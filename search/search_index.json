{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Teranode Documentation","text":""},{"location":"#index","title":"Index","text":"<ul> <li>Tutorials<ul> <li>Development Tutorials</li> <li>Miner Tutorials</li> </ul> </li> <li>How-to Guides<ul> <li>Development</li> <li>Miners</li> </ul> </li> <li>Key Topics<ul> <li>Introduction</li> <li>Architecture<ul> <li>Core Services</li> <li>Overlay Services</li> <li>Infrastructure Components</li> </ul> </li> <li>Additional Topics</li> </ul> </li> <li>Reference<ul> <li>Service Documentation</li> <li>Store Documentation</li> <li>Data Model</li> <li>API Documentation</li> <li>Additional Reference</li> </ul> </li> <li>Additional Resources</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":""},{"location":"#development-tutorials","title":"Development Tutorials","text":"<ul> <li>Fork and Pull Request Guidelines</li> <li>Setting Up for Development</li> </ul>"},{"location":"#miner-tutorials","title":"Miner Tutorials","text":"<ul> <li>Initial Setup Walkthrough</li> </ul>"},{"location":"#how-to-guides","title":"How-to Guides","text":""},{"location":"#development","title":"Development","text":"<ol> <li>Running Services Locally</li> <li>Using the Makefile</li> <li>Running Tests</li> <li>Setting Up Automated Test Environment</li> <li>Generating Protobuf Files</li> <li>Adding new Protobuf Services</li> <li>Configuring gRPC Logging</li> <li>Kubernetes - Remote Debugging Guide</li> <li>Developer's Guide to Teranode-CLI</li> </ol>"},{"location":"#miners","title":"Miners","text":""},{"location":"#docker-compose-setup","title":"Docker Compose Setup","text":"<ol> <li>Installation Guide</li> <li>Starting and Stopping Teranode</li> <li>Configuration Guide</li> <li>Update Procedures</li> <li>Troubleshooting Guide</li> <li>Security Best Practices</li> </ol>"},{"location":"#kubernetes-deployment","title":"Kubernetes Deployment","text":"<ol> <li>Installation with Kubernetes Operator</li> <li>Starting and Stopping Teranode</li> <li>Configuration Guide</li> <li>Update Procedures</li> <li>Backup Procedures</li> <li>Troubleshooting Guide</li> <li>Security Best Practices</li> </ol>"},{"location":"#common-tasks","title":"Common Tasks","text":"<ol> <li>CPU Mining Setup</li> <li>Interacting with Asset Server</li> <li>Interacting with RPC Service</li> <li>Interacting with the FSM via RPC</li> <li>Interacting with the Teranode CLI</li> <li>Managing Disk Space</li> <li>Aerospike Configuration Considerations</li> <li>How To Reset Teranode</li> <li>Blockchain Synchronization</li> <li>Using Listen Mode</li> </ol>"},{"location":"#key-topics","title":"Key Topics","text":""},{"location":"#introduction","title":"Introduction","text":"<ul> <li>What is Teranode?</li> <li>Transaction Lifecycle</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<ul> <li>Overall System Design</li> <li>Microservices Overview</li> <li>State Management</li> </ul>"},{"location":"#core-services","title":"Core Services","text":"<ul> <li>Asset Server</li> <li>Propagation Service</li> <li>Validator Service</li> <li>Subtree Validation Service</li> <li>Block Validation Service</li> <li>Block Assembly Service</li> <li>Blockchain Service</li> <li>Alert Service</li> </ul>"},{"location":"#overlay-services","title":"Overlay Services","text":"<ul> <li>Block Persister Service</li> <li>UTXO Persister Service</li> <li> <p>P2P Service</p> </li> <li> <p>Legacy Service</p> </li> <li>RPC Service</li> </ul>"},{"location":"#infrastructure-components","title":"Infrastructure Components","text":"<ul> <li>Stores<ul> <li>Blob Server</li> <li>UTXO Store</li> </ul> </li> <li>Messaging<ul> <li>Kafka</li> </ul> </li> <li>Utilities<ul> <li>UTXO Seeder</li> </ul> </li> </ul>"},{"location":"#additional-topics","title":"Additional Topics","text":"<ul> <li>Technology Stack</li> <li>Testing Framework</li> <li>QA Guide &amp; Instructions for Functional Requirement Tests</li> <li>Double Spends</li> <li>Two Phase Commit</li> </ul>"},{"location":"#reference","title":"Reference","text":""},{"location":"#service-documentation","title":"Service Documentation","text":"<ul> <li>Alert Service</li> <li>Asset Service</li> <li>Block Assembly</li> <li>Blockchain Server</li> <li>Block Persister</li> <li> <p>Block Validation</p> </li> <li> <p>Legacy Server</p> </li> <li>P2P Server</li> <li>Propagation Server</li> <li>RPC Service</li> <li>RPC API Docs</li> <li>Subtree Validation</li> <li>UTXO Persister</li> <li>TX Validator</li> </ul>"},{"location":"#store-documentation","title":"Store Documentation","text":"<ul> <li>Blob Store</li> <li>UTXO Store</li> </ul>"},{"location":"#data-model","title":"Data Model","text":"<ul> <li>Block Data Model</li> <li>Block Header Data Model</li> <li>Subtree Data Model</li> <li>Transaction Data Model</li> <li>UTXO Data Model</li> </ul>"},{"location":"#api-documentation","title":"API Documentation","text":"<ul> <li>Alert gRPC API</li> <li>Block Assembly gRPC API</li> <li>Blockchain gRPC API</li> <li> <p>Block Validation gRPC API</p> </li> <li> <p>Propagation gRPC API</p> </li> <li>Subtree Validation gRPC API</li> <li>Validator gRPC API</li> </ul>"},{"location":"#additional-reference","title":"Additional Reference","text":"<ul> <li>Third Party Software Requirements</li> <li>Project Structure</li> <li>Coding Conventions</li> <li>Error Handling Guidelines</li> <li>Configuration Settings</li> <li>Testing Framework Technical Reference</li> <li>Teranode Daemon Reference</li> <li>Prometheus Metrics</li> <li>Network Consensus Rules</li> <li>Git Commit Signing Setup Guide</li> </ul>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ol> <li>Glossary</li> <li>Contributing to Teranode</li> <li>License Information</li> </ol>"},{"location":"#conclusion","title":"Conclusion","text":"<p>Teranode represents a significant advancement in blockchain infrastructure, designed to provide a scalable, reliable, and high-performance foundation for the Bitcoin SV network. This documentation serves as a comprehensive resource for developers, miners, and other stakeholders involved with Teranode implementation and operation.</p> <p>By leveraging a microservices architecture and modern technologies, Teranode addresses the challenges of building a truly scalable blockchain system. Whether you're developing against Teranode, operating mining infrastructure, or simply exploring its architecture, this documentation provides the necessary guidance to understand and utilize the platform effectively.</p> <p>We encourage you to explore the various sections of this documentation based on your specific needs and to contribute to the ongoing development and improvement of Teranode.</p> <p>Copyright 2025 BSV Association.</p> <p>Licensed under the Open BSV License Version 6; you may not use this software except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>https://github.com/bsv-blockchain/teranode/blob/main/LICENSE\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"MARKDOWN_VALIDATION_GUIDE/","title":"Markdown Validation Guide for Teranode Documentation","text":"<p>This guide provides comprehensive solutions to prevent and catch markdown formatting issues that cause problems in Material MkDocs rendering.</p>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#available-validation-tools","title":"\ud83d\udee0\ufe0f Available Validation Tools","text":""},{"location":"MARKDOWN_VALIDATION_GUIDE/#1-pre-commit-hooks-automatic","title":"1. Pre-commit Hooks (Automatic)","text":"<p>Setup: Already configured in <code>.pre-commit-config.yaml</code> - markdownlint: Catches formatting issues before commits - Configuration: <code>.markdownlint.yml</code> with rules tailored for Material MkDocs</p> <p>Usage: <pre><code># Install pre-commit hooks\npre-commit install\n\n# Run manually on all files\npre-commit run markdownlint --all-files\n</code></pre></p>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#2-custom-validation-script","title":"2. Custom Validation Script","text":"<p>Location: <code>scripts/validate-markdown.py</code></p> <p>Features:</p> <ul> <li>Detects missing blank lines before lists</li> <li>Finds inconsistent nested bullet indentation</li> <li>Identifies configuration parameters missing Type/Default/Impact details</li> <li>Can automatically fix issues with <code>--fix</code> flag</li> </ul> <p>Usage: <pre><code># Validate all documentation\npython3 scripts/validate-markdown.py docs/\n\n# Validate and auto-fix issues\npython3 scripts/validate-markdown.py docs/ --fix\n\n# Validate single file\npython3 scripts/validate-markdown.py docs/topics/services/alert.md\n</code></pre></p>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#3-vs-code-extensions-real-time","title":"3. VS Code Extensions (Real-time)","text":"<p>Recommended Extensions:</p> <ul> <li>markdownlint: Real-time linting in editor</li> <li>Markdown All in One: Preview and formatting</li> <li>Material Theme: Better syntax highlighting</li> </ul> <p>Setup: 1. Install extensions 2. Add to VS Code settings.json: <pre><code>{\n  \"markdownlint.config\": {\n    \"MD007\": { \"indent\": 4 },\n    \"MD032\": true,\n    \"MD013\": false\n  }\n}\n</code></pre></p>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#common-issues-prevention","title":"\ud83c\udfaf Common Issues &amp; Prevention","text":""},{"location":"MARKDOWN_VALIDATION_GUIDE/#issue-1-missing-blank-lines-before-lists","title":"Issue 1: Missing Blank Lines Before Lists","text":"<p>Problem: Lists render inline instead of as proper lists Solution: Always add blank line before lists</p> <pre><code>\u274c Wrong:\nConfiguration options:\n\n- Option 1\n- Option 2\n\n\u2705 Correct:\nConfiguration options:\n\n- Option 1\n- Option 2\n</code></pre>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#issue-2-inconsistent-nested-indentation","title":"Issue 2: Inconsistent Nested Indentation","text":"<p>Problem: Nested items don't render as nested Solution: Use exactly 4 spaces for nested items</p> <pre><code>\u274c Wrong:\n\n- Main item\n  - Nested item (2 spaces)\n\n\u2705 Correct:\n\n- Main item\n    - Nested item (4 spaces)\n</code></pre>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#issue-3-configuration-parameter-formatting","title":"Issue 3: Configuration Parameter Formatting","text":"<p>Problem: Inconsistent parameter documentation Solution: Use standard format for all parameters</p> <pre><code>\u2705 Standard Format:\n1. **`parameter_name`**: Brief description.\n    - Type: string\n    - Default: \"value\"\n    - Impact: Detailed explanation of what this controls.\n</code></pre>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#workflow-integration","title":"\ud83d\ude80 Workflow Integration","text":""},{"location":"MARKDOWN_VALIDATION_GUIDE/#daily-development","title":"Daily Development","text":"<ol> <li>VS Code Extensions: Real-time feedback while editing</li> <li>Pre-commit Hooks: Automatic validation before commits</li> <li>Custom Script: Bulk validation and fixes</li> </ol>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#cicd-pipeline","title":"CI/CD Pipeline","text":"<ol> <li>GitHub Actions: Validate all PRs</li> <li>Deployment: Only deploy if validation passes</li> </ol>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#periodic-maintenance","title":"Periodic Maintenance","text":"<pre><code># Weekly validation run\npython3 scripts/validate-markdown.py docs/ --fix\ngit add -A\ngit commit -m \"docs: fix markdown formatting issues\"\n</code></pre>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#validation-checklist","title":"\ud83d\udccb Validation Checklist","text":"<p>Before committing documentation changes:</p> <ul> <li> Run custom validation script</li> <li> Check pre-commit hooks pass</li> <li> Preview in Material MkDocs locally</li> <li> Verify nested lists render correctly</li> <li> Confirm configuration parameters follow standard format</li> </ul>"},{"location":"MARKDOWN_VALIDATION_GUIDE/#quick-fixes","title":"\ud83d\udd27 Quick Fixes","text":"<p>Fix all formatting issues in docs: <pre><code>python3 scripts/validate-markdown.py docs/ --fix\n</code></pre></p> <p>Run markdownlint: <pre><code>markdownlint docs/ --config .markdownlint.yml --fix\n</code></pre></p> <p>Preview locally: <pre><code>mkdocs serve\n# Open http://localhost:8000\n</code></pre></p> <p>This multi-layered approach ensures high-quality, consistently formatted documentation that renders perfectly in Material MkDocs.</p>"},{"location":"P2P_NAT_TRAVERSAL/","title":"P2P NAT Traversal and Address Filtering","text":""},{"location":"P2P_NAT_TRAVERSAL/#problem","title":"Problem","text":"<p>When a node is behind NAT with only private IP addresses (192.168.x.x, 10.x.x.x, 172.16.x.x), proper address discovery is critical:</p> <ol> <li>The node only has private IP addresses on its network interfaces</li> <li>go-p2p v1.1.18 aggressively filters private IPs when <code>advertiseAddresses</code> is empty</li> <li>This filtering prevents libp2p's observed address mechanism from working</li> <li>Nodes end up invisible to the network</li> </ol>"},{"location":"P2P_NAT_TRAVERSAL/#how-libp2p-handles-nat","title":"How libp2p Handles NAT","text":"<p>libp2p has several mechanisms for NAT traversal:</p> <ol> <li> <p>Identify Protocol: When a peer connects to you, it tells you what address it sees you coming from. This \"observed address\" is your public IP as seen from the outside.</p> </li> <li> <p>AutoNAT Service: Peers help each other determine if they are behind NAT and what their external address is.</p> </li> <li> <p>UPnP/NAT-PMP: Automatically configures port forwarding on compatible routers.</p> </li> <li> <p>Hole Punching (DCUtR): Direct Connection Upgrade through Relay - allows peers behind NAT to establish direct connections.</p> </li> <li> <p>Circuit Relay: Use other peers as relays when direct connection is not possible.</p> </li> </ol>"},{"location":"P2P_NAT_TRAVERSAL/#solutions","title":"Solutions","text":""},{"location":"P2P_NAT_TRAVERSAL/#option-1-enable-autonat-service-recommended-for-nodes-behind-nat","title":"Option 1: Enable AutoNAT Service (Recommended for nodes behind NAT)","text":"<pre><code># In settings_local.conf\np2p_enable_nat_service = true\n</code></pre> <p>This enables libp2p's AutoNAT service which:</p> <ul> <li>Helps determine your external address</li> <li>Allows the node to learn its public IP from other peers</li> <li>Automatically adds observed addresses to the advertised list</li> </ul>"},{"location":"P2P_NAT_TRAVERSAL/#option-2-explicitly-set-advertise-address","title":"Option 2: Explicitly Set Advertise Address","text":"<p>If you know your public IP or domain:</p> <pre><code># In settings_local.conf\np2p_advertise_addresses = [\"/ip4/203.0.113.1/tcp/9905\"]\n</code></pre> <p>This is ideal for:</p> <ul> <li>Nodes with static public IPs</li> <li>Kubernetes deployments with known ingress IPs</li> <li>Nodes behind reverse proxies</li> </ul>"},{"location":"P2P_NAT_TRAVERSAL/#option-3-enable-nat-port-mapping","title":"Option 3: Enable NAT Port Mapping","text":"<p>For routers that support UPnP or NAT-PMP:</p> <pre><code># In settings_local.conf\np2p_enable_nat_port_map = true\n</code></pre>"},{"location":"P2P_NAT_TRAVERSAL/#option-4-enable-full-nat-traversal-suite","title":"Option 4: Enable Full NAT Traversal Suite","text":"<p>For maximum connectivity in challenging network environments:</p> <pre><code># In settings_local.conf\np2p_enable_nat_service = true\np2p_enable_nat_port_map = true\np2p_enable_hole_punching = true\np2p_enable_relay = true\n</code></pre>"},{"location":"P2P_NAT_TRAVERSAL/#option-5-share-private-addresses-development-only","title":"Option 5: Share Private Addresses (Development Only)","text":"<p>For local development or private networks:</p> <pre><code># In settings_local.conf\np2p_share_private_addresses = true\n</code></pre> <p>\u26a0\ufe0f Warning: Only use this in controlled environments where peers can actually reach private IPs.</p>"},{"location":"P2P_NAT_TRAVERSAL/#how-the-observed-address-mechanism-works","title":"How the Observed Address Mechanism Works","text":"<ol> <li>Node A (behind NAT) connects outbound to Node B</li> <li>Node B sees the connection coming from Node A's public IP (e.g., 203.0.113.1:45678)</li> <li>Node B tells Node A: \"I see you at 203.0.113.1:45678\" via the Identify protocol</li> <li>Node A adds this \"observed address\" to its list of advertised addresses</li> <li>Node A can now tell other peers: \"You can reach me at 203.0.113.1:45678\"</li> <li>Other peers can now connect to Node A using this address</li> </ol>"},{"location":"P2P_NAT_TRAVERSAL/#debugging","title":"Debugging","text":"<p>To debug NAT traversal issues:</p> <ol> <li>Check the logs for warnings about unreachable nodes:</li> </ol> <pre><code>grep \"No peers connected\\|NAT\\|observed\\|advertise\" teranode.log\n</code></pre> <ol> <li>Verify your configuration:</li> </ol> <pre><code>grep \"p2p_enable_nat\\|p2p_advertise\" settings_local.conf\n</code></pre> <ol> <li>Test connectivity from another node:</li> </ol> <pre><code>telnet &lt;your-public-ip&gt; 9905\n</code></pre>"},{"location":"P2P_NAT_TRAVERSAL/#best-practices","title":"Best Practices","text":"<ol> <li>For Production: Always set explicit <code>p2p_advertise_addresses</code> with your public IP</li> <li>For Cloud/Kubernetes: Use the load balancer or ingress IP in <code>p2p_advertise_addresses</code></li> <li>For Development: Enable <code>p2p_enable_nat_service</code> for automatic address detection</li> <li>For Home Networks: Enable <code>p2p_enable_nat_port_map</code> if your router supports it</li> </ol>"},{"location":"P2P_NAT_TRAVERSAL/#implementation-details","title":"Implementation Details","text":""},{"location":"P2P_NAT_TRAVERSAL/#address-advertisement-vs-filtering","title":"Address Advertisement vs Filtering","text":"<p>The solution involves two stages:</p> <ol> <li>Advertisement Stage: Nodes advertise their listen addresses (including private IPs) to enable libp2p's observed address detection</li> <li>Filtering Stage: When sharing peer lists via GetPeers, nodes filter out private addresses unless explicitly configured to share them</li> </ol>"},{"location":"P2P_NAT_TRAVERSAL/#why-this-approach-works","title":"Why This Approach Works","text":"<ul> <li>Nodes behind NAT advertise their private addresses</li> <li>When they connect to public nodes, those nodes observe the NAT's public IP</li> <li>The public nodes tell the NAT'd node its observed public address via libp2p's Identify protocol</li> <li>The NAT'd node can then share this observed public address with other peers</li> <li>Private addresses are filtered when sharing peer lists (not when advertising own addresses)</li> </ul>"},{"location":"P2P_NAT_TRAVERSAL/#the-go-p2p-v1118-issue","title":"The go-p2p v1.1.18 Issue","text":"<p>The go-p2p library's AddrsFactory aggressively filters private IPs when advertiseAddresses is empty. This prevents the observed address mechanism from working because:</p> <ul> <li>The node has no addresses to advertise</li> <li>Peers can't tell it what public address they observe</li> <li>The node remains invisible to the network</li> </ul> <p>The solution is to always pass listen addresses to override this filtering, allowing the observed address mechanism to function properly.</p>"},{"location":"state-machine.diagram/","title":"State Machine","text":"<p>The mermaid diagram outlined below represents the various states and events that dictate the functionality of the node. To create and visualize the state machine diagram, you can use https://mermaid.live/. This tool allows you to generate the diagram visualization interactively.</p> <pre><code>stateDiagram-v2\n    [*] --&gt; IDLE\n    CATCHINGBLOCKS --&gt; RUNNING: RUN\n    CATCHINGBLOCKS --&gt; IDLE: STOP\n    IDLE --&gt; LEGACYSYNCING: LEGACYSYNC\n    IDLE --&gt; RUNNING: RUN\n    LEGACYSYNCING --&gt; RUNNING: RUN\n    LEGACYSYNCING --&gt; IDLE: STOP\n    RUNNING --&gt; CATCHINGBLOCKS: CATCHUPBLOCKS\n    RUNNING --&gt; IDLE: STOP\n</code></pre>"},{"location":"howto/addingNewProtobufServices/","title":"Adding Protobuf","text":""},{"location":"howto/addingNewProtobufServices/#how-to-add-a-new-protobuf-service","title":"How to Add a New Protobuf Service","text":"<p>In addition to generating existing Protobuf files, you may need to extend the project by adding a new Protobuf service or message (e.g., defining a new RPC endpoint) or adding a dependency on a new <code>.proto</code> file. This section explains how to:</p> <ol> <li>Create or modify a <code>.proto</code> file to define a new service or message.</li> <li>Update the <code>Makefile</code> to ensure your new Protobuf definitions are compiled correctly.</li> <li>Verify the changes by generating the necessary Go files.</li> </ol>"},{"location":"howto/addingNewProtobufServices/#step-1-define-the-new-protobuf-service-or-endpoint","title":"Step 1: Define the New Protobuf Service or Endpoint","text":"<p>To start, you\u2019ll need to define your new service or message by creating or modifying a <code>.proto</code> file. For example, if you want to add a new RPC method to an existing service or create a completely new service, you would write it in a <code>.proto</code> file.</p> <p>For example, to add a new RPC method in <code>subtreevalidation_api.proto</code>:</p> <pre><code>service SubtreeValidationAPI {\n  rpc CheckSubtreeFromBlock (CheckSubtreeFromBlockRequest) returns (CheckSubtreeFromBlockResponse) {};\n  rpc NewValidationEndpoint (NewValidationRequest) returns (NewValidationResponse) {}; // New RPC method\n}\n\nmessage NewValidationRequest {\n  string data = 1;\n}\n\nmessage NewValidationResponse {\n  bool success = 1;\n  string message = 2;\n}\n</code></pre> <p>Note: All service names in Teranode use the <code>API</code> suffix (e.g., <code>SubtreeValidationAPI</code>, <code>ValidatorAPI</code>, <code>BlockchainAPI</code>).</p> <p>You can also define a completely new service and corresponding messages in a new <code>.proto</code> file, such as <code>services/newservice/newservice_api/newservice_api.proto</code>.</p>"},{"location":"howto/addingNewProtobufServices/#step-2-update-the-makefile","title":"Step 2: Update the <code>Makefile</code>","text":"<p>Once your new or modified <code>.proto</code> file is ready, you need to update the <code>Makefile</code> to ensure the file is included in the Protobuf generation process.</p> <p>To add a new service or dependency, follow these steps:</p> <ol> <li>Locate the <code>Makefile</code> in the project root.</li> <li>Add a new <code>protoc</code> command for your new <code>.proto</code> file under the <code>gen</code> target.</li> </ol> <p>For example, if you created a new <code>newservice_api.proto</code> file in the <code>services/newservice/newservice_api/</code> directory, add a new <code>protoc</code> command like this:</p> <pre><code>protoc \\\n--proto_path=. \\\n--go_out=. \\\n--go_opt=paths=source_relative \\\n--go-grpc_out=. \\\n--go-grpc_opt=paths=source_relative \\\nservices/newservice/newservice_api/newservice_api.proto\n</code></pre> <p>Note: The directory structure follows the pattern <code>services/&lt;service_name&gt;/&lt;service_name&gt;_api/&lt;service_name&gt;_api.proto</code>.</p> <p>This ensures that the new <code>.proto</code> file is processed and generates the corresponding Go code (both the message definitions and the gRPC stubs).</p>"},{"location":"howto/addingNewProtobufServices/#step-3-regenerate-the-protobuf-files","title":"Step 3: Regenerate the Protobuf Files","text":"<p>After updating the <code>Makefile</code>, you need to regenerate the Go files to reflect the changes in your new or modified <code>.proto</code> files.</p> <p>Simply run the following command:</p> <pre><code>make gen\n</code></pre> <p>This will generate the necessary Go files for all services, including the newly added ones. The generated files will be located alongside the <code>.proto</code> files, following the standard naming conventions (e.g., <code>newservice_api.pb.go</code> and <code>newservice_api_grpc.pb.go</code>).</p>"},{"location":"howto/addingNewProtobufServices/#step-4-verify-the-changes","title":"Step 4: Verify the Changes","text":"<p>Once the Go files have been generated, you can verify that the new service or endpoint is available by checking the generated <code>.pb.go</code> and <code>_grpc.pb.go</code> files. For instance, after adding a new method, you should see:</p> <ul> <li>The new RPC method in the <code>SubtreeValidationAPIServer</code> interface in the <code>_grpc.pb.go</code> file.</li> <li>The new request and response message types in the <code>.pb.go</code> file.</li> </ul> <p>Here's what you might expect in <code>subtreevalidation_api_grpc.pb.go</code>:</p> <pre><code>// SubtreeValidationAPIServer is the server API for SubtreeValidationAPI.\ntype SubtreeValidationAPIServer interface {\n  CheckSubtreeFromBlock(context.Context, *CheckSubtreeFromBlockRequest) (*CheckSubtreeFromBlockResponse, error)\n  NewValidationEndpoint(context.Context, *NewValidationRequest) (*NewValidationResponse, error) // New RPC method\n}\n</code></pre>"},{"location":"howto/addingNewProtobufServices/#example-creating-a-complete-new-service","title":"Example: Creating a Complete New Service","text":"<p>When creating a new service from scratch, your <code>.proto</code> file should include all required elements. Here's a complete example for <code>services/newservice/newservice_api/newservice_api.proto</code>:</p> <pre><code>syntax = \"proto3\";\n\noption go_package = \"./;newservice_api\";\n\npackage newservice_api;\n\nimport \"google/protobuf/timestamp.proto\";\n\n// NewServiceAPI provides methods for the new service functionality\nservice NewServiceAPI {\n  // HealthGRPC checks the service's health status\n  rpc HealthGRPC (EmptyMessage) returns (HealthResponse) {}\n\n  // ProcessData performs the main service operation\n  rpc ProcessData (ProcessDataRequest) returns (ProcessDataResponse) {}\n}\n\n// EmptyMessage represents an empty message structure used for health check requests\nmessage EmptyMessage {}\n\n// HealthResponse encapsulates the service health status information\nmessage HealthResponse {\n  bool ok = 1;\n  string details = 2;\n  google.protobuf.Timestamp timestamp = 3;\n}\n\n// ProcessDataRequest defines the input for data processing\nmessage ProcessDataRequest {\n  string data = 1;\n}\n\n// ProcessDataResponse contains the processing result\nmessage ProcessDataResponse {\n  bool success = 1;\n  string result = 2;\n}\n</code></pre> <p>Key elements to include:</p> <ul> <li><code>syntax = \"proto3\";</code> - Specifies the protobuf version</li> <li><code>option go_package = \"./;newservice_api\";</code> - Required for Go code generation</li> <li><code>package newservice_api;</code> - Package name matching the directory structure</li> <li>Import statements for dependencies (e.g., <code>google/protobuf/timestamp.proto</code>)</li> <li>Service definition with <code>API</code> suffix</li> <li>Common patterns like <code>HealthGRPC</code> endpoint (present in all services)</li> <li>Comprehensive comments for all messages and RPC methods</li> </ul>"},{"location":"howto/addingNewProtobufServices/#example-using-shared-protobuf-models","title":"Example: Using Shared Protobuf Models","text":"<p>Teranode already provides shared data models in <code>model/model.proto</code> that can be used across services. To use these in your new service:</p> <ol> <li>Import the model in your service's <code>.proto</code> file:</li> </ol> <pre><code>import \"model/model.proto\";\n</code></pre> <ol> <li>Reference the model types in your messages:</li> </ol> <pre><code>message MyServiceRequest {\n  model.MiningCandidate candidate = 1;\n  string additional_data = 2;\n}\n</code></pre> <p>If you need to add new shared models, update <code>model/model.proto</code> directly. The Makefile already includes this file in the <code>gen</code> target, so running <code>make gen</code> will regenerate the code.</p> <p>Existing shared proto files:</p> <ul> <li><code>model/model.proto</code> - Core data models (MiningCandidate, BlockInfo, etc.)</li> <li><code>errors/error.proto</code> - Error handling structures</li> <li><code>stores/utxo/status.proto</code> - UTXO status definitions</li> </ul>"},{"location":"howto/addingNewProtobufServices/#step-5-implement-the-service-interface","title":"Step 5: Implement the Service Interface","text":"<p>After generating the protobuf files, you need to implement the service interface in your Go code. Create an implementation file (e.g., <code>services/newservice/service.go</code>):</p> <pre><code>package newservice\n\nimport (\n    \"context\"\n    \"github.com/bsv-blockchain/teranode/services/newservice/newservice_api\"\n    \"google.golang.org/protobuf/types/known/timestamppb\"\n)\n\n// Service implements the NewServiceAPIServer interface\ntype Service struct {\n    newservice_api.UnimplementedNewServiceAPIServer\n    // Add your service dependencies here\n}\n\n// NewService creates a new instance of the service\nfunc NewService() *Service {\n    return &amp;Service{}\n}\n\n// HealthGRPC implements the health check endpoint\nfunc (s *Service) HealthGRPC(ctx context.Context, req *newservice_api.EmptyMessage) (*newservice_api.HealthResponse, error) {\n    return &amp;newservice_api.HealthResponse{\n        Ok:        true,\n        Details:   \"Service is running\",\n        Timestamp: timestamppb.Now(),\n    }, nil\n}\n\n// ProcessData implements the main service operation\nfunc (s *Service) ProcessData(ctx context.Context, req *newservice_api.ProcessDataRequest) (*newservice_api.ProcessDataResponse, error) {\n    // Implement your business logic here\n    return &amp;newservice_api.ProcessDataResponse{\n        Success: true,\n        Result:  \"Processed: \" + req.Data,\n    }, nil\n}\n</code></pre> <p>Next steps:</p> <ol> <li>Register the service with the gRPC server in your main application</li> <li>Add configuration for the service in <code>settings.conf</code></li> <li>Write unit tests for your service implementation</li> <li>Add integration tests if needed</li> </ol>"},{"location":"howto/addingNewProtobufServices/#step-6-cleaning-up-generated-files-optional","title":"Step 6: Cleaning Up Generated Files (Optional)","text":"<p>If you need to remove the previously generated files for some reason (e.g., during refactoring), you can use the following command:</p> <pre><code>make clean_gen\n</code></pre> <p>This will delete all generated <code>.pb.go</code> files, allowing you to start fresh when running <code>make gen</code> again.</p>"},{"location":"howto/addingNewProtobufServices/#additional-makefile-commands","title":"Additional Makefile Commands","text":"<p>For more information about the <code>Makefile</code> and additional commands you can use, please refer to the Makefile Documentation.</p>"},{"location":"howto/automatedTestingHowTo/","title":"Automated Testing","text":""},{"location":"howto/automatedTestingHowTo/#setting-up-test-environment","title":"Setting Up Test Environment","text":""},{"location":"howto/automatedTestingHowTo/#how-to-set-up-your-local-test-environment","title":"How to Set Up Your Local Test Environment","text":"<ol> <li>Prerequisites:</li> </ol> <pre><code># Install required tools\ndocker compose\ngo 1.21 or higher\nmake\n</code></pre> <ol> <li>Initial setup:</li> </ol> <pre><code># Clone and build\ngit clone [repository]\ncd teranode\ndocker compose build\n</code></pre> <ol> <li>Verify setup:</li> </ol> <pre><code># Run smoke tests\nmake smoketest\n</code></pre>"},{"location":"howto/automatedTestingHowTo/#how-to-configure-test-settings","title":"How to Configure Test Settings","text":"<ol> <li>Create local settings:</li> </ol> <pre><code>touch settings_local.conf\n</code></pre> <ol> <li> <p>Add required settings for your specific testing activity. Common test contexts:</p> </li> <li> <p><code>test</code> - Standard testing environment</p> </li> <li><code>docker</code> - Docker-based testing</li> <li><code>docker.ci</code> - CI environment testing</li> </ol>"},{"location":"howto/automatedTestingHowTo/#running-tests","title":"Running Tests","text":""},{"location":"howto/automatedTestingHowTo/#how-to-run-test-suites","title":"How to Run Test Suites","text":"<ol> <li>Run all tests in a suite:</li> </ol> <pre><code># Format\ncd /teranode/test/&lt;suite-name&gt;\ngo test -v -tags test_&lt;suite-code&gt;\n\n# Examples\n# Run TNA tests\ncd /teranode/test/tna\ngo test -v -tags test_tna\n\n# Run TNB tests\ncd /teranode/test/tnb\ngo test -v -tags test_tnb\n\n# Run TEC tests\ncd /teranode/test/tec\ngo test -v -tags test_tec\n</code></pre> <ol> <li>Run specific test:</li> </ol> <pre><code># Format\ngo test -v -run \"^&lt;TestSuiteName&gt;$/^&lt;TestName&gt;$\" -tags &lt;test-tag&gt;\n\n# Example: Run specific TNA test\ngo test -v -run \"^TestTNA1TestSuite$/^TestBroadcastNewTxAllNodes$\" -tags test_tna\n</code></pre> <ol> <li>Run with different options:</li> </ol> <pre><code># With timeout\ngo test -v -tags test_tna -timeout 10m\n\n# With race detection\ngo test -v -race -tags test_tna\n\n# With specific cache size\ngo test -v -tags \"test_tna,testtxmetacache\"\n</code></pre>"},{"location":"howto/automatedTestingHowTo/#how-to-debug-failed-tests","title":"How to Debug Failed Tests","text":"<ol> <li>Enable verbose logging:</li> </ol> <pre><code># Set log level to DEBUG\nexport LOG_LEVEL=DEBUG\ngo test -v -tags test_tna\n</code></pre> <ol> <li>Check container logs:</li> </ol> <pre><code># View logs for specific node\ndocker logs teranode1\ndocker logs teranode2\ndocker logs teranode3\n</code></pre> <ol> <li>Inspect test state:</li> </ol> <pre><code># Check node health (verify actual ports in your settings)\ncurl http://localhost:18090/api/v1/bestblockheader/json\ncurl http://localhost:28090/api/v1/bestblockheader/json\ncurl http://localhost:38090/api/v1/bestblockheader/json\n</code></pre>"},{"location":"howto/automatedTestingHowTo/#adding-new-tests","title":"Adding New Tests","text":""},{"location":"howto/automatedTestingHowTo/#how-to-add-a-new-test-case","title":"How to Add a New Test Case","text":"<ol> <li>Create test file with proper structure:</li> </ol> <pre><code>//go:build test_tna || debug\n\n// How to run this test manually:\n// $ go test -v -run \"^TestTNA5TestSuite$/^TestNewFeature$\" -tags test_tna\n\npackage tna\n\nimport (\n    \"testing\"\n    \"time\"\n\n    helper \"github.com/bsv-blockchain/teranode/test/utils\"\n    \"github.com/bsv-blockchain/teranode/test/utils/tconfig\"\n    \"github.com/stretchr/testify/suite\"\n)\n\ntype TNA5TestSuite struct {\n    helper.TeranodeTestSuite\n}\n\nfunc TestTNA5TestSuite(t *testing.T) {\n    suite.Run(t, &amp;TNA5TestSuite{\n        TeranodeTestSuite: helper.TeranodeTestSuite{\n            TConfig: tconfig.LoadTConfig(\n                map[string]any{\n                    tconfig.KeyTeranodeContexts: []string{\n                        \"docker.teranode1.test.tna5Test\",\n                        \"docker.teranode2.test.tna5Test\",\n                        \"docker.teranode3.test.tna5Test\",\n                    },\n                },\n            ),\n        },\n    },\n    )\n}\n\nfunc (suite *TNA5TestSuite) TestNewFeature() {\n    // Access test environment\n    testEnv := suite.TeranodeTestEnv\n    ctx := testEnv.Context\n    t := suite.T()\n    logger := testEnv.Logger\n\n    // Access nodes\n    node1 := testEnv.Nodes[0]\n    blockchainClient := node1.BlockchainClient\n\n    // Your test logic here\n    t.Log(\"Starting test\")\n\n    // Example: Check node health\n    health, err := blockchainClient.Health(ctx)\n    if err != nil {\n        t.Fatalf(\"Failed to get health: %v\", err)\n    }\n    if !health.Ok {\n        t.Error(\"Expected health to be OK\")\n    }\n\n    t.Log(\"Test completed successfully\")\n}\n</code></pre> <ol> <li>Common test patterns:</li> </ol> <pre><code>// Create and send transaction\ntx, err := helper.CreateAndSendTx(ctx, node1)\nif err != nil {\n    t.Fatalf(\"Failed to create transaction: %v\", err)\n}\n\n// Mine a block\nblockHash, err := helper.MineBlock(ctx, node1.Settings, node1.BlockAssemblyClient, logger)\nif err != nil {\n    t.Fatalf(\"Failed to mine block: %v\", err)\n}\n\n// Wait for block height\nerr = helper.WaitForNodeBlockHeight(ctx, blockchainClient, targetHeight, 30*time.Second)\nif err != nil {\n    t.Fatalf(\"Failed to reach target height: %v\", err)\n}\n\n// Wait for nodes to sync\nblockchainClients := []blockchain.ClientI{\n    testEnv.Nodes[0].BlockchainClient,\n    testEnv.Nodes[1].BlockchainClient,\n    testEnv.Nodes[2].BlockchainClient,\n}\nerr = helper.WaitForNodesToSync(ctx, blockchainClients, 30*time.Second)\nif err != nil {\n    t.Fatalf(\"Nodes failed to sync: %v\", err)\n}\n</code></pre>"},{"location":"howto/automatedTestingHowTo/#how-to-add-custom-test-configuration","title":"How to Add Custom Test Configuration","text":"<ol> <li>Use TConfig system to customize settings:</li> </ol> <pre><code>func TestCustomTestSuite(t *testing.T) {\n    suite.Run(t, &amp;CustomTestSuite{\n        TeranodeTestSuite: helper.TeranodeTestSuite{\n            TConfig: tconfig.LoadTConfig(\n                map[string]any{\n                    tconfig.KeyTeranodeContexts: []string{\n                        \"docker.teranode1.test.customTest\",\n                    },\n                    tconfig.KeyComposeFiles: []string{\n                        \"docker-compose.e2etest.yml\",\n                        \"docker-compose.custom.override.yml\",\n                    },\n                },\n            ),\n        },\n    })\n}\n</code></pre> <ol> <li>Create custom docker compose override:</li> </ol> <pre><code># docker-compose.custom.override.yml\nservices:\n  teranode1:\n    environment:\n\n      - CUSTOM_SETTING=value\n</code></pre>"},{"location":"howto/automatedTestingHowTo/#working-with-multiple-nodes","title":"Working with Multiple Nodes","text":""},{"location":"howto/automatedTestingHowTo/#how-to-test-node-communication","title":"How to Test Node Communication","text":"<ol> <li>Setup multiple nodes:</li> </ol> <pre><code>// Ensure all nodes are running\nfor i, node := range testEnv.Nodes {\n    health, err := node.BlockchainClient.Health(ctx)\n    if err != nil {\n        t.Errorf(\"Node %d health check failed: %v\", i, err)\n    }\n    if !health.Ok {\n        t.Errorf(\"Node %d is not healthy\", i)\n    }\n}\n</code></pre> <ol> <li>Test transaction propagation:</li> </ol> <pre><code>// Send transaction to node 0\ntx, err := helper.CreateAndSendTx(ctx, testEnv.Nodes[0])\nif err != nil {\n    t.Fatalf(\"Failed to create transaction: %v\", err)\n}\n\n// Allow time for propagation\ntime.Sleep(2 * time.Second)\n\n// Verify on other nodes\nblock, err := testEnv.Nodes[1].BlockchainClient.GetBestBlockHeader(ctx)\nif err != nil {\n    t.Fatalf(\"Failed to get block from node 1: %v\", err)\n}\n</code></pre>"},{"location":"howto/automatedTestingHowTo/#how-to-test-node-failure-scenarios","title":"How to Test Node Failure Scenarios","text":"<ol> <li>Stop specific containers:</li> </ol> <pre><code># Stop a specific node container\ndocker stop teranode2\n</code></pre> <ol> <li>Restart containers:</li> </ol> <pre><code># Restart the node\ndocker start teranode2\n</code></pre> <ol> <li>Test resilience in code:</li> </ol> <pre><code>// Send transaction before node failure\ntx1, _ := helper.CreateAndSendTx(ctx, testEnv.Nodes[0])\n\n// Simulate node failure by stopping docker container\n// (typically done manually or via docker SDK)\n\n// Verify other nodes continue working\ntx2, err := helper.CreateAndSendTx(ctx, testEnv.Nodes[1])\nif err != nil {\n    t.Fatalf(\"Remaining nodes should continue working: %v\", err)\n}\n</code></pre>"},{"location":"howto/automatedTestingHowTo/#cleanup-and-maintenance","title":"Cleanup and Maintenance","text":""},{"location":"howto/automatedTestingHowTo/#how-to-clean-test-environment","title":"How to Clean Test Environment","text":"<ol> <li>Remove test data:</li> </ol> <pre><code># Clean data directory\nrm -rf data/test/*\n</code></pre> <ol> <li>Remove containers:</li> </ol> <pre><code># Remove all test containers\ndocker ps -a -q --filter label=com.docker.compose.project=e2e | xargs docker rm -f\n</code></pre> <ol> <li>Full cleanup in test code:</li> </ol> <pre><code>// TearDown is handled automatically by TeranodeTestSuite\n// But you can add custom cleanup in TearDownTest:\nfunc (suite *CustomTestSuite) TearDownTest() {\n    // Custom cleanup here\n    suite.TeranodeTestSuite.TearDownTest() // Call parent teardown\n}\n</code></pre>"},{"location":"howto/automatedTestingHowTo/#how-to-handle-common-issues","title":"How to Handle Common Issues","text":"<ol> <li>Port conflicts:</li> </ol> <pre><code># Check port usage\nlsof -i :8087\nlsof -i :8090\nlsof -i :9292\n</code></pre> <ol> <li>Resource cleanup:</li> </ol> <pre><code># Force remove hanging containers\ndocker ps -a -q --filter label=com.docker.compose.project=e2e | xargs docker rm -f\n\n# Remove volumes\ndocker volume prune -f\n</code></pre> <ol> <li>Data persistence issues:</li> </ol> <pre><code># Reset all data (use sudo if needed)\nrm -rf ./data/test\n\n# Or if permission issues:\nsudo rm -rf ./data/test\n</code></pre>"},{"location":"howto/automatedTestingHowTo/#available-test-tags","title":"Available Test Tags","text":"<p>Teranode uses build tags to control which tests run:</p> <ul> <li><code>test_tna</code> - TNA test suite</li> <li><code>test_tnb</code> - TNB test suite</li> <li><code>test_tec</code> - TEC test suite</li> <li><code>test_tnd</code> - TND test suite</li> <li><code>test_tnf</code> - TNF test suite</li> <li><code>test_tnj</code> - TNJ test suite</li> <li><code>test_smoke</code> - Smoke tests</li> <li><code>test_functional</code> - Functional tests</li> <li><code>testtxmetacache</code> - Use small transaction metadata cache</li> <li><code>largetxmetacache</code> - Use production-sized cache</li> <li><code>aerospike</code> - Tests requiring Aerospike</li> <li><code>debug</code> - Enable debug mode for tests</li> </ul>"},{"location":"howto/automatedTestingHowTo/#other-resources","title":"Other Resources","text":"<ul> <li>QA Guide &amp; Instructions for Functional Requirement Tests</li> <li>Understanding The Testing Framework</li> <li>Testing Technical Reference</li> <li>Settings Reference</li> <li>Teranode Daemon Reference</li> </ul>"},{"location":"howto/bugReporting/","title":"Bug Reporting","text":"<p>When you encounter issues, please follow these guidelines to report bugs to the Teranode support team:</p>"},{"location":"howto/bugReporting/#before-reporting","title":"Before Reporting","text":"<ol> <li>Check the troubleshooting documentation (Docker) to ensure the behavior is indeed a bug.</li> <li>Search existing GitHub issues to see if the bug has already been reported.</li> </ol>"},{"location":"howto/bugReporting/#collecting-information","title":"Collecting Information","text":"<p>Before submitting a bug report, gather the following information:</p> <ol> <li> <p>Environment Details:</p> <ul> <li>Operating System and version</li> <li>Docker version</li> <li>Docker Compose version</li> <li> <p>Configuration Files:</p> </li> <li> <p><code>settings_local.conf</code></p> </li> <li>Docker Compose file</li> <li> <p>System Resources:</p> </li> <li> <p>CPU usage</p> </li> <li>Memory usage</li> <li>Disk space and I/O statistics</li> <li> <p>Network Information</p> </li> <li> <p>Firewall configuration</p> </li> <li>Any relevant network errors</li> <li> <p>Steps to Reproduce:</p> </li> <li> <p>Detailed, step-by-step description of how to reproduce the issue</p> </li> <li> <p>Expected vs Actual Behavior:</p> </li> <li> <p>What you expected to happen</p> </li> <li>What actually happened</li> <li> <p>Screenshots or Error Messages:</p> </li> <li> <p>Include any relevant visual information</p> </li> </ul> </li> </ol>"},{"location":"howto/bugReporting/#submitting-the-bug-report","title":"Submitting the Bug Report","text":"<ol> <li>Go to the Teranode GitHub repository.</li> <li>Click on \"Issues\" and then \"New Issue\"</li> <li>Select the \"Bug Report\" template</li> <li>Fill out the template with the information you've gathered</li> <li>Submit the issue</li> </ol>"},{"location":"howto/bugReporting/#bug-report-template","title":"Bug Report Template","text":"<p>When creating a new issue, GitHub will automatically load a template. The template includes the following sections:</p> <pre><code>## Describe the bug\nA clear and concise description of what the bug is.\n\n## To Reproduce\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '...'\n3. Scroll down to '...'\n4. See error\n\n## Expected behavior\nA clear and concise description of what you expected to happen.\n\n## Screenshots\nIf applicable, add screenshots to help explain your problem.\n\n## Timeline\nWhen did the bug first occur, or when did you first notice it?\n\n## Desktop (please complete the following information):\n\n- OS: [e.g. iOS]\n- Browser [e.g. chrome, safari]\n- Version [e.g. 22]\n\n## TERANODE Env\nYou can get that at the start of your program and looks something like:\n\nSETTINGS_CONTEXT\n----------------\nscaling.m1\n\nSETTINGS\n--------\nSERVICE_NAME=validation-service\nadvertisingInterval=10s\nadvertisingURL=\nclientName=M1\nasset_grpcAddress=blob-service.blob-service.svc.cluster.local:8091\nasset_grpcListenAddress=:8091\nasset_httpAddress=https://m1.scaling.teranode.network\nasset_httpListenAddress=:8090\n...\n\n## Additional context\n\nAdd any other context about the problem here.\n</code></pre>"},{"location":"howto/bugReporting/#after-submitting","title":"After Submitting","text":"<ul> <li>Be responsive to any follow-up questions from the development team.</li> <li>If you discover any new information about the bug, update the issue.</li> <li>If the bug is resolved in a newer version, please confirm and close the issue.</li> </ul>"},{"location":"howto/chain-integrity-testing/","title":"Chain Integrity Testing","text":"<p>This document explains how to run chain integrity tests locally using the Make jobs that replicate the GitHub workflow workflow.</p>"},{"location":"howto/chain-integrity-testing/#overview","title":"Overview","text":"<p>The chain integrity test verifies that multiple Teranode nodes maintain consistent blockchain state by: 1. Starting 3 Teranode nodes with block generators 2. Waiting for all nodes to reach a specified block height 3. Running the chainintegrity tool to compare log file hashes across nodes 4. Checking for consensus among the nodes</p>"},{"location":"howto/chain-integrity-testing/#prerequisites","title":"Prerequisites","text":"<p>Before running the chain integrity tests, ensure you have:</p> <ul> <li>Docker and Docker Compose installed</li> <li><code>jq</code> command-line JSON processor installed</li> <li><code>curl</code> command-line tool installed</li> <li>Sufficient system resources (recommended: 8GB RAM, 4 CPU cores)</li> <li>AWS CLI installed and configured (for ECR access)</li> </ul>"},{"location":"howto/chain-integrity-testing/#available-make-jobs","title":"Available Make Jobs","text":""},{"location":"howto/chain-integrity-testing/#1-standard-chain-integrity-test","title":"1. Standard Chain Integrity Test","text":"<pre><code>make chain-integrity-test\n</code></pre> <p>This runs the full chain integrity test with default parameters:</p> <ul> <li>Required block height: 120</li> <li>Maximum wait time: 10 minutes (120 attempts \u00d7 5 seconds)</li> <li>Sleep interval: 5 seconds</li> </ul>"},{"location":"howto/chain-integrity-testing/#2-custom-chain-integrity-test","title":"2. Custom Chain Integrity Test","text":"<pre><code>make chain-integrity-test-custom REQUIRED_HEIGHT=100 MAX_ATTEMPTS=60 SLEEP=3\n</code></pre> <p>This allows you to customize the test parameters:</p> <ul> <li><code>REQUIRED_HEIGHT</code>: The minimum block height all nodes must reach (default: 120)</li> <li><code>MAX_ATTEMPTS</code>: Maximum number of attempts to check node heights (default: 120)</li> <li><code>SLEEP</code>: Sleep interval between checks in seconds (default: 5)</li> </ul>"},{"location":"howto/chain-integrity-testing/#3-quick-chain-integrity-test","title":"3. Quick Chain Integrity Test","text":"<pre><code>make chain-integrity-test-quick\n</code></pre> <p>This runs a faster version of the test with shorter 20 minutes:</p> <ul> <li>Required block height: 50</li> <li>Maximum wait time: 3 minutes (60 attempts \u00d7 3 seconds)</li> <li>Sleep interval: 3 seconds</li> </ul>"},{"location":"howto/chain-integrity-testing/#4-display-hash-analysis","title":"4. Display Hash Analysis","text":"<pre><code>make show-hashes\n</code></pre> <p>This displays the hash analysis results from a previous chain integrity test:</p> <ul> <li>Shows individual node hash values</li> <li>Indicates consensus status</li> <li>Provides clear success/failure indicators</li> <li>Can be run independently after a test completes</li> </ul>"},{"location":"howto/chain-integrity-testing/#5-aws-ecr-login","title":"5. AWS ECR Login","text":"<pre><code>make ecr-login\n</code></pre> <p>This logs into AWS ECR to enable pulling required Docker images:</p> <ul> <li>Authenticates with AWS ECR in eu-north-1 region</li> <li>Enables pulling teranode-commands and teranode-coinbase images</li> <li>Required before running tests if ECR images are needed</li> <li>Automatically handled during chain integrity tests</li> </ul>"},{"location":"howto/chain-integrity-testing/#6-clean-up","title":"6. Clean Up","text":"<pre><code>make clean-chain-integrity\n</code></pre> <p>This cleans up all artifacts from chain integrity tests:</p> <ul> <li>Removes log files</li> <li>Stops and removes Docker containers</li> <li>Removes the chainintegrity binary</li> </ul>"},{"location":"howto/chain-integrity-testing/#test-process","title":"Test Process","text":"<p>The chain integrity test follows these steps:</p> <ol> <li>Build chainintegrity binary - Compiles the chain integrity tool</li> <li>Clean up old data - Removes previous test data</li> <li>Build teranRM teranode image - Creates a local Docker image</li> <li>Start Teranode nodes - Launches 3 nodes with block generators using Docker Compose</li> <li>Wait for mining completion - Monitors all nodes until they reach the required block height</li> <li>Stop Teranode nodes - Gracefully stops the nodes</li> <li>Run chainintegrity test - Runs the integrity verification tool</li> <li>Check for hash mismatch - Verifies consensus among nodes</li> <li>Cleanup - Stops all containers and removes resources</li> </ol>"},{"location":"howto/chain-integrity-testing/#monitoring","title":"Monitoring","text":"<p>During the test, you can monitor progress by:</p> <ul> <li>Checking the console output for height updates</li> <li>Viewing container logs: <code>docker compose -f compose/docker-compose-3blasters.yml logs</code></li> <li> <p>Checking individual node APIs:</p> </li> <li> <p>Node 1: http://localhost:18090/api/v1/bestblockheader/json</p> </li> <li>Node 2: http://localhost:28090/api/v1/bestblockheader/json</li> <li>Node 3: http://localhost:38090/api/v1/bestblockheader/json</li> </ul>"},{"location":"howto/chain-integrity-testing/#output-files","title":"Output Files","text":"<p>After a successful test, the following files are generated:</p> <ul> <li><code>chainintegrity_output.log</code> - Main output from the chainintegrity tool</li> <li><code>chainintegrity-teranode1.log</code> - Log file for node 1</li> <li><code>chainintegrity-teranode2.log</code> - Log file for node 2</li> <li><code>chainintegrity-teranode3.log</code> - Log file for node 3</li> <li><code>chainintegrity-teranode1.filtered.log</code> - Filtered log for node 1</li> <li><code>chainintegrity-teranode2.filtered.log</code> - Filtered log for node 2</li> <li><code>chainintegrity-teranode3.filtered.log</code> - Filtered log for node 3</li> </ul>"},{"location":"howto/chain-integrity-testing/#hash-analysis","title":"Hash Analysis","text":"<p>The chain integrity test compares SHA256 hashes of filtered log files across all nodes. You can view the hash analysis results using:</p> <pre><code>make show-hashes\n</code></pre> <p>This will display output similar to: <pre><code>\ud83d\udcca Hash Analysis Results:\n==========================\n  - Extracting hash information...\n\n    chainintegrity-teranode1.filtered.log: a1b2c3d4e5f6...\n    chainintegrity-teranode2.filtered.log: a1b2c3d4e5f6...\n    chainintegrity-teranode3.filtered.log: 9f8e7d6c5b4a...\n\n  \u2713 Consensus achieved: At least two nodes have matching hashes\n</code></pre></p> <p>The hash comparison helps verify that nodes are maintaining consistent blockchain state.</p>"},{"location":"howto/chain-integrity-testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"howto/chain-integrity-testing/#common-issues","title":"Common Issues","text":"<ol> <li>Timeout waiting for nodes to reach height</li> <li>Increase <code>MAX_ATTEMPTS</code> or decrease <code>REQUIRED_HEIGHT</code></li> <li>Check system resources (CPU, memory, disk space)</li> <li> <p>Verify Docker containers are running: <code>docker ps</code></p> </li> <li> <p>ECR login issues</p> </li> <li>Ensure AWS CLI is installed and configured</li> <li>Check AWS credentials: <code>aws sts get-caller-identity</code></li> <li>Verify ECR access permissions</li> <li> <p>Run <code>make ecr-login</code> manually if needed</p> </li> <li> <p>Nodes not responding</p> </li> <li>Check container logs: <code>docker compose -f compose/docker-compose-3blasters.yml logs teranode1</code></li> <li>Verify ports are not in use: <code>lsof -i :18090,28090,38090</code></li> <li> <p>Restart Docker if needed</p> </li> <li> <p>Chain integrity test fails</p> </li> <li>Check the <code>chainintegrity_output.log</code> for detailed error messages</li> <li>Verify all nodes are in sync before running the test</li> <li>Consider running with debug mode for more verbose output</li> </ol>"},{"location":"howto/chain-integrity-testing/#debug-mode","title":"Debug Mode","text":"<p>To get more detailed output, manually run the chainintegrity tool:</p> <pre><code># Build the tool\nmake build-chainintegrity\n\n# Run with debug output\n./chainintegrity.run --logfile=chainintegrity --debug\n</code></pre>"},{"location":"howto/chain-integrity-testing/#manual-cleanup","title":"Manual Cleanup","text":"<p>If the test fails or gets stuck, you can manually clean up:</p> <pre><code># Stop all containers\ndocker compose -f compose/docker-compose-3blasters.yml down\n\n# Remove test artifacts\nmake clean-chain-integrity\n\n# Remove data directory\nrm -rf data\n</code></pre>"},{"location":"howto/chain-integrity-testing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>The test requires significant system resources</li> <li>Running on a machine with less than 8GB RAM may cause issues</li> <li>SSD storage is recommended for faster I/O</li> <li>Consider using the quick test for development iterations</li> </ul>"},{"location":"howto/chain-integrity-testing/#integration-with-awkward-cicd","title":"Integration with Awkward CI/CD","text":"<p>The local invokes the same process as the GitHub workflow:</p> <ul> <li>Uses the same Docker Compose configuration</li> <li>Follows the same error checking logic</li> <li>Generates the same output format</li> <li>Can be used to reproduce CI/CD issues locally </li> </ul>"},{"location":"howto/configuringGrpcLogging/","title":"Configuring gRPC Logging","text":""},{"location":"howto/configuringGrpcLogging/#how-to-enable-additional-grpc-logs","title":"How to Enable Additional gRPC Logs","text":"<p>By setting specific environment variables, you can obtain more detailed logs that provide insight into gRPC's internal workings, such as the client channel and round-robin load balancing. These logs can help in debugging and troubleshooting gRPC-related issues.</p>"},{"location":"howto/configuringGrpcLogging/#step-by-step-enabling-grpc-logs","title":"Step-by-Step: Enabling gRPC Logs","text":""},{"location":"howto/configuringGrpcLogging/#step-1-understanding-grpc-log-levels","title":"Step 1: Understanding gRPC Log Levels","text":"<p>By default, gRPC provides minimal logging. However, for debugging purposes, you can increase the verbosity of the logs by setting the environment variable <code>GRPC_VERBOSITY</code>. The available levels for <code>GRPC_VERBOSITY</code> are:</p> <ul> <li><code>ERROR</code>: Logs only error messages.</li> <li><code>INFO</code>: Logs informational messages (this is the default level).</li> <li><code>DEBUG</code>: Logs detailed debug messages, useful for tracking detailed behaviors.</li> </ul> <p>In addition to setting the verbosity, you can specify which gRPC components you want to trace by setting the <code>GRPC_TRACE</code> environment variable. Some of the commonly traced components include:</p> <ul> <li><code>client_channel</code>: Provides logs related to the client-side gRPC channel.</li> <li><code>round_robin</code>: Provides logs related to the round-robin load balancing mechanism.</li> </ul>"},{"location":"howto/configuringGrpcLogging/#step-2-setting-the-environment-variables","title":"Step 2: Setting the Environment Variables","text":"<p>To enable additional logs when running the node, set the following environment variables:</p> <pre><code>export GRPC_VERBOSITY=debug\nexport GRPC_TRACE=client_channel,round_robin\n</code></pre> <p>Here\u2019s what each of these does:</p> <ul> <li><code>GRPC_VERBOSITY=debug</code>: This sets the log verbosity to <code>debug</code>, which provides detailed logging of gRPC operations.</li> <li><code>GRPC_TRACE=client_channel,round_robin</code>: This specifies that logs should be generated for the <code>client_channel</code> and <code>round_robin</code> components of gRPC, which are useful for debugging connection handling and load balancing behaviors.</li> </ul>"},{"location":"howto/configuringGrpcLogging/#step-3-running-the-node-with-grpc-logging-enabled","title":"Step 3: Running the Node with gRPC Logging Enabled","text":"<p>Once the environment variables are set, you can run the node as usual. The additional gRPC logs will now be printed to the console or log file, depending on your node\u2019s logging configuration.</p> <p>Here\u2019s an example of how to run the node with the environment variables enabled:</p> <pre><code>GRPC_VERBOSITY=debug GRPC_TRACE=client_channel,round_robin go run .\n</code></pre>"},{"location":"howto/configuringGrpcLogging/#step-4-interpreting-the-logs","title":"Step 4: Interpreting the Logs","text":"<p>After enabling gRPC logs, you will see more detailed output related to the gRPC client channels and round-robin load balancing. Some of the key log entries to watch for include:</p> <ul> <li>Connection and Reconnection Events: Logs showing when the gRPC client establishes or re-establishes connections to a server.</li> <li>Load Balancing Events: Logs detailing how the round-robin load balancer distributes calls across available servers.</li> <li>Error Details: Detailed errors or warnings that may occur during RPCs, including retries or failures.</li> </ul> <p>These logs can provide valuable insights into how the node interacts with other services via gRPC and can help pinpoint issues with connection management, load balancing, or request handling.</p>"},{"location":"howto/configuringGrpcLogging/#step-5-disabling-grpc-logs","title":"Step 5: Disabling gRPC Logs","text":"<p>Once you\u2019re done debugging or collecting logs, you may want to disable the additional logging to reduce the verbosity of the output. To do this, simply unset the environment variables or set them to less verbose options. For example:</p> <pre><code>unset GRPC_VERBOSITY\nunset GRPC_TRACE\n</code></pre> <p>Alternatively, you can set <code>GRPC_VERBOSITY</code> to a less detailed level, like <code>ERROR</code>:</p> <pre><code>export GRPC_VERBOSITY=error\n</code></pre>"},{"location":"howto/configuringGrpcLogging/#summary","title":"Summary","text":"<p>By setting the <code>GRPC_VERBOSITY</code> and <code>GRPC_TRACE</code> environment variables, you can enable additional logging for gRPC to help debug and troubleshoot issues with client channels and load balancing. This can be particularly useful when diagnosing connectivity issues or fine-tuning the node\u2019s interaction with other services.</p> <p>For a more complete overview of gRPC logs and tracing, refer to the official gRPC documentation or additional project-specific logging documentation if available.</p>"},{"location":"howto/developersHowToTeranodeCLI/","title":"Developer's Guide to Teranode-CLI","text":"<p>Last Modified: 21-May-2025</p>"},{"location":"howto/developersHowToTeranodeCLI/#overview","title":"Overview","text":"<p>The Teranode-CLI is a command-line interface tool designed for developers to interact with Teranode services during development and testing. Unlike the production environment where you might access it through Docker containers, as a developer you'll build and run it directly on your machine.</p> <p>This guide provides a comprehensive walkthrough of using the Teranode-CLI in a development environment.</p>"},{"location":"howto/developersHowToTeranodeCLI/#building-the-cli","title":"Building the CLI","text":"<p>Before using Teranode-CLI, you need to build it from source:</p> <pre><code>go build -o teranode-cli ./cmd/teranodecli\n</code></pre> <p>This will create a <code>teranode-cli</code> executable in your current directory.</p>"},{"location":"howto/developersHowToTeranodeCLI/#basic-usage","title":"Basic Usage","text":"<pre><code># General format\nSETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli &lt;command&gt; [options]\n\n# Getting help\n./teranode-cli\n</code></pre>"},{"location":"howto/developersHowToTeranodeCLI/#important-settings-context","title":"Important Settings Context","text":"<p>All commands require your <code>SETTINGS_CONTEXT</code> environment variable to be set correctly. This ensures the CLI uses your development settings:</p> <pre><code># Either set it for the session\nexport SETTINGS_CONTEXT=dev.[YOUR_CONTEXT]\n\n# Or prefix each command\nSETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli &lt;command&gt;\n</code></pre> <p>Replace <code>[YOUR_CONTEXT]</code> with your specific development context (e.g., <code>dev.johndoe</code> or simply <code>dev</code>).</p>"},{"location":"howto/developersHowToTeranodeCLI/#available-commands","title":"Available Commands","text":""},{"location":"howto/developersHowToTeranodeCLI/#fsm-state-management","title":"FSM State Management","text":"<p>One of the most common uses of Teranode-CLI during development is managing the Finite State Machine (FSM) state of your Teranode instance.</p>"},{"location":"howto/developersHowToTeranodeCLI/#getting-the-current-state","title":"Getting the Current State","text":"<pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli getfsmstate\n</code></pre> <p>Typical output: <pre><code>Current FSM State: IDLE\n</code></pre></p>"},{"location":"howto/developersHowToTeranodeCLI/#setting-a-new-state","title":"Setting a New State","text":"<pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli setfsmstate --fsmstate &lt;state&gt;\n</code></pre> <p>Valid states you can issue are:</p> <ul> <li><code>running</code> - Normal operation mode (processes transactions and creates blocks)</li> <li><code>idle</code> - Idle mode (default startup state)</li> </ul> <p>Example to switch to RUNNING state: <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli setfsmstate --fsmstate running\n</code></pre></p> <p>Expected output: <pre><code>Setting FSM state to: running\nFSM state successfully set to: RUNNING\n</code></pre></p>"},{"location":"howto/developersHowToTeranodeCLI/#view-system-configuration","title":"View System Configuration","text":"<p>To inspect your current system settings:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli settings\n</code></pre> <p>This will display a comprehensive list of all settings currently in effect, including which specific settings are overridden by your <code>[YOUR_CONTEXT]</code> configuration.</p>"},{"location":"howto/developersHowToTeranodeCLI/#data-management-commands","title":"Data Management Commands","text":""},{"location":"howto/developersHowToTeranodeCLI/#aerospike-reader","title":"Aerospike Reader","text":"<p>Retrieve transaction data from Aerospike using a transaction ID:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli aerospikereader &lt;txid&gt;\n</code></pre> <p>The <code>&lt;txid&gt;</code> must be a valid 64-character transaction ID.</p>"},{"location":"howto/developersHowToTeranodeCLI/#file-reader","title":"File Reader","text":"<p>Inspect data files:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli filereader [path] [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--verbose</code> - Enable verbose output</li> <li><code>--checkHeights</code> - Check heights in UTXO headers</li> <li><code>--useStore</code> - Use store</li> </ul>"},{"location":"howto/developersHowToTeranodeCLI/#bitcoin-to-utxo-set-conversion","title":"Bitcoin to UTXO Set Conversion","text":"<p>Convert Bitcoin blockchain data to UTXO set format:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli bitcointoutxoset --bitcoinDir=&lt;bitcoin-data-path&gt; --outputDir=&lt;output-dir-path&gt; [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--bitcoinDir</code> - Location of Bitcoin data (required)</li> <li><code>--outputDir</code> - Output directory for UTXO set (required)</li> <li><code>--skipHeaders</code> - Skip processing headers</li> <li><code>--skipUTXOs</code> - Skip processing UTXOs</li> <li><code>--blockHash</code> - Block hash to start from</li> <li><code>--previousBlockHash</code> - Previous block hash</li> <li><code>--blockHeight</code> - Block height to start from</li> <li><code>--dumpRecords</code> - Dump records from index</li> </ul>"},{"location":"howto/developersHowToTeranodeCLI/#utxo-persister-management","title":"UTXO Persister Management","text":"<p>Manage UTXO persistence:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli utxopersister\n</code></pre>"},{"location":"howto/developersHowToTeranodeCLI/#seeder","title":"Seeder","text":"<p>Seed initial blockchain data:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli seeder --inputDir=&lt;input-dir&gt; --hash=&lt;hash&gt; [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--inputDir</code> - Input directory for UTXO set and headers (required)</li> <li><code>--hash</code> - Hash of the UTXO set / headers to process (required)</li> <li><code>--skipHeaders</code> - Skip processing headers</li> <li><code>--skipUTXOs</code> - Skip processing UTXOs</li> </ul>"},{"location":"howto/developersHowToTeranodeCLI/#block-data-importexport","title":"Block Data Import/Export","text":""},{"location":"howto/developersHowToTeranodeCLI/#export-blockchain-to-csv","title":"Export Blockchain to CSV","text":"<pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli export-blocks --file=&lt;file-path&gt;\n</code></pre>"},{"location":"howto/developersHowToTeranodeCLI/#import-blockchain-from-csv","title":"Import Blockchain from CSV","text":"<pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli import-blocks --file=&lt;file-path&gt;\n</code></pre>"},{"location":"howto/developersHowToTeranodeCLI/#block-template-verification","title":"Block Template Verification","text":"<p>Check if the current block template is valid:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli checkblocktemplate\n</code></pre>"},{"location":"howto/developersHowToTeranodeCLI/#common-development-workflows","title":"Common Development Workflows","text":""},{"location":"howto/developersHowToTeranodeCLI/#starting-a-fresh-development-node","title":"Starting a Fresh Development Node","text":"<ol> <li>Start your Teranode node:</li> </ol> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] go run .\n</code></pre> <ol> <li>Check the initial FSM state:</li> </ol> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli getfsmstate\n</code></pre> <ol> <li>Transition to RUNNING state:</li> </ol> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli setfsmstate --fsmstate running\n</code></pre> <ol> <li>Verify the FSM state change:</li> </ol> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli getfsmstate\n</code></pre>"},{"location":"howto/developersHowToTeranodeCLI/#debugging-tips","title":"Debugging Tips","text":"<ul> <li>If your teranode-cli commands aren't working, ensure your <code>SETTINGS_CONTEXT</code> is correctly set</li> <li>Verify the node is actually running before attempting to change its state</li> <li>Look for error messages in both the CLI output and your node's logs</li> <li>Use the <code>settings</code> command to confirm your configuration settings are applied correctly</li> </ul>"},{"location":"howto/developersHowToTeranodeCLI/#extending-the-cli","title":"Extending the CLI","text":"<p>Developers can extend the Teranode-CLI by adding new commands to the <code>cmd/teranodecli/teranodecli/cli.go</code> file. Follow the existing pattern for creating new commands and adding them to the command help map.</p>"},{"location":"howto/developersHowToTeranodeCLI/#further-resources","title":"Further Resources","text":"<ul> <li>Developer Setup Guide</li> <li>Locally Running Services</li> <li>FSM State Management</li> </ul>"},{"location":"howto/generatingProtobuf/","title":"Generating Protobuf","text":""},{"location":"howto/generatingProtobuf/#how-to-generate-protobuf-files","title":"How to Generate Protobuf Files","text":"<p>This guide will walk you through the process of generating Protobuf files for the Teranode project. Protobuf files define the structure and services used across the project, and the <code>Makefile</code> provides a simple way to compile these <code>.proto</code> files into Go source code using the <code>protoc</code> compiler.</p>"},{"location":"howto/generatingProtobuf/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, make sure the following tools and resources are available:</p> <ol> <li>Protobuf Compiler (<code>protoc</code>): Ensure you have the <code>protoc</code> tool installed. If you have followed the Installation Guide for Developers and Contributors, you are ready to use it. For more information, you can follow the official guide too.</li> <li>Makefile: This project uses the Teranode <code>Makefile</code> to manage protobuf compilation tasks. Familiarity with running <code>make</code> commands is useful.</li> </ol>"},{"location":"howto/generatingProtobuf/#step-by-step-generating-protobuf-files","title":"Step-by-Step: Generating Protobuf Files","text":""},{"location":"howto/generatingProtobuf/#step-1-overview-of-proto-files","title":"Step 1: Overview of <code>.proto</code> Files","text":"<p>The project uses <code>.proto</code> files to define the structure of its services and messages. Each service is typically located in the <code>services</code> directory, and they follow the pattern:</p> <pre><code>services/&lt;service_name&gt;/&lt;service_name&gt;_api.proto\n</code></pre> <p>For example, the file <code>services/subtreevalidation/subtreevalidation_api.proto</code> defines the RPC services and message types for the <code>SubtreeValidationService</code>.</p> <pre><code>syntax = \"proto3\";\n\npackage subtreevalidation;\n\nservice SubtreeValidationService {\nrpc ValidateSubtree (SubtreeValidationRequest) returns (SubtreeValidationResponse);\n}\n\nmessage SubtreeValidationRequest {\nstring tree_id = 1;\nint32 max_depth = 2;\n}\n\nmessage SubtreeValidationResponse {\nbool is_valid = 1;\nstring validation_message = 2;\n}\n</code></pre>"},{"location":"howto/generatingProtobuf/#step-2-generating-the-protobuf-files","title":"Step 2: Generating the Protobuf Files","text":"<p>To generate the Go files from the <code>.proto</code> definitions, simply run the following command from the project\u2019s root directory:</p> <pre><code>make gen\n</code></pre> <p>This command will:</p> <ul> <li>Process all <code>.proto</code> files listed in the <code>Makefile</code> using <code>protoc</code>.</li> <li>Generate the corresponding Go source code files (<code>*.pb.go</code> and <code>*_grpc.pb.go</code>) in the same directory as the <code>.proto</code> files.</li> </ul>"},{"location":"howto/generatingProtobuf/#how-the-make-gen-command-works","title":"How the <code>make gen</code> Command Works","text":"<p>The <code>make gen</code> command runs several <code>protoc</code> commands, which are defined in the <code>Makefile</code>. Each <code>protoc</code> command processes a <code>.proto</code> file and generates its corresponding Go files.</p> <p>For example, to generate files for the <code>blockassembly</code> service, the command looks like this:</p> <pre><code>protoc \\\n--proto_path=. \\\n--go_out=. \\\n--go_opt=paths=source_relative \\\n--go-grpc_out=. \\\n--go-grpc_opt=paths=source_relative \\\nservices/blockassembly/blockassembly_api/blockassembly_api.proto\n</code></pre> <p>This generates:</p> <ul> <li><code>services/blockassembly/blockassembly_api/blockassembly_api.pb.go</code> (message definitions)</li> <li><code>services/blockassembly/blockassembly_api/blockassembly_api_grpc.pb.go</code> (gRPC client and server stubs)</li> </ul>"},{"location":"howto/generatingProtobuf/#step-3-handling-dependencies","title":"Step 3: Handling Dependencies","text":"<p>Some services have dependencies on common <code>.proto</code> files, like <code>model.proto</code>. The <code>Makefile</code> is configured to handle these dependencies.</p> <p>For example: <pre><code>protoc \\\n--proto_path=. \\\n--go_out=. \\\n--go_opt=paths=source_relative \\\nmodel/model.proto\n</code></pre> This command generates the necessary Go files for <code>model.proto</code> and ensures that services depending on <code>model.proto</code> can compile successfully.</p>"},{"location":"howto/generatingProtobuf/#step-4-cleaning-generated-files","title":"Step 4: Cleaning Generated Files","text":"<p>If you need to remove the generated Protobuf files, you can run the following command:</p> <pre><code>make clean_gen\n</code></pre> <p>This will remove all <code>.pb.go</code> files generated by the <code>make gen</code> command across the different services, as defined in the <code>Makefile</code>.</p>"},{"location":"howto/generatingProtobuf/#additional-make-commands","title":"Additional Make Commands","text":"<p>More information on available <code>make</code> commands, including options for customization and dependency handling, can be found in the Makefile Documentation.</p>"},{"location":"howto/generatingProtobuf/#common-errors-and-troubleshooting","title":"Common Errors and Troubleshooting","text":"<ol> <li><code>protoc</code> not found: Ensure that the <code>protoc</code> tool is installed and available in your system\u2019s PATH.</li> <li>Missing dependencies: If a <code>.proto</code> file depends on another file (e.g., <code>model.proto</code>), ensure that all required <code>.proto</code> files are included in the <code>proto_path</code> during compilation.</li> </ol>"},{"location":"howto/generatingProtobuf/#conclusion","title":"Conclusion","text":"<p>You should now be able to generate Protobuf files for the Teranode Go services. The <code>make gen</code> command simplifies the process by automating the <code>protoc</code> compilation.</p> <p>For more advanced usage and detailed configuration, check out the Makefile Documentation.</p>"},{"location":"howto/howToRemoteDebugTeranode/","title":"Remotely Debugging a Teranode Operator","text":"<p>For advanced users who need to debug a running Teranode operator, follow these steps:</p> <ol> <li>Edit the Cluster Custom Resource (CR): Use the following command, replacing <code>mainnet-1</code> with your namespace if different:</li> </ol> <pre><code>kubectl edit clusters.teranode.bsvblockchain.org -n mainnet-1 mainnet-1\n</code></pre> <ol> <li>Modify the Service Configuration: Find the service you want to debug (e.g., legacy) and override the <code>command</code> key under <code>deploymentOverrides</code>:</li> </ol> <pre><code>command:\n\n- ./dlv\n- --listen=:4040\n- --continue\n- --accept-multiclient\n- --headless=true\n- --api-version=2\n- exec\n- ./teranode.run\n- --\n</code></pre> <p>Note: If you need to debug multiple services, add this command to all necessary services.</p> <ol> <li>Port Forward the Debugger: You have two options:</li> </ol> <p>a. Using kubectl (easier for a single port): <pre><code>kubectl port-forward -n mainnet-1 &lt;replace_with_pod_name&gt; 4040:4040\n</code></pre> This will provide you with an IP:PORT (often localhost) to access the debugger.</p> <p>b. Using kubefwd (useful for multiple services): <pre><code>sudo kubefwd svc -n mainnet-1\n</code></pre> This maps ports with service names (e.g., asset:4040, legacy:4040, blockchain:4040).</p> <ol> <li> <p>Configure VS Code:</p> <ul> <li>Set up VS Code to connect to the remote debugger now running locally on your computer.</li> <li>Avoid using the \"cursor\" feature, as it may cause connection issues.</li> <li>If using kubefwd, ensure your debugger IP points to the correct service (e.g., use <code>asset:4040</code> for the asset service).</li> </ul> </li> <li> <p>Start Debugging:</p> <ul> <li>Add breakpoints as needed.</li> <li>Begin your debugging session.</li> </ul> </li> </ol> <p>Note: When debugging multiple services, make sure to use the correct local DNS (e.g., <code>asset:4040</code>) for each service you're debugging.</p>"},{"location":"howto/locallyRunningServices/","title":"\u25b6\ufe0f Developer Guides - Running the Services Locally","text":"<p>This section will walk you through the commands and configurations needed to run your services locally for development purposes.</p>"},{"location":"howto/locallyRunningServices/#quickstart-run-all-services","title":"\ud83d\ude80 Quickstart: Run All Services","text":""},{"location":"howto/locallyRunningServices/#prerequisites","title":"Prerequisites","text":"<p>Before running Teranode, ensure the required infrastructure services are started:</p> <pre><code># Start Kafka in Docker\n./scripts/kafka.sh\n\n# Start PostgreSQL in Docker\n./scripts/postgres.sh\n</code></pre> <p>Note: If you configure your settings to use Aerospike for UTXO storage, you'll also need to run:</p> <pre><code># Start Aerospike in Docker\n./scripts/aerospike.sh\n</code></pre>"},{"location":"howto/locallyRunningServices/#start-teranode","title":"Start Teranode","text":"<p>Execute all services in a single terminal window with the command below. Replace <code>[YOUR_USERNAME]</code> with your specific username.</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] go run .\n</code></pre> <p>\ud83d\udcdd Note: Confirm that settings for your context are correctly established as outlined in the Installation Guide.</p> <p>\u26a0\ufe0f Warning: When restarting services, it's recommended to clean the data directory first:</p> <pre><code>rm -rf data\n</code></pre>"},{"location":"howto/locallyRunningServices/#advanced-configuration","title":"\ud83d\udcdd Advanced Configuration","text":""},{"location":"howto/locallyRunningServices/#database-backend-configuration","title":"Database Backend Configuration","text":"<p>Teranode supports multiple database backends for UTXO storage, configured via settings rather than build tags:</p> <ol> <li>PostgreSQL (Default for development):</li> </ol> <pre><code># Make sure PostgreSQL is running\n./scripts/postgres.sh\n\n# Your settings_local.conf should have a PostgreSQL connection string\nutxostore.dev.[YOUR_USERNAME] = postgres://teranode:teranode@localhost:5432/teranode?blockHeightRetention=5\n\n# Run with the PostgreSQL backend\nSETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run .\n</code></pre> <ol> <li>SQLite (Lightweight option):</li> </ol> <pre><code># Your settings_local.conf should have an SQLite connection string\nutxostore.dev.[YOUR_USERNAME] = sqlite:///utxostore?blockHeightRetention=5\n\n# Run with SQLite backend\nSETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run .\n</code></pre> <ol> <li>Aerospike (High-performance option):</li> </ol> <p>Warning: Aerospike Requirements</p> <ul> <li>Requires both the appropriate settings AND the 'aerospike' build tag</li> <li>See the Aerospike Integration section below</li> <li>Important: Unlike PostgreSQL and SQLite, Aerospike requires the build tag because the Aerospike driver code won't be compiled into the binary without it. If you configure Aerospike in settings but don't use the tag, the application will fail at runtime with an 'unknown database driver' error.</li> </ul> <p>Note: The database backend is determined by the connection string prefix in your settings:</p> <ul> <li>PostgreSQL: <code>postgres://</code></li> <li>SQLite: <code>sqlite:///</code></li> <li>Aerospike: <code>aerospike://</code></li> </ul>"},{"location":"howto/locallyRunningServices/#build-tags","title":"\ud83c\udff7\ufe0f Build Tags","text":"<p>Teranode supports various build tags that enable specific features or configurations. These tags are specified using the <code>-tags</code> flag with the <code>go run</code> or <code>go build</code> commands.</p>"},{"location":"howto/locallyRunningServices/#aerospike-integration","title":"Aerospike Integration","text":"<p>To use Aerospike as the UTXO storage backend:</p> <ol> <li>First, start the Aerospike Docker container:</li> </ol> <pre><code>./scripts/aerospike.sh\n</code></pre> <ol> <li>Run Teranode with the aerospike tag:</li> </ol> <pre><code>rm -rf data &amp;&amp; SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -tags aerospike .\n</code></pre>"},{"location":"howto/locallyRunningServices/#transaction-metadata-cache-configurations","title":"Transaction Metadata Cache Configurations","text":"<p>Teranode supports different transaction metadata cache sizes through build tags:</p> <ul> <li>Large Cache (Default): Used when no specific tx metadata cache tag is specified</li> </ul> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -tags aerospike .\n</code></pre> <ul> <li>Small Cache: Reduces memory usage with a smaller transaction metadata cache</li> </ul> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -tags aerospike,smalltxmetacache .\n</code></pre> <ul> <li>Test Cache: Configured specifically for testing scenarios</li> </ul> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -tags aerospike,testtxmetacache .\n</code></pre>"},{"location":"howto/locallyRunningServices/#multiple-tags","title":"Multiple Tags","text":"<p>You can combine multiple tags by separating them with commas:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -tags aerospike,smalltxmetacache .\n</code></pre>"},{"location":"howto/locallyRunningServices/#network-configuration","title":"Network Configuration","text":"<p>Teranode supports different Bitcoin networks (mainnet, testnet, etc.). This is primarily controlled through settings but can be overridden using the <code>network</code> environment variable:</p> <pre><code># Run on testnet\nnetwork=testnet SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run .\n\n# Run on testnet with Aerospike\nnetwork=testnet SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -tags aerospike .\n</code></pre> <p>Note: The network setting defaults to what's specified in your settings_local.conf under <code>network.dev.[YOUR_USERNAME]</code>. The environment variable overrides this setting.</p>"},{"location":"howto/locallyRunningServices/#testing-tags","title":"Testing Tags","text":"<p>For running various test suites (not typically needed for development):</p> <ul> <li><code>test_all</code>: Runs all tests</li> <li><code>test_smoke_rpc</code>: Runs smoke tests for RPC functionality</li> <li><code>test_services</code>: Tests specific to services</li> <li><code>test_longlong</code>: For extended duration tests</li> </ul>"},{"location":"howto/locallyRunningServices/#component-options","title":"Component Options","text":"<p>Launch the node with specific components using command-line options. This allows you to enable only the components you need for your development tasks.</p> <pre><code>rm -rf data &amp;&amp; SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] go run -tags aerospike . [OPTIONS]\n</code></pre> <p>Enable or disable components by setting the corresponding option to <code>1</code> or <code>0</code>. Note: Options are case-sensitive and must be lowercase.</p> Component Option Description Alert <code>-alert=1</code> Alert system for network notifications Asset <code>-asset=1</code> Asset handling service Block Assembly <code>-blockassembly=1</code> Block assembly service Block Persister <code>-blockpersister=1</code> Block persistence service Block Validation <code>-blockvalidation=1</code> Block validation service Blockchain <code>-blockchain=1</code> Blockchain processing service Legacy <code>-legacy=1</code> Legacy API support P2P <code>-p2p=1</code> Peer-to-peer networking service Propagation <code>-propagation=1</code> Data propagation service RPC <code>-rpc=1</code> RPC interface service Subtree Validation <code>-subtreevalidation=1</code> Subtree validation service UTXO Persister <code>-utxopersister=1</code> UTXO persistence service Validator <code>-validator=1</code> Transaction validation service"},{"location":"howto/locallyRunningServices/#additional-options","title":"Additional Options","text":"Option Description <code>-all=&lt;1\\|0&gt;</code> Enable/disable all services unless explicitly overridden by other flags. By default (when no flags are specified), the system behaves as if <code>-all=1</code> was set. <code>-help=1</code> Display command-line help information <code>-wait_for_postgres=1</code> Wait for PostgreSQL to be available before starting <code>-localTestStartFromState=X</code> Start blockchain FSM from a specific state (for testing)"},{"location":"howto/locallyRunningServices/#examples","title":"Examples","text":"<p>To start the node with only validation and UTXO storage:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -tags aerospike . -validator=1 -utxopersister=1\n</code></pre>"},{"location":"howto/locallyRunningServices/#wait-for-postgresql","title":"Wait For PostgreSQL","text":"<p>If you want Teranode to wait for PostgreSQL to be available before starting:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run . -wait_for_postgres=1\n</code></pre> <p>This is useful in containerized environments or when PostgreSQL might not be immediately ready.</p>"},{"location":"howto/locallyRunningServices/#health-checks","title":"Health Checks","text":"<p>Teranode exposes health check endpoints on port 8000 (configurable in settings):</p> <ul> <li><code>/health/readiness</code> - Indicates if the system is ready to accept requests</li> <li><code>/health/liveness</code> - Indicates if the system is running properly</li> </ul>"},{"location":"howto/locallyRunningServices/#logging-configuration","title":"Logging Configuration","text":"<p>Teranode respects the <code>NO_COLOR</code> environment variable to disable colored output in logs.</p> <pre><code>NO_COLOR=1 SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run .\n</code></pre>"},{"location":"howto/locallyRunningServices/#component-selection-examples","title":"Component Selection Examples","text":"<p>Running specific components only:</p> <p>To initiate the node with only specific components, such as <code>Validator</code>:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -tags aerospike . -validator=1\n</code></pre> <p>Disabling all services by default and enabling only specific ones:</p> <p>This is particularly useful for development:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -tags aerospike . -all=0 -validator=1 -rpc=1\n</code></pre>"},{"location":"howto/locallyRunningServices/#running-individual-services","title":"\ud83d\udd27 Running Individual Services","text":"<p>You can also run each service on its own:</p> <ol> <li>Navigate to a service's directory:</li> </ol> <pre><code>cd services/validator\n</code></pre> <ol> <li>Run the service:</li> </ol> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] go run .\n</code></pre>"},{"location":"howto/locallyRunningServices/#running-specific-commands","title":"\ud83d\udcdc Running Specific Commands","text":"<p>For executing particular tasks, use commands found under the cmd/ directory.</p>"},{"location":"howto/locallyRunningServices/#running-ui-dashboard","title":"\ud83d\udda5 Running UI Dashboard","text":"<p>For UI Dashboard:</p> <pre><code>make dev-dashboard\n</code></pre> <p>Remember to replace <code>[YOUR_CONTEXT]</code> with your actual username throughout all commands.</p> <p>This guide aims to provide a streamlined process for running services and nodes during development.</p> <p>If you encounter any issues, consult the detailed documentation or reach out to the development team for assistance.</p>"},{"location":"howto/makefile/","title":"Makefile Documentation","text":"<p>This Makefile facilitates a variety of development and build tasks for the Teranode project.</p> <ul> <li>Makefile Documentation<ul> <li>Environment Configuration</li> <li>Key Commands</li> <li>All Commands<ul> <li>General Configuration</li> <li>Setting Debug Flags</li> <li>Setting Transaction Metadata Cache Flags</li> <li>Build All Components</li> <li>Dependencies</li> <li>Development</li> <li>Building</li> <li>Building Tools and Utilities</li> <li>Testing</li> <li>Chain Integrity Testing</li> <li>Code Generation</li> <li>Cleanup</li> <li>Linting and Static Analysis</li> <li>Installation</li> <li>Utilities</li> </ul> </li> <li>Environment Variables</li> <li>Usage Examples</li> <li>Notes</li> </ul> </li> </ul>"},{"location":"howto/makefile/#environment-configuration","title":"Environment Configuration","text":"<ul> <li><code>SHELL</code>: Indicates the shell for the make process. Set to <code>/bin/bash</code>.</li> <li><code>DEBUG_FLAGS</code>: Flags to use in debug mode.</li> <li><code>TXMETA_TAG</code>: Tag for transaction metadata cache configuration.</li> <li><code>SETTINGS_CONTEXT_DEFAULT</code>: Default settings context (set to <code>docker.ci</code>).</li> <li><code>LOCAL_TEST_START_FROM_STATE</code>: Optional parameter to configure test start state.</li> </ul>"},{"location":"howto/makefile/#key-commands","title":"Key Commands","text":"<p>Type: Target | Description: Description | Impact: What it does</p> <ul> <li>all: Executes the following tasks in order: <code>deps</code>, <code>install</code>, <code>lint</code>, <code>build</code>, and <code>test</code>.</li> <li>deps: Downloads required Go modules.</li> <li>install: Installs all development dependencies including linting tools, protobuf tools, build tools, and git hooks.</li> <li>dev: Runs both the <code>dev-dashboard</code> and <code>dev-teranode</code> concurrently.</li> <li>dev-teranode: Executes the Go project.</li> <li>dev-dashboard: Installs and runs the Node.js dashboard project located in <code>./ui/dashboard</code>.</li> <li>build: Builds the Teranode application and dashboard.</li> <li>build-teranode: Builds the main Teranode binary without the dashboard.</li> <li>build-teranode-ci: Builds Teranode with race detection for CI environments.</li> <li>test: Executes Go tests with race detection (excludes <code>test/</code> directory).</li> <li>testall: Runs all tests: <code>test</code>, <code>longtest</code>, and <code>sequentialtest</code>.</li> <li>gen: Generates required Go code from <code>.proto</code> files for various services.</li> <li>clean: Cleans up generated binaries and build artifacts.</li> <li>lint: Executes lint checks on changed files compared to main branch.</li> <li>chain-integrity-test: Runs comprehensive chain integrity test with 3 nodes.</li> </ul>"},{"location":"howto/makefile/#all-commands","title":"All Commands","text":""},{"location":"howto/makefile/#general-configuration","title":"General Configuration","text":"<ul> <li><code>SHELL=/bin/bash</code>: Sets the shell to bash for running commands.</li> <li><code>DEBUG_FLAGS=</code>: Initializes an empty variable for debug flags.</li> <li><code>TXMETA_TAG=</code>: Initializes transaction metadata cache tag.</li> <li><code>SETTINGS_CONTEXT_DEFAULT := docker.ci</code>: Sets default settings context.</li> </ul>"},{"location":"howto/makefile/#setting-debug-flags","title":"Setting Debug Flags","text":"<ul> <li>set_debug_flags: Sets the debug flags (<code>-N -l</code>) if the <code>DEBUG</code> environment variable is set to <code>true</code>.</li> </ul>"},{"location":"howto/makefile/#setting-transaction-metadata-cache-flags","title":"Setting Transaction Metadata Cache Flags","text":"<ul> <li> <p>set_txmetacache_flag: Sets the <code>TXMETA_TAG</code> based on environment variables:</p> <ul> <li><code>TXMETA_SMALL_TAG=true</code>: Uses <code>smalltxmetacache</code></li> <li><code>TXMETA_TEST_TAG=true</code>: Uses <code>testtxmetacache</code></li> <li>Default: Uses <code>largetxmetacache</code></li> </ul> </li> </ul>"},{"location":"howto/makefile/#build-all-components","title":"Build All Components","text":"<ul> <li>all: A composite task that runs <code>deps</code>, <code>install</code>, <code>lint</code>, <code>build</code>, and <code>test</code>.</li> </ul>"},{"location":"howto/makefile/#dependencies","title":"Dependencies","text":"<ul> <li>deps: Downloads necessary Go modules using <code>go mod download</code>.</li> </ul>"},{"location":"howto/makefile/#development","title":"Development","text":"<ul> <li>dev: Runs both <code>dev-dashboard</code> and <code>dev-teranode</code> concurrently.</li> <li>dev-teranode: Executes the main Go project with <code>go run .</code>.</li> <li>dev-dashboard: Installs npm dependencies and runs the Node.js project in development mode at <code>./ui/dashboard</code>.</li> </ul>"},{"location":"howto/makefile/#building","title":"Building","text":"<ul> <li>build: A composite task that runs <code>update_config</code>, <code>build-teranode-with-dashboard</code>, <code>build-teranode-cli</code>, and <code>clean_backup</code>.</li> <li>update_config: Updates the <code>settings_local.conf</code> configuration file based on <code>LOCAL_TEST_START_FROM_STATE</code> parameter.</li> <li>clean_backup: Removes backup configuration files (<code>settings_local.conf.bak</code>).</li> <li>build-teranode-with-dashboard: Builds Teranode with the dashboard UI integrated. Includes debug flags and transaction metadata cache configuration.</li> <li>build-teranode: Builds the main Teranode project with specific build tags (<code>aerospike</code>, transaction metadata cache tag).</li> <li>build-teranode-no-debug: Builds Teranode without debug symbols for production use (stripped binary).</li> <li>build-teranode-ci: Builds Teranode for continuous integration environments with race detection enabled.</li> <li>build-teranode-cli: Builds the Teranode CLI tool at <code>./cmd/teranodecli</code>.</li> <li>build-dashboard: Installs npm dependencies and builds the dashboard UI for production.</li> </ul>"},{"location":"howto/makefile/#building-tools-and-utilities","title":"Building Tools and Utilities","text":"<ul> <li>build-chainintegrity: Builds the chain integrity testing tool at <code>./compose/cmd/chainintegrity/</code>.</li> <li>build-tx-blaster: Builds the transaction blaster performance testing tool at <code>./cmd/txblaster/</code>.</li> <li>build-blockchainstatus: Builds the blockchain status utility at <code>./cmd/blockchainstatus/</code>.</li> </ul>"},{"location":"howto/makefile/#testing","title":"Testing","text":"<ul> <li>test: Runs unit tests with race detection and configurable output format. Excludes tests in the <code>test/</code> directory. Uses <code>testtxmetacache</code> tag and <code>SETTINGS_CONTEXT=test</code>.</li> <li>buildtest: Builds tests without running them for separate execution.</li> <li>sequentialtest: Executes tests in the <code>test/sequentialtest/</code> directory sequentially for more stable results.</li> <li>longtest: Executes long-running tests in the <code>test/longtest/</code> directory with 5-minute timeout.</li> <li>testall: Runs all test suites: <code>test</code>, <code>longtest</code>, and <code>sequentialtest</code>.</li> <li>nightly-tests: Runs comprehensive tests typically scheduled for nightly builds. Builds Docker images and uses CTRF JSON reporter for results.</li> <li>smoketest: Runs smoke tests in the <code>test/e2e/daemon/ready/</code> directory focused on basic functionality.</li> <li>install-tools: Installs testing tools like the CTRF JSON reporter.</li> </ul>"},{"location":"howto/makefile/#chain-integrity-testing","title":"Chain Integrity Testing","text":"<p>The chain integrity test suite validates that multiple Teranode instances maintain consensus and produce identical blockchain states.</p> <ul> <li> <p>chain-integrity-test: Full chain integrity test that:</p> <ul> <li>Builds the chainintegrity binary</li> <li>Cleans up old test data</li> <li>Builds Docker image for teranode</li> <li>Starts 3 teranode instances with block generators</li> <li>Waits for all nodes to reach height 120+ and synchronize</li> <li>Validates chain integrity by comparing node states</li> <li>Checks for error logs during execution</li> <li>Target: 120 blocks, Max wait: 10 minutes (120 attempts \u00d7 5 seconds)</li> </ul> </li> <li> <p>chain-integrity-test-custom: Chain integrity test with customizable parameters:</p> <ul> <li>Usage: <code>make chain-integrity-test-custom REQUIRED_HEIGHT=&lt;height&gt; MAX_ATTEMPTS=&lt;attempts&gt; SLEEP=&lt;seconds&gt;</code></li> <li>Default values: <code>REQUIRED_HEIGHT=120</code>, <code>MAX_ATTEMPTS=120</code>, <code>SLEEP=5</code></li> <li>Allows testing different block heights and wait times</li> </ul> </li> <li> <p>chain-integrity-test-quick: Quick chain integrity test for faster development iterations:</p> <ul> <li>Target: 50 blocks</li> <li>Max wait: 3 minutes (60 attempts \u00d7 3 seconds)</li> <li>Check interval: 3 seconds</li> <li>Use this for rapid testing during development</li> </ul> </li> <li> <p>clean-chain-integrity: Cleans up chain integrity test artifacts:</p> <ul> <li>Removes all chainintegrity log files</li> <li>Removes chainintegrity binary</li> <li>Stops Docker Compose services</li> <li>Use this to reset between test runs</li> </ul> </li> <li> <p>ecr-login: AWS ECR login for pulling required Docker images:</p> <ul> <li>Logs into AWS ECR (eu-north-1 region)</li> <li>Required before building or pulling ECR-hosted images</li> <li>Uses AWS CLI credentials</li> </ul> </li> <li> <p>show-hashes: Displays hash analysis results from chainintegrity test:</p> <ul> <li>Extracts hash information from test output</li> <li>Shows consensus status among nodes</li> <li>Useful for debugging chain integrity issues</li> </ul> </li> </ul>"},{"location":"howto/makefile/#code-generation","title":"Code Generation","text":"<ul> <li> <p>gen: Generates Go code from Protocol Buffers for various services:</p> <ul> <li><code>model/model.proto</code></li> <li><code>errors/error.proto</code></li> <li><code>stores/utxo/status.proto</code></li> <li><code>services/validator/validator_api/validator_api.proto</code></li> <li><code>services/propagation/propagation_api/propagation_api.proto</code></li> <li><code>services/blockassembly/blockassembly_api/blockassembly_api.proto</code></li> <li><code>services/blockvalidation/blockvalidation_api/blockvalidation_api.proto</code></li> <li><code>services/subtreevalidation/subtreevalidation_api/subtreevalidation_api.proto</code></li> <li><code>services/blockchain/blockchain_api/blockchain_api.proto</code></li> <li><code>services/asset/asset_api/asset_api.proto</code></li> <li><code>services/alert/alert_api/alert_api.proto</code></li> <li><code>services/legacy/peer_api/peer_api.proto</code></li> <li><code>services/p2p/p2p_api/p2p_api.proto</code></li> <li><code>util/kafka/kafka_message/kafka_messages.proto</code></li> </ul> </li> </ul>"},{"location":"howto/makefile/#cleanup","title":"Cleanup","text":"<ul> <li> <p>clean_gen: Removes generated Go code (<code>.pb.go</code> files) from:</p> <ul> <li>All service API directories</li> <li>Model and error directories</li> <li>UTXO store directory</li> </ul> </li> <li> <p>clean: Cleans up built artifacts:</p> <ul> <li>Removes <code>.tar.gz</code> archives</li> <li>Removes binary executables (<code>blaster.run</code>, <code>blockchainstatus.run</code>)</li> <li>Removes build directory</li> <li>Removes coverage output files</li> </ul> </li> </ul>"},{"location":"howto/makefile/#linting-and-static-analysis","title":"Linting and Static Analysis","text":"<ul> <li> <p>install-lint: Installs tools for linting and static analysis:</p> <ul> <li><code>golangci-lint</code> (comprehensive Go linter)</li> <li><code>staticcheck</code> (static analysis tool)</li> </ul> </li> <li> <p>lint: Runs linters to check changed files compared to main branch:</p> <ul> <li>Fetches latest <code>origin/main</code></li> <li>Shows new linting errors/warnings introduced by your changes</li> <li>Only checks files modified since branching from main</li> </ul> </li> <li> <p>lint-new: Checks only unstaged/untracked changes:</p> <ul> <li>Useful for quick validation during development</li> <li>Falls back to checking last commit if no uncommitted changes</li> <li>Faster than <code>lint</code> for incremental work</li> </ul> </li> <li> <p>lint-full: Runs linters on the entire codebase:</p> <ul> <li>Shows all lint errors and warnings across all files</li> <li>Use before major releases or when refactoring</li> </ul> </li> <li> <p>lint-full-changed-dirs: Runs linters on changed directories only:</p> <ul> <li>Checks all files in directories containing <code>.go</code> file changes</li> <li>More comprehensive than <code>lint</code> but faster than <code>lint-full</code></li> <li>Shows all errors in modified directories, not just new ones</li> </ul> </li> </ul>"},{"location":"howto/makefile/#installation","title":"Installation","text":"<ul> <li> <p>install: Comprehensive installation command that sets up a complete development environment:</p> <ul> <li>Quality tools: golangci-lint and staticcheck for linting</li> <li>Core dependencies: Protocol Buffers compiler and Go protoc plugins (protobuf, protoc-gen-go, protoc-gen-go-grpc)</li> <li>Build dependencies: Build tools for native code components (libtool, autoconf, automake)</li> <li>Workflow tools: pre-commit hooks for team collaboration</li> </ul> <p>This is the preferred command for setting up a new development environment, as it installs all necessary dependencies.</p> </li> </ul>"},{"location":"howto/makefile/#utilities","title":"Utilities","text":"<ul> <li> <p>reset-data: Resets test data from archive:</p> <ul> <li>Unzips <code>data.zip</code></li> <li>Sets proper permissions on data directory</li> <li>Use when test data becomes corrupted</li> </ul> </li> <li> <p>generate_fsm_diagram: Generates finite state machine diagram:</p> <ul> <li>Runs FSM visualizer tool</li> <li>Outputs diagram to <code>docs/state-machine.diagram.md</code></li> <li>Visualizes blockchain service state transitions</li> </ul> </li> </ul>"},{"location":"howto/makefile/#environment-variables","title":"Environment Variables","text":"<p>The Makefile supports several environment variables to customize builds and tests:</p> <p>Build Configuration:</p> <ul> <li> <p><code>DEBUG</code>: Set to <code>true</code> to enable debug flags (<code>-N -l</code>) for debugging with delve or other debuggers</p> <ul> <li>Example: <code>DEBUG=true make build</code></li> </ul> </li> <li> <p><code>TXMETA_SMALL_TAG</code>: Set to <code>true</code> to use small transaction metadata cache</p> <ul> <li>Example: <code>TXMETA_SMALL_TAG=true make build-teranode</code></li> </ul> </li> <li> <p><code>TXMETA_TEST_TAG</code>: Set to <code>true</code> to use test-sized transaction metadata cache</p> <ul> <li>Example: <code>TXMETA_TEST_TAG=true make build-teranode</code></li> </ul> </li> <li> <p><code>LOCAL_TEST_START_FROM_STATE</code>: Configures the starting state for local tests</p> <ul> <li>Example: <code>make build LOCAL_TEST_START_FROM_STATE=genesis</code></li> </ul> </li> </ul> <p>Version Information (auto-detected from git):</p> <ul> <li><code>GIT_VERSION</code>: Git version string (auto-detected by <code>scripts/determine-git-version.sh</code>)</li> <li><code>GIT_COMMIT</code>: Full git commit hash</li> <li><code>GIT_SHA</code>: Short git SHA</li> <li><code>GIT_TAG</code>: Git tag if on a tagged commit</li> <li><code>GIT_TIMESTAMP</code>: Timestamp of the git commit</li> </ul> <p>Test Configuration:</p> <ul> <li><code>SETTINGS_CONTEXT</code>: Settings context for test execution (default: <code>test</code> for unit tests, <code>docker.ci</code> for integration tests)<ul> <li>Example: <code>SETTINGS_CONTEXT=docker go test ./...</code></li> </ul> </li> </ul> <p>Chain Integrity Test Configuration:</p> <ul> <li><code>REQUIRED_HEIGHT</code>: Target block height for chain integrity tests (default: 120)</li> <li><code>MAX_ATTEMPTS</code>: Maximum polling attempts (default: 120)</li> <li><code>SLEEP</code>: Seconds between polling attempts (default: 5)</li> </ul>"},{"location":"howto/makefile/#usage-examples","title":"Usage Examples","text":"<p>Development workflow:</p> <pre><code># First-time setup\nmake install\n\n# Run in development mode (auto-reload)\nmake dev\n\n# Run only teranode in development\nmake dev-teranode\n</code></pre> <p>Building:</p> <pre><code># Build with dashboard (production)\nmake build\n\n# Build with debug symbols\nDEBUG=true make build\n\n# Build for CI with race detection\nmake build-teranode-ci\n\n# Build with small cache for testing\nTXMETA_SMALL_TAG=true make build-teranode\n</code></pre> <p>Testing:</p> <pre><code># Run all unit tests\nmake test\n\n# Run all test suites (unit + long + sequential)\nmake testall\n\n# Run only smoke tests\nmake smoketest\n\n# Run tests in specific directory\ngo test -v -race -tags \"testtxmetacache\" -count=1 ./services/validator/...\n</code></pre> <p>Chain Integrity Testing:</p> <pre><code># Run full chain integrity test (120 blocks)\nmake chain-integrity-test\n\n# Quick test (50 blocks)\nmake chain-integrity-test-quick\n\n# Custom parameters\nmake chain-integrity-test-custom REQUIRED_HEIGHT=200 MAX_ATTEMPTS=240 SLEEP=3\n\n# View hash analysis results\nmake show-hashes\n\n# Clean up after testing\nmake clean-chain-integrity\n</code></pre> <p>Code generation:</p> <pre><code># Generate Go code from protobuf definitions\nmake gen\n\n# Clean generated code\nmake clean_gen\n</code></pre> <p>Linting:</p> <pre><code># Check only your changes vs main\nmake lint\n\n# Check only uncommitted changes\nmake lint-new\n\n# Check entire codebase\nmake lint-full\n\n# Check changed directories fully\nmake lint-full-changed-dirs\n</code></pre> <p>Utilities:</p> <pre><code># Generate FSM state machine diagram\nmake generate_fsm_diagram\n\n# Login to AWS ECR\nmake ecr-login\n\n# Reset test data\nmake reset-data\n</code></pre>"},{"location":"howto/makefile/#notes","title":"Notes","text":"<ul> <li> <p>PHONY declarations: All targets are declared as <code>.PHONY</code> to indicate that they do not produce or depend on any files. This ensures that the associated command is executed every time it's called, regardless of file timestamps.</p> </li> <li> <p>Go modules: The Makefile uses <code>-mod=readonly</code> for production builds to ensure reproducible builds and prevent accidental dependency changes.</p> </li> <li> <p>Build tags: Several build commands use tags like <code>aerospike</code> and transaction metadata cache tags (<code>smalltxmetacache</code>, <code>testtxmetacache</code>, <code>largetxmetacache</code>) to control conditional compilation.</p> </li> <li> <p>Race detection: The <code>-race</code> flag is only used in CI builds (<code>build-teranode-ci</code>) and test commands to detect race conditions. It's not enabled in standard development builds due to performance overhead.</p> </li> <li> <p>Trimpath: Production builds use <code>--trimpath</code> to remove absolute file paths from compiled binaries for reproducibility and security.</p> </li> <li> <p>ldflags: Build commands inject version information at compile time using <code>-ldflags</code> to set <code>main.commit</code> and <code>main.version</code> variables.</p> </li> <li> <p>Commented-out targets: Several targets are commented out in the Makefile but may be re-enabled as needed:</p> <ul> <li><code>build-propagation-blaster</code>: Propagation performance testing tool</li> <li><code>build-utxostore-blaster</code>: UTXO store performance testing tool</li> <li><code>build-s3-blaster</code>: S3 storage performance testing tool</li> <li><code>build-blockassembly-blaster</code>: Block assembly performance testing tool</li> <li><code>build-aerospiketest</code>: Aerospike database testing utility</li> </ul> </li> <li> <p>Platform differences: The <code>update_config</code> target contains macOS-specific <code>sed</code> commands (BSD sed). Linux users should uncomment the GNU sed variant.</p> </li> <li> <p>Docker Compose: Chain integrity tests use <code>docker-compose-3blasters.yml</code> to orchestrate multiple Teranode instances for consensus testing.</p> </li> <li> <p>Test isolation: Unit tests use <code>SETTINGS_CONTEXT=test</code> to isolate test configuration from development and production settings.</p> </li> <li> <p>Coverage reports: Test commands generate <code>coverage.out</code> files for code coverage analysis.</p> </li> </ul>"},{"location":"howto/runningTests/","title":"\u2714\ufe0f Running Tests","text":"<p>There are 2 commands to run tests:</p> <pre><code>make test  # Executes Go tests excluding the playground and PoC directories.\n</code></pre> <pre><code>make testall  # Executes Go tests excluding the playground and PoC directories.\n</code></pre>"},{"location":"howto/submitting_transactions/","title":"How to Submit Transactions to Teranode","text":"<p>This guide explains how to submit Bitcoin transactions to a Teranode node and covers the different transaction formats supported by the system.</p>"},{"location":"howto/submitting_transactions/#supported-transaction-formats","title":"Supported Transaction Formats","text":"<p>Teranode accepts transactions in two formats:</p>"},{"location":"howto/submitting_transactions/#1-standard-bitcoin-transaction-format","title":"1. Standard Bitcoin Transaction Format","text":"<p>The traditional Bitcoin transaction format used by most Bitcoin implementations:</p> <ul> <li>No special markers or extensions required</li> <li>Compatible with all existing Bitcoin libraries (libbitcoin, bitcoin-js, go-bt, etc.)</li> <li>Smaller transaction size for network transmission (no duplicate previous output data)</li> <li>Universally supported across the Bitcoin ecosystem</li> </ul> <p>When to use: This is the recommended format for most use cases. It's simpler to implement, widely supported, and Teranode handles the rest automatically.</p> <p>Example using go-bt:</p> <pre><code>package main\n\nimport (\n    \"github.com/libsv/go-bt/v2\"\n)\n\nfunc main() {\n    // Create a new transaction\n    tx := bt.NewTx()\n\n    // Add input (standard format - no previous output data)\n    err := tx.From(\n        \"previous_txid_here\",\n        0,  // output index\n        \"locking_script_hex\",\n        1000000, // satoshis\n    )\n    if err != nil {\n        panic(err)\n    }\n\n    // Add output\n    err = tx.PayTo(\"address_or_script\", 900000)\n    if err != nil {\n        panic(err)\n    }\n\n    // Sign the transaction\n    // ... signing logic ...\n\n    // Serialize to standard Bitcoin format\n    txBytes := tx.Bytes()\n\n    // Submit txBytes to Teranode\n}\n</code></pre>"},{"location":"howto/submitting_transactions/#2-extended-format-bip-239","title":"2. Extended Format (BIP-239)","text":"<p>Enhanced transaction format that includes additional metadata in each input:</p> <ul> <li>Includes previous output satoshi values in each input</li> <li>Includes previous output locking scripts in each input</li> <li>Marked with special header (<code>0000000000EF</code> after version number)</li> <li>Slightly faster validation (skips UTXO lookup step)</li> </ul> <p>When to use: Use extended format when:</p> <ul> <li>You need offline transaction validation without UTXO store access</li> <li>You're building high-throughput submission tools where every millisecond counts</li> <li>You're implementing BIP-239-specific features</li> </ul> <p>Example using go-bt (conceptual):</p> <pre><code>package main\n\nimport (\n    \"github.com/libsv/go-bt/v2\"\n)\n\nfunc main() {\n    // Create a new transaction\n    tx := bt.NewTx()\n\n    // Add input with extended format data\n    input := &amp;bt.Input{\n        PreviousTxID:       previousTxIDBytes,\n        PreviousTxOutIndex: 0,\n        UnlockingScript:    unlockingScript,\n        SequenceNumber:     0xFFFFFFFF,\n        // Extended format fields\n        PreviousTxSatoshis: 1000000,\n        PreviousTxScript:   lockingScript,\n    }\n    tx.AddInput(input)\n\n    // Add output\n    tx.AddOutput(&amp;bt.Output{\n        Satoshis:      900000,\n        LockingScript: outputScript,\n    })\n\n    // Serialize to extended format\n    // Note: Actual API may vary by library version\n    txBytes := tx.ExtendedBytes()\n\n    // Submit txBytes to Teranode\n}\n</code></pre>"},{"location":"howto/submitting_transactions/#submission-methods","title":"Submission Methods","text":""},{"location":"howto/submitting_transactions/#http-api","title":"HTTP API","text":"<p>Teranode's Propagation Service provides HTTP endpoints for transaction submission.</p>"},{"location":"howto/submitting_transactions/#single-transaction","title":"Single Transaction","text":"<pre><code>curl -X POST http://localhost:8090/tx \\\n  -H \"Content-Type: application/octet-stream\" \\\n  --data-binary @transaction.bin\n</code></pre> <p>Response:</p> <ul> <li><code>200 OK</code>: Transaction accepted</li> <li><code>400 Bad Request</code>: Malformed transaction</li> <li><code>422 Unprocessable Entity</code>: Transaction validation failed</li> </ul>"},{"location":"howto/submitting_transactions/#multiple-transactions-batch","title":"Multiple Transactions (Batch)","text":"<pre><code>curl -X POST http://localhost:8090/txs \\\n  -H \"Content-Type: application/octet-stream\" \\\n  --data-binary @transactions.bin\n</code></pre> <p>The batch endpoint expects transactions concatenated together. Each transaction result is returned in order.</p> <p>Go Example:</p> <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n\n    \"github.com/libsv/go-bt/v2\"\n)\n\nfunc submitTransaction(tx *bt.Tx, nodeURL string) error {\n    txBytes := tx.Bytes()\n\n    resp, err := http.Post(\n        fmt.Sprintf(\"%s/tx\", nodeURL),\n        \"application/octet-stream\",\n        bytes.NewReader(txBytes),\n    )\n    if err != nil {\n        return fmt.Errorf(\"failed to submit transaction: %w\", err)\n    }\n    defer resp.Body.Close()\n\n    if resp.StatusCode != http.StatusOK {\n        body, _ := io.ReadAll(resp.Body)\n        return fmt.Errorf(\"transaction rejected: %s\", string(body))\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"howto/submitting_transactions/#grpc-api","title":"gRPC API","text":"<p>For higher performance and lower latency, use the gRPC API.</p> <p>Go Example:</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n\n    \"github.com/libsv/go-bt/v2\"\n    \"google.golang.org/grpc\"\n    \"google.golang.org/grpc/credentials/insecure\"\n\n    propagation_api \"github.com/bsv-blockchain/teranode/services/propagation/propagation_api\"\n)\n\nfunc submitTransactionGRPC(tx *bt.Tx, nodeAddr string) error {\n    // Connect to Teranode\n    conn, err := grpc.Dial(nodeAddr, grpc.WithTransportCredentials(insecure.NewCredentials()))\n    if err != nil {\n        return fmt.Errorf(\"failed to connect: %w\", err)\n    }\n    defer conn.Close()\n\n    // Create propagation client\n    client := propagation_api.NewPropagationAPIClient(conn)\n\n    // Submit transaction\n    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n    defer cancel()\n\n    response, err := client.ProcessTransaction(ctx, &amp;propagation_api.ProcessTransactionRequest{\n        Tx: tx.Bytes(),\n    })\n    if err != nil {\n        return fmt.Errorf(\"failed to process transaction: %w\", err)\n    }\n\n    if response.Status != propagation_api.Status_SUCCESS {\n        return fmt.Errorf(\"transaction rejected: %s\", response.RejectReason)\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"howto/submitting_transactions/#format-recommendation","title":"Format Recommendation","text":""},{"location":"howto/submitting_transactions/#for-most-use-cases-use-standard-bitcoin-format","title":"For Most Use Cases: Use Standard Bitcoin Format","text":"<p>Reasons:</p> <ol> <li>Simplicity: Easier to implement with existing Bitcoin libraries</li> <li>Compatibility: Works with all Bitcoin tools and wallets</li> <li>Smaller size: Reduces network bandwidth usage</li> <li>No changes needed: Existing applications work without modification</li> </ol>"},{"location":"howto/submitting_transactions/#performance-comparison","title":"Performance Comparison","text":"<p>In Teranode's architecture, the performance difference between formats is negligible:</p> <ul> <li>Extended format advantage: Saves ~0.1-1ms per transaction (skips UTXO lookup)</li> <li>Teranode's UTXO lookup: Highly optimized with Aerospike/SQL + txmeta cache</li> <li>Typical UTXO lookup time: Sub-millisecond (0.1-0.5ms)</li> </ul> <p>Conclusion: Unless you're processing millions of transactions per second, the format choice won't significantly impact performance.</p>"},{"location":"howto/submitting_transactions/#error-handling","title":"Error Handling","text":""},{"location":"howto/submitting_transactions/#common-errors","title":"Common Errors","text":""},{"location":"howto/submitting_transactions/#1-transaction-with-no-inputs-or-malformed-transaction","title":"1. \"transaction with no inputs\" or \"malformed transaction\"","text":"<p>Cause: Transaction binary data is corrupted or improperly formatted.</p> <p>Solution: Verify serialization logic and ensure transaction is properly constructed.</p>"},{"location":"howto/submitting_transactions/#2-parent-transaction-not-found","title":"2. \"parent transaction not found\"","text":"<p>Cause: The transaction references a parent UTXO that hasn't been validated yet.</p> <p>Solution:</p> <ul> <li>Ensure parent transactions are submitted and validated first</li> <li>For transaction chains (CPFP), submit parent before child</li> <li>Wait a moment and retry</li> </ul>"},{"location":"howto/submitting_transactions/#3-insufficient-fee","title":"3. \"insufficient fee\"","text":"<p>Cause: Transaction fee doesn't meet the node's minimum fee policy.</p> <p>Solution: Increase the transaction fee to meet the node's requirements.</p>"},{"location":"howto/submitting_transactions/#4-double-spend-detected","title":"4. \"double spend detected\"","text":"<p>Cause: One or more inputs have already been spent.</p> <p>Solution: Verify UTXO status before creating transactions. The first transaction to reach the node wins.</p>"},{"location":"howto/submitting_transactions/#error-response-example","title":"Error Response Example","text":"<pre><code>{\n  \"status\": \"REJECTED\",\n  \"reject_reason\": \"transaction validation failed: parent transaction 5f3a... not found\",\n  \"tx_id\": \"abc123...\",\n  \"error_code\": \"MISSING_PARENT\"\n}\n</code></pre>"},{"location":"howto/submitting_transactions/#best-practices","title":"Best Practices","text":""},{"location":"howto/submitting_transactions/#1-always-include-transaction-fees","title":"1. Always Include Transaction Fees","text":"<p>Teranode validates transaction fees. Ensure your transaction includes sufficient fees based on size and complexity.</p> <pre><code>// Calculate fee based on transaction size\ntxSize := len(tx.Bytes())\nfeeRate := 50 // satoshis per byte\nminimumFee := txSize * feeRate\n</code></pre>"},{"location":"howto/submitting_transactions/#2-check-parent-transactions","title":"2. Check Parent Transactions","text":"<p>Before submitting a transaction, verify that all parent transactions (UTXOs being spent) are confirmed or have been submitted.</p>"},{"location":"howto/submitting_transactions/#3-handle-retries-gracefully","title":"3. Handle Retries Gracefully","text":"<p>Network conditions may require transaction resubmission:</p> <pre><code>func submitWithRetry(tx *bt.Tx, nodeURL string, maxRetries int) error {\n    var err error\n    for i := 0; i &lt; maxRetries; i++ {\n        err = submitTransaction(tx, nodeURL)\n        if err == nil {\n            return nil\n        }\n\n        // Check if error is retryable\n        if isRetryableError(err) {\n            time.Sleep(time.Duration(i+1) * time.Second)\n            continue\n        }\n\n        // Non-retryable error, fail immediately\n        return err\n    }\n    return fmt.Errorf(\"max retries exceeded: %w\", err)\n}\n</code></pre>"},{"location":"howto/submitting_transactions/#4-monitor-transaction-status","title":"4. Monitor Transaction Status","text":"<p>After submission, track transaction status using the Asset Server API:</p> <pre><code># Check transaction status\ncurl http://localhost:8090/tx/abc123.../json\n\n# Check UTXO status\ncurl http://localhost:8090/utxo/output_hash/json\n</code></pre>"},{"location":"howto/submitting_transactions/#5-batch-submissions-for-high-volume","title":"5. Batch Submissions for High Volume","text":"<p>For high-throughput applications, use batch submission:</p> <pre><code>func submitBatch(txs []*bt.Tx, nodeURL string) error {\n    var buffer bytes.Buffer\n\n    for _, tx := range txs {\n        buffer.Write(tx.Bytes())\n    }\n\n    resp, err := http.Post(\n        fmt.Sprintf(\"%s/txs\", nodeURL),\n        \"application/octet-stream\",\n        &amp;buffer,\n    )\n    // ... handle response\n}\n</code></pre>"},{"location":"howto/submitting_transactions/#transaction-lifecycle","title":"Transaction Lifecycle","text":"<ol> <li>Submission: Transaction sent to Propagation Service (HTTP/gRPC)</li> <li>Storage: Transaction stored in blob store</li> <li>Format Check: Validator checks if transaction is extended</li> <li>If not extended: Automatically extended in-memory using UTXO store</li> <li>If extended: Proceeds directly to validation</li> <li>Validation: Full consensus and policy validation</li> <li>UTXO Update: UTXOs marked as spent, new outputs created</li> <li>Block Assembly: Valid transactions forwarded to mining</li> <li>Mining: Transaction included in a block</li> <li>Confirmation: Block validated and added to blockchain</li> </ol>"},{"location":"howto/submitting_transactions/#testing","title":"Testing","text":""},{"location":"howto/submitting_transactions/#local-testing-setup","title":"Local Testing Setup","text":"<pre><code># Start Teranode in development mode\nSETTINGS_CONTEXT=dev make dev-teranode\n\n# Submit a test transaction\ncurl -X POST http://localhost:8090/tx \\\n  -H \"Content-Type: application/octet-stream\" \\\n  --data-binary @test_tx.bin\n</code></pre>"},{"location":"howto/submitting_transactions/#integration-testing","title":"Integration Testing","text":"<p>For automated testing with Teranode, refer to the test utilities in the codebase:</p> <ul> <li>Transaction helpers: <code>test/utils/transaction_helper.go</code> provides functions like <code>GenerateNewValidSingleInputTransaction()</code></li> <li>Test environment setup: <code>test/utils/testenv.go</code> contains the <code>TeranodeTestClient</code> structure for integration tests</li> <li>E2E test examples: See <code>test/e2e/</code> directory for complete end-to-end test examples</li> </ul> <p>Example test pattern from the codebase:</p> <pre><code>import (\n    \"github.com/bsv-blockchain/teranode/test/utils\"\n    \"github.com/bsv-blockchain/go-bt/v2\"\n)\n\nfunc TestTransactionSubmission(t *testing.T) {\n    // Setup test client (see test/utils/testenv.go)\n    node := utils.NewTeranodeTestClient()\n\n    // Generate a valid transaction using test helpers\n    tx, err := utils.GenerateNewValidSingleInputTransaction(node)\n    require.NoError(t, err)\n\n    // Submit via distributor client\n    _, err = node.DistributorClient.SendTransaction(context.Background(), tx)\n    require.NoError(t, err)\n}\n</code></pre> <p>For more detailed testing patterns, examine the existing tests in <code>test/e2e/</code>, <code>test/longtest/</code>, and <code>test/sequentialtest/</code> directories.</p>"},{"location":"howto/submitting_transactions/#additional-resources","title":"Additional Resources","text":"<ul> <li>Transaction Data Model: Detailed explanation of both transaction formats</li> <li>Propagation Service: How transactions are received and distributed</li> <li>Validator Service: Transaction validation process including automatic format extension</li> <li>BIP-239 Reference: Extended Transaction Format specification</li> </ul>"},{"location":"howto/submitting_transactions/#summary","title":"Summary","text":"<ul> <li>Teranode accepts both standard and extended transaction formats</li> <li>Standard format is recommended for most use cases</li> <li>Automatic extension happens transparently during validation</li> <li>HTTP and gRPC APIs are available for submission</li> <li>Performance difference between formats is negligible</li> <li>Proper error handling and retries are essential</li> </ul> <p>For questions or issues, refer to the Teranode documentation or contact the development team.</p>"},{"location":"howto/miners/minersHowToAerospikeTuning/","title":"Aerospike - Configuration Considerations","text":"<p>Last Modified - 13-December-2024</p>"},{"location":"howto/miners/minersHowToAerospikeTuning/#general-notes","title":"General notes","text":"<p>Teranode is tested at scaled-out configuration (1m tps throughput) with Aerospike. The Aerospike database is used to store the UTXO set, where indexes are stored in-memory, and data on disk.This document outlines some configuration notes for Aerospike.</p> <p>If using the <code>Docker compose</code> installation path, your compose file will automatically install the Aerospike 7.1 community edition for you.</p> <p>Expiration settings:</p> <ul> <li> <p>For mainnet, it's recommended to enable expiration of spent UTXO records. This will reduce the memory and disk usage. You can control the expiration with the &amp;expiration=${seconds} query parameter of the utxostore setting.</p> </li> <li> <p>For testnet, it's recommended to not use expiration because of an issue with 0-satoshis outputs being spent. This will be fixed in a future release.</p> </li> </ul> <p>The default configuration is set stop writing when 50% of the system memory has been consumed. To future-proof, consider running a dedicated cluster, with more memory and disk space allocated.</p> <ul> <li>See the sample aerospike.conf for an example of configuration of the <code>stop-writes-sys-memory-pct</code> parameter.</li> </ul> <p>By default, the Aerospike data is written to a single mount mounted in the aerospike container. For performance reasons, it is recommended to use at least 4 dedicated disks for the Aerospike data.</p> <ul> <li>See the sample aerospike.conf for an example of configuration of the <code>storage-engine</code> parameter.</li> </ul>"},{"location":"howto/miners/minersHowToAerospikeTuning/#sanity-checking","title":"Sanity Checking","text":"<pre><code>asadm -e \"info\"\nasadm -e \"summary -l\"\n</code></pre> <p>For more information about Aerospike, you can access the Aerospike documentation.</p>"},{"location":"howto/miners/minersHowToCPUMiner/","title":"How to Set Up CPU Mining with Teranode","text":"<p>Last modified: 02-Aug-2025</p>"},{"location":"howto/miners/minersHowToCPUMiner/#index","title":"Index","text":"<ul> <li>Introduction</li> <li>Prerequisites</li> <li>BSV CPU Miner Setup</li> <li>Configuration Parameters</li> <li>Usage Examples</li> <li>Troubleshooting</li> </ul>"},{"location":"howto/miners/minersHowToCPUMiner/#introduction","title":"Introduction","text":"<p>This guide provides instructions for setting up CPU mining with Teranode using the BSV CPU miner. CPU mining is primarily intended for testing purposes and small-scale mining operations.</p> <p>Important Note: CPU mining is not recommended for production mining due to its low hash rate compared to ASIC miners. This setup is ideal for:</p> <ul> <li>Testing Teranode functionality</li> <li>Development environments</li> <li>Educational purposes</li> <li>Small-scale testnet mining</li> </ul>"},{"location":"howto/miners/minersHowToCPUMiner/#prerequisites","title":"Prerequisites","text":"<ul> <li>Running Teranode instance with RPC service enabled</li> <li>Docker installed and configured</li> <li>Access to the Teranode network (Docker network or direct network access)</li> <li>Valid Bitcoin SV address for receiving mining rewards</li> </ul>"},{"location":"howto/miners/minersHowToCPUMiner/#bsv-cpu-miner-setup","title":"BSV CPU Miner Setup","text":"<p>The recommended CPU miner for Teranode is the BSV CPU miner available as a Docker container.</p>"},{"location":"howto/miners/minersHowToCPUMiner/#basic-configuration","title":"Basic Configuration","text":"<pre><code>docker run -it \\\n  --network my-teranode-network \\\n  ghcr.io/bitcoin-sv/cpuminer:latest \\\n  --algo=sha256d --debug --always-gmc --retries=1 --retry-pause=5 \\\n  --url=http://rpc:9292 --userpass=bitcoin:bitcoin \\\n  --coinbase-addr=mgqipciCS56nCYSjB1vTcDGskN82yxfo1G \\\n  --threads=2 --coinbase-sig=\"Teranode us-1\"\n</code></pre>"},{"location":"howto/miners/minersHowToCPUMiner/#configuration-for-external-network","title":"Configuration for External Network","text":"<p>If your Teranode RPC service is not on the same Docker network:</p> <pre><code>docker run -it \\\n  ghcr.io/bitcoin-sv/cpuminer:latest \\\n  --algo=sha256d --debug --always-gmc --retries=1 --retry-pause=5 \\\n  --url=http://YOUR_TERANODE_IP:9292 --userpass=bitcoin:bitcoin \\\n  --coinbase-addr=YOUR_BSV_ADDRESS \\\n  --threads=4 --coinbase-sig=\"Your Mining Pool\"\n</code></pre>"},{"location":"howto/miners/minersHowToCPUMiner/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"howto/miners/minersHowToCPUMiner/#required-parameters","title":"Required Parameters","text":"Parameter Short Form Description Example <code>--algo</code> <code>-a</code> Mining algorithm (always sha256d for BSV) <code>--algo=sha256d</code> <code>--url</code> <code>-o</code> RPC server URL <code>--url=http://rpc:9292</code> <code>--userpass</code> <code>-O</code> RPC credentials (username:password) <code>--userpass=bitcoin:bitcoin</code> <code>--coinbase-addr</code> Address to receive mining rewards <code>--coinbase-addr=1ABC...</code>"},{"location":"howto/miners/minersHowToCPUMiner/#optional-parameters","title":"Optional Parameters","text":"Parameter Short Form Description Default Example <code>--threads</code> <code>-t</code> Number of mining threads CPU cores <code>--threads=4</code> <code>--coinbase-sig</code> Custom signature for mined blocks Empty <code>--coinbase-sig=\"My Pool\"</code> <code>--retries</code> <code>-r</code> Number of retries for failed requests 3 <code>--retries=10</code> <code>--debug</code> <code>-D</code> Enable debug output false <code>--debug</code> <code>--always-gmc</code> Always use getminingcandidate RPC false <code>--always-gmc</code>"},{"location":"howto/miners/minersHowToCPUMiner/#advanced-parameters","title":"Advanced Parameters","text":"Parameter Description Example <code>--scantime</code> Time to spend on each work unit (seconds) <code>--scantime=30</code> <code>--timeout</code> Timeout for RPC requests (seconds) <code>--timeout=60</code> <code>--retry-pause</code> Pause between retries (seconds) <code>--retry-pause=5</code>"},{"location":"howto/miners/minersHowToCPUMiner/#usage-examples","title":"Usage Examples","text":""},{"location":"howto/miners/minersHowToCPUMiner/#testnet-mining","title":"Testnet Mining","text":"<pre><code>docker run -it \\\n  --network teranode-testnet \\\n  ghcr.io/bitcoin-sv/cpuminer:latest \\\n  --algo=sha256d --always-gmc --retries=1 --retry-pause=5 \\\n  --url=http://rpc:9292 \\\n  --userpass=bitcoin:bitcoin \\\n  --coinbase-addr=mgqipciCS56nCYSjB1vTcDGskN82yxfo1G \\\n  --threads=2 \\\n  --coinbase-sig=\"Testnet Miner\" \\\n  --debug\n</code></pre>"},{"location":"howto/miners/minersHowToCPUMiner/#high-performance-configuration","title":"High-Performance Configuration","text":"<p>For systems with many CPU cores:</p> <pre><code>docker run -it \\\n  --network my-teranode-network \\\n  ghcr.io/bitcoin-sv/cpuminer:latest \\\n  --algo=sha256d --always-gmc --retries=1 --retry-pause=5 \\\n  --url=http://rpc:9292 \\\n  --userpass=bitcoin:bitcoin \\\n  --coinbase-addr=YOUR_BSV_ADDRESS \\\n  --threads=16 \\\n  --scantime=60 \\\n  --coinbase-sig=\"High Performance Miner\"\n</code></pre>"},{"location":"howto/miners/minersHowToCPUMiner/#quiet-mode-no-debug-output","title":"Quiet Mode (No Debug Output)","text":"<pre><code>docker run -it \\\n  --network my-teranode-network \\\n  ghcr.io/bitcoin-sv/cpuminer:latest \\\n  --algo=sha256d --always-gmc --retries=1 --retry-pause=5 \\\n  --url=http://rpc:9292 \\\n  --userpass=bitcoin:bitcoin \\\n  --coinbase-addr=YOUR_BSV_ADDRESS \\\n  --threads=4 \\\n  --coinbase-sig=\"Production Miner\"\n</code></pre>"},{"location":"howto/miners/minersHowToCPUMiner/#troubleshooting","title":"Troubleshooting","text":""},{"location":"howto/miners/minersHowToCPUMiner/#common-issues","title":"Common Issues","text":"<p>Issue: Connection refused errors</p> <pre><code>[ERROR] HTTP request failed: Connection refused\n</code></pre> <p>Solution:</p> <ul> <li>Verify Teranode RPC service is running: <code>docker ps | grep rpc</code></li> <li>Check network connectivity between miner and RPC service</li> <li>Ensure correct URL format: <code>http://hostname:port</code></li> </ul> <p>Issue: Authentication failures</p> <pre><code>[ERROR] JSON-RPC call failed: Authentication failed\n</code></pre> <p>Solution:</p> <ul> <li>Verify RPC credentials in Teranode configuration</li> <li>Default credentials are <code>bitcoin:bitcoin</code></li> <li>Check userpass format: <code>username:password</code></li> </ul> <p>Issue: Invalid address errors</p> <pre><code>[ERROR] Invalid coinbase address\n</code></pre> <p>Solution:</p> <ul> <li>Use a valid BSV address for your network (mainnet/testnet)</li> <li>Testnet addresses start with 'm' or 'n'</li> <li>Mainnet addresses start with '1' or '3'</li> </ul> <p>Issue: Low hash rate or no shares</p> <pre><code>[INFO] No shares submitted\n</code></pre> <p>Solution:</p> <ul> <li>Increase thread count if system can handle it</li> <li>Verify Teranode is fully synchronized</li> <li>Check network difficulty - CPU mining may take time to find shares</li> </ul>"},{"location":"howto/miners/minersHowToCPUMiner/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Thread Count: Start with CPU core count, adjust based on system performance</li> <li>Scan Time: Increase for better efficiency, decrease for faster response to new work</li> <li>System Resources: Ensure adequate cooling and power for sustained mining</li> </ol>"},{"location":"howto/miners/minersHowToCPUMiner/#monitoring","title":"Monitoring","text":"<p>Monitor your mining progress:</p> <pre><code># View miner logs\ndocker logs &lt;container_id&gt;\n\n# Check Teranode RPC status\ncurl --user bitcoin:bitcoin \\\n  --data-binary '{\"jsonrpc\":\"1.0\",\"id\":\"test\",\"method\":\"getmininginfo\",\"params\":[]}' \\\n  -H 'content-type: text/plain;' \\\n  http://localhost:9292/\n</code></pre>"},{"location":"howto/miners/minersHowToCPUMiner/#getting-mining-statistics","title":"Getting Mining Statistics","text":"<p>Check current mining information:</p> <pre><code># Get network hash rate and difficulty\ncurl --user bitcoin:bitcoin \\\n  --data-binary '{\"jsonrpc\":\"1.0\",\"id\":\"test\",\"method\":\"getnetworkhashps\",\"params\":[]}' \\\n  -H 'content-type: text/plain;' \\\n  http://localhost:9292/\n\n# Check latest blocks\ncurl --user bitcoin:bitcoin \\\n  --data-binary '{\"jsonrpc\":\"1.0\",\"id\":\"test\",\"method\":\"getbestblockhash\",\"params\":[]}' \\\n  -H 'content-type: text/plain;' \\\n  http://localhost:9292/\n</code></pre>"},{"location":"howto/miners/minersHowToCPUMiner/#security-considerations","title":"Security Considerations","text":"<ol> <li>RPC Access: Ensure RPC service is not exposed to untrusted networks</li> <li>Credentials: Use strong RPC credentials in production environments</li> <li>Network Isolation: Run miner in isolated network environment when possible</li> <li>Address Security: Keep private keys for mining addresses secure</li> </ol>"},{"location":"howto/miners/minersHowToCPUMiner/#additional-resources","title":"Additional Resources","text":"<ul> <li>Interacting with RPC Service</li> <li>Teranode CLI Guide</li> <li>Docker Installation Guide</li> <li>BSV CPU Miner GitHub Repository</li> </ul> <p>Note: CPU mining is primarily for testing and development. For production mining operations, consider using ASIC miners or other specialized mining hardware.</p>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/","title":"How to Interact with the Asset Server","text":"<p>Last Modified: 28-May-2025</p> <p>There are 2 primary ways to interact with the node, using the RPC Server, and using the Asset Server. This document will focus on the Asset Server. The Asset Server provides an HTTP API for interacting with the node. Below is a list of implemented endpoints with their parameters and return values.</p>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#teranode-asset-server-http-api","title":"Teranode Asset Server HTTP API","text":"<p>The Teranode Asset Server provides the following HTTP endpoints organized by category. Unless otherwise specified, all endpoints are GET requests.</p> <p>Base URL: <code>/api/v1</code> (configurable)</p> <p>Port: <code>8090</code> (configurable)</p>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#response-formats","title":"Response Formats","text":"<p>Many endpoints support multiple response formats, indicated by the URL path or an optional format parameter:</p> <ul> <li>Default/Binary: Returns raw binary data (Content-Type: <code>application/octet-stream</code>)</li> <li>Hex: Returns hexadecimal string (Content-Type: <code>text/plain</code>)</li> <li>JSON: Returns structured JSON (Content-Type: <code>application/json</code>)</li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#health-and-status-endpoints","title":"Health and Status Endpoints","text":"<ul> <li> <p>GET <code>/alive</code></p> <ul> <li>Description: Returns the service status and uptime</li> <li>Parameters: None</li> <li>Returns: Text message with uptime information</li> <li>Status Code: 200 on success</li> </ul> </li> <li> <p>GET <code>/health</code></p> <ul> <li>Description: Performs a health check on the service and its dependencies</li> <li>Parameters: None</li> <li>Returns: Status information of service and dependencies</li> <li>Status Code: 200 on success, 503 on failure</li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#transaction-endpoints","title":"Transaction Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/tx/:hash</code></p> <ul> <li>Description: Retrieves a transaction in binary stream format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Transaction hash (hex string)</li> </ul> </li> <li> <p>Returns: Transaction data in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/tx/:hash/hex</code></p> <ul> <li>Description: Retrieves a transaction in hexadecimal format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Transaction hash (hex string)</li> </ul> </li> <li> <p>Returns: Transaction data as hex string</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/tx/:hash/json</code></p> <ul> <li>Description: Retrieves a transaction in JSON format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Transaction hash (hex string)</li> </ul> </li> <li> <p>Returns: Transaction data in structured JSON format</p> </li> </ul> </li> <li> <p>POST <code>/api/v1/txs</code></p> <ul> <li>Description: Batch retrieves multiple transactions</li> <li> <p>Request Body:</p> <p>Concatenated series of 32-byte transaction hashes without separators - Format: <code>[32-byte hash][32-byte hash][32-byte hash]...</code></p> </li> <li> <p>Returns: Concatenated transactions in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/txmeta/:hash/json</code></p> <ul> <li>Description: Retrieves transaction metadata</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Transaction hash (hex string)</li> </ul> </li> <li> <p>Returns: Transaction metadata in JSON format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/txmeta_raw/:hash</code></p> <ul> <li>Description: Retrieves raw transaction metadata</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Transaction hash (hex string)</li> </ul> </li> <li> <p>Returns: Raw transaction metadata (binary)</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/txmeta_raw/:hash/hex</code></p> <ul> <li>Description: Retrieves raw transaction metadata in hex format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Transaction hash (hex string)</li> </ul> </li> <li> <p>Returns: Raw transaction metadata as hex string</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/txmeta_raw/:hash/json</code></p> <ul> <li>Description: Retrieves raw transaction metadata in JSON format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Transaction hash (hex string)</li> </ul> </li> <li> <p>Returns: Raw transaction metadata in JSON format</p> </li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#block-endpoints","title":"Block Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/block/:hash</code></p> <ul> <li>Description: Retrieves a block by hash in binary format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: Block data in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/block/:hash/hex</code></p> <ul> <li>Description: Retrieves a block by hash in hexadecimal format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: Block data as hex string</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/block/:hash/json</code></p> <ul> <li>Description: Retrieves a block by hash in JSON format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: Block data in structured JSON format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/block/:hash/forks</code></p> <ul> <li>Description: Retrieves fork information for a block</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: JSON object with fork data</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/block/:hash/subtrees/json</code></p> <ul> <li>Description: Retrieves subtree information for a specific block</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: JSON object with block subtree information including subtree hashes and metadata</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/blocks</code></p> <ul> <li>Description: Retrieves a paginated list of blocks</li> <li> <p>Parameters:</p> <ul> <li><code>offset</code> (optional): Number of blocks to skip from the tip (default: 0)</li> <li><code>limit</code> (optional): Maximum number of blocks to return (default: 20, max: 100)</li> <li><code>includeOrphans</code> (optional): Whether to include orphaned blocks (default: false)</li> </ul> </li> <li> <p>Returns: JSON array of block data</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/blocks/:hash</code></p> <ul> <li>Description: Retrieves multiple blocks starting with the specified hash</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Starting block hash (hex string)</li> <li><code>n</code> (optional): Number of blocks to retrieve</li> </ul> </li> <li> <p>Returns: Block data in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/blocks/:hash/hex</code></p> <ul> <li>Description: Same as above but in hexadecimal format</li> </ul> </li> <li> <p>GET <code>/api/v1/blocks/:hash/json</code></p> <ul> <li>Description: Same as above but in JSON format</li> </ul> </li> <li> <p>GET <code>/api/v1/lastblocks</code></p> <ul> <li>Description: Retrieves the most recent blocks in the blockchain</li> <li> <p>Parameters:</p> <ul> <li><code>n</code> (optional): Number of blocks to retrieve (default: 10)</li> <li><code>fromHeight</code> (optional): Starting block height for retrieval (default: 0)</li> <li><code>includeOrphans</code> (optional): Whether to include orphaned blocks (default: false)</li> </ul> </li> <li> <p>Response Format:</p> <p>JSON array of recent block information including:</p> <ul> <li><code>hash</code>: Block hash</li> <li><code>height</code>: Block height</li> <li><code>time</code>: Block timestamp</li> <li><code>txCount</code>: Number of transactions in the block</li> <li><code>size</code>: Block size in bytes</li> <li><code>orphan</code>: Boolean indicating if the block is an orphan</li> </ul> </li> <li> <p>Returns: JSON array of recent block information</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/blockstats</code></p> <ul> <li>Description: Retrieves statistical information about the blockchain</li> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: JSON object with block statistics</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/blockgraphdata/:period</code></p> <ul> <li>Description: Retrieves time-series data for graphing purposes</li> <li> <p>Parameters:</p> <ul> <li><code>period</code>: Period in milliseconds for data aggregation</li> </ul> </li> <li> <p>Returns: JSON object with time-series block data</p> </li> </ul> </li> <li> <p>GET <code>/rest/block/:hash.bin</code></p> <ul> <li>Description: Legacy endpoint for retrieving a block in binary format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: Block data in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/block_legacy/:hash</code></p> <ul> <li>Description: Alternative legacy endpoint for retrieving a block in binary format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: Block data in binary format</p> </li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#block-header-endpoints","title":"Block Header Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/header/:hash</code></p> <ul> <li>Description: Retrieves a block header by hash in binary format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: Block header in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/header/:hash/hex</code></p> <ul> <li>Description: Retrieves a block header by hash in hexadecimal format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: Block header as hex string</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/header/:hash/json</code></p> <ul> <li>Description: Retrieves a block header by hash in JSON format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: Block header in structured JSON format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/headers/:hash</code></p> <ul> <li>Description: Retrieves multiple headers starting from a hash</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Starting block hash (hex string)</li> <li><code>n</code> (optional): Number of headers to retrieve</li> </ul> </li> <li> <p>Returns: Headers in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/headers/:hash/hex</code></p> <ul> <li>Description: Same as above but in hexadecimal format</li> </ul> </li> <li> <p>GET <code>/api/v1/headers/:hash/json</code></p> <ul> <li>Description: Same as above but in JSON format</li> </ul> </li> <li> <p>GET <code>/api/v1/headers_to_common_ancestor/:hash</code></p> <ul> <li>Description: Retrieves headers from specified hash to common ancestor</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Target block hash (hex string)</li> <li><code>locator</code> (required): Comma-separated list of block hashes for locator</li> </ul> </li> <li> <p>Returns: Headers in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/headers_to_common_ancestor/:hash/hex</code></p> <ul> <li>Description: Same as above but in hexadecimal format</li> </ul> </li> <li> <p>GET <code>/api/v1/headers_to_common_ancestor/:hash/json</code></p> <ul> <li>Description: Same as above but in JSON format</li> </ul> </li> <li> <p>GET <code>/api/v1/headers_from_common_ancestor/:hash</code></p> <ul> <li>Description: Retrieves headers from common ancestor to specified hash</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Target block hash (hex string)</li> <li><code>locator</code> (required): Comma-separated list of block hashes for locator</li> </ul> </li> <li> <p>Returns: Headers in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/headers_from_common_ancestor/:hash/hex</code></p> <ul> <li>Description: Same as above but in hexadecimal format</li> </ul> </li> <li> <p>GET <code>/api/v1/headers_from_common_ancestor/:hash/json</code></p> <ul> <li>Description: Same as above but in JSON format</li> </ul> </li> <li> <p>GET <code>/api/v1/block_locator</code></p> <ul> <li>Description: Retrieves block locator information for blockchain synchronization</li> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: JSON object with block locator data</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/bestblockheader</code></p> <ul> <li>Description: Retrieves the current best block header</li> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: Best block header in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/bestblockheader/hex</code></p> <ul> <li>Description: Same as above but in hexadecimal format</li> </ul> </li> <li> <p>GET <code>/api/v1/bestblockheader/json</code></p> <ul> <li>Description: Same as above but in JSON format</li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#utxo-endpoints","title":"UTXO Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/utxo/:hash</code></p> <ul> <li>Description: Retrieves UTXO information in binary format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: UTXO transaction hash (hex string)</li> <li><code>vout</code> (required): Output index (query parameter)</li> </ul> </li> <li> <p>Returns: UTXO data in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/utxo/:hash/hex</code></p> <ul> <li>Description: Retrieves UTXO information in hexadecimal format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: UTXO transaction hash (hex string)</li> <li><code>vout</code> (required): Output index (query parameter)</li> </ul> </li> <li> <p>Returns: UTXO data as hex string</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/utxo/:hash/json</code></p> <ul> <li>Description: Retrieves UTXO information in JSON format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: UTXO transaction hash (hex string)</li> <li><code>vout</code> (required): Output index (query parameter)</li> </ul> </li> <li> <p>Returns: UTXO data in structured JSON format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/utxos/:hash/json</code></p> <ul> <li>Description: Retrieves all UTXOs for a given transaction</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Transaction hash (hex string)</li> </ul> </li> <li> <p>Returns: Array of UTXO data in JSON format</p> </li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#subtree-endpoints","title":"Subtree Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/subtree/:hash</code></p> <ul> <li>Description: Retrieves a subtree in binary format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Subtree hash (hex string)</li> </ul> </li> <li> <p>Returns: Subtree data in binary format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/subtree/:hash/hex</code></p> <ul> <li>Description: Retrieves a subtree in hexadecimal format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Subtree hash (hex string)</li> </ul> </li> <li> <p>Returns: Subtree data as hex string</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/subtree/:hash/json</code></p> <ul> <li>Description: Retrieves a subtree in JSON format</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Subtree hash (hex string)</li> </ul> </li> <li> <p>Returns: Subtree data in structured JSON format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/subtree_data/:hash</code></p> <ul> <li>Description: Retrieves subtree metadata and information</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Subtree hash (hex string)</li> </ul> </li> <li> <p>Returns: Subtree metadata in JSON format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/subtree/:hash/txs/json</code></p> <ul> <li>Description: Retrieves transactions within a subtree</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Subtree hash (hex string)</li> </ul> </li> <li> <p>Returns: Array of transaction data in JSON format</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/block/:hash/subtrees/json</code></p> <ul> <li>Description: Retrieves all subtrees for a block</li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: Block hash (hex string)</li> </ul> </li> <li> <p>Returns: Array of subtree data in JSON format</p> </li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#search-endpoints","title":"Search Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/search</code></p> <ul> <li>Description: Searches for blockchain entities by hash or height</li> <li> <p>Parameters:</p> <ul> <li><code>query</code> (required): Search query (hash or block height)</li> </ul> </li> <li> <p>Returns: JSON object with search results and entity type</p> </li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#block-management-endpoints","title":"Block Management Endpoints","text":"<ul> <li> <p>POST <code>/api/v1/block/invalidate</code></p> <ul> <li>Description: Marks a block as invalid, forcing a chain reorganization</li> <li> <p>Parameters:</p> <ul> <li>Request body: JSON object with block hash information</li> </ul> <pre><code>{\n  \"hash\": \"000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f\"\n}\n</code></pre> </li> <li> <p>Returns: JSON object with status of the invalidation operation</p> </li> </ul> </li> <li> <p>POST <code>/api/v1/block/revalidate</code></p> <ul> <li>Description: Reconsiders a previously invalidated block</li> <li> <p>Parameters:</p> <ul> <li>Request body: JSON object with block hash information</li> </ul> <pre><code>{\n  \"hash\": \"000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f\"\n}\n</code></pre> </li> <li> <p>Returns: JSON object with status of the revalidation operation</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/blocks/invalid</code></p> <ul> <li>Description: Retrieves a list of currently invalidated blocks</li> <li> <p>Parameters:</p> <ul> <li><code>limit</code> (optional): Maximum number of blocks to retrieve</li> </ul> </li> <li> <p>Returns: Array of invalid block information</p> </li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#finite-state-machine-fsm-endpoints","title":"Finite State Machine (FSM) Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/fsm/state</code></p> <ul> <li>Description: Returns current blockchain FSM state</li> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: JSON object with current state information including:</p> <ul> <li><code>state</code>: Current state name</li> <li><code>metadata</code>: Additional state information</li> <li><code>allowedTransitions</code>: Events that can be triggered from this state</li> </ul> </li> <li> <p>Example response:</p> <pre><code>{\n  \"state\": \"Running\",\n  \"metadata\": {\n    \"syncedHeight\": 700001,\n    \"bestHeight\": 700001,\n    \"isSynchronized\": true\n  },\n  \"allowedTransitions\": [\"stop\", \"pause\"]\n}\n</code></pre> </li> </ul> </li> <li> <p>POST <code>/api/v1/fsm/state</code></p> <ul> <li>Description: Sends an event to the blockchain FSM to trigger a state transition</li> <li> <p>Parameters:</p> <p>JSON object with event details</p> <ul> <li><code>event</code> (string, required): The event name to trigger</li> <li><code>data</code> (object, optional): Additional data for the event</li> </ul> </li> <li> <p>Example request:</p> <pre><code>{\n  \"event\": \"pause\",\n  \"data\": {\n    \"reason\": \"maintenance\"\n  }\n}\n</code></pre> </li> <li> <p>Returns: JSON object with updated state information and transition result</p> </li> </ul> </li> <li> <p>GET <code>/api/v1/fsm/events</code></p> <ul> <li>Description: Lists all possible FSM events</li> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: JSON array of available events with descriptions</p> </li> <li> <p>Example response:</p> <pre><code>[\n  {\n    \"name\": \"start\",\n    \"description\": \"Start the blockchain service\"\n  },\n  {\n    \"name\": \"stop\",\n    \"description\": \"Stop the blockchain service\"\n  },\n  {\n    \"name\": \"pause\",\n    \"description\": \"Temporarily pause operations\"\n  }\n]\n</code></pre> </li> </ul> </li> <li> <p>GET <code>/api/v1/fsm/states</code></p> <ul> <li>Description: Lists all possible FSM states</li> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: JSON array of available states with descriptions</p> </li> <li> <p>Example response:</p> <pre><code>[\n  {\n    \"name\": \"Idle\",\n    \"description\": \"Service is idle and not processing blocks\"\n  },\n  {\n    \"name\": \"Running\",\n    \"description\": \"Service is active and processing blocks\"\n  },\n  {\n    \"name\": \"Paused\",\n    \"description\": \"Service is temporarily paused\"\n  }\n]\n</code></pre> </li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#error-handling","title":"Error Handling","text":"<p>All endpoints return appropriate HTTP status codes to indicate success or failure:</p> <ul> <li>200 OK: Request successful</li> <li>400 Bad Request: Invalid input parameters</li> <li>404 Not Found: Resource not found</li> <li>500 Internal Server Error: Server-side error</li> </ul> <p>Error responses include a JSON object with an error message:</p> <pre><code>{\n  \"error\": \"Error message description\"\n}\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#examples","title":"Examples","text":""},{"location":"howto/miners/minersHowToInteractWithAssetServer/#retrieving-a-transaction","title":"Retrieving a Transaction","text":"<pre><code>GET /api/v1/tx/abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789/json\n</code></pre> <p>Response:</p> <pre><code>{\n  \"txid\": \"abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789\",\n  \"version\": 1,\n  \"locktime\": 0,\n  \"vin\": [...],\n  \"vout\": [...]\n}\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#batch-retrieving-transactions","title":"Batch Retrieving Transactions","text":"<p>Request:</p> <pre><code>POST /api/v1/txs\nContent-Type: application/octet-stream\n\n&lt;binary data: concatenated 32-byte transaction hashes&gt;\n</code></pre> <p>Response: Binary data containing the concatenated transactions</p>"},{"location":"howto/miners/minersHowToInteractWithAssetServer/#retrieving-block-information","title":"Retrieving Block Information","text":"<pre><code>GET /api/v1/block/000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f/json\n</code></pre> <p>Response:</p> <pre><code>{\n  \"hash\": \"000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f\",\n  \"confirmations\": 123456,\n  \"size\": 285,\n  \"height\": 0,\n  \"version\": 1,\n  \"merkleroot\": \"4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b\",\n  \"tx\": [...],\n  \"time\": 1231006505,\n  \"nonce\": 2083236893,\n  \"bits\": \"1d00ffff\",\n  \"difficulty\": 1.0\n}\n</code></pre> <p>For more information on the Asset Server, see the Asset Server Reference.</p>"},{"location":"howto/miners/minersHowToInteractWithFSM/","title":"How to Manage Teranode States Using RPC","text":"<p>This guide explains how to change and monitor Teranode's state using gRPC commands. Note that Teranode instances start in IDLE state and require manual state transitions.</p>"},{"location":"howto/miners/minersHowToInteractWithFSM/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to a running Teranode instance</li> <li> <p>One of the following:</p> <ul> <li><code>grpcurl</code> installed on your system (requires network access to the RPC Server on port 18087)</li> <li>Access to the <code>teranode-cli</code> (recommended, requires direct access to RPC container)</li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToInteractWithFSM/#methods","title":"Methods","text":"<p>You can manage Teranode states using either <code>teranode-cli</code> (recommended) or <code>grpcurl</code> directly. The exact commands will depend on your deployment environment (Docker Compose or Kubernetes).</p>"},{"location":"howto/miners/minersHowToInteractWithFSM/#using-teranode-cli-recommended","title":"Using teranode-cli (Recommended)","text":""},{"location":"howto/miners/minersHowToInteractWithFSM/#docker-compose-environment","title":"Docker Compose Environment","text":""},{"location":"howto/miners/minersHowToInteractWithFSM/#1-check-current-state","title":"1. Check Current State","text":"<pre><code>docker exec -it blockchain teranode-cli getfsmstate\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithFSM/#2-set-new-state","title":"2. Set New State","text":"<pre><code>docker exec -it blockchain teranode-cli setfsmstate --fsmstate RUNNING\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithFSM/#kubernetes-environment","title":"Kubernetes Environment","text":""},{"location":"howto/miners/minersHowToInteractWithFSM/#1-check-current-state_1","title":"1. Check Current State","text":"<p>Access any Teranode pod and use teranode-cli directly:</p> <pre><code># Get the name of a pod (blockchain or asset are good options)\nkubectl get pods -n teranode-operator -l app=blockchain\n\n# Access the pod and run the command\nkubectl exec -it &lt;pod-name&gt; -n teranode-operator -- teranode-cli getfsmstate\n\n# Alternative one-liner\nkubectl exec -it $(kubectl get pods -n teranode-operator -l app=blockchain -o jsonpath='{.items[0].metadata.name}') -n teranode-operator -- teranode-cli getfsmstate\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithFSM/#2-set-new-state_1","title":"2. Set New State","text":"<pre><code># Change state to RUNNING\nkubectl exec -it $(kubectl get pods -n teranode-operator -l app=blockchain -o jsonpath='{.items[0].metadata.name}') -n teranode-operator -- teranode-cli setfsmstate --fsmstate RUNNING\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithFSM/#valid-fsm-states","title":"Valid FSM States","text":"<p>The following states are valid for all environments:</p> <ul> <li>IDLE</li> <li>RUNNING</li> <li>LEGACYSYNCING</li> <li>CATCHINGBLOCKS</li> </ul>"},{"location":"howto/miners/minersHowToInteractWithFSM/#using-grpcurl","title":"Using grpcurl","text":""},{"location":"howto/miners/minersHowToInteractWithFSM/#docker-compose-environment_1","title":"Docker Compose Environment","text":""},{"location":"howto/miners/minersHowToInteractWithFSM/#1-check-current-state_2","title":"1. Check Current State","text":"<p>To check the current state of Teranode in a Docker Compose environment:</p> <pre><code>grpcurl -plaintext rpcserver:18087 blockchain_api.BlockchainAPI.GetFSMCurrentState\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithFSM/#kubernetes-environment_1","title":"Kubernetes Environment","text":""},{"location":"howto/miners/minersHowToInteractWithFSM/#1-check-current-state_3","title":"1. Check Current State","text":"<p>For Kubernetes, you need to forward the RPC server port or access it through a service:</p> <pre><code># Port forward the RPC service\nkubectl port-forward service/rpcserver -n teranode-operator 18087:18087\n\n# In a new terminal\ngrpcurl -plaintext localhost:18087 blockchain_api.BlockchainAPI.GetFSMCurrentState\n</code></pre> <p>Expected output: <pre><code>{\n\"state\": \"Idle\"\n}\n</code></pre></p>"},{"location":"howto/miners/minersHowToInteractWithFSM/#2-send-state-transition-events-docker-compose","title":"2. Send State Transition Events (Docker Compose)","text":"<p>To start Teranode's normal operations in Docker Compose:</p> <pre><code>grpcurl -plaintext rpcserver:18087 blockchain_api.BlockchainAPI.Run\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithFSM/#2-send-state-transition-events-kubernetes","title":"2. Send State Transition Events (Kubernetes)","text":"<p>In a Kubernetes environment with port forwarding active:</p> <pre><code>grpcurl -plaintext localhost:18087 blockchain_api.BlockchainAPI.Run\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithFSM/#available-events","title":"Available Events","text":"<p>The following events are available for both environments:</p> <ul> <li><code>Run</code> - Transitions to RUNNING state</li> <li><code>LegacySync</code> - Transitions to LEGACYSYNCING state</li> <li><code>CatchUpBlocks</code> - Transitions to CATCHINGBLOCKS state</li> <li><code>Idle</code> - Transitions to IDLE state</li> </ul>"},{"location":"howto/miners/minersHowToInteractWithFSM/#3-wait-for-state-change","title":"3. Wait for State Change","text":"<p>To wait for a specific state transition:</p> <pre><code>grpcurl -plaintext -d '{\"state\":\"Running\"}' rpcserver:18087 blockchain_api.BlockchainAPI.WaitForFSMtoTransitionToGivenState\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithFSM/#validation","title":"Validation","text":"<p>After each state change, verify the new state:</p> <ol> <li>Use the \"get current state\" command (see instructions above for <code>grpcurl</code> or <code>teranode-cli</code>)</li> <li>Check the logs for transition messages</li> <li>Verify that expected services are running/stopped according to the state</li> </ol>"},{"location":"howto/miners/minersHowToInteractWithFSM/#further-reading","title":"Further Reading","text":"<ul> <li>How To Interact With the RPC Server</li> <li>State Management Documentation</li> </ul>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/","title":"How to Interact with the RPC Server","text":"<p>Last Modified: 28-May-2025</p> <p>There are 2 primary ways to interact with the node, using the RPC Server, and using the Asset Server. This document will focus on the RPC Server. The RPC server provides a JSON-RPC interface for interacting with the node. Below is a list of implemented RPC methods with their parameters and return values.</p>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#teranode-rpc-http-api","title":"Teranode RPC HTTP API","text":"<p>The Teranode RPC server provides a JSON-RPC interface for interacting with the node. Below is a list of implemented RPC methods:</p>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#block-related-methods","title":"Block-related Methods","text":"<ol> <li><code>getbestblockhash</code>: Returns the hash of the best (tip) block</li> <li><code>getblock</code>: Retrieves a block by its hash</li> <li><code>getblockbyheight</code>: Retrieves a block by its height</li> <li><code>getblockhash</code>: Returns the hash of a block at specified height</li> <li><code>getblockheader</code>: Returns information about a block's header</li> <li><code>getblockchaininfo</code>: Returns information about the blockchain</li> <li><code>invalidateblock</code>: Marks a block as invalid</li> <li><code>reconsiderblock</code>: Removes invalidity status of a block</li> </ol>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#mining-related-methods","title":"Mining-related Methods","text":"<ol> <li> <p><code>getdifficulty</code>: Returns the current network difficulty</p> <ul> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: Current difficulty as a floating point number</p> </li> </ul> </li> <li> <p><code>getmininginfo</code>: Returns mining-related information</p> <ul> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: Object containing block height, current block size and weight, current difficulty, and estimated network hashrate</p> </li> </ul> </li> <li> <p><code>getminingcandidate</code>: Obtain a mining candidate</p> <ul> <li> <p>Parameters:</p> <ul> <li> <p>Optional object containing:</p> <ul> <li><code>coinbaseValue</code> (numeric, optional): Custom coinbase value in satoshis</li> </ul> </li> </ul> </li> <li> <p>Returns: Object containing candidate ID, previous block hash, coinbase transaction, and merkle branches</p> </li> <li> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"mining\",\n    \"method\": \"getminingcandidate\",\n    \"params\": []\n}\n</code></pre> </li> <li> <p>Example Request with custom coinbase value:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"mining\",\n    \"method\": \"getminingcandidate\",\n    \"params\": [{\"coinbaseValue\": 5000000000}]\n}\n</code></pre> </li> <li> <p>Example Response:</p> <pre><code>{\n    \"result\": {\n        \"id\": \"00000000000000000000000000000000...\",\n        \"prevhash\": \"000000000000000004a1b6d6fdfa0d0a...\",\n        \"coinbase\": \"01000000010000000000000000000000000000...\",\n        \"coinbaseValue\": 5000000000,\n        \"version\": 536870912,\n        \"merkleproof\": [...],\n        \"time\": 1621500000,\n        \"bits\": \"180d60e3\",\n        \"height\": 700001,\n        \"nBits\": 402947203,\n        \"num_tx\": 5620,\n        \"sizeWithoutCoinbase\": 2300000,\n        \"minTime\": 1621498888,\n        \"fullCurrentTime\": 1621500000\n    },\n    \"error\": null,\n    \"id\": \"mining\"\n}\n</code></pre> </li> </ul> </li> <li> <p><code>submitminingsolution</code>: Submits a new mining solution</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>id</code> (string, required): Mining candidate ID</li> <li><code>nonce</code> (hexadecimal string, required): Nonce value found</li> <li><code>coinbase</code> (hexadecimal string, required): Complete coinbase transaction</li> <li><code>time</code> (numeric, required): Block time</li> <li><code>version</code> (numeric, optional): Block version</li> </ul> </li> <li> <p>Returns: Boolean <code>true</code> if block accepted, error if rejected</p> </li> <li>Validation process: The solution is validated for proof-of-work correctness, block structure, and consensus rules before being accepted and propagated to the network</li> <li> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"mining\",\n    \"method\": \"submitminingsolution\",\n    \"params\": [{\n        \"id\": \"00000000000000000000000000000000...\",\n        \"nonce\": \"17aab479321b85\",\n        \"coinbase\": \"01000000010000000000000000000000000000...\",\n        \"time\": 1621500004\n    }]\n}\n</code></pre> </li> </ul> </li> <li> <p><code>generate</code>: Generates new blocks (for testing only)</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>nblocks</code> (numeric, required): Number of blocks to generate</li> <li><code>maxtries</code> (numeric, optional): Maximum iterations to try</li> </ul> </li> <li> <p>Returns: Array of block hashes generated</p> </li> </ul> </li> <li> <p><code>generatetoaddress</code>: Generates new blocks with rewards going to a specified address (for testing only)</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>nblocks</code> (numeric, required): Number of blocks to generate</li> <li><code>address</code> (string, required): Bitcoin address to receive the rewards</li> <li><code>maxtries</code> (numeric, optional): Maximum iterations to try</li> </ul> </li> <li> <p>Returns: Array of block hashes generated</p> </li> </ul> </li> </ol>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#transaction-related-methods","title":"Transaction-related Methods","text":"<ol> <li> <p><code>createrawtransaction</code>: Creates a raw transaction</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>inputs</code> (array, required): Array of transaction inputs<ul> <li> <p>Each input is an object with:</p> <ul> <li><code>txid</code> (string, required): The transaction id</li> <li><code>vout</code> (numeric, required): The output number         - <code>outputs</code> (object, required): JSON object with outputs as key-value pairs             - Key is the Bitcoin address             - Value is the amount in BTC</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Returns: Hex-encoded raw transaction</p> </li> <li>Note: The transaction is not signed and cannot be submitted until signed</li> </ul> </li> <li> <p><code>getrawtransaction</code>: Gets a raw transaction</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>txid</code> (string, required): Transaction ID</li> <li><code>verbose</code> (boolean, optional): If true, returns detailed information</li> </ul> </li> <li> <p>Returns:</p> <ul> <li>If verbose=false: Hex-encoded transaction data</li> <li>If verbose=true: Detailed transaction object with txid, hash, size, version, locktime, and transaction inputs/outputs</li> </ul> </li> </ul> </li> <li> <p><code>sendrawtransaction</code>: Sends a raw transaction</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>hexstring</code> (string, required): Hex-encoded raw transaction</li> <li><code>allowhighfees</code> (boolean, optional): Allow high fees</li> </ul> </li> <li> <p>Returns: Transaction hash (txid) if successful</p> </li> <li>Validation process: Transaction is validated for correct format, script correctness, and fee policy before being accepted and propagated to the network</li> <li>Example Request:</li> </ul> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltext\",\n    \"method\": \"sendrawtransaction\",\n    \"params\": [\"0100000001bd2b5ba3d4a3a05c8ef31e8b6f8ab3e73b1f9ff5c617130cdf55e150d97a06ef000000006b483045022100c23a6432950e1ca96e438c95ce51bda58500ffa3a7a9941495a838bc7d3aee10022072ed0da7d7879f9ac7308a41c0e8ec7823e1b7932e211cf13a83a3ada10dacb141210386536695a23ba3ed37a18d542990f9b1df30a13952659d2820df3f47be78dcd3ffffffff01801a0600000000001976a914c5c25b16fa949402a8712e8e5fb3568eb87aee7288ac00000000\"]\n}\n</code></pre> </li> <li> <p><code>freeze</code>: Freezes a specific UTXO, preventing it from being spent</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>txid</code> (string, required): The transaction ID of the UTXO</li> <li><code>vout</code> (numeric, required): The output index</li> </ul> </li> <li> <p>Returns: Boolean <code>true</code> if successful</p> </li> <li>Note: Frozen UTXOs remain frozen until explicitly unfrozen</li> </ul> </li> <li> <p><code>unfreeze</code>: Unfreezes a previously frozen UTXO, allowing it to be spent</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>txid</code> (string, required): The transaction ID of the frozen UTXO</li> <li><code>vout</code> (numeric, required): The output index</li> </ul> </li> <li> <p>Returns: Boolean <code>true</code> if successful</p> </li> </ul> </li> <li> <p><code>reassign</code>: Reassigns ownership of a specific UTXO to a new Bitcoin address</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>txid</code> (string, required): The transaction ID of the UTXO</li> <li><code>vout</code> (numeric, required): The output index</li> <li><code>destination</code> (string, required): The Bitcoin address to reassign to</li> </ul> </li> <li> <p>Returns: Boolean <code>true</code> if successful</p> </li> <li>Note: The UTXO must be frozen before it can be reassigned</li> </ul> </li> </ol>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#network-related-methods","title":"Network-related Methods","text":"<ol> <li> <p><code>getinfo</code>: Returns information about the node</p> <ul> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: Object containing node information including:</p> <ul> <li><code>version</code>: Server version</li> <li><code>protocolversion</code>: Protocol version</li> <li><code>blocks</code>: Current block count</li> <li><code>connections</code>: Current connection count</li> <li><code>difficulty</code>: Current network difficulty</li> <li><code>errors</code>: Current error messages</li> <li><code>testnet</code>: Whether running on testnet</li> <li><code>timeoffset</code>: Time offset in seconds</li> </ul> </li> </ul> </li> <li> <p><code>getchaintips</code>: Returns information about all known chain tips</p> <ul> <li> <p>Parameters:     None</p> </li> <li> <p>Returns:</p> <ul> <li><code>height</code>: Height of the chain tip</li> <li><code>hash</code>: Block hash of the chain tip</li> <li><code>branchlen</code>: Zero for main chain, otherwise length of branch</li> <li><code>status</code>: Status of the chain tip (\"active\" for main chain, \"valid-fork\", etc.)</li> </ul> </li> </ul> </li> <li> <p><code>getpeerinfo</code>: Returns information about connected peers</p> <ul> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: Array of objects with detailed information about each connected peer, including:</p> <ul> <li><code>id</code>: Peer index</li> <li><code>addr</code>: IP address and port</li> <li><code>addrlocal</code>: Local address</li> <li><code>services</code>: Services provided by the peer</li> <li><code>lastsend</code>: Time since last message sent to this peer</li> <li><code>lastrecv</code>: Time since last message received from this peer</li> <li><code>bytessent</code>: Total bytes sent to this peer</li> <li><code>bytesrecv</code>: Total bytes received from this peer</li> <li><code>conntime</code>: Connection time in seconds</li> <li><code>pingtime</code>: Ping time in seconds</li> <li><code>version</code>: Peer protocol version</li> <li><code>subver</code>: Peer user agent</li> <li><code>inbound</code>: Whether connection is inbound</li> <li><code>startingheight</code>: Starting height of the peer</li> <li><code>banscore</code>: Ban score (for misbehavior)</li> </ul> </li> </ul> </li> <li> <p><code>setban</code>: Manages banned IP addresses/subnets</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>subnet</code> (string, required): The IP/Subnet to ban (e.g. 192.168.0.0/24)</li> <li><code>command</code> (string, required): 'add' to add to banlist, 'remove' to remove from banlist</li> <li><code>bantime</code> (numeric, optional): Time in seconds how long to ban (0 = permanently)</li> <li><code>absolute</code> (boolean, optional): If set to true, the bantime is interpreted as an absolute timestamp</li> </ul> </li> <li> <p>Returns: null on success</p> </li> <li>Note: Successfully executes across both P2P and legacy peer services. The underlying GRPC operations require API key authentication, which is handled automatically by the RPC server.</li> </ul> </li> <li> <p><code>isbanned</code>: Checks if a network address is currently banned</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>subnet</code> (string, required): The IP/Subnet to check</li> </ul> </li> <li> <p>Returns: Boolean <code>true</code> if the address is banned, <code>false</code> otherwise</p> </li> <li>Note: This command accesses GRPC ban status methods which require API key authentication when accessed directly. The RPC command handles this authentication automatically.</li> </ul> </li> <li> <p><code>listbanned</code>: Returns list of all banned IP addresses/subnets</p> <ul> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: Array of objects containing banned addresses with:</p> <ul> <li><code>address</code>: The banned IP/subnet</li> <li><code>banned_until</code>: The timestamp when the ban expires</li> <li><code>ban_created</code>: The timestamp when the ban was created</li> <li><code>ban_reason</code>: The reason for the ban (if provided)</li> </ul> </li> </ul> </li> <li> <p><code>clearbanned</code>: Removes all IP address bans</p> <ul> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: Boolean <code>true</code> on success</p> </li> </ul> </li> </ol>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#server-control-methods","title":"Server Control Methods","text":"<ol> <li> <p><code>stop</code>: Stops the Teranode server</p> <ul> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: String 'Teranode server stopping' when successful</p> </li> </ul> </li> <li> <p><code>version</code>: Returns version information about the node</p> <ul> <li> <p>Parameters:     None</p> </li> <li> <p>Returns: Object containing version information including:</p> <ul> <li><code>version</code>: The server version</li> <li><code>subversion</code>: The server subversion string</li> <li><code>protocolversion</code>: The protocol version</li> <li><code>localservices</code>: The services supported by this node</li> <li><code>localrelay</code>: Whether transaction relay is active</li> <li><code>timeoffset</code>: The time offset</li> <li><code>buildinfo</code>: Additional build information (compiler, OS, etc.)</li> </ul> </li> </ul> </li> </ol>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#error-handling","title":"Error Handling","text":"<p>When an error occurs during RPC method execution, the server returns a standardized error response in the following format:</p> <pre><code>{\n    \"result\": null,\n    \"error\": {\n        \"code\": -32601,\n        \"message\": \"Method not found\"\n    },\n    \"id\": \"1\"\n}\n</code></pre> <p>Common error codes include:</p> <ul> <li><code>-32600</code>: Invalid request</li> <li><code>-32601</code>: Method not found</li> <li><code>-32602</code>: Invalid parameters</li> <li><code>-32603</code>: Internal error</li> <li><code>-1</code>: Misc error</li> <li><code>-2</code>: Request rejected (e.g., node not synced)</li> <li><code>-5</code>: Invalid address or key</li> <li><code>-8</code>: Out of memory</li> <li><code>-32</code>: Server error (e.g., shutting down)</li> </ul> <p>Note: Teranode implements a subset of Bitcoin Core's RPC methods. If you encounter a \"Command unimplemented\" error, that method is not currently supported. See the RPC reference documentation for the complete list of supported methods.</p>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#authentication","title":"Authentication","text":"<p>The RPC server uses HTTP Basic Authentication. Credentials are configured in the settings (see the section 4.1 for details). There are two levels of access:</p> <ol> <li>Admin access: Full access to all RPC methods.</li> <li>Limited access: Access to a subset of RPC methods defined in <code>rpcLimited</code>.</li> </ol>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#grpc-api-key-authentication","title":"GRPC API Key Authentication","text":"<p>For direct GRPC service access, certain administrative operations require additional API key authentication:</p> <ul> <li>Protected Operations: <code>BanPeer</code> and <code>UnbanPeer</code> methods in both P2P and Legacy GRPC services</li> <li>Usage: API key must be included in GRPC requests as metadata with the key <code>x-api-key</code></li> </ul> <p>Note: When using RPC commands like <code>setban</code> and <code>isbanned</code>, the API key authentication is handled automatically by the RPC server. Direct GRPC access requires manual API key inclusion.</p>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#request-format","title":"Request Format","text":"<p>Requests should be sent as HTTP POST requests with a JSON-RPC 1.0 or 2.0 formatted body. For example:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"1\",\n    \"method\": \"getbestblockhash\",\n    \"params\": []\n}\n</code></pre>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#response-format","title":"Response Format","text":"<p>Responses are JSON objects containing the following fields:</p> <ul> <li><code>result</code>: The result of the method call (if successful).</li> <li><code>error</code>: Error information (if an error occurred).</li> <li><code>id</code>: The id of the request.</li> </ul>"},{"location":"howto/miners/minersHowToInteractWithRPCServer/#example-request","title":"Example Request","text":"<p>The default credentials are <code>bitcoin:bitcoin</code>. The default credentials can be changed via settings.</p> <pre><code>curl --user bitcoin:bitcoin --data-binary '{\"jsonrpc\":\"1.0\",\"id\":\"curltext\",\"method\":\"version\",\"params\":[]}' -H 'content-type: text/plain;' http://localhost:9292/\n</code></pre> <p>For detailed information on each method's parameters and return values, refer to the Bitcoin SV protocol documentation or the specific Teranode RPC Reference.</p>"},{"location":"howto/miners/minersHowToResetTeranode/","title":"How to Reset Teranode","text":"<p>If you require to sync a Teranode from scratch or need to restore from a backup, you will need to clean-up pre-existing data from your UTXO store, Blockchain store and your filesystem.</p> <p>The below is an example procedure, assuming Aerospike as your UTXO Store and Postgres as your Blockchain store.</p> <p>1. Aerospike clean-up</p> <p>See the Aerospike documentation for more information.</p> <pre><code>asadm --enable -e \"manage truncate ns utxo-store set utxo\"\n\n# verify the total records count, should slowly decrease to 0\nasadm -e \"info\"\n</code></pre> <p>2. Postgres clean-up</p> <pre><code># Connect to your the db used by Teranode.\npostgres=&gt; \\c &lt;db_name&gt;\n\n# Sanity count check.\nteranode_mainnet=&gt; SELECT COUNT(*) FROM blocks;\ncount\n--------\n123123\n(1 row)\n\n# truncate the blocks and state table.\nDROP TABLE IF EXISTS blocks CASCADE;\nDROP TABLE IF EXISTS state CASCADE;\nDROP TABLE IF EXISTS bans CASCADE;\n</code></pre> <p>3. Filesystem clean-up.</p> <pre><code>DATA_MOUNT_POINT=/mnt/teranode # Correct the exact path as required\nsudo rm -rf $DATA_MOUNT_POINT/*\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/","title":"Syncing the Blockchain","text":"<p>Last modified: 15-October-2025</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Synchronization Methods Comparison</li> <li>Method 1: Default Network Sync (P2P)</li> <li>Method 2: Seeding from Legacy SV Node</li> <li>Method 3: Seeding from Existing Teranode</li> <li>Recovery and Troubleshooting</li> <li>Monitoring and Verification</li> <li>Additional Resources</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#overview","title":"Overview","text":"<p>This guide covers the different methods available for synchronizing a Teranode instance with the Bitcoin SV blockchain. Whether you're setting up a fresh node or recovering from downtime, this document will help you choose the most appropriate synchronization method for your situation.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#synchronization-methods-comparison","title":"Synchronization Methods Comparison","text":"<p>Choose the synchronization method that best fits your situation:</p> Method Use Case Advantages Disadvantages Time Required Default Network Sync Fresh install, no existing data \u2022 Simple setup\u2022 No additional requirements\u2022 Complete validation \u2022 Slowest method\u2022 High bandwidth usage 5-8 days Legacy SV Node Seeding Have existing BSV node \u2022 Faster than P2P\u2022 Proven data source\u2022 Reduced bandwidth \u2022 Requires SV Node setup\u2022 Additional export steps 1 Hour(assumes SV nodealready in sync) Teranode Data Seeding Have existing Teranode \u2022 Fastest method\u2022 Direct data transfer\u2022 Minimal processing \u2022 Requires access to existing data\u2022 Version compatibility needed 1 Hour <p></p> <p>\ud83d\udca1 Recommendation: For production deployments, we recommend using the Legacy SV Node seeding method when possible, as it provides the best balance of speed and data integrity verification.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#method-1-default-network-sync-p2p","title":"Method 1: Default Network Sync (P2P)","text":"<p>This is the standard synchronization method where Teranode downloads the complete blockchain from other nodes in the network via peer-to-peer connections.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#prerequisites","title":"Prerequisites","text":"<ul> <li>\u2705 Teranode instance deployed and running</li> <li>\u2705 Network connectivity to BSV peers</li> <li>\u2705 Sufficient storage space (minimum 4TB with default prune settings of 288 blocks)</li> <li>\u2705 Stable internet connection with adequate bandwidth</li> </ul> <p>\u26a0\ufe0f Important: This method can take around 14 days depending on your network connection and hardware specifications.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#process-overview","title":"Process Overview","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#step-1-initialize-sync-process","title":"Step 1: Initialize Sync Process","text":"<p>Upon startup, Teranode begins in IDLE state. You must explicitly set the state to <code>legacysyncing</code> to begin synchronization.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#for-kubernetes-deployments","title":"For Kubernetes Deployments","text":"<pre><code># Set FSM state to begin legacy syncing\nkubectl exec -it $(kubectl get pods -n teranode-operator -l app=blockchain -o jsonpath='{.items[0].metadata.name}') -n teranode-operator -- teranode-cli setfsmstate -fsmstate legacysyncing\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#for-docker-deployments","title":"For Docker Deployments","text":"<pre><code># Set FSM state to begin legacy syncing\ndocker exec -it blockchain teranode-cli setfsmstate -fsmstate legacysyncing\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-2-peer-discovery-and-block-download","title":"Step 2: Peer Discovery and Block Download","text":"<ul> <li>Peer Connection: Teranode automatically discovers and connects to BSV network peers</li> <li>Block Requests: Downloads blocks sequentially from genesis, starting with the first available peer</li> <li>Legacy Mode: In <code>legacysyncing</code> state, connects to traditional BSV nodes for compatibility</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-3-validation-and-storage","title":"Step 3: Validation and Storage","text":"<p>As blocks are received, multiple Teranode services work in parallel:</p> <ul> <li>Block Validation: <code>block-validator</code> service validates block headers and structure</li> <li>Subtree Validation: <code>subtree-validator</code> service validates transaction subtrees</li> <li>Storage: <code>blockchain</code> service stores validated blocks in the database</li> <li>UTXO Updates: <code>asset</code> service maintains the UTXO set</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-4-monitor-progress","title":"Step 4: Monitor Progress","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#kubernetes-monitoring","title":"Kubernetes Monitoring","text":"<pre><code># View real-time sync logs\nkubectl logs -n teranode-operator -l app=blockchain -f\n\n# Check service health\nkubectl get pods -n teranode-operator | grep -E 'aerospike|postgres|kafka|teranode-operator'\n\n# Wait for services to be ready\nkubectl wait --for=condition=ready pod -l app=blockchain -n teranode-operator --timeout=300s\n\n# Get detailed blockchain info\nkubectl exec &lt;blockchain-pod-name&gt; -n teranode-operator -- teranode-cli getblockchaininfo\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#docker-monitoring","title":"Docker Monitoring","text":"<pre><code># View real-time sync logs\ndocker-compose logs -f blockchain\n\n# Check service health\ndocker-compose ps\n\n# Get detailed blockchain info\ndocker exec -it blockchain teranode-cli getblockchaininfo\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#expected-timeline","title":"Expected Timeline","text":"Phase Duration Description Initial Setup 5-10 minutes Peer discovery and connection establishment Early Blocks 1-2 days Genesis to block ~500,000 (smaller blocks, faster processing) Recent Blocks 3-6 days Block ~500,000 to current tip (larger blocks, slower processing) Catch-up Ongoing Maintaining sync with new blocks <p>\ud83d\udcca Performance Tip: Monitor your system resources during sync. CPU and I/O intensive operations are normal during this process.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#method-2-seeding-from-legacy-sv-node","title":"Method 2: Seeding from Legacy SV Node","text":"<p>This method allows you to bootstrap a Teranode instance using data exported from an existing Bitcoin SV node (bitcoind). This significantly reduces synchronization time compared to P2P sync.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>\u2705 Access to a fully synchronized Bitcoin SV node (bitcoind)</li> <li>\u2705 SV Node gracefully shut down (using <code>bitcoin-cli stop</code>)</li> <li>\u2705 Fresh Teranode instance with no existing blockchain data (use reset guide to clear existing data if needed)</li> <li>\u2705 Sufficient disk space for export files (~1TB recommended, temporary during process)</li> <li>\u2705 Sufficient disk space for Teranode data (~10TB recommended, permanent)</li> <li>\u2705 Docker or Kubernetes environment set up</li> </ul> <p>\u26a0\ufe0f Critical: Only perform this operation on a gracefully shut down SV Node to ensure data consistency.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#overview_1","title":"Overview","text":"<p>This process involves two main phases:</p> <ol> <li>Export Phase: Convert SV Node database data (UTXO set and headers) into a Teranode-compatible format</li> <li>Seeding Phase: Import the converted data files into Teranode</li> </ol> <p>\ud83d\udca1 What's happening: The <code>bitcointoutxoset</code> tool reads the SV Node's LevelDB database files (<code>chainstate</code> and <code>blocks</code>) and converts them into Teranode's native format for fast import.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#phase-1-convert-sv-node-data-to-teranode-format","title":"Phase 1: Convert SV Node Data to Teranode Format","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#step-1-verify-sv-node-requirements","title":"Step 1: Verify SV Node Requirements","text":"<p>Before proceeding with the export, ensure your Bitcoin SV node meets these critical requirements:</p> <p>Data Location Requirements</p> <ul> <li>For this guide, we assume SV node data is located at <code>/mnt/bitcoin-sv-data</code></li> <li>Replace this path with your actual SV node data directory in all commands</li> <li>Verify the directory contains both <code>blocks</code> and <code>chainstate</code> subdirectories</li> </ul> <p>Node State Requirements</p> <ul> <li>\u2705 SV Node must be completely stopped - Use <code>bitcoin-cli stop</code> for graceful shutdown</li> <li>\u274c Node cannot be actively syncing - Export will fail if the node is processing blocks</li> <li>\u274c Node cannot have active network connections during export</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#optional-ensure-data-consistency","title":"Optional: Ensure Data Consistency","text":"<p>If you failed to stop gracefully, and you want to guarantee data consistency before export, you can start the SV node one final time in isolated mode:</p> <pre><code># Start SV node in isolated mode (no network activity)\nbitcoind -listen=0 -connect=0 -daemon\n\n# Wait for any pending operations to complete (check logs)\ntail -f ~/.bitcoin/debug.log\n\n# Once stable, stop gracefully\nbitcoin-cli stop\n</code></pre> <p>Critical Warning</p> <p>The SV node must be completely stopped before proceeding. Active syncing or network activity during export will cause data corruption or export failure.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-2-prepare-export-directory","title":"Step 2: Prepare Export Directory","text":"<pre><code># Create export directory\nsudo mkdir -p /mnt/teranode/seed/export\nsudo chown $USER:$USER /mnt/teranode/seed/export\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-3-export-utxo-data","title":"Step 3: Export UTXO Data","text":"<pre><code># Export UTXO set from Bitcoin SV node\n# Replace /mnt/bitcoin-sv-data with your actual SV node data directory\ndocker run -it \\\n    -v /mnt/bitcoin-sv-data:/home/ubuntu/bitcoin-data:ro \\\n    -v /mnt/teranode/seed:/mnt/teranode/seed \\\n    --entrypoint=\"\" \\\n    ghcr.io/bsv-blockchain/teranode:v0.11.13 \\\n    /app/teranode-cli bitcointoutxoset \\\n        -bitcoinDir=/home/ubuntu/bitcoin-data \\\n        -outputDir=/mnt/teranode/seed/export\n</code></pre> <p>Expected Output Files:</p> <ul> <li><code>{blockhash}.utxo-headers</code> - Block headers data</li> <li><code>{blockhash}.utxo-set</code> - UTXO set data</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-3-verify-export","title":"Step 3: Verify Export","text":"<pre><code># Check exported files\nls -la /mnt/teranode/seed/export/\n# You should see .utxo-headers and .utxo-set files\n</code></pre> <p>\ud83d\udd27 Troubleshooting: If you encounter a <code>Block hash mismatch between last block and chainstate</code> error, restart your SV Node once more with <code>bitcoin-cli stop</code> followed by a clean restart.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#important-cleanup-for-retries","title":"\u26a0\ufe0f Important: Cleanup for Retries","text":"<p>If you encounter issues during export or need to retry the process:</p> <ol> <li>Clear the export directory completely:</li> </ol> <pre><code>sudo rm -rf /mnt/teranode/seed/export/*\n</code></pre> <ol> <li> <p>Reset the target Teranode instance (see reset guide)</p> </li> <li> <p>Verify SV node was gracefully shutdown and repeat from Step 1</p> </li> </ol> <p>Important for Retries</p> <p>Partial or corrupted export files can cause seeding failures. Always start with a clean export directory and fresh Teranode instance when retrying.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#phase-2-seed-teranode-with-exported-data","title":"Phase 2: Seed Teranode with Exported Data","text":"<p>Critical Service Requirements</p> <p>During the seeding process, only the following services should be running for Teranode:</p> <ul> <li>Aerospike (database)</li> <li>Postgres (database)</li> <li>Kafka (messaging)</li> </ul> <p>All other Teranode services must be stopped (blockchain, asset, blockvalidation, etc.) to prevent conflicts during data import.</p> <p>Network Configuration Alignment</p> <p>The export and import target networks must be consistent:</p> <ul> <li>If SV node was running mainnet, Teranode must be configured for mainnet</li> <li>If SV node was running testnet, Teranode must be configured for testnet</li> <li>Check development configurations - Some setups may be set to \"regtest\" mode</li> <li>Verify network settings in Teranode configuration before proceeding with Step 1</li> </ul> <p>Mismatched networks will cause seeding failure or data corruption.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-1-prepare-teranode-environment","title":"Step 1: Prepare Teranode Environment","text":"<p>For Docker Deployments:</p> <pre><code># Start required services (adjust service names as needed)\ndocker compose up -d aerospike aerospike-2 postgres kafka-shared\n\n# Verify services are running\ndocker compose ps\n\n# CRITICAL: Ensure Teranode services are NOT running\ndocker compose stop blockchain asset blockvalidation # Add other services as needed\n</code></pre> <p>For Kubernetes Deployments:</p> <p>You can scale down the Teranode services using the <code>spec.enabled</code> option in the CR:</p> <pre><code>---\napiVersion: teranode.bsvblockchain.org/v1alpha1\nkind: Cluster\nmetadata:\n  name: teranode-cluster\nspec:\n  enabled: false\n  alertSystem:\n    ...\n</code></pre> <pre><code>kubectl apply -f &lt;path-to-your-cr-file&gt;.yaml\n\n# Verify pods are terminated\nkubectl get pods -n teranode-operator\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-2-identify-the-block-hash","title":"Step 2: Identify the Block Hash","text":"<p>Before running the seeder, you need to identify the correct block hash from your exported files:</p> <pre><code># List the exported files to see the block hash\nls -la /mnt/teranode/seed/export/\n\n# You should see files like:\n# 0000000000013b8ab2cd513b0261a14096412195a72a0c4827d229dcc7e0f7af.utxo-headers\n# 0000000000013b8ab2cd513b0261a14096412195a72a0c4827d229dcc7e0f7af.utxo-set\n</code></pre> <p>Understanding the Hash Parameter</p> <p>The <code>-hash</code> parameter specifies the block hash of the last block in the exported UTXO set. This hash:</p> <ul> <li>Identifies the exported files - Files are named <code>{blockhash}.utxo-headers</code> and <code>{blockhash}.utxo-set</code></li> <li>Represents the blockchain tip at the time of export from the SV node</li> <li>Must match exactly with the filenames in your export directory</li> </ul> <p>How to find it: Extract the hash from your exported filenames (the part before <code>.utxo-headers</code> or <code>.utxo-set</code>)</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-3-run-seeder","title":"Step 3: Run Seeder","text":"<p>For Docker Deployments:</p> <pre><code># Run the seeder (replace the hash with your actual block hash from Step 2)\n# Make sure to add any environment variables you have defined in your docker-compose.yml\ndocker run -it \\\n    -e SETTINGS_CONTEXT=docker.m \\\n    -e network=mainnet \\\n    -v ${PWD}/docker/mainnet/data/teranode:/app/data \\\n    -v /mnt/teranode/seed:/mnt/teranode/seed \\\n    --network my-teranode-network \\\n    --entrypoint=\"\" \\\n    ghcr.io/bsv-blockchain/teranode:v0.11.13 \\\n    /app/teranode-cli seeder \\\n        -inputDir /mnt/teranode/seed/export \\\n        -hash 0000000000013b8ab2cd513b0261a14096412195a72a0c4827d229dcc7e0f7af\n</code></pre> <p>For Kubernetes Deployments:</p> <pre><code># Create a temporary seeder pod\nkubectl run teranode-seeder \\\n    --image=ghcr.io/bsv-blockchain/teranode:v0.11.13 \\\n    --restart=Never \\\n    --rm -i --tty \\\n    -n teranode-operator \\\n    -- /app/teranode-cli seeder \\\n        -inputDir /mnt/teranode/seed/export \\\n        -hash 0000000000013b8ab2cd513b0261a14096412195a72a0c4827d229dcc7e0f7af\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-3-monitor-seeding-progress","title":"Step 3: Monitor Seeding Progress","text":"<pre><code># Monitor seeder logs\n# For Docker:\ndocker logs -f &lt;seeder-container-id&gt;\n\n# For Kubernetes:\nkubectl logs -f teranode-seeder -n teranode-operator\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-4-start-teranode-services","title":"Step 4: Start Teranode Services","text":"<p>After successful seeding:</p> <p>For Docker:</p> <pre><code># Start all Teranode services\ndocker compose up -d\n</code></pre> <p>For Kubernetes:</p> <pre><code># Scale services back up\nkubectl scale deployment blockchain --replicas=1 -n teranode-operator\nkubectl scale deployment asset --replicas=1 -n teranode-operator\n# Scale up other services as needed\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#expected-timeline_1","title":"Expected Timeline","text":"Phase Duration Description Export 2-4 hours Extracting UTXO set from SV Node Seeding 4-8 hours Importing data into Teranode Verification 30 minutes Starting services and verifying sync Total 1-12 hours Complete process <p>\ud83d\udcbe Storage Note: The seeder writes directly to Aerospike, PostgreSQL, and the filesystem. Ensure your environment variables and volume mounts are correctly configured.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#method-3-seeding-from-existing-teranode","title":"Method 3: Seeding from Existing Teranode","text":"<p>This is the fastest synchronization method, using UTXO set and header files from an existing Teranode instance. This method is ideal when you have access to another synchronized Teranode node.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>\u2705 Access to a synchronized Teranode instance</li> <li>\u2705 UTXO set files from Block/UTXO Persister</li> <li>\u2705 Fresh target Teranode instance with no existing blockchain data (use reset guide to clear existing data if needed)</li> <li>\u2705 Network access between source and target systems</li> <li>\u2705 Sufficient storage for data transfer</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#overview_2","title":"Overview","text":"<p>This method uses the same seeder tool as Method 2, but with data files generated by Teranode's Block Persister and UTXO Persister services instead of exporting from a legacy SV node.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-1-locate-source-data","title":"Step 1: Locate Source Data","text":"<p>On your source Teranode instance, locate the persisted data files:</p> <pre><code># Typical locations for persisted data\n# Docker deployments:\nls -la /app/data/blockstore/\nls -la /app/data/utxo-persister/\n\n# Kubernetes deployments:\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- ls -la /app/data/blockstore/\n</code></pre> <p>Required Files:</p> <ul> <li><code>{blockhash}.utxo-headers</code> - Block headers</li> <li><code>{blockhash}.utxo-set</code> - UTXO set data</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-2-make-data-available","title":"Step 2: Make Data Available","text":"<p>Ensure the required UTXO files are available in your target Teranode's export directory:</p> <pre><code># Target location for the files\n/mnt/teranode/seed/export/{blockhash}.utxo-headers\n/mnt/teranode/seed/export/{blockhash}.utxo-set\n</code></pre> <p>Data Transfer</p> <p>Use your preferred method to transfer the files from the source Teranode to the target location (scp, rsync, container volumes, shared storage, etc.).</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-3-run-seeder_1","title":"Step 3: Run Seeder","text":"<p>Use the same seeder process as described in Method 2:</p> <p>For Docker:</p> <pre><code># Prepare environment\ndocker compose up -d aerospike aerospike-2 postgres kafka-shared\ndocker compose stop blockchain asset blockvalidation\n\n# Run seeder\ndocker run -it \\\n    -e SETTINGS_CONTEXT=docker.m \\\n    -v ${PWD}/docker/mainnet/data/teranode:/app/data \\\n    -v /mnt/teranode/seed:/mnt/teranode/seed \\\n    --network my-teranode-network \\\n    --entrypoint=\"\" \\\n    ghcr.io/bsv-blockchain/teranode:v0.11.13 \\\n    /app/teranode-cli seeder \\\n        -inputDir /mnt/teranode/seed/export \\\n        -hash &lt;blockhash-from-filename&gt;\n</code></pre> <p>For Kubernetes:</p> <pre><code># Scale down services\nkubectl scale deployment blockchain --replicas=0 -n teranode-operator\n\n# Run seeder\nkubectl run teranode-seeder \\\n    --image=ghcr.io/bsv-blockchain/teranode:v0.11.13 \\\n    --restart=Never --rm -i --tty \\\n    -n teranode-operator \\\n    -- /app/teranode-cli seeder \\\n        -inputDir /mnt/teranode/seed/export \\\n        -hash &lt;blockhash-from-filename&gt;\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#expected-timeline_2","title":"Expected Timeline","text":"Phase Duration Description Data Transfer 1-3 hours Copying files between systems Seeding 2-4 hours Importing data into target Teranode Verification 15 minutes Starting services and verification Total 1-6 hours Complete process <p>\u26a1 Speed Advantage: This method is typically 2-3x faster than Method 2 since the data is already in Teranode's optimized format.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#recovery-and-troubleshooting","title":"Recovery and Troubleshooting","text":"<p>Teranode is designed for resilience and can recover from various types of downtime or disconnections. This section covers different recovery scenarios and troubleshooting approaches.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#normal-recovery-short-downtime","title":"Normal Recovery (Short Downtime)","text":"<p>For brief interruptions (minutes to hours), Teranode typically recovers automatically.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#automatic-recovery-process","title":"Automatic Recovery Process","text":"<ol> <li> <p>Service Restart</p> <ul> <li>Kubernetes automatically restarts crashed pods via ReplicaSet controllers</li> <li>Docker Compose can be configured with restart policies</li> </ul> </li> <li> <p>Peer Reconnection</p> <ul> <li>The <code>peer</code> service re-establishes network connections</li> <li>Automatic discovery of available BSV network peers</li> </ul> </li> <li> <p>Block Catch-up</p> <ul> <li>Node determines last processed block height</li> <li>Requests missing blocks from peers automatically</li> <li>Processes blocks sequentially to catch up</li> </ul> </li> </ol>"},{"location":"howto/miners/minersHowToSyncTheNode/#monitor-normal-recovery","title":"Monitor Normal Recovery","text":"<p>For Kubernetes:</p> <pre><code># Check pod status and restarts\nkubectl get pods -n teranode-operator -o wide\n\n# View recovery logs\nkubectl logs -n teranode-operator -l app=blockchain -f --tail=100\n\n# Check for pod events and issues\nkubectl describe pod -n teranode-operator -l app=blockchain\n\n# Verify sync progress\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getblockchaininfo\n</code></pre> <p>For Docker:</p> <pre><code># Check container status\ndocker-compose ps\n\n# View recovery logs\ndocker-compose logs -f blockchain --tail=100\n\n# Verify sync progress\ndocker exec -it blockchain teranode-cli getblockchaininfo\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#extended-downtime-recovery","title":"Extended Downtime Recovery","text":"<p>For longer outages (days to weeks), additional considerations apply.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#assessment-phase","title":"Assessment Phase","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#step-1-check-data-integrity","title":"Step 1: Check Data Integrity","text":"<pre><code># Examine logs for corruption warnings\nkubectl logs -n teranode-operator -l app=blockchain --previous | grep -i \"corrupt\\|error\\|fail\"\n\n# Check database connectivity\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli settings | grep -E \"postgres\\|aerospike\"\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#step-2-evaluate-catch-up-requirements","title":"Step 2: Evaluate Catch-up Requirements","text":"<pre><code># Check current block height vs network tip\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getblockchaininfo\n\n# Calculate blocks behind (compare with block explorer)\n# If &gt;10,000 blocks behind, consider reseeding\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#recovery-options","title":"Recovery Options","text":"Blocks Behind Recommended Action Expected Time &lt; 1,000 Normal catch-up 1-4 hours 1,000 - 10,000 Monitor catch-up progress 4-24 hours &gt; 10,000 Consider reseeding (Method 2 or 3) 6-12 hours"},{"location":"howto/miners/minersHowToSyncTheNode/#manual-intervention-steps","title":"Manual Intervention Steps","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#option-1-force-catch-up","title":"Option 1: Force Catch-up","text":"<pre><code># Reset FSM state and restart sync\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli setfsmstate -fsmstate legacysyncing\n\n# Monitor progress closely\nkubectl logs -n teranode-operator -l app=blockchain -f\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#option-2-reseed-from-recent-data","title":"Option 2: Reseed from Recent Data","text":"<p>If catch-up is too slow, use Method 2 or Method 3 from this guide with recent data.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#issue-sync-stalled","title":"Issue: Sync Stalled","text":"<p>Symptoms:</p> <ul> <li>Block height not increasing</li> <li>No new blocks being processed</li> <li>Peer connections established but inactive</li> </ul> <p>Solutions:</p> <pre><code># Check peer connections\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getpeerinfo\n\n# Restart peer service\nkubectl delete pod -n teranode-operator -l app=peer\n\n# Reset FSM state\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli setfsmstate -fsmstate legacysyncing\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#issue-database-connection-errors","title":"Issue: Database Connection Errors","text":"<p>Symptoms:</p> <ul> <li>Connection timeouts to PostgreSQL/Aerospike</li> <li>\"Database unavailable\" errors in logs</li> </ul> <p>Solutions:</p> <pre><code># Check database pod status\nkubectl get pods -n teranode-operator | grep -E \"postgres\\|aerospike\"\n\n# Test database connectivity\nkubectl exec -it postgres-pod -n teranode-operator -- psql -U &lt;username&gt; -d &lt;database&gt; -c \"SELECT 1;\"\n\n# Restart database pods if needed\nkubectl delete pod -n teranode-operator -l app=postgres\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#issue-storage-space-exhausted","title":"Issue: Storage Space Exhausted","text":"<p>Symptoms:</p> <ul> <li>\"No space left on device\" errors</li> <li>Pods in CrashLoopBackOff state</li> </ul> <p>Solutions:</p> <pre><code># Check storage usage\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- df -h\n\n# Clean up old logs (if applicable)\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- find /app/logs -name \"*.log\" -mtime +7 -delete\n\n# Scale up storage (requires cluster admin)\n# Consider implementing log rotation\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#performance-monitoring-during-recovery","title":"Performance Monitoring During Recovery","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#key-metrics-to-watch","title":"Key Metrics to Watch","text":"<ul> <li>Block Processing Rate: Blocks per minute</li> <li>Peer Connection Count: Active peer connections</li> <li>Database Performance: Query response times</li> <li>Storage I/O: Disk read/write rates</li> <li>Memory Usage: RAM consumption patterns</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#monitoring-commands","title":"Monitoring Commands","text":"<pre><code># Watch block height progress\nwatch \"kubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getblockchaininfo | grep height\"\n\n# Monitor resource usage\nkubectl top pods -n teranode-operator\n\n# Check service health endpoints\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- curl -s http://localhost:8000/health\n</code></pre> <p>\ud83d\udcc8 Tip: Use monitoring tools like Prometheus and Grafana to track recovery progress and estimate completion times. Set up alerts for stalled sync or resource exhaustion.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#monitoring-and-verification","title":"Monitoring and Verification","text":"<p>After completing synchronization using any method, it's important to verify that your Teranode instance is properly synchronized and functioning correctly.</p>"},{"location":"howto/miners/minersHowToSyncTheNode/#verification-checklist","title":"Verification Checklist","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#basic-connectivity","title":"\u2705 Basic Connectivity","text":"<pre><code># Check if services are running\nkubectl get pods -n teranode-operator\n# All pods should be in \"Running\" status\n\n# Test CLI connectivity\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getblockchaininfo\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#synchronization-status","title":"\u2705 Synchronization Status","text":"<pre><code># Check current block height\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getblockchaininfo | grep -E \"height|hash\"\n\n# Compare with network tip (use block explorer or other nodes)\n# Heights should match within 1-2 blocks\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#peer-connections","title":"\u2705 Peer Connections","text":"<pre><code># Verify peer connections\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getpeerinfo | grep -E \"addr|version\"\n\n# Should have 8+ active peer connections\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#database-health","title":"\u2705 Database Health","text":"<pre><code># Check PostgreSQL connectivity\nkubectl exec -it &lt;postgres-pod&gt; -n teranode-operator -- psql -U &lt;username&gt; -d &lt;database&gt; -c \"SELECT COUNT(*) FROM blocks;\"\n\n# Check Aerospike connectivity\nkubectl exec -it &lt;aerospike-pod&gt; -n teranode-operator -- asinfo -v \"statistics\" | grep \"objects\"\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#fsm-state","title":"\u2705 FSM State","text":"<pre><code># Verify FSM is in correct state\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getfsmstate\n\n# Should show \"running\" or \"synced\" for a fully synchronized node\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#ongoing-monitoring","title":"Ongoing Monitoring","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#daily-health-checks","title":"Daily Health Checks","text":"<pre><code>#!/bin/bash\n# Save as teranode-health-check.sh\n\necho \"=== Teranode Health Check ===\"\necho \"Timestamp: $(date)\"\n\n# Check pod status\necho \"\\n--- Pod Status ---\"\nkubectl get pods -n teranode-operator\n\n# Check block height\necho \"\\n--- Block Height ---\"\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getblockchaininfo | grep height\n\n# Check peer count\necho \"\\n--- Peer Count ---\"\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getpeerinfo | wc -l\n\n# Check FSM state\necho \"\\n--- FSM State ---\"\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- teranode-cli getfsmstate\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#performance-metrics","title":"Performance Metrics","text":"<pre><code># Monitor resource usage\nkubectl top pods -n teranode-operator\n\n# Check storage usage\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- df -h\n\n# Monitor network traffic\nkubectl exec -it &lt;blockchain-pod&gt; -n teranode-operator -- netstat -i\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#log-analysis","title":"Log Analysis","text":"<pre><code># Check for errors in recent logs\nkubectl logs -n teranode-operator -l app=blockchain --tail=1000 | grep -i error\n\n# Monitor sync progress\nkubectl logs -n teranode-operator -l app=blockchain -f | grep -E \"height|block|sync\"\n\n# Check for warnings\nkubectl logs -n teranode-operator -l app=blockchain --tail=1000 | grep -i warn\n</code></pre>"},{"location":"howto/miners/minersHowToSyncTheNode/#additional-resources","title":"Additional Resources","text":""},{"location":"howto/miners/minersHowToSyncTheNode/#documentation","title":"Documentation","text":"<ul> <li>Teranode CLI Reference - Complete CLI command reference</li> <li>Reset Teranode Guide - How to reset and clean Teranode data</li> <li>Seeder Command Details - Advanced seeder configuration</li> <li>UTXO Persister Service - UTXO persistence configuration</li> <li>UTXO Store Documentation - UTXO storage mechanisms</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#installation-and-configuration","title":"Installation and Configuration","text":"<ul> <li>Docker Installation - Docker-based setup</li> <li>Kubernetes Installation - Kubernetes deployment</li> <li>Docker Configuration - Docker configuration guide</li> <li>Kubernetes Configuration - Kubernetes configuration guide</li> <li>Third Party Requirements - External dependencies</li> </ul>"},{"location":"howto/miners/minersHowToSyncTheNode/#support-and-community","title":"Support and Community","text":"<p>For additional support:</p> <ol> <li>Check Logs: Always start with examining service logs</li> <li>Documentation: Review related documentation sections</li> <li>Community Forums: Search existing discussions and solutions</li> <li>Issue Reporting: Report bugs with detailed logs and reproduction steps</li> </ol>"},{"location":"howto/miners/minersHowToTeranodeCLI/","title":"Teranode CLI Documentation","text":"<p>Last Modified: 4-May-2025</p>"},{"location":"howto/miners/minersHowToTeranodeCLI/#overview","title":"Overview","text":"<p>The teranode-cli is a command-line interface tool for interacting with Teranode services. It provides various commands for maintenance, debugging, and operational tasks.</p>"},{"location":"howto/miners/minersHowToTeranodeCLI/#basic-usage","title":"Basic Usage","text":"<p>To access the CLI in a Docker container:</p> <pre><code>docker exec -it blockchain teranode-cli\n\n\nUsage: teranode-cli &lt;command&gt; [options]\n\n    Available Commands:\n    aerospikereader      Aerospike Reader\n    bitcointoutxoset     Bitcoin to Utxoset\n    checkblock           Validate an existing block\n    checkblocktemplate   Check block template\n    export-blocks        Export blockchain to CSV\n    filereader           File Reader\n    fix-chainwork        Fix incorrect chainwork values in blockchain database\n    getfsmstate          Get the current FSM State\n    import-blocks        Import blockchain from CSV\n    seeder               Seeder\n    setfsmstate          Set the FSM State\n    settings             Settings\n    utxopersister        Utxo Persister\n    utxovalidator        Validate UTXO sets\n\n    Use 'teranode-cli &lt;command&gt; --help' for more information about a command\n</code></pre>"},{"location":"howto/miners/minersHowToTeranodeCLI/#available-commands","title":"Available Commands","text":""},{"location":"howto/miners/minersHowToTeranodeCLI/#configuration","title":"Configuration","text":"Command Description Key Options <code>settings</code> View system configuration None"},{"location":"howto/miners/minersHowToTeranodeCLI/#data-management","title":"Data Management","text":"Command Description Key Options <code>aerospikereader</code> Read transaction data from Aerospike <code>&lt;txid&gt;</code> - Transaction ID to lookup <code>bitcointoutxoset</code> Convert Bitcoin data to UTXO set <code>--bitcoinDir</code> - Location of bitcoin data <code>--outputDir</code> - Output directory for UTXO set <code>--skipHeaders</code> - Skip processing headers <code>--skipUTXOs</code> - Skip processing UTXOs <code>--blockHash</code> - Block hash to start from <code>--previousBlockHash</code> - Previous block hash <code>--blockHeight</code> - Block height to start from <code>--dumpRecords</code> - Dump records from index <code>export-blocks</code> Export blockchain data to CSV <code>--file</code> - CSV file path to export <code>import-blocks</code> Import blockchain data from CSV <code>--file</code> - CSV file path to import <code>utxopersister</code> Manage UTXO persistence None"},{"location":"howto/miners/minersHowToTeranodeCLI/#system-tools","title":"System Tools","text":"Command Description Key Options <code>checkblock</code> Validate an existing block <code>&lt;blockhash&gt;</code> - Hash of the block to validate <code>checkblocktemplate</code> Check block template validity None <code>seeder</code> Seed initial blockchain data <code>--inputDir</code> - Input directory for data <code>--hash</code> - Hash of the data to process <code>--skipHeaders</code> - Skip processing headers <code>--skipUTXOs</code> - Skip processing UTXOs <code>filereader</code> Read and process files <code>--verbose</code> - Enable verbose output <code>--checkHeights</code> - Check heights in UTXO headers <code>--useStore</code> - Use store <code>utxovalidator</code> Validate UTXO sets <code>--inputDir</code> - Input directory containing UTXO data <code>--blockHash</code> - Block hash to validate against <code>--verbose</code> - Enable verbose output <code>getfsmstate</code> Get the current FSM state None <code>setfsmstate</code> Set the FSM state <code>--fsmstate</code> - Target FSM state \u00a0\u00a0Values: running, idle, catchingblocks, legacysyncing"},{"location":"howto/miners/minersHowToTeranodeCLI/#database-maintenance","title":"Database Maintenance","text":"Command Description Key Options <code>fix-chainwork</code> Fix incorrect chainwork values in blockchain <code>--db-url</code> - Database URL (required) database <code>--dry-run</code> - Preview changes without updating (default: true) <code>--batch-size</code> - Updates per transaction (default: 1000) <code>--start-height</code> - Starting block height (default: 650286) <code>--end-height</code> - Ending block height (default: 0 for tip)"},{"location":"howto/miners/minersHowToTeranodeCLI/#detailed-command-reference","title":"Detailed Command Reference","text":""},{"location":"howto/miners/minersHowToTeranodeCLI/#aerospike-reader","title":"Aerospike Reader","text":"<pre><code>teranode-cli aerospikereader &lt;txid&gt;\n</code></pre> <p>Retrieves transaction data from an Aerospike database using the provided transaction ID.</p>"},{"location":"howto/miners/minersHowToTeranodeCLI/#bitcoin-to-utxo-set","title":"Bitcoin to UTXO Set","text":"<pre><code>teranode-cli bitcointoutxoset --bitcoinDir=&lt;bitcoin-data-path&gt; --outputDir=&lt;output-dir-path&gt; [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--bitcoinDir</code>: Location of Bitcoin data (required)</li> <li><code>--outputDir</code>: Output directory for UTXO set (required)</li> <li><code>--skipHeaders</code>: Skip processing headers</li> <li><code>--skipUTXOs</code>: Skip processing UTXOs</li> <li><code>--blockHash</code>: Block hash to start from</li> <li><code>--previousBlockHash</code>: Previous block hash</li> <li><code>--blockHeight</code>: Block height to start from</li> <li><code>--dumpRecords</code>: Dump records from index</li> </ul>"},{"location":"howto/miners/minersHowToTeranodeCLI/#check-block","title":"Check Block","text":"<pre><code>teranode-cli checkblock &lt;blockhash&gt;\n</code></pre> <p>Validates an existing block by its hash. This command performs comprehensive validation including:</p> <ul> <li>Transaction validation</li> <li>Merkle tree verification</li> <li>Proof of work validation</li> <li>Consensus rule checks</li> </ul> <p>Example:</p> <pre><code>teranode-cli checkblock 000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f\n</code></pre>"},{"location":"howto/miners/minersHowToTeranodeCLI/#file-reader","title":"File Reader","text":"<pre><code>teranode-cli filereader [path] [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--verbose</code>: Enable verbose output</li> <li><code>--checkHeights</code>: Check heights in UTXO headers</li> <li><code>--useStore</code>: Use store</li> </ul>"},{"location":"howto/miners/minersHowToTeranodeCLI/#fsm-state-management","title":"FSM State Management","text":"<pre><code>teranode-cli getfsmstate\n</code></pre> <p>Gets the current FSM state of the system.</p> <pre><code>teranode-cli setfsmstate --fsmstate=&lt;state&gt;\n</code></pre> <p>Options:</p> <ul> <li><code>--fsmstate</code>: Target FSM state (required)<ul> <li>Valid values: running, idle, catchingblocks, legacysyncing</li> </ul> </li> </ul>"},{"location":"howto/miners/minersHowToTeranodeCLI/#export-blocks","title":"Export Blocks","text":"<pre><code>teranode-cli export-blocks --file=&lt;path&gt;\n</code></pre> <p>Exports blockchain data to a CSV file.</p> <p>Options:</p> <ul> <li><code>--file</code>: CSV file path to export (required)</li> </ul>"},{"location":"howto/miners/minersHowToTeranodeCLI/#import-blocks","title":"Import Blocks","text":"<pre><code>teranode-cli import-blocks --file=&lt;path&gt;\n</code></pre> <p>Import blockchain data from a CSV file.</p> <p>Options:</p> <ul> <li><code>--file</code>: CSV file path to import (required)</li> </ul>"},{"location":"howto/miners/minersHowToTeranodeCLI/#check-block-template","title":"Check Block Template","text":"<pre><code>teranode-cli checkblocktemplate\n</code></pre> <p>Validates the current block template. Useful for miners to ensure block templates are correctly formed.</p>"},{"location":"howto/miners/minersHowToTeranodeCLI/#seeder","title":"Seeder","text":"<pre><code>teranode-cli seeder --inputDir=&lt;input-dir&gt; --hash=&lt;hash&gt; [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--inputDir</code>: Input directory for UTXO set and headers (required)</li> <li><code>--hash</code>: Hash of the UTXO set / headers to process (required)</li> <li><code>--skipHeaders</code>: Skip processing headers</li> <li><code>--skipUTXOs</code>: Skip processing UTXOs</li> </ul>"},{"location":"howto/miners/minersHowToTeranodeCLI/#utxo-validator","title":"UTXO Validator","text":"<pre><code>teranode-cli utxovalidator --inputDir=&lt;input-dir&gt; [options]\n</code></pre> <p>Validates UTXO sets by reading and verifying UTXO data from specified directories. This tool is useful for ensuring UTXO set integrity and detecting any inconsistencies.</p> <p>Options:</p> <ul> <li><code>--inputDir</code>: Input directory containing UTXO data (required)</li> <li><code>--blockHash</code>: Specific block hash to validate against</li> <li><code>--verbose</code>: Enable verbose output for detailed validation information</li> <li><code>--batchSize</code>: Number of UTXOs to process in each batch (default: 10000)</li> </ul> <p>Example:</p> <pre><code>teranode-cli utxovalidator --inputDir=/data/utxos --verbose\n</code></pre>"},{"location":"howto/miners/minersHowToTeranodeCLI/#fix-chainwork","title":"Fix Chainwork","text":"<pre><code>teranode-cli fix-chainwork --db-url=&lt;database-url&gt; [options]\n</code></pre> <p>Fixes incorrect chainwork values in the blockchain database. This command is used for database maintenance and should be used with caution.</p> <p>Options:</p> <ul> <li><code>--db-url</code>: Database URL (postgres://... or sqlite://...) (required)</li> <li><code>--dry-run</code>: Preview changes without updating database (default: true)</li> <li><code>--batch-size</code>: Number of updates to batch in a transaction (default: 1000)</li> <li><code>--start-height</code>: Starting block height (default: 650286)</li> <li><code>--end-height</code>: Ending block height (0 for current tip) (default: 0)</li> </ul> <p>\u26a0\ufe0f Warning: This command modifies blockchain database records. Always run with <code>--dry-run=true</code> first to preview changes before applying them to production databases.</p>"},{"location":"howto/miners/minersHowToTeranodeCLI/#error-handling","title":"Error Handling","text":"<p>The CLI will exit with status code 1 when:</p> <ul> <li>Invalid commands are provided</li> <li>Required arguments are missing</li> <li>Command execution fails</li> </ul>"},{"location":"howto/miners/minersHowToTeranodeCLI/#environment","title":"Environment","text":"<p>The CLI is available in all Teranode containers and automatically configured to work with the local Teranode instance.</p>"},{"location":"howto/miners/minersHowToUseListenMode/","title":"How to Use Listen Mode for Mining Operations","text":""},{"location":"howto/miners/minersHowToUseListenMode/#overview","title":"Overview","text":"<p>Listen Mode is a special operational mode for Teranode that allows nodes to receive blockchain data without broadcasting or propagating information to the network. This mode is particularly useful for mining operations that need to stay synchronized with the network while maintaining a low profile or reducing network traffic.</p>"},{"location":"howto/miners/minersHowToUseListenMode/#what-is-listen-mode","title":"What is Listen Mode?","text":"<p>Listen Mode configures a Teranode instance to:</p> <ul> <li>Receive all network messages (blocks, transactions, subtrees)</li> <li>Process incoming blockchain data normally</li> <li>Skip outbound message propagation</li> <li>Maintain peer connections through handshakes only</li> </ul> <p>This creates a \"silent observer\" node that stays synchronized without contributing to network message propagation.</p>"},{"location":"howto/miners/minersHowToUseListenMode/#use-cases-for-miners","title":"Use Cases for Miners","text":""},{"location":"howto/miners/minersHowToUseListenMode/#1-mining-pool-monitoring-nodes","title":"1. Mining Pool Monitoring Nodes","text":"<ul> <li>Monitor blockchain state without affecting network traffic</li> <li>Reduce bandwidth costs for monitoring-only nodes</li> <li>Maintain multiple observer nodes without network overhead</li> </ul>"},{"location":"howto/miners/minersHowToUseListenMode/#2-development-and-testing","title":"2. Development and Testing","text":"<ul> <li>Test mining software without propagating test blocks</li> <li>Monitor network behavior without participation</li> <li>Debug connectivity issues in isolation</li> </ul>"},{"location":"howto/miners/minersHowToUseListenMode/#3-geographic-distribution","title":"3. Geographic Distribution","text":"<ul> <li>Deploy lightweight monitoring nodes in multiple regions</li> <li>Reduce cross-region bandwidth usage</li> <li>Maintain global network visibility with minimal overhead</li> </ul>"},{"location":"howto/miners/minersHowToUseListenMode/#4-backup-and-failover-nodes","title":"4. Backup and Failover Nodes","text":"<ul> <li>Keep backup nodes synchronized without active participation</li> <li>Quick failover capability when needed</li> <li>Reduced resource usage for standby nodes</li> </ul>"},{"location":"howto/miners/minersHowToUseListenMode/#configuration","title":"Configuration","text":""},{"location":"howto/miners/minersHowToUseListenMode/#setting-listen-mode","title":"Setting Listen Mode","text":"<p>Add the following setting:</p> <pre><code># Enable listen-only mode\nlisten_mode = listen_only\n\n# Default mode (normal operation)\n# listen_mode = full\n</code></pre>"},{"location":"howto/miners/minersHowToUseListenMode/#available-modes","title":"Available Modes","text":"Mode Description Use Case <code>full</code> Normal operation - receives and sends all messages Active mining nodes <code>listen_only</code> Receives messages only - no outbound propagation Monitoring, backup nodes"},{"location":"howto/miners/minersHowToUseListenMode/#environment-variable","title":"Environment Variable","text":"<p>You can also set listen mode via environment variable:</p> <pre><code>export listen_mode=listen_only\n</code></pre>"},{"location":"howto/miners/minersHowToUseListenMode/#behavior-in-listen-mode","title":"Behavior in Listen Mode","text":""},{"location":"howto/miners/minersHowToUseListenMode/#what-still-works","title":"What Still Works","text":"<ol> <li>Peer Discovery and Connections</li> <li>Handshake messages are still sent (required for peer connections)</li> <li>Node maintains connections to peers</li> <li> <p>Receives peer announcements</p> </li> <li> <p>Data Reception</p> </li> <li>Receives all blocks</li> <li>Receives all transactions</li> <li>Receives all subtrees</li> <li> <p>Processes blockchain updates normally</p> </li> <li> <p>Local Operations</p> </li> <li>RPC interface remains fully functional</li> <li>Mining operations work normally</li> <li>Block validation continues</li> <li>UTXO management unchanged</li> </ol>"},{"location":"howto/miners/minersHowToUseListenMode/#what-is-disabled","title":"What is Disabled","text":"<ol> <li>Network Propagation</li> <li>Does not broadcast new blocks</li> <li>Does not propagate transactions</li> <li>Does not announce subtrees</li> <li> <p>Does not relay peer messages</p> </li> <li> <p>P2P Participation</p> </li> <li>Does not contribute to network topology</li> <li>Does not help with message distribution</li> <li>Minimal bandwidth usage</li> </ol>"},{"location":"howto/miners/minersHowToUseListenMode/#mining-considerations","title":"Mining Considerations","text":""},{"location":"howto/miners/minersHowToUseListenMode/#for-mining-pools","title":"For Mining Pools","text":"<p>When running a mining pool with listen mode:</p> <ol> <li>Primary Mining Node: Run in <code>full</code> mode</li> <li>This node submits mined blocks<ul> <li>Handles transaction propagation</li> </ul> </li> <li> <p>Participates fully in the network</p> </li> <li> <p>Monitor Nodes: Run in <code>listen_only</code> mode</p> </li> <li>Track blockchain state<ul> <li>Provide redundancy</li> </ul> </li> <li> <p>Reduce operational costs</p> </li> <li> <p>Configuration Example:</p> </li> </ol> <pre><code># Primary miner (full mode)\nlisten_mode = full\np2p_port = 9906\n\n# Monitor nodes (listen only)\nlisten_mode = listen_only\np2p_port = 9907  # Different port for each monitor\n</code></pre>"},{"location":"howto/miners/minersHowToUseListenMode/#for-solo-miners","title":"For Solo Miners","text":"<p>Solo miners typically should NOT use listen mode on their primary mining node because:</p> <ul> <li>Mined blocks won't be propagated</li> <li>You cannot claim mining rewards</li> <li>Your blocks won't be recognized by the network</li> </ul> <p>Use listen mode only for:</p> <ul> <li>Secondary monitoring nodes</li> <li>Development and testing</li> <li>Network analysis</li> </ul>"},{"location":"howto/miners/minersHowToUseListenMode/#best-practices","title":"Best Practices","text":""},{"location":"howto/miners/minersHowToUseListenMode/#1-network-architecture","title":"1. Network Architecture","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Internet  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Primary Node \u2502 (full mode)\n                    \u2502  (Mining)    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                 \u2502                 \u2502\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502Monitor Node\u2502   \u2502Monitor Node\u2502   \u2502Monitor Node\u2502\n  \u2502(listen_only)\u2502  \u2502(listen_only)\u2502  \u2502(listen_only)\u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"howto/miners/minersHowToUseListenMode/#2-switching-modes","title":"2. Switching Modes","text":"<p>To switch from listen mode to full mode:</p> <ol> <li>Stop the node</li> <li>Update configuration:</li> </ol> <pre><code>listen_mode = full\n</code></pre> <ol> <li>Restart the node</li> <li>Verify full participation via logs</li> </ol>"},{"location":"howto/miners/minersHowToUseListenMode/#conclusion","title":"Conclusion","text":"<p>Listen Mode provides miners with a powerful configuration option for:</p> <ul> <li>Reducing operational costs</li> <li>Improving network monitoring</li> <li>Enhancing security posture</li> <li>Optimizing resource usage</li> </ul> <p>Use it wisely as part of a comprehensive mining infrastructure strategy, ensuring your primary mining nodes always run in full mode to participate in the network and claim rewards.</p>"},{"location":"howto/miners/minersManagingDiskSpace/","title":"Managing Disk Space - Teranode","text":"<p>Key considerations and strategies:</p> <ol> <li> <p>Monitoring Disk Usage:</p> <ul> <li>Regularly check available disk space using tools like <code>df -h</code> or through the Grafana dashboard.</li> <li>Set up alerts to notify you when disk usage reaches certain thresholds (e.g., 80% or 85% full).</li> </ul> </li> <li> <p>Understanding Data Growth:</p> <ul> <li>The blockchain data, transaction store, and subtree store will grow over time as new blocks are added to the network.</li> <li>Growth rate depends on network activity and can vary significantly.</li> </ul> </li> <li> <p>Pruning Strategies:</p> <ul> <li>Teranode implements regular pruning of old data that's no longer needed for immediate operations.</li> <li>While retention policies for different data types are configurable, this is not documented in this guide.</li> </ul> </li> <li> <p>Log Management:</p> <ul> <li>Consider offloading logs to a separate storage system or log management service.</li> </ul> </li> <li> <p>Backup and Recovery:</p> <ul> <li>Implement a backup strategy that doesn't interfere with disk space management.</li> <li>Ensure backups are stored on separate physical media or cloud storage.</li> </ul> </li> <li> <p>Performance Impact:</p> <ul> <li>Be aware that very high disk usage (&gt;90%) can negatively impact performance.</li> <li>Monitor I/O performance alongside disk space usage.</li> </ul> </li> </ol>"},{"location":"howto/miners/docker/minersHowToConfigureTheNode/","title":"Configuring Docker Compose Teranode","text":"<p>Last modified: 22-January-2025</p>"},{"location":"howto/miners/docker/minersHowToConfigureTheNode/#index","title":"Index","text":"<ul> <li>Configuring Setting Files</li> <li>Optional vs Required services</li> <li>Reference Settings</li> </ul>"},{"location":"howto/miners/docker/minersHowToConfigureTheNode/#configuring-setting-files","title":"Configuring Setting Files","text":"<p>The Docker Compose installation provides a pre-configured set of settings to make the installation fully operational.</p> <p>However, you can change the configuration and override any of the default settings in your local <code>settings_local.conf</code> file. To check how this file is created, please review the Docker installation guide here.</p> <p>For a list of settings, and their default values, please refer to the reference at the end of this document.</p>"},{"location":"howto/miners/docker/minersHowToConfigureTheNode/#environment-variables","title":"Environment Variables","text":"<p>As an alternative to configuring settings in <code>settings_local.conf</code>, you can also overwrite any setting using environment variable. Please see the <code>x-teranode-settings</code> in your <code>docker-compose.yml</code> for an example of how to proceed.</p>"},{"location":"howto/miners/docker/minersHowToConfigureTheNode/#optional-vs-required-services","title":"Optional vs Required services","text":"<p>While most services are required for the proper functioning of Teranode, some services are optional and are disabled in Docker Compose. The following table provides an overview of the services and their status:</p> Required Optional Asset Server Block Persister Block Assembly UTXO Persister Block Validator Subtree Validator Blockchain Propagation P2P Legacy Gateway <p>The Block and UTXO persister services are optional and can be disabled. If enabled, your node will be in Archive Mode, storing historical block and UTXO data. This data can be useful for analytics and historical lookups but comes with additional storage and processing overhead. Additionally, it can be used as a backup for the UTXO store.</p>"},{"location":"howto/miners/docker/minersHowToConfigureTheNode/#settings-reference","title":"Settings Reference","text":"<p>You can find the pre-configured settings file here. You can refer to this document in order to identify the current system behaviour and in order to override desired settings in your <code>settings_local.conf</code>.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/","title":"How to Install Teranode with Docker Compose","text":"<p>Last modified: 28-Jul-2025</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#index","title":"Index","text":"<ul> <li>Introduction</li> <li>Prerequisites</li> <li>Hardware Requirements</li> <li>Software Requirements</li> <li>Network Considerations</li> <li>Installation Process<ul> <li>Teranode Initial Block Synchronization<ul> <li>Full P2P Download</li> <li>Initial Data Set Installation</li> </ul> </li> <li>Teranode Installation Types<ul> <li>Pre-built Binaries</li> <li>Docker Container</li> <li>Docker Compose</li> </ul> </li> <li>Installing Teranode with Docker Compose</li> </ul> </li> <li>Reference Settings</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#introduction","title":"Introduction","text":"<p>This guide provides step-by-step instructions for installing Teranode using Docker Compose. Notice that this approach is only recommended for testing purposes and not for production usage.</p> <p>This guide is applicable to:</p> <ol> <li> <p>Miners and node operators testing Teranode.</p> </li> <li> <p>Single-node Teranode setups, using Docker Compose setup.</p> </li> <li> <p>Configurations designed to connect to and process the BSV mainnet with current production load.</p> </li> </ol> <p>This guide does not cover:</p> <ol> <li> <p>Advanced clustering configurations.</p> </li> <li> <p>Scaled-out Teranode configurations. The Teranode installation described in this document is capable of handling the regular BSV production load, not the scaled-out 1 million tps configuration.</p> </li> <li> <p>Custom mining software integration.</p> </li> <li> <p>Advanced network configurations.</p> </li> <li> <p>Any sort of source code build or manipulation of any kind.</p> </li> </ol>"},{"location":"howto/miners/docker/minersHowToInstallation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine 17.03+</li> <li>Docker Compose</li> <li>AWS CLI</li> <li>100GB+ available disk space</li> <li>Stable internet connection</li> <li>Root or sudo access</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#hardware-requirements","title":"Hardware Requirements","text":"<p>The Teranode team will provide you with current hardware recommendations. These recommendations will be:</p> <ol> <li>Tailored to your specific configuration settings</li> <li>Designed to handle the expected production transaction volume</li> <li>Updated regularly to reflect the latest performance requirements</li> </ol> <p>This ensures your system is appropriately equipped to manage the projected workload efficiently.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#software-requirements","title":"Software Requirements","text":"<p>Teranode relies on a number of third-party software dependencies, some of which can be sourced from different vendors.</p> <p>BSV provides both a Docker Compose that initialises all dependencies within a single server node, and a <code>Kubernetes operator</code> that provides a production-live multi-node setup. This document covers the Docker Compose approach.</p> <p>This section will outline the various vendors in use in Teranode.</p> <p>To know more, please refer to the Third Party Reference Documentation</p> <p>Note - if using Docker Compose, the shared Docker storage, which is automatically managed by Docker Compose, is used instead. This approach provides a more accessible testing environment while still allowing for essential functionality and performance evaluation.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#network-considerations","title":"Network Considerations","text":"<p>Network requirements for running a Teranode BSV node:</p> <ul> <li>Inbound: 5-50 GB per day</li> <li>Outbound: 50-150 GB per day</li> </ul> <p>Key considerations:</p> <ol> <li>Ensure reliable internet connection with sufficient bandwidth</li> <li>Plan for higher bandwidth during initial blockchain synchronization</li> <li>Monitor network usage against ISP limits</li> </ol>"},{"location":"howto/miners/docker/minersHowToInstallation/#installation-process","title":"Installation Process","text":""},{"location":"howto/miners/docker/minersHowToInstallation/#teranode-initial-block-synchronization","title":"Teranode Initial Block Synchronization","text":"<p>Teranode requires an initial block synchronization to function properly. There are two approaches for completing the synchronization process.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#full-p2p-download","title":"Full P2P Download","text":"<ul> <li>Start the node and download all blocks from genesis using peer-to-peer (P2P) network.</li> <li>This method downloads the entire blockchain history.</li> </ul> <p>Pros:</p> <ul> <li>Simple to implement</li> <li>Ensures the node has the complete blockchain history</li> </ul> <p>Cons:</p> <ul> <li>Time-consuming process</li> <li>Can take 5-8 days, depending on available bandwidth</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#initial-data-set-installation","title":"Initial Data Set Installation","text":"<p>To speed up the initial synchronization process, you have the option to seed Teranode from pre-existing data. To know more about this approach, please refer to the How to Sync the Node guide.</p> <p>Pros:</p> <ul> <li>Significantly faster than full P2P download</li> <li>Allows for quicker node setup</li> </ul> <p>Cons:</p> <ul> <li>Requires additional steps</li> <li>The data set must be validated, to ensure it has not been tampered with</li> </ul> <p>Where possible, BSV recommends using the Initial Data Set Installation approach.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#teranode-installation-types","title":"Teranode Installation Types","text":"<p>Teranode supports four installation methods:</p> <ul> <li>Pre-built Binaries</li> <li>Docker Container</li> <li>Docker Compose</li> <li>Kubernetes Operator</li> </ul> <p>Note: Only Docker Compose (testing) and Kubernetes Operator (production) are officially supported.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#pre-built-binaries","title":"Pre-built Binaries","text":"<ul> <li>Pre-built executables are available for both arm64 and amd64 architectures.</li> <li>This method provides the most flexibility but requires manual setup of dependencies.</li> <li>Suitable for users who need fine-grained control over their installation.</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#docker-container","title":"Docker Container","text":"<ul> <li>A Docker image is published to a Docker registry, containing the Teranode binary and internal dependencies.</li> <li>This method offers a balance between ease of use and customization.</li> <li>Ideal for users familiar with Docker who want to integrate Teranode into existing container ecosystems.</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#docker-compose","title":"Docker Compose","text":"<ul> <li>A Docker Compose file is provided to alpha testing miners.</li> <li>This method sets up a single-node Teranode instance along with all required external dependencies.</li> <li> <p>Components included:</p> <ul> <li>Teranode Docker image</li> <li>Kafka</li> <li>PostgreSQL</li> <li>Aerospike</li> <li>Grafana</li> <li>Prometheus</li> <li>Docker Shared Storage</li> </ul> </li> <li> <p>Advantages:</p> <ul> <li>Easiest method to get a full Teranode environment running quickly.</li> <li>Automatically manages start order and networking between components.</li> <li>Used by developers and QA for testing, ensuring reliability.</li> </ul> </li> <li> <p>Considerations:</p> <ul> <li>This setup uses predefined external dependencies, which may not be customizable.</li> <li>While convenient for development and testing, it is not optimized, nor intended, for production usage.</li> </ul> </li> </ul> <p>Note: The Docker Compose method is recommended for testing in single node environments as it provides a consistent environment that mirrors the development setup. However, for production deployments or specific performance requirements, users need to consider using the Kubernetes operator approach (separate document).</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#installing-teranode-with-docker-compose","title":"Installing Teranode with Docker Compose","text":"<p>Prerequisites:</p> <ul> <li> <p>Docker and Docker Compose installed on your system</p> </li> <li> <p>The Alpha testing Teranode docker compose file should have been checked out in your system (see the next section)</p> </li> <li> <p>Sufficient disk space (at least 100GB recommended)</p> </li> <li> <p>A stable internet connection</p> </li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-1-create-repository","title":"Step 1: Create Repository","text":""},{"location":"howto/miners/docker/minersHowToInstallation/#open-terminal","title":"Open Terminal","text":"<p>Open a terminal or command prompt.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#clone-repository","title":"Clone Repository","text":"<p>Clone the Teranode repository:</p> <pre><code>cd $YOUR_WORKING_DIR\ngit clone git@github.com:bsv-blockchain/teranode.git\ncd teranode\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-2-configure-environment-settings-context-optional","title":"Step 2: Configure Environment Settings Context [Optional]","text":""},{"location":"howto/miners/docker/minersHowToInstallation/#create-environment-file","title":"Create Environment File","text":"<p>Create a <code>.env</code> file in the root directory of the project.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#set-context-optional","title":"Set Context (Optional)","text":"<p>While not required (your docker compose file will be preconfigured), the following line can be used to set the context:</p> <pre><code>SETTINGS_CONTEXT_1=docker.m\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#authenticate-with-aws-ecr","title":"Authenticate with AWS ECR","text":"<p>Authenticate with AWS ECR (please check with your Teranode team for the latest credentials).</p> <p>This step is not mandatory, but useful if you want to create settings variants for specific contexts.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-3-prepare-local-settings","title":"Step 3: Prepare Local Settings","text":""},{"location":"howto/miners/docker/minersHowToInstallation/#review-settings-file","title":"Review Settings File","text":"<p>You can see the current settings under the <code>$YOUR_WORKING_DIR/teranode/settings.conf</code> file. You can override any settings here.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#settings-reference","title":"Settings Reference","text":"<p>For a list of settings, and their default values, please refer to the reference at the end of this document.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-4-pull-docker-images","title":"Step 4: Pull Docker Images","text":""},{"location":"howto/miners/docker/minersHowToInstallation/#navigate-to-docker-compose-directory","title":"Navigate to Docker Compose Directory","text":"<p>Go to either the testnet Docker compose folder:</p> <pre><code>cd $YOUR_WORKING_DIR/teranode/deploy/docker/testnet\n</code></pre> <p>Or to the mainnet Docker compose folder:</p> <pre><code>cd $YOUR_WORKING_DIR/teranode/deploy/docker/mainnet\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#pull-docker-images","title":"Pull Docker Images","text":"<p>Pull the required Docker images:</p> <pre><code>docker-compose pull\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-5-start-the-teranode-stack","title":"Step 5: Start the Teranode Stack","text":"<ol> <li> <p>When running on a box without a public IP, you should enable <code>legacy_config_Upnp</code> (in your settings file), so you don't get banned by the SV Nodes.</p> </li> <li> <p>Launch the entire Teranode stack using Docker Compose:</p> </li> </ol> <pre><code>docker-compose up -d\n</code></pre> <p>Force the node to transition to Run mode:</p> <pre><code>docker exec -it blockchain teranode-cli setfsmstate --fsmstate running\n</code></pre> <p>or LegacySync mode:</p> <pre><code>docker exec -it blockchain teranode-cli setfsmstate --fsmstate legacysyncing\n</code></pre> <p>You can verify the current state with:</p> <pre><code>docker exec -it blockchain teranode-cli getfsmstate\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-6-verify-services","title":"Step 6: Verify Services","text":""},{"location":"howto/miners/docker/minersHowToInstallation/#check-service-status","title":"Check Service Status","text":"<p>Check if all services are running correctly:</p> <pre><code>docker-compose ps\n</code></pre> <p>Example output:</p> NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS aerospike aerospike:ce-7.1.0.6 <code>/usr/bin/as-tini-st\u2026</code> aerospike 2 minutes ago Up 2 minutes 0.0.0.0:3000-&gt;3000/tcp, :::3000-&gt;3000/tcp, 3001-3002/tcp aerospike-exporter aerospike/aerospike-prometheus-exporter:latest <code>/docker-entrypoint.\u2026</code> aerospike-exporter 2 minutes ago Up 2 minutes 9145/tcp asset ghcr.io/bsv-blockchain/teranode:v0.5.50 <code>/app/entrypoint.sh \u2026</code> asset 2 minutes ago Up 2 minutes (healthy) 0/tcp, 3005/tcp, 8098/tcp, 9292/tcp, 0.0.0.0:8090-&gt;8090/tcp, :::8090-&gt;8090/tcp, 0.0.0.0:32796-&gt;4040/tcp, [::]:32796-&gt;4040/tcp, 0.0.0.0:32797-&gt;8000/tcp, [::]:32797-&gt;8000/tcp, 0.0.0.0:32798-&gt;8091/tcp, [::]:32798-&gt;8091/tcp, 0.0.0.0:32799-&gt;9091/tcp, [::]:32799-&gt;9091/tcp blockassembly ghcr.io/bsv-blockchain/teranode:v0.5.50 <code>/app/entrypoint.sh \u2026</code> blockassembly 2 minutes ago Up 2 minutes (healthy) 0/tcp, 3005/tcp, 8098/tcp, 9292/tcp, 0.0.0.0:32768-&gt;4040/tcp, [::]:32768-&gt;4040/tcp, 0.0.0.0:32772-&gt;8000/tcp, [::]:32772-&gt;8000/tcp, 0.0.0.0:32774-&gt;8085/tcp, [::]:32774-&gt;8085/tcp, 0.0.0.0:32777-&gt;8285/tcp, [::]:32777-&gt;8285/tcp, 0.0.0.0:32779-&gt;9091/tcp, [::]:32779-&gt;9091/tcp blockchain ghcr.io/bsv-blockchain/teranode:v0.5.50 <code>/app/entrypoint.sh \u2026</code> blockchain 2 minutes ago Up 2 minutes (healthy) 0/tcp, 3005/tcp, 8098/tcp, 9292/tcp, 0.0.0.0:32769-&gt;4040/tcp, [::]:32769-&gt;4040/tcp, 0.0.0.0:32771-&gt;8000/tcp, [::]:32771-&gt;8000/tcp, 0.0.0.0:32775-&gt;8082/tcp, [::]:32775-&gt;8082/tcp, 0.0.0.0:32776-&gt;8087/tcp, [::]:32776-&gt;8087/tcp, 0.0.0.0:32778-&gt;9091/tcp, [::]:32778-&gt;9091/tcp blockvalidation ghcr.io/bsv-blockchain/teranode:v0.5.50 <code>/app/entrypoint.sh \u2026</code> blockvalidation 2 minutes ago Up 2 minutes (healthy) 0/tcp, 3005/tcp, 8098/tcp, 9292/tcp, 0.0.0.0:32781-&gt;4040/tcp, [::]:32781-&gt;4040/tcp, 0.0.0.0:32787-&gt;8000/tcp, [::]:32787-&gt;8000/tcp, 0.0.0.0:32788-&gt;8088/tcp, [::]:32788-&gt;8088/tcp, 0.0.0.0:32790-&gt;8188/tcp, [::]:32790-&gt;8188/tcp, 0.0.0.0:32792-&gt;9091/tcp, [::]:32792-&gt;9091/tcp grafana grafana/grafana:latest <code>/run.sh</code> grafana 2 minutes ago Up 2 minutes 0.0.0.0:3005-&gt;3000/tcp, [::]:3005-&gt;3000/tcp kafka-console-shared docker.redpanda.com/redpandadata/console:latest <code>/bin/sh -c 'echo \\\"$\u2026'</code> kafka-console-shared 2 minutes ago Up 2 minutes 0.0.0.0:8080-&gt;8080/tcp, :::8080-&gt;8080/tcp kafka-shared vectorized/redpanda:latest <code>/entrypoint.sh 'red\u2026'</code> kafka-shared 2 minutes ago Up 2 minutes 8082/tcp, 9644/tcp, 0.0.0.0:9092-9093-&gt;9092-9093/tcp, :::9092-9093-&gt;9092-9093/tcp, 0.0.0.0:32794-&gt;8081/tcp, [::]:32794-&gt;8081/tcp legacy ghcr.io/bsv-blockchain/teranode:v0.5.50 <code>/app/entrypoint.sh \u2026</code> legacy 2 minutes ago Up 2 minutes 0/tcp, 3005/tcp, 0.0.0.0:8333-&gt;8333/tcp, :::8333-&gt;8333/tcp, 9292/tcp, 0.0.0.0:18333-&gt;18333/tcp, :::18333-&gt;18333/tcp, 0.0.0.0:32782-&gt;4040/tcp, [::]:32782-&gt;4040/tcp, 0.0.0.0:32783-&gt;8000/tcp, [::]:32783-&gt;8000/tcp, 0.0.0.0:32784-&gt;8098/tcp, [::]:32784-&gt;8098/tcp, 0.0.0.0:32785-&gt;8099/tcp, [::]:32785-&gt;8099/tcp, 0.0.0.0:32786-&gt;9091/tcp, [::]:32786-&gt;9091/tcp postgres postgres:latest <code>docker-entrypoint.s\u2026</code> postgres 2 minutes ago Up 2 minutes (healthy) 0.0.0.0:5432-&gt;5432/tcp, :::5432-&gt;5432/tcp prometheus prom/prometheus:v2.44.0 <code>/bin/prometheus --c\u2026</code> prometheus 2 minutes ago Up 2 minutes 0.0.0.0:9090-&gt;9090/tcp, :::9090-&gt;9090/tcp rpc ghcr.io/bsv-blockchain/teranode:v0.5.50 <code>/app/entrypoint.sh \u2026</code> rpc 2 minutes ago Up 2 minutes 0/tcp, 3005/tcp, 8098/tcp, 0.0.0.0:9292-&gt;9292/tcp, :::9292-&gt;9292/tcp, 0.0.0.0:32770-&gt;4040/tcp, [::]:32770-&gt;4040/tcp, 0.0.0.0:32773-&gt;8000/tcp, [::]:32773-&gt;8000/tcp, 0.0.0.0:32780-&gt;9091/tcp, [::]:32780-&gt;9091/tcp subtreevalidation ghcr.io/bsv-blockchain/teranode:v0.5.50 <code>/app/entrypoint.sh \u2026</code> subtreevalidation 2 minutes ago Up 2 minutes (healthy) 0/tcp, 3005/tcp, 8098/tcp, 9292/tcp, 0.0.0.0:32789-&gt;4040/tcp, [::]:32789-&gt;4040/tcp, 0.0.0.0:32791-&gt;8000/tcp, [::]:32791-&gt;8000/tcp, 0.0.0.0:32793-&gt;8086/tcp, [::]:32793-&gt;8086/tcp, 0.0.0.0:32795-&gt;9091/tcp, [::]:32795-&gt;9091/tcp"},{"location":"howto/miners/docker/minersHowToInstallation/#verify-service-health","title":"Verify Service Health","text":"<p>Ensure all services show a status of \"Up\" or \"Healthy\".</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-7-access-monitoring-tools","title":"Step 7: Access Monitoring Tools","text":"<ol> <li> <p>Grafana: Access the Grafana dashboard at <code>http://localhost:3005</code></p> <ul> <li>Default credentials are <code>admin/admin</code></li> <li>Navigate to the \"Teranode - Service Overview\" dashboard for key metrics</li> <li>Explore other dashboards for detailed service metrics. For example, you can check the Legacy sync metrics in the \"Teranode - Legacy Service\" dashboard.</li> </ul> </li> <li> <p>Kafka Console: Available internally on port 8080</p> </li> <li> <p>Prometheus: Available internally on port 9090</p> </li> <li> <p>Teranode Blockchain Viewer: A basic blockchain viewer is available and can be accessed via the asset container. It provides an interface to browse blockchain data.</p> <ul> <li>Port: Exposed on port 8090 of the asset container.</li> <li>Access URL: http://localhost:8090/viewer</li> </ul> </li> </ol> <p>Note: You must set the setting <code>dashboard_enabled</code> as true in order to see the viewer.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-8-interact-with-teranode","title":"Step 8: Interact with Teranode","text":"<ul> <li> <p>The various Teranode services expose different ports for interaction:</p> <ul> <li>Blockchain service: Port 8082</li> <li>Asset service: Ports 8090</li> <li>Miner service: Ports 8089, 8092, 8099</li> <li>P2P service: Ports 9905, 9906</li> <li>RPC service: 9292</li> </ul> </li> </ul> <p>Notice that those ports might be mapped to random ports on your host machine. You can check the mapping by running <code>docker-compose ps</code>.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-9-logging-and-troubleshooting","title":"Step 9: Logging and Troubleshooting","text":""},{"location":"howto/miners/docker/minersHowToInstallation/#view-all-service-logs","title":"View All Service Logs","text":"<p>View logs for all services:</p> <pre><code>docker-compose logs\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#view-specific-service-logs","title":"View Specific Service Logs","text":"<p>View logs for a specific service (e.g., teranode-blockchain):</p> <pre><code>docker-compose logs -f legacy\ndocker-compose logs -f blockchain\ndocker-compose logs -f asset\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-10-docker-log-rotation","title":"Step 10: Docker Log Rotation","text":"<p>Teranode is very verbose and will output a lot of information, especially with logLevel=DEBUG. To avoid running out of disk space, you can specify logging options directly in your docker-compose.yml file for each service.</p> <pre><code>services:\n  [servicename]:\n    logging:\n      options:\n        max-size: \"100m\"\n        max-file: \"3\"\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-11-stopping-the-stack","title":"Step 11: Stopping the Stack","text":"<ol> <li>To stop all services:</li> </ol> <pre><code>docker-compose down\n</code></pre> <p>Additional Notes:</p> <ul> <li>The <code>data</code> directory (under <code>testnet</code> or <code>mainnet</code>) will contain persistent data. This includes blockchain data and other persistent storage required by the Teranode components. By default, Docker Compose is configured to mount these directories into the respective containers, ensuring that data persists across restarts and container recreation. Ensure regular backups.</li> <li>Under no circumstances should you use this Docker Compose approach for production usage.</li> <li>Please discuss with the Teranode support team for advanced configuration options and optimizations not covered in the present document.</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#optimizations","title":"Optimizations","text":"<p>If you have local access to SV Nodes, you can use them to speed up the initial block synchronization too. You can set <code>legacy_connect_peers: \"172.x.x.x:8333|10.x.x.x:8333\"</code> in your <code>docker-compose.yml</code> to force the legacy service to only connect to those peers.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#aerospike-operations-and-configuration","title":"Aerospike Operations and Configuration","text":"<p>Teranode includes Aerospike 8.0 community edition for storing the UTXO set. The Aerospike database stores all indexes in memory and the data on disk.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#aerospike-configuration","title":"Aerospike Configuration","text":"<p>The default configuration is set to stop writing when 50% of the system memory has been consumed. To future proof, you might want to run a dedicated cluster, with more memory and disk space allocated.</p> <p>See aerospike.conf / stop-writes-sys-memory-pct</p> <p>By default, the Aerospike data is written to a single mount mounted in the <code>aerospike</code> container. For performance reasons, it is recommended to use at least 4 dedicated disks for the Aerospike data.</p> <p>See aerospike.conf / storage-engine</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#aerospike-administration","title":"Aerospike Administration","text":"<p>You can access the Aerospike container and run administrative commands:</p> <pre><code>docker exec -it aerospike /bin/bash\n\n# Command for basic sanity checking\nasadm -e \"info\"\nasadm -e \"summary -l\"\n</code></pre> <p>For more information about Aerospike, you can access the Aerospike documentation.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#docker-logs-management","title":"Docker Logs Management","text":"<p>Teranode is very verbose and will output a lot of information, especially with <code>logLevel=DEBUG</code>. Make sure to setup log rotation for the containers to avoid running out of disk space.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#system-wide-docker-log-configuration","title":"System-wide Docker Log Configuration","text":"<pre><code>sudo bash -c 'cat &lt;&lt;EOF &gt; /etc/docker/daemon.json\n{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"100m\"\n  }\n}\nEOF'\nsudo systemctl restart docker\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#per-service-log-configuration","title":"Per-Service Log Configuration","text":"<p>Alternatively, you can specify logging options directly in your docker-compose.yml file for each service:</p> <pre><code>services:\n  [servicename]:\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"100m\"\n        max-file: \"3\"\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#teranode-network-architecture","title":"Teranode Network Architecture","text":"<p>Teranode offers two network connectivity options:</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#svnode-p2p-network","title":"SVNode P2P Network","text":"<ul> <li>Description: All data transmitted over P2P connections (via <code>legacy</code> service)</li> <li>Use Case: Traditional Bitcoin SV network connectivity</li> <li>Characteristics: Compatible with existing SV Node infrastructure</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#teranode-p2p-network","title":"Teranode P2P Network","text":"<ul> <li>Description: Only small messages over P2P, with bulk data downloaded via HTTP(S) (via <code>peer</code> and <code>asset</code> services)</li> <li>Use Case: Enhanced performance and stability</li> <li>Characteristics: Significant performance advantages over traditional SV Node network</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#teranode-network-requirements","title":"Teranode Network Requirements","text":"<p>To connect to the Teranode network, you'll need:</p> <ol> <li>Public-facing <code>peer</code> service exposed via TCP (defaults to port 9905, P2P_PORT setting)</li> <li>Public-facing <code>asset</code> service exposed via HTTP(S) (selective endpoint exposure recommended)</li> <li>Valid SSL certificates if using HTTPS for the asset service</li> </ol>"},{"location":"howto/miners/docker/minersHowToInstallation/#asset-service-setup","title":"Asset Service Setup","text":"<p>The asset service provides HTTP access to blockchain data. For production deployments, you should configure proper caching and security.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#basic-asset-service-configuration","title":"Basic Asset Service Configuration","text":"<p>Update your configuration with your publicly accessible endpoints:</p> <pre><code>asset_httpPublicAddress = https://teranode-1-asset-cache.example.com/api/v1\npeer_p2pPublicAddress = /ip4/1.2.3.4/tcp/9905\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#asset-service-with-nginx-caching","title":"Asset Service with Nginx Caching","text":"<p>For the <code>asset</code> service, we have provided an <code>asset-cache</code> example using nginx to limit endpoints and provide a caching mechanism. This improves performance and provides better security by exposing only necessary endpoints.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#asset-service-viewer","title":"Asset Service Viewer","text":"<p>The asset service includes a web-based viewer for blockchain data:</p> <ul> <li>Port: Exposed on port 8090 of the asset container</li> <li>Access URL: <code>http://localhost:8090/viewer</code></li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#rpc-interface","title":"RPC Interface","text":"<p>A SVNode compatible RPC interface is available for interacting with Teranode programmatically.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#rpc-configuration","title":"RPC Configuration","text":"<ul> <li>Port: 9292</li> <li>Default Credentials: <code>bitcoin:bitcoin</code></li> <li>Protocol: JSON-RPC over HTTP</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#rpc-usage-example","title":"RPC Usage Example","text":"<pre><code>curl --user bitcoin:bitcoin --data-binary '{\"jsonrpc\":\"1.0\",\"id\":\"curltext\",\"method\":\"version\",\"params\":[]}' -H 'content-type: text/plain;' http://localhost:9292/\n</code></pre> <p>Note: Most methods have not been implemented yet. The RPC interface is primarily for compatibility and basic operations.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#teranode-cli-command-reference","title":"teranode-cli Command Reference","text":"<p>The <code>teranode-cli</code> is a command-line tool that can be used to interact with the Teranode services. It is available in all containers.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#accessing-teranode-cli","title":"Accessing teranode-cli","text":"<pre><code>docker exec -it blockchain teranode-cli\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#available-commands","title":"Available Commands","text":"<p>Running <code>teranode-cli</code> without arguments will show a list of all available commands:</p> <pre><code>Usage: teranode-cli &lt;command&gt; [options]\n\nAvailable Commands:\n  aerospikereader      Aerospike Reader\n  checkblocktemplate   Check block template\n  export-blocks        Export blockchain to CSV\n  filereader           File Reader\n  getfsmstate          Get the current FSM State\n  import-blocks        Import blockchain from CSV\n  seeder               Seeder\n  setfsmstate          Set the FSM State\n  settings             Settings\n\nUse 'teranode-cli &lt;command&gt; --help' for more information about a command\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#common-cli-operations","title":"Common CLI Operations","text":"<p>Check FSM State:</p> <pre><code>docker exec -it blockchain teranode-cli getfsmstate\n</code></pre> <p>Set FSM State to Legacy Syncing:</p> <pre><code>docker exec -it blockchain teranode-cli setfsmstate --fsmstate LEGACYSYNCING\n</code></pre> <p>Set FSM State to Running:</p> <pre><code>docker exec -it blockchain teranode-cli setfsmstate --fsmstate RUNNING\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#teranode-reset-procedures","title":"Teranode Reset Procedures","text":"<p>If you require to sync a Teranode from scratch, you will need to clean-up data from Aerospike, Postgres, and your filesystem.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#1-aerospike-clean-up","title":"1. Aerospike Clean-up","text":"<p>See the Aerospike documentation for more information.</p> <pre><code># Access Aerospike container\ndocker exec -it aerospike /bin/bash\n\n# Truncate the UTXO set\nasadm --enable -e \"manage truncate ns utxo-store set utxo\"\n\n# Verify the total records count, should slowly decrease to 0\nasadm -e \"info\"\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#2-postgres-clean-up","title":"2. Postgres Clean-up","text":"<pre><code># Access Postgres container\ndocker exec -it postgres psql -U postgres\n\n# Connect to the database used by Teranode\npostgres=&gt; \\c &lt;db_name&gt;\n\n# Sanity count check\nteranode_mainnet=&gt; SELECT COUNT(*) FROM blocks;\n count\n--------\n 123123\n(1 row)\n\n# Truncate the blocks and state tables\nTRUNCATE TABLE blocks RESTART IDENTITY CASCADE;\nTRUNCATE TABLE state RESTART IDENTITY CASCADE;\nTRUNCATE TABLE bans RESTART IDENTITY CASCADE;\nTRUNCATE TABLE alert_system_alert_messages RESTART IDENTITY CASCADE;\nTRUNCATE TABLE alert_system_public_keys RESTART IDENTITY CASCADE;\n\n# Verify the table was truncated\nSELECT COUNT(*) FROM blocks;\n count\n-------\n     0\n(1 row)\n\n# Dropping these indexes will significantly speed up seeding\n# The blockchain service will re-create them\nDROP INDEX idx_chain_work_id;\nDROP INDEX idx_chain_work_peer_id;\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#3-filesystem-clean-up","title":"3. Filesystem Clean-up","text":"<pre><code># Define your data mount point\nDATA_MOUNT_POINT=/mnt/teranode\nsudo rm -rf $DATA_MOUNT_POINT/*\n\n# Or for Docker Compose setup\nsudo rm -rf ~/teranode-public/docker/testnet/data/*\nsudo rm -rf ~/teranode-public/docker/mainnet/data/*\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#teranode-seeding","title":"Teranode Seeding","text":"<p>If you have access to an SV Node, you can speed up the Initial Block Download (IBD) by seeding the Teranode with an export from SV Node. Only run this on a gracefully shut down SV Node instance (e.g., after using the RPC <code>stop</code> method).</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-1-export-from-sv-node","title":"Step 1: Export from SV Node","text":"<pre><code>docker run -it \\\n  -v /mnt/teranode/seed:/mnt/teranode/seed \\\n  --entrypoint=\"\" \\\n  434394763103.dkr.ecr.eu-north-1.amazonaws.com/teranode-public:v0.8.12 \\\n  /app/teranode-cli bitcointoutxoset -bitcoinDir=/home/ubuntu/bitcoin-data -outputDir=/mnt/teranode/seed/export\n</code></pre> <p>This script assumes your <code>bitcoin-data</code> directory is located in <code>/home/ubuntu/bitcoin-data</code> and contains the <code>blocks</code> and <code>chainstate</code> directories. It will generate <code>${blockhash}.utxo-headers</code> and <code>${blockhash}.utxo-set</code> files.</p> <p>Note: If you get a <code>Block hash mismatch between last block and chainstate</code> error message, you should try starting and stopping the SV Node again, it means there are still a few unprocessed blocks.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-2-seed-teranode","title":"Step 2: Seed Teranode","text":"<p>Once you have the export, you can seed any fresh Teranode with the following command:</p> <pre><code>docker run -it \\\n  -e SETTINGS_CONTEXT=docker.m \\\n  -v ${teranode-location}/docker/base/settings_local.conf:/app/settings_local.conf \\\n  -v ${teranode-location}/docker/mainnet/data/teranode:/app/data \\\n  -v /mnt/teranode/seed:/mnt/teranode/seed \\\n  --network my-teranode-network \\\n  --entrypoint=\"\" \\\n  434394763103.dkr.ecr.eu-north-1.amazonaws.com/teranode-public:v0.8.12 \\\n  /app/teranode-cli seeder -inputDir /mnt/teranode/seed -hash 0000000000013b8ab2cd513b0261a14096412195a72a0c4827d229dcc7e0f7af\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#step-3-complete-seeding-process","title":"Step 3: Complete Seeding Process","text":"<p>For Docker Compose setup, use the following instructions:</p> <pre><code># Stop all services\ndocker compose down\n\n# Clear out the postgres, aerospike and external data\nsudo rm -rf ~/teranode-public/docker/testnet/data/*\n\n# Bring up the dependent services\n# Blockchain service will insert the correct genesis block for your selected network\ndocker compose up -d aerospike postgres kafka-shared blockchain\n\n# Wait to make sure genesis block gets inserted\n# You will see it in the blockchain logs as `genesis block inserted`\ndocker compose logs -f -n 100 blockchain\n\n# Run the seeder (see command above)\ndocker run -it ...\n\n# Bring down blockchain to reset the internal caches\ndocker compose down blockchain\n\n# Bring all other services back online\ndocker compose up -d\n\n# Transition Teranode to LEGACYSYNCING\ndocker exec -it blockchain teranode-cli setfsmstate --fsmstate LEGACYSYNCING\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#cpu-mining","title":"CPU Mining","text":"<p>For CPU mining setup with Teranode, please refer to the dedicated guide:</p> <ul> <li>CPU Mining Guide - Complete setup instructions for CPU mining with Teranode</li> </ul> <p>This guide covers the BSV CPU miner configuration, parameters, troubleshooting, and usage examples.</p> <p>Note: The example address used here is a testnet address, linked to the BSV Faucet. For mainnet mining, use a valid mainnet address.</p>"},{"location":"howto/miners/docker/minersHowToInstallation/#advanced-configuration-examples","title":"Advanced Configuration Examples","text":""},{"location":"howto/miners/docker/minersHowToInstallation/#private-network-configuration","title":"Private Network Configuration","text":"<p>When running on a box without a public IP, you should enable <code>legacy_config_Upnp</code> in your settings file to avoid getting banned by the SV Nodes:</p> <pre><code>legacy_config_Upnp = true\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#peer-connection-optimization","title":"Peer Connection Optimization","text":"<p>If you have local access to SV Nodes, that will speed up the IBD. You can set specific peer connections in your docker-compose.yml:</p> <pre><code>environment:\n\n  - legacy_config_ConnectPeers=172.x.x.x:8333|10.x.x.x:8333\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#settings-override-examples","title":"Settings Override Examples","text":"<p>You can override any setting using environment variables in your docker-compose.yml. See the <code>x-teranode-settings</code> section for examples:</p> <pre><code>x-teranode-settings: &amp;teranode-settings\n  SETTINGS_CONTEXT: docker.m\n  # Override specific settings\n  logLevel: DEBUG\n  legacy_config_Upnp: \"true\"\n  asset_httpPublicAddress: \"https://your-domain.com/api/v1\"\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#network-specific-settings","title":"Network-Specific Settings","text":"<p>Testnet Configuration:</p> <pre><code>environment:\n\n  - SETTINGS_CONTEXT=docker.t\n  - legacy_config_TestNet=true\n</code></pre> <p>Mainnet Configuration:</p> <pre><code>environment:\n\n  - SETTINGS_CONTEXT=docker.m\n  - legacy_config_TestNet=false\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"howto/miners/docker/minersHowToInstallation/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Issue: Container fails to start with permission errors Solution: Ensure proper file permissions on data directories:</p> <pre><code>sudo chown -R 1000:1000 ./data/\n</code></pre> <p>Issue: Aerospike stops writing due to memory limits Solution: Increase system memory or adjust <code>stop-writes-sys-memory-pct</code> in aerospike.conf</p> <p>Issue: Postgres connection errors Solution: Check database initialization and ensure proper network connectivity between containers</p> <p>Issue: Slow synchronization Solution:</p> <ul> <li>Use seeding process for faster initial sync</li> <li>Configure specific peer connections for better connectivity</li> <li>Ensure adequate system resources (CPU, memory, disk I/O)</li> </ul>"},{"location":"howto/miners/docker/minersHowToInstallation/#log-analysis","title":"Log Analysis","text":"<p>Monitor service logs for troubleshooting:</p> <pre><code># Follow logs for all containers\ndocker compose logs -f -n 100\n\n# Follow logs for specific container\ndocker logs -f legacy -n 100\n\n# Check container health status\ndocker ps --format \"table {{.Names}}\\t{{.Status}}\\t{{.Ports}}\"\n</code></pre>"},{"location":"howto/miners/docker/minersHowToInstallation/#settings-reference_1","title":"Settings Reference","text":"<p>You can find the pre-configured settings file. You can refer to this document in order to identify the current system behaviour and in order to override desired settings in your <code>settings_local.conf</code>.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/","title":"Docker Compose - Starting and Stopping Teranode","text":"<p>Last modified: 22-January-2025</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#index","title":"Index","text":"<ul> <li>Starting the Node</li> <li>Stopping the Node</li> </ul>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#starting-the-node","title":"Starting the Node","text":"<p>Starting your Teranode instance involves initializing all the necessary services in the correct order. The Docker Compose configuration file handles this complexity for us.</p> <p>This guide assumes you have previously followed the installation guide and have a working Teranode setup.</p> <p>Follow these steps to start your node:</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-1-navigate-to-the-teranode-directory","title":"Step 1: Navigate to the Teranode Directory","text":"<p>Go to either the testnet Docker compose folder:</p> <pre><code>cd $YOUR_WORKING_DIR/teranode/deploy/docker/testnet\n</code></pre> <p>Or to the mainnet Docker compose folder:</p> <pre><code>cd $YOUR_WORKING_DIR/teranode/deploy/docker/mainnet\n</code></pre>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-2-pull-latest-images-optional-but-recommended-before-each-start","title":"Step 2: Pull Latest Images (optional, but recommended before each start)","text":"<pre><code>docker-compose pull\n</code></pre>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-3-start-the-teranode-stack","title":"Step 3: Start the Teranode Stack","text":"<pre><code>docker-compose up -d\n</code></pre> <p>This command starts all services defined in the <code>docker-compose.yml</code> file in detached mode.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-4-verify-service-startup","title":"Step 4: Verify Service Startup","text":"<pre><code>docker-compose ps\n</code></pre> <p>Ensure all services show a status of \"Up\" or \"Healthy\".</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-5-monitor-startup-logs","title":"Step 5: Monitor Startup Logs","text":"<pre><code>docker-compose logs -f\n</code></pre> <p>This allows you to watch the startup process in real-time.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-6-check-individual-service-logs","title":"Step 6: Check Individual Service Logs","text":"<p>If a specific service isn't starting correctly, check its logs. Example:</p> <pre><code>docker-compose logs -f legacy\ndocker-compose logs -f blockchain\ndocker-compose logs -f asset\n</code></pre>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-7-verify-network-connections","title":"Step 7: Verify Network Connections","text":"<p>Once all services are up, ensure the node is responsive:</p> <pre><code>curl --user bitcoin:bitcoin --data-binary '{\"jsonrpc\": \"1.0\", \"id\": \"curltest\", \"method\": \"version\", \"params\": []}' -H 'content-type: text/plain;' http://127.0.0.1:9292/\n</code></pre> <pre><code>curl --user bitcoin:bitcoin --data-binary '{\"jsonrpc\": \"1.0\", \"id\": \"curltest\", \"method\": \"getinfo\", \"params\": []}' -H 'content-type: text/plain;' http://127.0.0.1:9292/\n</code></pre> <p>And that it is connected to its peers:</p> <pre><code>curl --user bitcoin:bitcoin --data-binary '{\"jsonrpc\": \"1.0\", \"id\": \"curltest\", \"method\": \"getpeerinfo\", \"params\": []}' -H 'content-type: text/plain;' http://127.0.0.1:9292/\n</code></pre>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-8-check-synchronization-status","title":"Step 8: Check Synchronization Status","text":"<p>Monitor the blockchain synchronization process:</p> <pre><code>curl --user bitcoin:bitcoin --data-binary '{\"jsonrpc\": \"1.0\", \"id\": \"curltest\", \"method\": \"getblockchaininfo\", \"params\": []}' -H 'content-type: text/plain;' http://127.0.0.1:9292/\n</code></pre>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-9-access-monitoring-tools","title":"Step 9: Access Monitoring Tools","text":"<ul> <li>Open Grafana at http://localhost:3005 to view system metrics.</li> <li>Check Prometheus at http://localhost:9090 for raw metrics data.</li> </ul>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-10-troubleshooting","title":"Step 10: Troubleshooting","text":"<ul> <li>If any service fails to start, check its specific logs and configuration.</li> <li>Ensure all required ports are open and not conflicting with other applications.</li> <li>Verify that all external dependencies (Kafka, Postgres, Aerospike) are running correctly.</li> </ul>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-11-post-start-checks","title":"Step 11: Post-start Checks","text":"<p>Remember, the initial startup may take some time, especially if this is the first time starting the node or if there's a lot of blockchain data to sync. Be patient and monitor the logs for any issues.</p> <p>For subsequent starts after the initial setup, you typically only need to run the <code>docker-compose up -d</code> command, unless you've made configuration changes or updates to the system.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#stopping-the-node","title":"Stopping the Node","text":"<p>Properly stopping your Teranode instance is crucial to maintain data integrity and prevent potential issues. Follow these steps to safely stop your node:</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-1-navigate-to-the-teranode-directory_1","title":"Step 1: Navigate to the Teranode Directory","text":"<p>Go to either the testnet Docker compose folder:</p> <pre><code>cd $YOUR_WORKING_DIR/teranode/deploy/docker/testnet\n</code></pre> <p>Or to the mainnet Docker compose folder:</p> <pre><code>cd $YOUR_WORKING_DIR/teranode/deploy/docker/mainnet\n</code></pre>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-2-graceful-shutdown","title":"Step 2: Graceful Shutdown","text":"<p>To stop all services defined in your <code>docker-compose.yml</code> file:</p> <pre><code>docker-compose down\n</code></pre> <p>This command stops and removes all containers, but preserves your data volumes.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-3-verify-shutdown","title":"Step 3: Verify Shutdown","text":"<p>Ensure all services have stopped:</p> <pre><code>docker-compose ps\n</code></pre> <p>This should show no running containers related to your Teranode setup.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-4-check-for-any-lingering-processes","title":"Step 4: Check for Any Lingering Processes","text":"<pre><code>docker ps\n</code></pre> <p>Verify that no Teranode-related containers are still running.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-5-stop-specific-services-if-needed","title":"Step 5: Stop Specific Services (if needed)","text":"<p>If you need to stop only specific services:</p> <pre><code>docker-compose stop [service-name]\n</code></pre> <p>Replace [service-name] with the specific service you want to stop, e.g., teranode-blockchain.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-6-forced-shutdown-use-with-caution","title":"Step 6: Forced Shutdown (use with caution!)","text":"<p>If services aren't responding to the normal shutdown command:</p> <pre><code>docker-compose down --timeout 30\n</code></pre> <p>This forces a shutdown after 30 seconds. Adjust the timeout as needed.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-7-cleanup-optional","title":"Step 7: Cleanup (optional)","text":"<p>To remove all stopped containers and free up space:</p> <pre><code>docker system prune\n</code></pre> <p>Be cautious with this command as it removes all stopped containers, not just Teranode-related ones.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-8-data-preservation","title":"Step 8: Data Preservation","text":"<p>The <code>docker-compose down</code> command doesn't remove volumes by default. Your data should be preserved in the ./data directory.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-9-backup-consideration","title":"Step 9: Backup Consideration","text":"<p>Consider creating a backup of your data before shutting down, especially if you plan to make changes to your setup.</p>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-10-monitoring-shutdown","title":"Step 10: Monitoring Shutdown","text":"<p>Watch the logs during shutdown to ensure services are stopping cleanly:</p> <pre><code>docker-compose logs -f\n</code></pre>"},{"location":"howto/miners/docker/minersHowToStopStartDockerTeranode/#step-11-restart-preparation","title":"Step 11: Restart Preparation","text":"<p>If you plan to restart soon, no further action is needed. Your data and configurations will be preserved for the next startup.</p> <p>Important: Always Use Graceful Shutdown</p> <p>Remember, always use the graceful shutdown method (<code>docker-compose down</code>) unless absolutely necessary to force a shutdown. This ensures that all services have time to properly close their connections, flush data to disk, and perform any necessary cleanup operations.</p>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/","title":"How to Troubleshoot Teranode (Docker Compose)","text":"<p>Last modified: 22-January-2025</p>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#index","title":"Index","text":"<ul> <li>Introduction</li> <li>Troubleshooting<ul> <li>Health Checks and System Monitoring<ul> <li>Service Status</li> <li>Detailed Container/Pod Health</li> <li>Monitoring System Resources</li> </ul> </li> <li>Recovery Procedures<ul> <li>Third Party Component Failure</li> </ul> </li> </ul> </li> </ul>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#introduction","title":"Introduction","text":"<p>This guide provides troubleshooting steps for common issues encountered while running Teranode with Docker Compose.</p>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"howto/miners/docker/minersHowToTroubleshooting/#health-checks-and-system-monitoring","title":"Health Checks and System Monitoring","text":""},{"location":"howto/miners/docker/minersHowToTroubleshooting/#service-status","title":"Service Status","text":"<p><pre><code>docker-compose ps\n</code></pre> This command lists all services defined in your docker-compose.yml file, along with their current status (Up, Exit, etc.) and health state if health checks are configured.</p>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#detailed-containerpod-health","title":"Detailed Container/Pod Health","text":"<pre><code>docker inspect --format='{{json .State.Health}}' container_name\n</code></pre> <p>Replace <code>container_name</code> with the name of your specific Teranode service container. For example:</p> <pre><code>docker inspect --format='{{json .State.Health}}' asset\n</code></pre>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#monitoring-system-resources","title":"Monitoring System Resources","text":"<ul> <li> <p>Use <code>docker stats</code> to monitor CPU, memory, and I/O usage of containers:   <pre><code>docker stats\n</code></pre></p> </li> <li> <p>Consider using Prometheus and Grafana for comprehensive monitoring.</p> </li> <li>Look for services consuming unusually high resources.</li> </ul>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#viewing-global-logs","title":"Viewing Global Logs","text":"<pre><code>docker-compose logs\ndocker-compose logs -f  # Follow logs in real-time\ndocker-compose logs --tail=100  # View only the most recent logs\n</code></pre>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#viewing-logs-for-specific-microservices","title":"Viewing Logs for Specific Microservices","text":"<pre><code>docker-compose logs [service_name]\n</code></pre>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#useful-options-for-log-viewing","title":"Useful Options for Log Viewing","text":"<ul> <li>Show timestamps:   <pre><code>docker-compose logs -t\n</code></pre></li> <li>Limit output:   <pre><code>docker-compose logs --tail=50 [service_name]\n</code></pre></li> <li>Since time:   <pre><code>docker-compose logs --since 2023-07-01T00:00:00 [service_name]\n</code></pre></li> </ul>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#checking-logs-for-specific-teranode-microservices","title":"Checking Logs for Specific Teranode Microservices","text":"<pre><code>docker-compose logs [service_name]\n</code></pre> <p>For Docker Compose, replace <code>[service_name]</code> with the appropriate service or pod name:</p> <ul> <li>Propagation Service (service name: <code>propagation</code>)</li> <li>Blockchain Service (service name: <code>blockchain</code>)</li> <li>Asset Service (service name: <code>asset</code>)</li> <li>Block Validation Service (service name: <code>blockvalidation</code>)</li> <li>P2P Service (service name: <code>p2p</code>)</li> <li>Block Assembly Service (service name: <code>blockassembly</code>)</li> <li>Subtree Validation Service (service name: <code>subtreevalidation</code>)</li> <li>RPC Server (service name: <code>rpc</code>)</li> <li>Postgres Database (service name: <code>postgres</code>)          [Only in Docker]</li> <li>Aerospike Database (service name: <code>aerospike</code>)        [Only in Docker]</li> <li>Kafka   (service name: <code>kafka-shared</code>)                [Only in Docker]</li> <li>Kafka Console (service name: <code>kafka-console-shared</code>)  [Only in Docker]</li> <li>Prometheus (service name: <code>prometheus</code>)               [Only in Docker]</li> <li>Grafana  (service name: <code>grafana</code>)                    [Only in Docker]</li> </ul>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#redirecting-logs-to-a-file","title":"Redirecting Logs to a File","text":"<pre><code>docker-compose logs &gt; teranode_logs.txt\ndocker-compose logs [service_name] &gt; [service_name]_logs.txt\n</code></pre> <p>Remember to replace placeholders like <code>[service_name]</code>, and label selectors with the appropriate values for your Teranode setup.</p>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#check-services-dashboard","title":"Check Services Dashboard","text":"<p>Check your Grafana <code>TERANODE Service Overview</code> dashboard:</p> <ul> <li> <p>Check that there's no blocks in the queue (<code>Queued Blocks in Block Validation</code>). We expect little or no queueing, and not creeping up. 3 blocks queued up are already a concern.</p> </li> <li> <p>Check that the propagation instances are handling around the same load to make sure the load is equally distributed among all the propagation servers. See the <code>Propagation Processed Transactions per Instance</code> diagram.</p> </li> <li> <p>Check that the cache is at a sustainable pattern rather than \"exponentially\" growing (see both the <code>Tx Meta Cache in Block Validation</code> and <code>Tx Meta Cache Size in Block Validation</code> diagrams).</p> </li> <li> <p>Check that go routines (<code>Goroutines</code> graph) are not creeping up or reaching excessive levels.</p> </li> </ul>"},{"location":"howto/miners/docker/minersHowToTroubleshooting/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"howto/miners/docker/minersHowToTroubleshooting/#third-party-component-failure","title":"Third Party Component Failure","text":"<p>Teranode is highly dependent on its third party dependencies. Postgres, Kafka and Aerospike are critical for Teranode operations, and the node cannot work without them.</p> <p>If a third party service fails, you must restore its functionality. Once it is back, please restart Teranode cleanly following the instructions in the How to Start and Stop Teranode in Docker guide.</p> <p>Should you encounter a bug, please report it following the instructions in the Bug Reporting section.</p>"},{"location":"howto/miners/docker/minersSecurityBestPractices/","title":"Security Best Practices","text":"<p>Last modified: 22-January-2025</p>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#firewall-configuration","title":"Firewall Configuration","text":"<p>While Docker Compose creates an isolated network for the Teranode services, some ports are exposed to the host system and potentially to the external network. Here are some firewall configuration recommendations:</p>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#1-publicly-exposed-ports","title":"1. Publicly Exposed Ports","text":"<p>Review the ports exposed in the Docker Compose configuration file(s) and ensure your firewall is configured to handle these appropriately:</p> <ul> <li><code>9292</code>: RPC Server. Open to receive RPC API requests.</li> <li><code>8090</code>: Asset Server. Open for incoming HTTP asset requests.</li> <li><code>9905,9906</code>: P2P Server. Open for incoming connections to allow peer discovery and communication.</li> </ul>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#2-host-firewall","title":"2. Host Firewall","text":"<ul> <li>Configure your host's firewall to allow incoming connections only on the necessary ports.</li> <li>For ports that don't need external access, strictly restrict them to localhost (127.0.0.1) or your internal network.</li> </ul>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#3-external-access","title":"3. External Access","text":"<ul> <li>Only expose ports to the internet that are absolutely necessary for node operation (e.g., P2P, RPC and Asset server ports).</li> <li>Use strong authentication for any services that require external access. See the section 4.1 of this document for more details.</li> </ul>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#4-dockers-built-in-firewall","title":"4. Docker's Built-in Firewall","text":"<ul> <li>Docker manages its own iptables rules. Ensure these don't conflict with your host firewall rules.</li> </ul>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#5-network-segmentation","title":"5. Network Segmentation","text":"<ul> <li>If possible, place your Teranode host on a separate network segment with restricted access to other parts of your infrastructure.</li> </ul>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#6-regular-audits","title":"6. Regular Audits","text":"<ul> <li>Periodically review your firewall rules and exposed ports to ensure they align with your security requirements.</li> </ul>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#7-service-specific-recommendations","title":"7. Service-Specific Recommendations","text":"<ul> <li>PostgreSQL (5432): If you want to expose it, restrict to internal network, never publicly.</li> <li>Kafka (9092, 9093): If you want to expose it, restrict to internal network, never publicly.</li> <li>Aerospike (3000): If you want to expose it, restrict to internal network, never publicly.</li> <li>Grafana (3005): Secure with strong authentication if exposed externally.</li> </ul>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#8-p2p-communication","title":"8. P2P Communication","text":"<ul> <li>Ensure ports 9905 and 9906 are open for incoming connections to allow peer discovery and communication.</li> </ul> <p>Important Security Principle</p> <p>Remember, the exact firewall configuration will depend on your specific network setup, security requirements, and how you intend to operate your Teranode. Always follow the principle of least privilege, exposing only what is necessary for operation.</p>"},{"location":"howto/miners/docker/minersSecurityBestPractices/#regular-system-updates","title":"Regular System Updates","text":"<p>System Update Recommendations</p> <p>In order to receive the latest bug fixes and vulnerability patches, please ensure you perform periodic system updates, as regularly as feasible. Please refer to the Teranode update process outlined in the Section 6 of this document.</p>"},{"location":"howto/miners/docker/minersUpdatingTeranode/","title":"Updating Teranode to a New Version - Docker Compose","text":"<ol> <li> <p>Download and copy any newer version of the docker compose file, if available, into your project repository.</p> </li> <li> <p>Update Docker Images</p> </li> </ol> <p>Pull the latest versions of all images:</p> <p><pre><code>docker-compose pull\n</code></pre> 3. Stop the Current Stack</p> <p><pre><code>docker-compose down\n</code></pre> 4. Start the Updated Stack</p> <p><pre><code>docker-compose up -d\n</code></pre> 5. Verify the Update</p> <p>Check that all services are running and healthy:</p> <p><pre><code>docker-compose ps\n</code></pre> 6. Check Logs for Any Issues</p> <pre><code>docker-compose logs\n</code></pre> <p>Important Considerations:</p> <ul> <li>Data Persistence: The update process should not affect data stored in volumes (./data directory). However, it's always good practice to backup important data before updating.</li> <li>Configuration Changes: Check the release notes or documentation for any required changes to <code>settings_local.conf</code> or environment variables.</li> <li>Database Migrations: Some updates may require database schema changes. Where applicable, this will be handled transparently by docker compose.</li> <li>Downtime: Be aware that this process involves stopping and restarting all services, which will result in some downtime. Rolling updates are not possible with the <code>docker compose</code> setup.</li> </ul> <p>After the update:</p> <ul> <li>Monitor the system closely for any unexpected behavior.</li> <li>Check the Grafana dashboards to verify that performance metrics are normal.</li> </ul> <p>If you encounter any issues during or after the update, you may need to:</p> <ul> <li>Check the specific service logs for error messages.</li> <li>Consult the release notes or documentation for known issues and solutions.</li> <li>Reach out to the Teranode support team for assistance.</li> </ul> <p>Remember, the exact update process may vary depending on the specific changes in each new version. Always refer to the official update instructions provided with each new release for the most accurate and up-to-date information.</p>"},{"location":"howto/miners/kubernetes/minersHowToBackup/","title":"How to Backup Teranode Data","text":"<p>Last modified: 6-March-2025</p> <p>Regular and secure backups are essential for protecting a Teranode installation, ensuring data integrity, and safeguarding your wallet. The next steps outline how to back up your Teranode wallet and data:</p> <p>There are three main options for backing up and restoring a Teranode system. Each option has its own use cases and procedures.</p>"},{"location":"howto/miners/kubernetes/minersHowToBackup/#option-1-full-system-backup","title":"Option 1: Full System Backup","text":"<p>This option involves stopping all services, then performing backups of both the UTXO data and the blockchain data, plus the node data (typically under the /data directory).</p> <p>In a production environment, UTXOs will be typically stored in Aerospike, and the blockchain data will be stored in Postgres.</p> <p>The below is an example procedure, however you must adapt this to your specific needs.</p>"},{"location":"howto/miners/kubernetes/minersHowToBackup/#procedure","title":"Procedure:","text":"<p>Note: This is not a hot backup. All services must be stopped before performing the backup.</p> <ol> <li>Stop all Teranode services.</li> <li>Perform an UTXO data backup.</li> <li>Perform a blockchain data backup.</li> <li>Perform a backup of the filesystem data (typically under /data).</li> <li>Restart services after backup completion.</li> </ol>"},{"location":"howto/miners/kubernetes/minersHowToBackup/#option-2-archive-mode-backup","title":"Option 2: Archive Mode Backup","text":"<p>If running the node in Archive mode (with Block Persister and UTXO Persister), users can use their own exported UTXO set as an alternative to exporting the Aerospike data.</p>"},{"location":"howto/miners/kubernetes/minersHowToBackup/#procedure_1","title":"Procedure:","text":"<ol> <li>Ensure Block Persister and UTXO Persister are enabled and running.</li> <li>Use the persisted UTXO set and header files, together with a blockchain postgres export, for backup purposes.</li> <li>Follow the restoration procedure in the Installation section.</li> </ol>"},{"location":"howto/miners/kubernetes/minersHowToConfigureTheNode/","title":"Configuring Kubernetes Operator Teranode","text":"<p>Last modified: 6-March-2025</p>"},{"location":"howto/miners/kubernetes/minersHowToConfigureTheNode/#index","title":"Index","text":"<ul> <li>Configuring Setting Files</li> <li>Optional vs Required services</li> <li>Reference Settings</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToConfigureTheNode/#configuring-setting-files","title":"Configuring Setting Files","text":"<p>The Teranode settings can be configured in the operator ConfigMap. You can refer to the notes on the installation and ConfigMap setup here.</p> <p>For a list of settings, and their default values, please refer to the reference at the end of this document.</p>"},{"location":"howto/miners/kubernetes/minersHowToConfigureTheNode/#optional-vs-required-services","title":"Optional vs Required services","text":"<p>While most services are required for the proper functioning of Teranode, some services are optional and can be disabled if not needed. The following table provides an overview of the services and their status:</p> Required Optional Asset Server Block Persister Block Assembly UTXO Persister Block Validator Subtree Validator Blockchain Propagation P2P Legacy Gateway <p>The Block and UTXO persister services are optional and can be disabled. If enabled, your node will be in Archive Mode, storing historical block and UTXO data. As Teranode does not retain historical transaction data, this data can be useful for analytics and historical lookups, but comes with additional storage and processing overhead. Additionally, it can be used as a backup for the UTXO store.</p>"},{"location":"howto/miners/kubernetes/minersHowToConfigureTheNode/#settings-reference","title":"Settings Reference","text":"<p>You can find the pre-configured settings file here. You can refer to this document in order to identify the current system behaviour and in order to override desired settings in your <code>settings_local.conf</code>.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/","title":"How to Install Teranode with Kubernetes Helm","text":"<p>Last modified: 28-Jul-2025</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#index","title":"Index","text":"<ul> <li>Introduction</li> <li>Prerequisites</li> <li>Deployment with Minikube<ul> <li>Start Minikube</li> <li>Deploy Dependencies</li> <li>Create Persistent Volume Provider</li> <li>Load Teranode Images</li> <li>Deploy Teranode</li> </ul> </li> <li>Verifying the Deployment</li> <li>Production Considerations</li> <li>Other Resources</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#introduction","title":"Introduction","text":"<p>This guide provides instructions for deploying Teranode in a Kubernetes environment. While this guide shows the steps to deploy on a single server cluster using Minikube, these configurations can be adapted for production use with appropriate modifications.</p> <p></p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following tools installed and configured:</p> <ul> <li>Docker</li> <li>Minikube</li> <li>kubectl</li> <li>Helm</li> <li>AWS CLI</li> </ul> <p>Additionally, ensure you have a storage provider capable of providing ReadWriteMany (RWX) storage. As an example, this guide includes setting up an NFS server via Docker for this purpose.</p> <p></p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#deployment-with-minikube","title":"Deployment with Minikube","text":"<p>Minikube creates a local Kubernetes cluster on your machine. For running Teranode, we recommend the following process:</p> <p></p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#start-minikube","title":"Start Minikube","text":"<p>Start minikube with recommended resources and verify its status:</p> <pre><code># Start minikube with recommended resources\nminikube start --cpus=4 --memory=8192 --disk-size=20gb\n\n# Verify minikube status\nminikube status\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#deploy-dependencies","title":"Deploy Dependencies","text":"<p>Teranode requires several backing services. While these services should be deployed separately in production, for local development we'll deploy them within the same cluster.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#create-namespace","title":"Create Namespace","text":"<p>Create a namespace for the deployment:</p> <pre><code>kubectl create namespace teranode-operator\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#deploy-backing-services","title":"Deploy Backing Services","text":"<p>Deploy all dependencies in the teranode namespace:</p> <pre><code>kubectl apply -f deploy/kubernetes/aerospike/ -n teranode-operator\nkubectl apply -f deploy/kubernetes/postgres/ -n teranode-operator\nkubectl apply -f deploy/kubernetes/kafka/ -n teranode-operator\n</code></pre> <p>To know more, please refer to the Third Party Reference Documentation</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#create-persistent-volume-provider","title":"Create Persistent Volume Provider","text":"<p>For this example, we will create a local folder and expose it to Minikube via a docker based NFS server.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#standard-x86x64-systems","title":"Standard x86/x64 Systems","text":"<pre><code>docker volume create nfs-volume\n\ndocker run -d \\\n    --name nfs-server \\\n    -e NFS_EXPORT_0='/minikube-storage *(rw,no_subtree_check,fsid=0,no_root_squash)' \\\n    -v nfs-volume:/minikube-storage \\\n    --cap-add SYS_ADMIN \\\n    -p 2049:2049 \\\n  erichough/nfs-server\n\n# connect the nfs-server to the minikube network\ndocker network connect minikube nfs-server\n\n# create the PersistentVolume\nkubectl apply -f deploy/kubernetes/nfs/\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#arm-based-systems","title":"ARM-based Systems","text":"<p>For arm based systems, you can use this variant:</p> <pre><code>docker volume create nfs-volume\n\ndocker run -d --name nfs-server --privileged \\\n    -v nfs-volume:/minikube-storage \\\n    alpine:latest \\\n    sh -c \"apk add --no-cache nfs-utils &amp;&amp; \\\n        mkdir -p /minikube-storage &amp;&amp; \\\n        chmod 777 /minikube-storage &amp;&amp; \\\n        echo '/minikube-storage *(rw,sync,no_subtree_check,no_root_squash,insecure,fsid=0)' &gt; /etc/exports &amp;&amp; \\\n        exportfs -r &amp;&amp; \\\n        rpcbind &amp;&amp; \\\n        rpc.statd &amp;&amp; \\\n        rpc.nfsd 8 &amp;&amp; \\\n        rpc.mountd &amp;&amp; \\\n        tail -f /dev/null\"\n\n# connect the nfs-server to the minikube network\ndocker network connect minikube nfs-server\n\n# create the PersistentVolume\nkubectl apply -f deploy/kubernetes/nfs/\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#load-teranode-images","title":"Load Teranode Images","text":"<p>Pull and load the required Teranode images into Minikube:</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#identify-available-versions","title":"Identify Available Versions","text":"<p>You can find the latest available version published on GitHub Container Registry:</p> <ul> <li>https://github.com/bsv-blockchain/teranode/pkgs/container/teranode</li> <li>https://github.com/bsv-blockchain/teranode-operator/pkgs/container/teranode-operator</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#set-image-versions","title":"Set Image Versions","text":"<pre><code># Set image versions (please derive the right TERANODE_VERSION from the results of the previous command)\nexport OPERATOR_VERSION=v0.5.5\nexport TERANODE_VERSION=v0.11.13\nexport ECR_REGISTRY=ghcr.io/bsv-blockchain\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#load-images-into-minikube","title":"Load Images into Minikube","text":"<pre><code># Load Teranode Operator\ndocker pull $ECR_REGISTRY/teranode-operator:$OPERATOR_VERSION\nminikube image load $ECR_REGISTRY/teranode-operator:$OPERATOR_VERSION\n\n# Load Teranode Public\ndocker pull $ECR_REGISTRY/teranode:$TERANODE_VERSION\nminikube image load $ECR_REGISTRY/teranode:$TERANODE_VERSION\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#deploy-teranode","title":"Deploy Teranode","text":"<p>The Teranode Operator manages the lifecycle of Teranode instances:</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#install-teranode-operator","title":"Install Teranode Operator","text":"<pre><code># Install operator\nhelm upgrade --install teranode-operator oci://ghcr.io/bsv-blockchain/teranode-operator \\\n    -n teranode-operator \\\n    -f deploy/kubernetes/teranode/teranode-operator.yaml\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#apply-teranode-configuration","title":"Apply Teranode Configuration","text":"<p>Apply the Teranode configuration and custom resources:</p> <pre><code>kubectl apply -f deploy/kubernetes/teranode/teranode-configmap.yaml -n teranode-operator\nkubectl apply -f deploy/kubernetes/teranode/teranode-cr.yaml -n teranode-operator\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#start-syncing-process","title":"Start Syncing Process","text":"<p>A fresh Teranode starts up in IDLE state by default. To start syncing from the legacy network, you can run:</p> <pre><code>kubectl exec -it $(kubectl get pods -n teranode-operator -l app=blockchain -o jsonpath='{.items[0].metadata.name}') -n teranode-operator -- teranode-cli setfsmstate -fsmstate legacysyncing\n</code></pre> <p>To know more about the syncing process, please refer to the Teranode Sync Guide</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#verifying-the-deployment","title":"Verifying the Deployment","text":"<p>To verify your deployment:</p> <pre><code># Check all pods are running\nkubectl get pods -n teranode-operator | grep -E 'aerospike|postgres|kafka|teranode-operator'\n\n# Check Teranode services are ready\nkubectl wait --for=condition=ready pod -l app=blockchain -n teranode-operator --timeout=300s\n\n# View Teranode logs\nkubectl logs -n teranode-operator -l app=blockchain -f\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#production-considerations","title":"Production Considerations","text":"<p>For production deployments, consider:</p> <ul> <li>Deploying dependencies (Aerospike, PostgreSQL, Kafka) in separate clusters or using managed services</li> <li>Implementing proper security measures (network policies, RBAC, etc.)</li> <li>Setting up monitoring and alerting</li> <li>Configuring appropriate resource requests and limits</li> <li>Setting up proper backup and disaster recovery procedures</li> </ul> <p>An example CR for a mainnet deployment is available in kubernetes/teranode/teranode-cr-mainnet.yaml.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallation/#other-resources","title":"Other Resources","text":"<ul> <li>Third Party Reference Documentation</li> <li>Teranode Sync Guide</li> <li>How-To Configure the Node.md</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/","title":"How to Install Teranode with Kubernetes Operator","text":"<p>Last modified: 29-January-2025</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#index","title":"Index","text":"<ul> <li>Introduction</li> <li>Prerequisites</li> <li>Hardware Requirements</li> <li>Software Requirements</li> <li>Network Considerations</li> <li>Installation Process</li> <li>Teranode Initial Synchronization<ul> <li>Full P2P Download</li> <li>Initial Data Set Installation</li> </ul> </li> <li>Teranode Installation - Introduction to the Kubernetes Operator</li> <li>Installing Teranode with the Custom Kubernetes Operator</li> <li>Optimizations</li> <li>Reference - Settings</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#introduction","title":"Introduction","text":"<p>This guide provides step-by-step instructions for installing Teranode with the Kubernetes Operator.</p> <p>This guide is applicable to:</p> <ol> <li> <p>Miners and node operators using <code>kubernetes operator</code>.</p> </li> <li> <p>Configurations designed to connect to and process the BSV mainnet with current production load.</p> </li> </ol> <p>This guide does not cover:</p> <ol> <li> <p>Advanced network configurations.</p> </li> <li> <p>Any sort of source code build or manipulation of any kind.</p> </li> </ol>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go version 1.20.0+</li> <li>Docker version 17.03+</li> <li>kubectl version 1.11.3+</li> <li>Access to a Kubernetes v1.11.3+ cluster</li> <li>Operator Lifecycle Manager (OLM) installed</li> <li>Operator SDK</li> <li>Sufficient cluster resources as defined in the Cluster spec</li> <li>A stable internet connection</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#hardware-requirements","title":"Hardware Requirements","text":"<p>The Teranode team will provide you with current hardware recommendations. These recommendations will be:</p> <ol> <li>Tailored to your specific configuration settings</li> <li>Designed to handle the expected production transaction volume</li> <li>Updated regularly to reflect the latest performance requirements</li> </ol> <p>This ensures your system is appropriately equipped to manage the projected workload efficiently.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#software-requirements","title":"Software Requirements","text":"<p>Teranode relies on a number of third-party software dependencies, some of which can be sourced from different vendors.</p> <p>BSV provides both a <code>Kubernetes operator</code> that provides a production-live multi-node setup. However, it is the operator responsibility to support and monitor the various third parties.</p> <p>This section will outline the various vendors in use in Teranode.</p> <p>To know more, please refer to the Third Party Reference Documentation</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#network-considerations","title":"Network Considerations","text":"<p>Running a Teranode BSV listener node has relatively low bandwidth requirements compared to many other server applications. The primary network traffic consists of receiving blockchain data, including new transactions and blocks.</p> <p>While exact bandwidth usage can vary depending on network activity and node configuration, Bitcoin nodes typically require:</p> <ul> <li>Inbound: 5-50 GB per day</li> <li>Outbound: 50-150 GB per day</li> </ul> <p>These figures are approximate. In general, any stable internet connection should be sufficient for running a Teranode instance.</p> <p>Key network considerations:</p> <ol> <li>Ensure your internet connection is reliable and has sufficient bandwidth to handle continuous data transfer.</li> <li>Be aware that initial blockchain synchronization, depending on your installation method, may require higher bandwidth usage. If you synchronise automatically starting from the genesis block, you will have to download every block. However, the BSV recommended approach is to install a seed UTXO Set and blockchain.</li> <li>Monitor your network usage to ensure it stays within your ISP's limits and adjust your node's configuration if needed.</li> </ol>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#installation-process","title":"Installation Process","text":""},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#teranode-initial-synchronization","title":"Teranode Initial Synchronization","text":"<p>Teranode requires an initial block synchronization to function properly. There are two approaches for completing the synchronization process.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#full-p2p-download","title":"Full P2P Download","text":"<ul> <li>Start the node and download all blocks from genesis using peer-to-peer (P2P) network.</li> <li>This method downloads the entire blockchain history.</li> </ul> <p>Pros:</p> <ul> <li>Simple to implement</li> <li>Ensures the node has the complete blockchain history</li> </ul> <p>Cons:</p> <ul> <li>Time-consuming process</li> <li>Can take 5-8 days, depending on available bandwidth</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#initial-data-set-installation","title":"Initial Data Set Installation","text":"<p>To speed up the initial synchronization process, you have the option to seed Teranode from pre-existing data. To know more about this approach, please refer to the How to Sync the Node guide.</p> <p>Pros:</p> <ul> <li>Significantly faster than full P2P download</li> <li>Allows for quicker node setup</li> </ul> <p>Cons:</p> <ul> <li>Requires additional steps</li> <li>The data set must be validated, to ensure it has not been tampered with</li> </ul> <p>Where possible, BSV recommends using the Initial Data Set Installation approach.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#teranode-installation-introduction-to-the-kubernetes-operator","title":"Teranode Installation - Introduction to the Kubernetes Operator","text":"<p>Teranode is a complex system that can be deployed on Kubernetes using a custom operator (Kubernetes Operator Pattern). The deployment is managed through Kubernetes manifests, specifically using a Custom Resource Definition (CRD) of kind \"Cluster\" in the teranode.bsvblockchain.org/v1alpha1 API group. This Cluster resource defines various components of the Teranode system, including Asset, Block Validator, Block Persister, UTXO Persister, Blockchain, Block Assembly, Miner, Peer-to-Peer, Propagation, and Subtree Validator services.</p> <p>The deployment uses kustomization for managing Kubernetes resources, allowing for easier customization and overlay configurations.</p> <p>The Cluster resource allows fine-grained control over resource allocation, enabling users to adjust CPU and memory requests and limits for each component.</p> <p>It must be noted that the <code>Kubernetes operator</code> production-like setup does not install or manage the third party dependencies, explicitly:</p> <ul> <li>Kafka</li> <li>PostgreSQL</li> <li>Aerospike</li> <li>Grafana</li> <li>Prometheus</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#configuration-management","title":"Configuration Management","text":"<p>Environment variables and settings are managed using ConfigMaps. The Cluster custom resource specifies a <code>configMapName</code> (e.g., \"shared-config\") which is referenced by the various Teranode components. Users will need to create this ConfigMap before deploying the Cluster resource.</p> <p>Sample config:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: teranode-operator-config\ndata:\n  SETTINGS_CONTEXT: operator.testnet\n  KAFKA_HOSTS: ...\n  blockchain_store: 'postgres://...'\n  utxostore: 'aerospike://...'\n</code></pre> <p>To review the list of settings you could configure in the ConfigMap, please refer to the list here.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#storage-requirements","title":"Storage Requirements","text":"<p>Teranode uses PersistentVolumeClaims (PVCs) for storage in some components. For example, the SubtreeValidator specifies storage resources and a storage class. Users should ensure their Kubernetes cluster has the necessary storage classes and capacity available.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#service-deployment","title":"Service Deployment","text":"<p>The Teranode services are deployed as separate components within the Cluster custom resource. Each service (e.g., Asset, BlockValidator, Blockchain, Peer-to-Peer, etc.) is defined with its own specification, including resource requests and limits. The Kubernetes operator manages the creation and lifecycle of these components based on the Cluster resource definition.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#namespace-usage","title":"Namespace Usage","text":"<p>Users can deploy Teranode to a specific namespace by specifying it during the operator installation or when applying the Cluster resource.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#networking-and-ingress","title":"Networking and Ingress","text":"<p>Networking is handled through Kubernetes Services and Ingress resources. The Cluster resource allows specification of ingress for Asset, Peer, and Propagation services. It supports customization of ingress class, annotations, and hostnames. The setup appears to use Traefik as the ingress controller, but it's designed to be flexible for different ingress providers.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#third-party-dependencies","title":"Third Party Dependencies","text":"<p>Tools like Grafana, Prometheus, Aerospike Postgres and Kafka are not included in the Teranode operator deployment. Users are expected to set up these tools separately in their Kubernetes environment (or outside of it).</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#logging-and-troubleshooting","title":"Logging and Troubleshooting","text":"<p>Standard Kubernetes logging and troubleshooting approaches apply. Users can use <code>kubectl logs</code> and <code>kubectl describe</code> commands to investigate issues with the deployed pods and resources.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#installing-teranode-with-the-custom-kubernetes-operator","title":"Installing Teranode with the Custom Kubernetes Operator","text":""},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-1-prepare-the-environment","title":"Step 1: Prepare the Environment","text":"<ol> <li> <p>Ensure you have kubectl installed and configured to access your Kubernetes cluster.</p> </li> <li> <p>Verify access to your Kubernetes cluster:</p> </li> </ol> <pre><code>kubectl cluster-info\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-2-install-operator-lifecycle-manager-olm","title":"Step 2: Install Operator Lifecycle Manager (OLM)","text":"<ol> <li>If OLM is not already installed, install it using the following command:</li> </ol> <pre><code>operator-sdk olm install\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-3-create-bsva-catalogsource","title":"Step 3: Create BSVA CatalogSource","text":"<ol> <li>Clone the Teranode repository:</li> </ol> <pre><code>cd $YOUR_WORKING_DIR\ngit clone git@github.com:bsv-blockchain/teranode-operator.git\ncd teranode-operator\n</code></pre> <ol> <li>Create the BSVA CatalogSource in the OLM namespace:</li> </ol> <pre><code>kubectl create -f olm/catalog-source.yaml\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-4-create-target-namespace","title":"Step 4: Create Target Namespace","text":"<ol> <li>Create the namespace where you want to install the Teranode operator (this example uses 'teranode-operator'):</li> </ol> <pre><code>kubectl create namespace teranode-operator\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-5-create-operatorgroup-and-subscription","title":"Step 5: Create OperatorGroup and Subscription","text":"<ol> <li>(Optional) If you're deploying to a namespace other than 'teranode-operator', modify the OperatorGroup to specify your installation namespace:</li> </ol> <pre><code>echo \"  - &lt;your-namespace&gt;\" &gt;&gt; olm/og.yaml\n</code></pre> <ol> <li>Create the OperatorGroup and Subscription resources:</li> </ol> <pre><code>kubectl create -f olm/og.yaml -n teranode-operator\nkubectl create -f olm/subscription.yaml -n teranode-operator\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-6-verify-deployment","title":"Step 6: Verify Deployment","text":"<ol> <li>Check if all pods are running (your output should be similar to the below):</li> </ol> <pre><code># Check catalog source pod\nkubectl get pods -n olm\n</code></pre> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\nbsva-catalog-8922m                  1/1     Running   0          22s\ncatalog-operator-577f8b4bf5-sczlj   1/1     Running   0          86m\nolm-operator-8685b95f84-8wkf4       1/1     Running   0          86m\noperatorhubio-catalog-thvck         1/1     Running   0          85m\npackageserver-b54f9549f-kzqn9       1/1     Running   0          85m\npackageserver-b54f9549f-tr24v       1/1     Running   0          85m\n</code></pre> <pre><code># Check operator deployment\nkubectl get pods -n teranode-operator\n</code></pre> <pre><code>NAME                                                              READY   STATUS      RESTARTS   AGE\nasset-5cc5745c75-6m5gf                                            1/1     Running     0          3d11h\nasset-5cc5745c75-84p58                                            1/1     Running     0          3d11h\nblock-assembly-649dfd8596-k8q29                                   1/1     Running     0          3d11h\nblock-assembly-649dfd8596-njdgn                                   1/1     Running     0          3d11h\nblock-persister-57784567d6-tdln7                                  1/1     Running     0          3d11h\nblock-persister-57784567d6-wdx84                                  1/1     Running     0          3d11h\nblock-validator-6c4bf46f8b-bvxmm                                  1/1     Running     0          3d11h\nblockchain-ccbbd894c-k95z9                                        1/1     Running     0          3d11h\ndkr-ecr-eu-north-1-amazonaws-com-teranode-operator-bundle-v0-1    1/1     Running     0          3d11h\nede69fe8f248328195a7b76b2fc4c65a4ae7b7185126cdfd54f61c7eadffnzv   0/1     Completed   0          3d11h\nminer-6b454ff67c-jsrgv                                            1/1     Running     0          3d11h\npeer-6845bc4749-24ms4                                             1/1     Running     0          3d11h\npropagation-648cd4cc56-cw5bp                                      1/1     Running     0          3d11h\npropagation-648cd4cc56-sllxb                                      1/1     Running     0          3d11h\nsubtree-validator-7879f559d5-9gg9c                                1/1     Running     0          3d11h\nsubtree-validator-7879f559d5-x2dd4                                1/1     Running     0          3d11h\nteranode-operator-controller-manager-768f498c4d-mk49k             2/2     Running     0          3d11h\n</code></pre> <ol> <li>Ensure all services show a status of \"Running\" or \"Completed\".</li> </ol>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-7-configure-ingress-if-applicable","title":"Step 7: Configure Ingress (if applicable)","text":"<ol> <li>Verify that ingress resources are created for Asset, Peer, and Propagation services:</li> </ol> <pre><code>kubectl get ingress\n</code></pre> <ol> <li>Configure your ingress controller or external load balancer as needed.</li> </ol>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-8-access-teranode-services","title":"Step 8: Access Teranode Services","text":"<ul> <li>The various Teranode services will be accessible through the configured ingress or service endpoints.</li> <li>Refer to your specific ingress or network configuration for exact URLs and ports.</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-9-change-the-node-status-to-run-or-legacysync","title":"Step 9: Change the node status to Run or LegacySync","text":"<ol> <li>Force the node to transition to Run mode:</li> </ol> <pre><code>grpcurl -plaintext SERVER:8087 blockchain_api.BlockchainAPI.Run\n</code></pre> <ol> <li>Or LegacySync mode:</li> </ol> <pre><code>grpcurl -plaintext SERVER:8087 blockchain_api.BlockchainAPI.LegacySync\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-10-access-monitoring-tools","title":"Step 10: Access Monitoring Tools","text":"<p>Teranode Blockchain Viewer: A basic blockchain viewer is available and can be accessed via the asset container. It provides an interface to browse blockchain data.</p> <ul> <li>Port: Exposed on port 8090 of the asset container.</li> <li>Access URL: http://localhost:8090/viewer</li> </ul> <p>Note</p> <p>You must set the setting <code>dashboard_enabled</code> as true in order to see the viewer.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-11-monitoring-and-logging","title":"Step 11: Monitoring and Logging","text":"<ol> <li> <p>Set up your preferred monitoring stack (e.g., Prometheus, Grafana) to monitor the Teranode cluster.</p> </li> <li> <p>Use standard Kubernetes logging practices to access logs:</p> </li> </ol> <pre><code>kubectl logs &lt;pod-name&gt;\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#step-12-troubleshooting","title":"Step 12: Troubleshooting","text":"<ol> <li>Check pod status:</li> </ol> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre> <ol> <li>View pod logs:</li> </ol> <pre><code>kubectl logs &lt;pod-name&gt;\n</code></pre> <ol> <li>Verify ConfigMaps and Secrets:</li> </ol> <pre><code>kubectl get configmaps\nkubectl get secrets\n</code></pre> <p>Additional Notes:</p> <ul> <li>You can also refer to the teranode-operator repository for up to date instructions.</li> <li>This installation uses the 'stable' channel of the BSVA Catalog, which includes automatic upgrades for minor releases.</li> <li>To change the channel or upgrade policy, modify the <code>olm/subscription.yaml</code> file before creating the Subscription.</li> <li>SharedPVCName represents a persistent volume shared across a number of services (Block Validation, Subtree Validation, Block Assembly, Asset Server, Block Persister, UTXO Persister). The Persistent Volume must be in access mode ReadWriteMany (Access Modes). While the implementation of the storage is left at the user's discretion, the BSV Association has successfully tested using an AWS FSX for Lustre volume at high throughput, and it can be considered as a reliable option for any Teranode deployment.</li> <li>Ensure proper network policies and security contexts are in place for your Kubernetes environment.</li> <li>Regularly back up any persistent data stored in PersistentVolumeClaims.</li> <li>The Teranode operator manages the lifecycle of the Teranode services. Direct manipulation of the underlying resources is not recommended.</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#optimizations","title":"Optimizations","text":"<p>When running on a box without a public IP, you should enable <code>legacy_config_Upnp</code> (in your settings), so you don't get banned by the SV Nodes.</p> <p>If you have local access to SV Nodes, you can use them to speed up the initial block synchronization too. You can set <code>legacy_connect_peers: \"172.x.x.x:8333|10.x.x.x:8333\"</code> in your <code>docker-compose.yml</code> to force the legacy service to only connect to those peers.</p>"},{"location":"howto/miners/kubernetes/minersHowToInstallationDeprecated/#reference-settings","title":"Reference - Settings","text":"<p>You can find the pre-configured settings file here. You can refer to this document in order to identify the current system behaviour and in order to override desired settings in your <code>settings_local.conf</code>.</p>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/","title":"How to Start and Stop Teranode in Kubernetes","text":"<p>Last modified: 6-March-2025</p>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#introduction","title":"Introduction","text":"<p>This guide provides instructions for starting and stopping Teranode in a Kubernetes environment using the Teranode Operator and associated configurations.</p>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have all components installed as described in the Installation Guide:</p> <ul> <li>Docker</li> <li>Minikube</li> <li>kubectl</li> <li>Helm</li> <li>AWS CLI</li> <li>All dependencies deployed (Aerospike, PostgreSQL, Kafka)</li> <li>Storage provider configured (NFS)</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#starting-teranode","title":"Starting Teranode","text":""},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#1-deploy-teranode-configuration","title":"1. Deploy Teranode Configuration","text":"<pre><code># Apply the Teranode configuration and custom resources\nkubectl apply -f kubernetes/teranode/teranode-configmap.yaml -n teranode-operator\nkubectl apply -f kubernetes/teranode/teranode-cr.yaml -n teranode-operator\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#2-verify-deployment","title":"2. Verify Deployment","text":"<pre><code># Check all pods are running\nkubectl get pods -n teranode-operator | grep -E 'aerospike|postgres|kafka|teranode-operator'\n\n# Check Teranode services are ready\nkubectl wait --for=condition=ready pod -l app=blockchain -n teranode-operator --timeout=300s\n\n# View Teranode logs\nkubectl logs -n teranode-operator -l app=blockchain -f\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#3-start-syncing-if-needed","title":"3. Start Syncing (if needed)","text":"<pre><code>kubectl exec -it $(kubectl get pods -n teranode-operator -l app=blockchain -o jsonpath='{.items[0].metadata.name}') -n teranode-operator -- teranode-cli setfsmstate -fsmstate legacysyncing\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#stopping-teranode","title":"Stopping Teranode","text":""},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#1-graceful-shutdown","title":"1. Graceful Shutdown","text":"<pre><code># Remove the Teranode custom resource\nkubectl delete -f kubernetes/teranode/teranode-cr.yaml -n teranode-operator\n\n# Remove the configmap\nkubectl delete -f kubernetes/teranode/teranode-configmap.yaml -n teranode-operator\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#2-verify-resource-removal","title":"2. Verify Resource Removal","text":"<pre><code># Check pod termination status\nkubectl get pods -n teranode-operator\n\n# Monitor shutdown events\nkubectl get events -n teranode-operator\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#3-force-deletion-if-necessary","title":"3. Force Deletion (if necessary)","text":"<p>Only use these commands if normal shutdown fails:</p> <pre><code># Force delete resources\nkubectl delete -f kubernetes/teranode/teranode-cr.yaml -n teranode-operator --grace-period=0 --force\nkubectl delete -f kubernetes/teranode/teranode-configmap.yaml -n teranode-operator --grace-period=0 --force\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToStopStartKubernetesTeranode/#other-resources","title":"Other Resources","text":"<ul> <li>How to Install Teranode</li> <li>Third Party Reference Documentation</li> <li>Teranode Sync Guide</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/","title":"How to Troubleshoot Teranode (Kubernetes Operator)","text":"<p>Last modified: 6-March-2025</p>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#index","title":"Index","text":"<ul> <li>Health Checks and System Monitoring<ul> <li>Service Status</li> <li>Detailed Container/Pod Health</li> <li>Configuring Health Checks</li> <li>Viewing Health Check Logs</li> <li>Monitoring System Resources</li> <li>Viewing Global Logs</li> <li>Viewing Logs for Specific Microservices</li> <li>Useful Options for Log Viewing</li> <li>Checking Logs for Specific Teranode Microservices</li> <li>Redirecting Logs to a File</li> <li>Check Services Dashboard**</li> </ul> </li> <li>Recovery Procedures<ul> <li>Third Party Component Failure</li> </ul> </li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#health-checks-and-system-monitoring","title":"Health Checks and System Monitoring","text":""},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#service-status","title":"Service Status","text":"<pre><code>kubectl get pods -n teranode-operator\n</code></pre> <p>This command lists all pods in the current namespace, showing their status and readiness.</p>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#detailed-containerpod-health","title":"Detailed Container/Pod Health","text":"<pre><code>kubectl describe pod &lt;pod-name&gt; -n teranode-operator\n</code></pre> <p>This provides detailed information about the pod, including its current state, recent events, and readiness probe results.</p>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#configuring-health-checks","title":"Configuring Health Checks","text":"<p>In your Deployment or StatefulSet specification:</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n\n      - name: teranode-blockchain\n        ...\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8087\n          periodSeconds: 30\n          timeoutSeconds: 10\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8087\n          periodSeconds: 30\n          timeoutSeconds: 10\n          failureThreshold: 3\n          initialDelaySeconds: 40\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#viewing-health-check-logs","title":"Viewing Health Check Logs","text":"<p>Health check results are typically logged in the pod events:</p> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>Look for events related to readiness and liveness probes.</p>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#monitoring-system-resources","title":"Monitoring System Resources","text":"<ul> <li>Use <code>kubectl top</code> to view resource usage:</li> </ul> <pre><code>kubectl top pods\nkubectl top nodes\n</code></pre> <p>For both environments:</p> <ul> <li>Consider setting up Prometheus and Grafana for more comprehensive monitoring.</li> <li>Look for services consuming unusually high resources.</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#viewing-global-logs","title":"Viewing Global Logs","text":"<pre><code>kubectl logs -n teranode-operator -l app.kubernetes.io/part-of=teranode-operator\nkubectl logs -n teranode-operator -f -l app.kubernetes.io/part-of=teranode-operator  # Follow logs in real-time\nkubectl logs -n teranode-operator --tail=100 -l app.kubernetes.io/part-of=teranode-operator  # View only the most recent logs\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#viewing-logs-for-specific-microservices","title":"Viewing Logs for Specific Microservices","text":"<pre><code>kubectl logs -n teranode-operator &lt;pod-name&gt;\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#useful-options-for-log-viewing","title":"Useful Options for Log Viewing","text":"<ul> <li>Show timestamps:</li> </ul> <pre><code>kubectl logs -n teranode-operator &lt;pod-name&gt; --timestamps=true\n</code></pre> <ul> <li>Limit output:</li> </ul> <pre><code>kubectl logs -n teranode-operator &lt;pod-name&gt; --tail=50\n</code></pre> <ul> <li>Since time:</li> </ul> <pre><code>kubectl logs -n teranode-operator &lt;pod-name&gt; --since-time=\"2023-07-01T00:00:00Z\"\n</code></pre>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#checking-logs-for-specific-teranode-microservices","title":"Checking Logs for Specific Teranode Microservices","text":"<p>Replace <code>[service_name]</code> or <code>&lt;pod-name&gt;</code> with the appropriate service or pod name:</p> <ul> <li>Propagation Service (service name: <code>propagation</code>)</li> <li>Blockchain Service (service name: <code>blockchain</code>)</li> <li>Asset Service (service name: <code>asset</code>)</li> <li>Block Validation Service (service name: <code>block-validator</code>)</li> <li>P2P Service (service name: <code>p2p</code>)</li> <li>Block Assembly Service (service name: <code>block-assembly</code>)</li> <li>Subtree Validation Service (service name: <code>subtree-validator</code>)</li> <li>Miner Service (service name: <code>miner</code>)</li> <li>RPC Server (service name: <code>rpc</code>)</li> <li>Block Persister Service (service name: <code>block-persister</code>)</li> <li>UTXO Persister Service (service name: <code>utxo-persister</code>)</li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#redirecting-logs-to-a-file","title":"Redirecting Logs to a File","text":"<pre><code>kubectl logs -n teranode-operator -l app.kubernetes.io/part-of=teranode-operator &gt; teranode_logs.txt\nkubectl logs -n teranode-operator &lt;pod-name&gt; &gt; pod_logs.txt\n</code></pre> <p>Remember to replace placeholders like <code>[service_name]</code>, <code>&lt;pod-name&gt;</code>, and label selectors with the appropriate values for your Teranode setup.</p>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#check-services-dashboard","title":"Check Services Dashboard**","text":"<p>Check your Grafana <code>TERANODE Service Overview</code> dashboard:</p> <ul> <li> <p>Check that there's no blocks in the queue (<code>Queued Blocks in Block Validation</code>). We expect little or no queueing, and not creeping up. 3 blocks queued up are already a concern.</p> </li> <li> <p>Check that the propagation instances are handling around the same load to make sure the load is equally distributed among all the propagation servers. See the <code>Propagation Processed Transactions per Instance</code> diagram.</p> </li> <li> <p>Check that the cache is at a sustainable pattern rather than \"exponentially\" growing (see both the <code>Tx Meta Cache in Block Validation</code> and <code>Tx Meta Cache Size in Block Validation</code> diagrams).</p> </li> <li> <p>Check that go routines (<code>Goroutines</code> graph) are not creeping up or reaching excessive levels.</p> </li> </ul>"},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"howto/miners/kubernetes/minersHowToTroubleshooting/#third-party-component-failure","title":"Third Party Component Failure","text":"<p>Teranode is highly dependent on its third party dependencies. Postgres, Kafka and Aerospike are critical for Teranode operations, and the node cannot work without them.</p> <p>If a third party service fails, you must restore its functionality. Once it is back, please restart Teranode cleanly following the instructions in the How to Start and Stop Teranode in Kubernetes guide.</p> <p>Should you encounter a bug, please report it following the instructions in the Bug Reporting section.</p>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/","title":"Security Best Practices","text":"<p>Last modified: 6-March-2025</p>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#firewall-configuration","title":"Firewall Configuration","text":"<p>Here are some firewall configuration recommendations:</p>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#1-publicly-exposed-ports","title":"1. Publicly Exposed Ports","text":"<p>Review the ports exposed in the Kubernetes operator configuration file(s) and ensure your firewall is configured to handle these appropriately:</p> <ul> <li><code>9292</code>: RPC Server. Open to receive RPC API requests.</li> <li><code>8090</code>: Asset Server. Open for incoming HTTP asset requests. This should not be exposed directly, but via a reverse proxy or caching mechanism.</li> <li><code>9905,9906</code>: P2P Server. Open for incoming connections to allow peer discovery and communication.</li> <li><code>8098, 8099</code>: Legacy P2P Service, open for HTTP and gRPC.</li> </ul>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#2-restricted-ports","title":"2. Restricted Ports","text":"<p>Review the ports exposed in the Kubernetes operator configuration file(s) and ensure your firewall is configured to handle these appropriately:</p> <ul> <li><code>9292</code>: RPC Server. Open to receive RPC API requests. Not open to public access.</li> </ul>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#3-host-firewall","title":"3. Host Firewall","text":"<ul> <li>Configure your host's firewall to allow incoming connections only on the necessary ports.</li> <li>For ports that don't need external access, strictly restrict them to localhost (127.0.0.1) or your internal network.</li> </ul>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#4-external-access","title":"4. External Access","text":"<ul> <li>Only expose ports to the internet that are absolutely necessary for node operation (e.g., P2P and Asset server ports).</li> <li>Use strong authentication for any services that require external access. See the section 4.1 of this document for more details.</li> </ul>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#5-network-segmentation","title":"5. Network Segmentation","text":"<ul> <li>If possible, place your Teranode host on a separate network segment with restricted access to other parts of your infrastructure.</li> </ul>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#6-regular-audits","title":"6. Regular Audits","text":"<ul> <li>Periodically review your firewall rules and exposed ports to ensure they align with your security requirements.</li> </ul>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#7-service-specific-recommendations","title":"7. Service-Specific Recommendations","text":"<ul> <li>PostgreSQL (5432): If you want to expose it, restrict to internal network, never publicly.</li> <li>Kafka (9092, 9093): If you want to expose it, restrict to internal network, never publicly.</li> <li>Aerospike (3000): If you want to expose it, restrict to internal network, never publicly.</li> <li>Grafana (3005): Secure with strong authentication if exposed externally.</li> </ul>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#8-p2p-communication","title":"8. P2P Communication","text":"<ul> <li>Ensure ports 9905 and 9906 are open for incoming connections to allow peer discovery and communication.</li> </ul> <p>Important Security Principle</p> <p>Remember, the exact firewall configuration will depend on your specific network setup, security requirements, and how you intend to operate your Teranode. Always follow the principle of least privilege, exposing only what is necessary for operation.</p>"},{"location":"howto/miners/kubernetes/minersSecurityBestPractices/#regular-system-updates","title":"Regular System Updates","text":"<p>System Update Recommendations</p> <p>In order to receive the latest bug fixes and vulnerability patches, please ensure you perform periodic system updates, as regularly as feasible. Please refer to the Teranode update process outlined in the Section 6 of this document.</p>"},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/","title":"How to Update Teranode with Kubernetes Helm","text":"<p>Last modified: 6-March-2025</p>"},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/#introduction","title":"Introduction","text":"<p>This guide provides instructions for updating Teranode in a Kubernetes environment using Helm.</p>"},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have:</p> <ul> <li>Access to the ECR registry</li> <li>AWS CLI configured</li> <li>Helm installed</li> <li>Existing Teranode deployment</li> </ul>"},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/#update-process","title":"Update Process","text":""},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/#1-set-version-variables","title":"1. Set Version Variables","text":"<pre><code># Set new versions\nexport OPERATOR_VERSION=&lt;new-version&gt;\nexport TERANODE_VERSION=&lt;new-version&gt;\nexport ECR_REGISTRY=ghcr.io/bsv-blockchain\n</code></pre>"},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/#2-update-images","title":"2. Update Images","text":"<pre><code># Pull new images\ndocker pull $ECR_REGISTRY/teranode-operator:$OPERATOR_VERSION\ndocker pull $ECR_REGISTRY/teranode:$TERANODE_VERSION\n\n# Load into Minikube\nminikube image load $ECR_REGISTRY/teranode-operator:$OPERATOR_VERSION\nminikube image load $ECR_REGISTRY/teranode:$TERANODE_VERSION\n</code></pre>"},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/#3-update-operator","title":"3. Update Operator","text":"<pre><code># Update operator\nhelm upgrade teranode-operator oci://ghcr.io/bsv-blockchain/teranode-operator \\\n    -n teranode-operator \\\n    -f kubernetes/teranode/teranode-operator.yaml\n</code></pre>"},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/#4-verify-update","title":"4. Verify Update","text":"<pre><code># Check all pods are running\nkubectl get pods -n teranode-operator | grep -E 'aerospike|postgres|kafka|teranode-operator'\n\n# Check Teranode services are ready\nkubectl wait --for=condition=ready pod -l app=blockchain -n teranode-operator --timeout=300s\n\n# View Teranode logs\nkubectl logs -n teranode-operator -l app=blockchain -f\n</code></pre>"},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/#production-considerations","title":"Production Considerations","text":"<p>For production deployments:</p> <ul> <li>Create backups before updating</li> <li>Review release notes for breaking changes</li> <li>Test updates in a staging environment first</li> <li>Monitor system during and after update</li> <li>Have a rollback plan prepared</li> </ul>"},{"location":"howto/miners/kubernetes/minersUpdatingTeranode/#other-resources","title":"Other Resources","text":"<ul> <li>How to Install Teranode</li> <li>Third Party Reference Documentation</li> <li>Teranode Sync Guide</li> </ul>"},{"location":"misc/BIP-239/","title":"BIP-239 (Transaction Extended Format)","text":"<pre>\n  BIP: 239\n  Layer: Applications\n  Title: Transaction Extended Format (TEF)\n  Author:\n      Simon Ordish (@ordishs)\n      Siggi Oskarsson (@icellan)\n  Comments-Summary: No comments yet.\n  Comments-URI: -\n  Status: Proposal\n  Type: Standards Track\n  Created: 2022-11-09\n</pre>"},{"location":"misc/BIP-239/#abstract","title":"Abstract","text":"<p>Regular Bitcoin transactions do not contain all the data that is needed to verify that the signatures in the transactions are valid. To sign an input of a Bitcoin transaction, the signer needs to know the transaction ID, output index, output satoshis and the locking script of the input transaction. When sending a Bitcoin transaction to a node, only the previous transaction ID and the output index are part of the serialized transaction, the node will look up the locking script and output amount of the input transaction.</p> <p>We propose an Extended Format (EF) for a Bitcoin transaction, that includes the locking script and the amount in satoshis of all inputs of the transaction. This allows a broadcast service to validate all aspects of a transaction without having to contact a node or an indexer for the utxos of the inputs of a transaction, speeding up the validation.</p>"},{"location":"misc/BIP-239/#copyright","title":"Copyright","text":"<p>This BIP is licensed under the Open BSV license.</p>"},{"location":"misc/BIP-239/#motivation","title":"Motivation","text":"<p>Verifying that a transaction is valid, including all signatures, is not possible at the moment without getting the unspent transaction outputs (utxos) from the transactions that are used as inputs from a Bitcoin node (or a Bitcoin indexer). This lookup of the utxos always happens inside a Bitcoin node when validating a transaction, but for a broadcast service to be able to fully validate a transaction (including the fee being paid) it also needs to look up the utxos being spent, which complicates scalability, since this lookup needs to happen on a node (via RPC), that might be too busy to react within an acceptable time frame.</p> <p>A broadcast service would be able to validate a transaction almost in full if the sender would also send the missing data (previous locking scripts and satoshi outputs) from the utxos being used in the transaction. When creating a new transaction, the previous locking scripts and satoshi outputs are needed to be able to properly sign the transaction, so the missing data is available at the time of the transaction creation. Serializing the transaction to Extended Format, instead of the standard format, is at the point of creating the transaction no extra work, but does make it much easier for a broadcast service to validate the transaction when being received, before sending the transaction to a node.</p> <p>The main motivation for this proposal is therefore scalability. When incoming transactions contain all the data that is needed to validate them, without having to contact an external service for missing data, the broadcast service becomes much more scalable.</p>"},{"location":"misc/BIP-239/#specification","title":"Specification","text":"<p>Current Transaction format:</p> Field Description Size Version no currently 2 4 bytes In-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of inputs Transaction Input  Structure  qty with variable length per input Out-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of outputs Transaction Output Structure  qty with variable length per output nLocktime if non-zero and sequence numbers are &lt; 0xFFFFFFFF: block height or timestamp when transaction is final 4 bytes <p>The Extended Format adds a marker to the transaction format:</p> Field Description Size Version no currently 2 4 bytes EF marker marker for extended format 0000000000EF In-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of inputs Extended Format transaction Input Structure  qty with variable length per input Out-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of outputs Transaction Output Structure  qty with variable length per output nLocktime if non-zero and sequence numbers are &lt; 0xFFFFFFFF: block height or timestamp when transaction is final 4 bytes <p>The Extended Format marker allows a library that supports the format to recognize that it is dealing with a transaction in extended format, while a library that does not support extended format will read the transaction as having 0 inputs, 0 outputs and a future nLock time. This has been done to minimize the possible problems a legacy library will have when reading the extended format. It can in no way be recognized as a valid transaction.</p> <p>The input structure is the only additional thing that is changed in the Extended Format. The current input structure looks like this:</p> Field Description Size Previous Transaction hash TXID of the transaction the output was created in 32 bytes Previous Txout-index Index of the output (Non negative integer) 4 bytes Txin-script length Non negative integer VI = VarInt 1 - 9 bytes Txin-script / scriptSig Script -many bytes Sequence_no Used to iterate inputs inside a payment channel. Input is final when nSequence = 0xFFFFFFFF 4 bytes <p>In the Extended Format, we extend the input structure to include the previous locking script and satoshi outputs:</p> Field Description Size Previous Transaction hash TXID of the transaction the output was created in 32 bytes Previous Txout-index Index of the output (Non negative integer) 4 bytes Txin-script length Non negative integer VI = VarInt 1 - 9 bytes Txin-script / scriptSig Script -many bytes Sequence_no Used to iterate inputs inside a payment channel. Input is final when nSequence = 0xFFFFFFFF 4 bytes Previous TX satoshi output Output value in satoshis of previous input 8 bytes Previous TX script length Non negative integer VI = VarInt 1 - 9 bytes Previous TX locking script Script \\"},{"location":"references/codingConventions/","title":"\ud83d\udcd8 Coding Conventions &amp; Standards","text":""},{"location":"references/codingConventions/#index","title":"Index","text":"<ul> <li>Naming Conventions<ul> <li>Introduction</li> <li>General Principles<ul> <li>Clarity and Readability Over Brevity</li> <li>Consistency Within the Project</li> <li>Use of Descriptive Names, Avoiding Generic Names When Possible</li> </ul> </li> <li>Package Names<ul> <li>Short, Lowercase, and One-word Names</li> <li>Avoidance of Common Names Like \"util\" or \"helper\"</li> <li>Examples and Exceptions</li> </ul> </li> <li>File Name Conventions</li> <li>Variable Names<ul> <li>Short Yet Descriptive Names</li> <li>CamelCase for Exportable Variables and camelCase for Internal Variables</li> <li>Common Idioms</li> </ul> </li> <li>Function Names<ul> <li>Use of Descriptive Verbs and Nouns</li> <li>Naming Conventions for Constructors, Getters, and Setters</li> <li>Error Handling Functions and Their Naming Patterns</li> </ul> </li> <li>Interface Names<ul> <li>Single Method Interfaces with \"er\" Suffix</li> <li>Use of Descriptive Names for More Complex Interfaces</li> </ul> </li> <li>Type Names<ul> <li>Avoidance of Redundant or Tautological Names</li> <li>Use of Clear and Specific Names for Custom Types</li> </ul> </li> <li>Commenting<ul> <li>Best Practices for Writing Clear and Helpful Comments</li> </ul> </li> </ul> </li> <li>Error Handling<ul> <li>Effective Use of Go's Error Handling Paradigm</li> <li>Patterns for Error Creation, Propagation, and Checking</li> </ul> </li> <li>Concurrency<ul> <li>Best Practices for Using Goroutines and Channels</li> <li>Patterns for Avoiding Common Concurrency Pitfalls</li> </ul> </li> <li>Testing<ul> <li>Writing Effective Unit Tests Using the \"testing\" Package</li> <li>Use of Table-Driven Tests for Comprehensive Coverage</li> <li>Use of <code>testify</code> for Cleaner Assertions</li> <li>Best Practices</li> </ul> </li> <li>Code Formatting and Linting<ul> <li>Formatting</li> <li>Linting<ul> <li>Linting Rules</li> </ul> </li> <li>Pull Requests</li> <li>Optional Tools</li> </ul> </li> <li>Dependency Management<ul> <li>Use of Modules for Managing Dependencies</li> <li>Strategies for Keeping Dependencies Up to Date and Secure</li> </ul> </li> <li>AI Ruleset</li> </ul>"},{"location":"references/codingConventions/#naming-conventions","title":"Naming Conventions","text":""},{"location":"references/codingConventions/#introduction","title":"Introduction","text":"<p>The Teranode BSV implementation follows the Go programming language's naming conventions and best practices. These conventions are based on the official Go documentation, effective Go, and the Go community's accepted practices. To know more about them, please check:</p> <ul> <li>Effective Go</li> <li>What's in a name?</li> </ul> <p>The naming conventions and best practices outlined in this document provide a summary of these coding and naming best practices, together with additional guidelines specific to the Teranode BSV implementation.</p>"},{"location":"references/codingConventions/#general-principles","title":"General Principles","text":""},{"location":"references/codingConventions/#clarity-and-readability-over-brevity","title":"Clarity and Readability Over Brevity","text":"<ul> <li>Goal: Clear and readable code.</li> <li>Descriptive names &gt; succinct names to avoid ambiguity.</li> <li>Longer, explicit names are preferred over cryptic abbreviations.</li> <li>Example: Use <code>bestBlockchainBlockHeader</code> instead of <code>bestBlkHdr</code> for clarity.</li> </ul>"},{"location":"references/codingConventions/#consistency-within-the-project","title":"Consistency Within the Project","text":"<ul> <li>Consistency maintains a coherent codebase.</li> <li>Applies to naming, formatting, commenting, and code structure.</li> <li>Helps developers quickly understand and navigate new code sections.</li> </ul>"},{"location":"references/codingConventions/#use-of-descriptive-names-avoiding-generic-names-when-possible","title":"Use of Descriptive Names, Avoiding Generic Names When Possible","text":"<ul> <li>Use descriptive and specific names for clarity on role and usage.</li> <li>Descriptive names serve as self-documenting elements.</li> <li>Avoid generic names (e.g., <code>data</code>, <code>info</code>, <code>manager</code>) that lack insight.</li> <li>Choose names reflecting the entity's purpose (e.g., <code>miningCandidate</code>, <code>coinbaseValue</code>).</li> <li>Enhances code intuitiveness, readability, and maintainability.</li> </ul>"},{"location":"references/codingConventions/#package-names","title":"Package Names","text":""},{"location":"references/codingConventions/#short-lowercase-and-one-word-names","title":"Short, Lowercase, and One-word Names","text":"<ul> <li>Package names: concise, lowercase, single word if possible.</li> <li>Improves readability and avoids variable name conflicts.</li> <li>Examples: Use <code>net</code> (not <code>networkOperations</code>), <code>time</code> (not <code>timeUtils</code>).</li> </ul>"},{"location":"references/codingConventions/#avoidance-of-common-names-like-util-or-helper","title":"Avoidance of Common Names Like \"util\" or \"helper\"","text":"<ul> <li>Avoid generic package names like <code>util</code>, <code>utils</code>, <code>helper</code>.</li> <li>Generic names lead to unclear purpose and mixed contents.</li> <li>Name packages after their function or representation, e.g., <code>blockassembly</code> for Block Assembly.</li> </ul>"},{"location":"references/codingConventions/#examples-and-exceptions","title":"Examples and Exceptions","text":"<ul> <li>Good Package Names: <code>http</code>, <code>os</code>, <code>json</code> \u2013 These names are short, descriptive, and specific to their functionality.</li> <li>Avoid: <code>utilities</code>, <code>common</code>, <code>shared</code> \u2013 These are generic and do not convey the package's contents or purpose.</li> <li>Exceptions may arise when a package is designed to extend or wrap standard library functionality with more specific features. In such cases, appending a descriptive term to a standard library package name can be acceptable, e.g., <code>httputil</code> or <code>ioutil</code>, though the latter is deprecated in favor of more descriptive package names like <code>io</code> and <code>os</code>.</li> </ul>"},{"location":"references/codingConventions/#file-name-conventions","title":"File Name Conventions","text":"<ul> <li>Use snake_case: File names should use all lowercase letters with words separated by underscores (e.g., <code>block_header.go</code>, <code>transaction_pool.go</code>).</li> <li>Be descriptive and concise: File names should clearly indicate their contents or purpose, avoiding generic names like <code>main.go</code> (except for the entry point), <code>misc.go</code>, or <code>util.go</code>.</li> <li>Avoid redundancy: Do not repeat the package name in the file name unless necessary for clarity (e.g., in <code>block/block.go</code>, just use <code>block.go</code>).</li> <li>Test files: Name test files with the <code>_test.go</code> suffix (e.g., <code>block_header_test.go</code>).</li> <li>Generated files: If a file is generated, include a comment at the top indicating it is generated and was not edited by hand. Use a suffix or prefix if appropriate (e.g., <code>zz_generated_types.go</code>).</li> <li>Grouping: For related types or logic, group them in a single file when practical but split into multiple files if the file becomes too large or unwieldy.</li> <li>No uppercase or special characters: Avoid uppercase letters, spaces, or special characters in file names.</li> </ul> <p>Examples:</p> <ul> <li><code>block_header.go</code></li> <li><code>transaction_pool.go</code></li> <li><code>network_manager.go</code></li> <li><code>block_header_test.go</code></li> </ul> <p>Scripts:</p> <ul> <li>Check Filenames</li> </ul>"},{"location":"references/codingConventions/#variable-names","title":"Variable Names","text":""},{"location":"references/codingConventions/#short-yet-descriptive-names","title":"Short Yet Descriptive Names","text":"<ul> <li>Choose brief, descriptive names for variables.</li> <li>Aim to convey purpose without sacrificing readability.</li> <li>Avoid overly long names.</li> </ul>"},{"location":"references/codingConventions/#camelcase-for-exportable-variables-and-camelcase-for-internal-variables","title":"CamelCase for Exportable Variables and camelCase for Internal Variables","text":"<ul> <li>Exportable Variables: Use CamelCase (capitalizing the first letter) for variables that need to be accessible outside the package, e.g., <code>CustomerID</code>.</li> <li>Internal Variables: Use camelCase (starting with a lowercase letter) for variables only used within the package, e.g., <code>localTime</code>.</li> </ul>"},{"location":"references/codingConventions/#common-idioms","title":"Common Idioms","text":"<ul> <li>Loop Indices: Use short names like <code>i</code>, <code>j</code>, <code>k</code> for loops.</li> <li>Errors: Use <code>err</code> to represent errors.</li> <li>Temporary Variables: Short names like <code>tmp</code> or <code>temp</code> are acceptable for temporary or insignificant variables.</li> </ul>"},{"location":"references/codingConventions/#function-names","title":"Function Names","text":""},{"location":"references/codingConventions/#use-of-descriptive-verbs-and-nouns","title":"Use of Descriptive Verbs and Nouns","text":"<ul> <li>Use descriptive verbs and nouns in function names.</li> <li>Clearly indicate function action and subject.</li> <li>Examples: <code>CalculateTotal</code>, <code>ReadFile</code>, <code>PrintMessage</code>.</li> </ul>"},{"location":"references/codingConventions/#naming-conventions-for-constructors-getters-and-setters","title":"Naming Conventions for Constructors, Getters, and Setters","text":"<ul> <li>Constructors: Prefixed with <code>New</code> or <code>Make</code> indicating creation, e.g., <code>NewUser</code> or <code>MakeConnection</code>.</li> <li>Getters: No prefix; use the property name directly, avoiding the <code>Get</code> prefix, e.g., <code>Name()</code> instead of <code>GetName()</code>.</li> <li>Setters: Prefixed with <code>Set</code> followed by the property name, e.g., <code>SetName(value)</code>.</li> </ul>"},{"location":"references/codingConventions/#error-handling-functions-and-their-naming-patterns","title":"Error Handling Functions and Their Naming Patterns","text":"<ul> <li>Name functions that return errors with action verbs: <code>Open</code>, <code>Read</code>, <code>Write</code>.</li> <li>It's clear from the context that an error can be returned. Context should suggest an error can be returned, e.g., <code>os.Open</code>.</li> <li>Avoid \"Error\" in names; the return type already implies an error possibility.</li> </ul>"},{"location":"references/codingConventions/#interface-names","title":"Interface Names","text":""},{"location":"references/codingConventions/#single-method-interfaces-with-er-suffix","title":"Single Method Interfaces with \"er\" Suffix","text":"<ul> <li>For interfaces with a single method, use a name ending in \"-er\" to describe the action performed by the method, such as <code>Reader</code>, <code>Writer</code>, or <code>Closer</code>.</li> </ul>"},{"location":"references/codingConventions/#use-of-descriptive-names-for-more-complex-interfaces","title":"Use of Descriptive Names for More Complex Interfaces","text":"<ul> <li>For interfaces with multiple methods, choose descriptive names that capture the overall functionality or role of the interface, rather than following the \"-er\" suffix rule.</li> <li>For example, <code>FileSystem</code> for an interface that encapsulates various file system operations or <code>DatabaseConnector</code> for an interface managing database connections.</li> </ul>"},{"location":"references/codingConventions/#type-names","title":"Type Names","text":""},{"location":"references/codingConventions/#avoidance-of-redundant-or-tautological-names","title":"Avoidance of Redundant or Tautological Names","text":"<ul> <li>Avoid names that repeat the package name or provide no additional information about the type.</li> <li>For instance, instead of <code>http.HttpClient</code>, simply use <code>http.Client</code> to prevent redundancy.</li> </ul>"},{"location":"references/codingConventions/#use-of-clear-and-specific-names-for-custom-types","title":"Use of Clear and Specific Names for Custom Types","text":"<ul> <li>Choose names that clearly and specifically describe what the custom type represents or does, ensuring they are intuitive and meaningful.</li> <li>For example, <code>Block</code> for a type representing block information, or <code>SubtreeProcessor</code> for a type that processes subtrees.</li> </ul>"},{"location":"references/codingConventions/#commenting","title":"Commenting","text":""},{"location":"references/codingConventions/#best-practices-for-writing-clear-and-helpful-comments","title":"Best Practices for Writing Clear and Helpful Comments","text":"<ul> <li>Descriptive Comments: Write comments that explain the \"why\" behind code logic, not just the \"what\". This helps readers understand the purpose and reasoning.</li> <li>Package Comments: Start with a package comment in a file named <code>doc.go</code> that describes the package's purpose and provides an overview of its functionality.</li> <li>Function Comments: Begin with the function name and describe what the function does, its parameters, return values, and any side effects.</li> <li>Avoid Redundant Comments: Don't state the obvious. Focus on providing additional context or information not readily clear from the code itself.</li> </ul>"},{"location":"references/codingConventions/#error-handling","title":"Error Handling","text":""},{"location":"references/codingConventions/#effective-use-of-gos-error-handling-paradigm","title":"Effective Use of Go's Error Handling Paradigm","text":"<ul> <li>Explicit Error Checking: Always check for errors by comparing the returned error to <code>nil</code>. Handle the error appropriately where it occurs.</li> </ul> <pre><code>if err != nil {\n // Handle error\n}\n</code></pre>"},{"location":"references/codingConventions/#patterns-for-error-creation-propagation-and-checking","title":"Patterns for Error Creation, Propagation, and Checking","text":"<ul> <li>Propagating Errors: When an error occurs, return it to the caller instead of handling it unless you can resolve it, or it's critical to continue execution.</li> </ul> <pre><code>if err != nil {\n return err\n}\n</code></pre> <ul> <li>Custom Error Types: For more complex error handling, define custom error types that implement the <code>error</code> interface.</li> </ul> <p>Additional patterns and examples for custom error types will be added in future revisions.</p>"},{"location":"references/codingConventions/#concurrency","title":"Concurrency","text":"<p>Go's concurrency model, centered around goroutines and channels, enables efficient parallel execution. Here are best practices and patterns for its effective use.</p>"},{"location":"references/codingConventions/#best-practices-for-using-goroutines-and-channels","title":"Best Practices for Using Goroutines and Channels","text":"<ul> <li>Start Simple: Begin with a simple design. Use goroutines for asynchronous tasks and channels for communication.</li> <li>Avoid Shared State: Prefer channels to share data between goroutines instead of shared memory to avoid race conditions.</li> <li>Buffered Channels: Use buffered channels when you know the capacity ahead of time or to limit the number of goroutines running concurrently.</li> </ul>"},{"location":"references/codingConventions/#patterns-for-avoiding-common-concurrency-pitfalls","title":"Patterns for Avoiding Common Concurrency Pitfalls","text":"<ul> <li>Worker Pools: Implement worker pools to control the number of goroutines performing work simultaneously, preventing excessive resource consumption.</li> <li>Select Statement: Use the <code>select</code> statement to wait on multiple channel operations, enhancing control over channel communication.</li> <li>Context Package: Use the <code>context</code> package to manage and cancel goroutines, providing a way to control goroutine lifecycles and prevent leaks.</li> </ul> <p>By following these guidelines, you can leverage Go's concurrency features effectively, creating programs that are scalable, efficient, and robust.</p> <pre><code>jobs := make(chan Job, 100)\nresults := make(chan Result, 100)\n\nfor w := 1; w &lt;= 3; w++ {\n    go worker(w, jobs, results)\n}\n\nfor _, j := range jobList {\n    jobs &lt;- j\n}\nclose(jobs)\n</code></pre>"},{"location":"references/codingConventions/#testing","title":"Testing","text":""},{"location":"references/codingConventions/#writing-effective-unit-tests-using-the-testing-package","title":"Writing Effective Unit Tests Using the \"testing\" Package","text":"<ul> <li>Basic Structure: Utilize the <code>testing.T</code> type to create tests. Each test function should be named <code>TestXxx</code>, where <code>Xxx</code> does not start with a lowercase letter.</li> </ul> <pre><code>func TestXxx(t *testing.T) {\n  // Test code here\n}\n</code></pre> <ul> <li>Assert Conditions: Use <code>if</code> statements or an assertion library like <code>testify</code> to test conditions within your tests.</li> </ul> <pre><code>if got != want {\n  t.Errorf(\"got %q, want %q\", got, want)\n}\n</code></pre>"},{"location":"references/codingConventions/#use-of-table-driven-tests-for-comprehensive-coverage","title":"Use of Table-Driven Tests for Comprehensive Coverage","text":"<ul> <li>Implement table-driven tests by defining test cases as structs in a slice, iterating over them in a single test function.</li> </ul> <pre><code>var tests = []struct {\n    input string\n    want  string\n}{\n    {\"input1\", \"want1\"},\n    {\"input2\", \"want2\"},\n    // More test cases\n}\n\nfor _, tt := range tests {\n    t.Run(tt.input, func(t *testing.T) {\n        got := MyFunc(tt.input)\n        require.Equal(t, tt.want, got, \"for input %s\", tt.input)\n    })\n}\n</code></pre>"},{"location":"references/codingConventions/#use-of-testify-for-cleaner-assertions","title":"Use of <code>testify</code> for Cleaner Assertions","text":"<ul> <li>The <code>testify</code> package offers expressive assertion functions. For Teranode, prefer the <code>require</code> package for assertions to ensure test failures stop further execution.</li> </ul> <pre><code>require.NoError(t, err)\nrequire.Equal(t, expected, actual)\nrequire.InDelta(t, 3.14, result, 0.01) // for float comparisons\n</code></pre> <ul> <li>This approach is compatible with tools like <code>golangci-lint</code> and <code>testifylint</code>.</li> </ul>"},{"location":"references/codingConventions/#best-practices","title":"Best Practices","text":"<ul> <li>Name your test methods with consistent, meaningful naming (e.g., <code>TestFoo_Create_WhenValidInput</code>).</li> <li>Prefer <code>require</code> over <code>assert</code> when failure should halt the test.</li> <li>Avoid using <code>t.Parallel()</code> unless concurrency is explicitly tested.</li> <li>Favor subtests (<code>t.Run</code>) for grouped behaviors.</li> </ul>"},{"location":"references/codingConventions/#code-formatting-and-linting","title":"Code Formatting and Linting","text":""},{"location":"references/codingConventions/#formatting","title":"Formatting","text":"<ul> <li>All code must be formatted using the standard Go toolchain:</li> </ul> <pre><code>go fmt ./...\n</code></pre> <ul> <li>Consistent formatting helps ensure clarity in code reviews and version control diffs.</li> </ul>"},{"location":"references/codingConventions/#linting","title":"Linting","text":"<ul> <li>Use <code>golangci-lint</code> to enforce code correctness and style.</li> </ul> <pre><code>golangci-lint run\n</code></pre> <ul> <li>Linting rules help catch common bugs, ensure consistent idioms, and avoid style drift.</li> <li>Make sure to configure <code>.golangci.yml</code> in the root of the project for consistent linting behavior across environments.</li> </ul>"},{"location":"references/codingConventions/#linting-rules","title":"Linting Rules","text":"<p>The project uses <code>golangci-lint</code> for enforcing code quality. Key configurations include:</p> <ul> <li>Filename Conventions: Checked via CI (separate script).</li> <li> <p>Enabled Linters:</p> </li> <li> <p><code>depguard</code>: Prevents importing disallowed packages like <code>errors</code> (use standard <code>errors</code> instead).</p> </li> <li><code>forbidigo</code>: Disallows use of <code>fmt.Errorf</code> (use <code>errors.New</code> instead).</li> <li><code>gocognit</code>: Flags functions exceeding complexity threshold (200).</li> <li>Additional linters for style, security, imports, and error handling (<code>gosec</code>, <code>goimports</code>, <code>errcheck</code>, etc.).</li> </ul>"},{"location":"references/codingConventions/#exceptions","title":"Exceptions","text":"<ul> <li> <p>Linting is excluded for:</p> </li> <li> <p><code>services/legacy</code> (multiple linters ignored)</p> </li> <li><code>errors</code> package (for <code>depguard</code> and <code>forbidigo</code>)</li> <li>All files in <code>vendor/</code></li> </ul>"},{"location":"references/codingConventions/#pull-requests","title":"Pull Requests","text":"<ul> <li>PRs should pass all formatting and linting checks before being merged.</li> <li>Reviewers may reject PRs that fail to meet formatting or linting guidelines.</li> </ul>"},{"location":"references/codingConventions/#optional-tools","title":"Optional Tools","text":"<ul> <li><code>goimports</code> \u2013 Formats and organizes import sections.</li> <li><code>go vet</code> \u2013 Reports suspicious constructs.</li> <li><code>staticcheck</code> \u2013 A robust static analysis tool for additional checks.</li> </ul>"},{"location":"references/codingConventions/#dependency-management","title":"Dependency Management","text":""},{"location":"references/codingConventions/#use-of-modules-for-managing-dependencies","title":"Use of Modules for Managing Dependencies","text":"<ul> <li> <p>Modules: Adopt Go modules for dependency management by initializing a new module via <code>go mod init &lt;module name&gt;</code>, which creates a <code>go.mod</code> file to track your project's dependencies.</p> <pre><code>go mod init mymodule\n</code></pre> </li> <li> <p>Dependency Tracking: The <code>go.mod</code> file lists the specific versions of external packages your project depends on, ensuring consistent builds.</p> </li> </ul>"},{"location":"references/codingConventions/#strategies-for-keeping-dependencies-up-to-date-and-secure","title":"Strategies for Keeping Dependencies Up to Date and Secure","text":"<ul> <li> <p>Regular Updates: Regularly run <code>go get -u</code> to update your dependencies to their latest minor or patch versions.</p> <pre><code>go get -u\n</code></pre> </li> <li> <p>Vulnerability Checks: Use tools like <code>go list -m all | go get -u</code> and <code>go mod tidy</code> to find and fix known vulnerabilities in dependencies, and to clean up unused dependencies.</p> <pre><code>go list -m all | go get -u\ngo mod tidy\n</code></pre> </li> <li> <p>Version Pinning: Pin dependencies to specific versions or ranges in your <code>go.mod</code> file to avoid breaking changes and ensure reproducibility.</p> </li> </ul>"},{"location":"references/codingConventions/#ai-ruleset","title":"AI Ruleset","text":"<p>The following is a set of rules that AI synthesized to help guide the development of Teranode. These rules are not exhaustive and may be updated as the project evolves.</p> <pre><code>Teranode Coding\u2011Standards Enforcement Rules\n\nThese rules are to be used on the Teranode project for Go conventions and best practices.\n\n1. Naming\n   1.1 Packages \u2013 Packages must be short, lowercase, one word; avoid generic names such as util, helper, common; wrapping a standard\u2011library helper with a clarifier like httputil is allowed only when you are strictly extending standard\u2011library behaviour.\n   1.2 Variables \u2013 Exported variables use CamelCase (HTTPTimeout, MaxAge); internal variables use camelCase (localTime, cfg); idiomatic short names are fine for loop indices, errors, and temporaries (i, j, err, tmp).\n   1.3 Functions / methods \u2013 Names follow a VerbNoun pattern (CalculateTotal, ReadFile); constructors are NewXxx or MakeXxx; getters use the field name directly (Name()), never GetName; setters are SetXxx(value); helper functions that can return an error use a simple verb such as Open or Write.\n   1.4 Interfaces \u2013 A single\u2011method interface ends in \u201c\u2011er\u201d (Reader); multi\u2011method interfaces use a descriptive collective name (FileSystem, DatabaseConnector).\n   1.5 Types \u2013 Do not repeat the package name (http.Client, not http.HttpClient); choose clear, domain\u2011specific names such as Block or SubtreeProcessor.\n   1.6 Filenames \u2013 All Go files must use snake\\_case.go format (e.g., handler\\_test.go, data\\_loader.go); camelCase, kebab-case, and PascalCase are not allowed. A CI step validates this rule and outputs a SonarQube-compatible JSON report if violated.\n\n2. Comments \u2013 Comments explain why, not the obvious what; include a package overview in doc.go; each function comment starts with the function name and describes parameters, returns, and side effects.\n\n3. Error handling \u2013 Always check errors immediately: if err != nil { handle or propagate }; propagate unless you can fully resolve; custom error types must implement the error interface.\n\n4. Concurrency \u2013 Keep designs simple; add goroutines only when needed; prefer channels over shared state; use buffered channels and worker pools to bound concurrency; always accept context.Context for cancellation.\n\n5. Testing \u2013 Use Go\u2019s testing package with testify; test functions are named TestXxx and may use subtests with t.Run; prefer table\u2011driven tests; assertions use require helpers such as require.NoError and require.InDelta; do not call t.Parallel() unless the test explicitly validates concurrency; when returning code snippets, omit package and import blocks.\n\n6. Formatting and linting \u2013\n  * Code must pass: go fmt ./...\n  * Code must pass: golangci-lint run\n  * Optional helpers: goimports, go vet, staticcheck\n  * Linter configuration includes the following rules:\n    \u2022 depguard: Prevents use of disallowed packages (e.g., \"errors\" package is blocked in favor of the standard library)\n    \u2022 forbidigo: Blocks use of fmt.Errorf; enforces errors.New instead\n    \u2022 gocognit: Flags functions with high cognitive complexity (threshold set at 200)\n    \u2022 Other enabled linters include: asciicheck, errcheck, goconst, gocritic, gci, goimports, gosec, misspell, prealloc, unconvert, whitespace, wsl\n</code></pre>"},{"location":"references/errorHandling/","title":"\ud83d\udcd8 Error Handling","text":""},{"location":"references/errorHandling/#index","title":"Index","text":"<ol> <li>Introduction<ul> <li>1.1. Go Errors</li> <li>1.2. Go Errors: Best Practices</li> <li>1.3. Sentinel Errors</li> <li>1.4 Wrapping Errors</li> </ul> </li> <li>Error Handling in Teranode<ul> <li>2.1. Error Handling Strategy</li> <li>2.2. Sentinel Errors in Teranode</li> <li>2.3. Error Wrapping in Teranode</li> <li>Error Creation and Wrapping</li> <li>Unwrapping Errors</li> <li>Best Practices for Error Wrapping</li> <li>2.4. gRPC Error Wrapping in Teranode</li> <li>Converting Teranode Errors to gRPC Errors</li> <li>Converting gRPC Errors back to Teranode Errors</li> <li>Practical Example</li> <li>Best Practices</li> <li>2.5. Extra Data in Error Handling</li> <li>Error Structure</li> <li>Purpose and Usage</li> <li>Example Implementation</li> <li>Type Assertions with Extra Data</li> <li>Usage Example</li> <li>Best Practices</li> <li>2.6. Error Protobuf</li> <li>Error Protocol Definition</li> <li>Key Components</li> <li>Purpose and Benefits</li> <li>Integration with Teranode's Error Handling</li> <li>Best Practices</li> <li>2.7. Unit Tests</li> </ul> </li> </ol>"},{"location":"references/errorHandling/#1-introduction","title":"1. Introduction","text":""},{"location":"references/errorHandling/#11-go-errors","title":"1.1. Go Errors","text":"<p>In Go (Golang), error handling is managed through an interface called <code>error</code>. This interface is defined in the built-in <code>errors</code> package.</p> <p>The <code>error</code> interface in Go is defined as follows:</p> <pre><code>type error interface {\n    Error() string\n}\n</code></pre> <p>Any type that implements this <code>Error() string</code> method satisfies the <code>error</code> interface. This \"convention over configuration\" approach encourages Go programs to handle errors explicitly by checking whether an error is nil before proceeding with normal operations.</p> <p>Custom error types can be created by defining types that implement the <code>error</code> interface. This is useful for conveying error context or state beyond just a text message. Here\u2019s a simple example:</p> <pre><code>type MyError struct {\n    Msg string\n    File string\n    Line int\n}\n\nfunc (e *MyError) Error() string {\n    return fmt.Sprintf(\"%s:%d: %s\", e.File, e.Line, e.Msg)\n}\n</code></pre> <p>Functions that can result in an error typically return an error object as their last return value. The calling function should always check this error before proceeding.</p> <pre><code>result, err := someFunction()\nif err != nil {\n    // handle error\n    log.Fatal(err)\n}\n// proceed with using result\n</code></pre> <p>This pattern encourages handling errors at the place they occur, rather than propagating them up the stack implicitly via exceptions.</p>"},{"location":"references/errorHandling/#12-go-errors-best-practices","title":"1.2. Go Errors: Best Practices","text":"<ul> <li>Always check for errors where they might occur. Do not ignore returned error values or propagate them up.</li> <li>Consider defining your custom error types for complex systems, which can add clarity and control in error handling.</li> <li>Use standard <code>errors.Is</code> (to check for a specific errors) and <code>errors.As</code> (to check for type compatibility and type assertion).</li> <li>Parsing the text of an error message to determine the type or cause of an error is considered an anti-pattern. Instead, use custom error types or error wrapping to provide context.</li> </ul>"},{"location":"references/errorHandling/#13-sentinel-errors","title":"1.3. Sentinel Errors","text":"<p>Sentinel errors in Go are predefined error variables that represent specific error conditions. These are declared at the package level and used consistently throughout the code to signify particular states or outcomes. This pattern is useful for comparing returned errors to known values, providing a consistent method for error checking.</p> <p>Since sentinel errors are variables, they can easily be compared using <code>errors.Is()</code> to check if a particular error has occurred. Using sentinel errors keeps error handling simple and explicit, requiring only basic checks against predefined values.</p> <p>Here's a basic example of defining and using sentinel errors in a Go program:</p> <pre><code>package main\n\nimport (\n    \"errors\"\n    \"fmt\"\n)\n\n// Define sentinel errors\nvar (\n    ErrNotFound = errors.New(\"not found\")\n    ErrPermissionDenied = errors.New(\"permission denied\")\n)\n\nfunc findItem(id string) error {\n    // Simulated condition: item not found\n    if id != \"expected_id\" {\n        return ErrNotFound\n    }\n    return nil\n}\n\nfunc main() {\n    err := findItem(\"unknown_id\")\n    if errors.Is(err, ErrNotFound) {\n        fmt.Println(\"Item not found!\")\n    } else if errors.Is(err, ErrPermissionDenied) {\n        fmt.Println(\"Access denied.\")\n    } else if err != nil {\n        fmt.Println(\"An unexpected error occurred:\", err)\n    } else {\n        fmt.Println(\"Item found.\")\n    }\n}\n</code></pre> <p>While useful, sentinel errors have limitations, especially as applications scale. Code that uses sentinel errors is tightly coupled to these errors, making changes to error definitions potentially disruptive.</p>"},{"location":"references/errorHandling/#14-wrapping-errors","title":"1.4 Wrapping Errors","text":"<p>In Go, error wrapping is a technique used to add additional context to an error while preserving the original error itself. This approach helps maintain a \"chain\" of errors, which can be useful for debugging and error handling in complex microservices.</p> <p>Example of wrapping an error:</p> <pre><code>package main\n\nimport (\n    \"errors\"\n    \"fmt\"\n)\n\nfunc operation1() error {\n    return errors.New(\"base error\")\n}\n\nfunc operation2() error {\n    err := operation1()\n    if err != nil {\n        // Wrap the error with additional context\n        return fmt.Errorf(\"operation2 failed: %w\", err)\n    }\n    return nil\n}\n\nfunc main() {\n    err := operation2()\n    if err != nil {\n        fmt.Println(\"An error occurred:\", err)\n    }\n}\n</code></pre> <p>On the other hand, unwrapping is the process of retrieving the original error from a wrapped error. You can unwrap errors manually using the <code>Unwrap</code> method provided by the <code>errors</code> package, or you can use higher-level utilities like <code>errors.Is</code> and <code>errors.As</code> to check for specific errors or extract errors of specific types.</p> <p>In this context, <code>errors.Is</code> and <code>errors.As</code> are used to check errors in an error chain without needing to manually call <code>Unwrap</code> repeatedly</p> <ul> <li><code>errors.Is</code>: checks whether any error in the chain matches a specific error.</li> <li><code>errors.As</code>: finds the first error in the chain that matches a specific type and provides access to it.</li> </ul> <p>Benefits of Wrapping Errors:</p> <ol> <li>Wrapping errors helps in preserving the context where an error occurred, without losing information about the original error.</li> <li>Maintaining an error chain aids in diagnosing issues by providing a traceable path of what went wrong and where.</li> <li>Allows developers to decide how much error information to expose to different parts of the application or to the end user, enhancing security and usability.</li> </ol>"},{"location":"references/errorHandling/#2-error-handling-in-teranode","title":"2. Error Handling in Teranode","text":""},{"location":"references/errorHandling/#21-error-handling-strategy","title":"2.1. Error Handling Strategy","text":"<p>Teranode follows a structured error handling strategy that combines the use of predefined error types, error wrapping, and consistent error creation patterns. This approach ensures clear, consistent, and traceable error handling throughout the application.</p> <p></p> <p>The <code>errors/Error.go</code> file contains the core error type definition and functions for creating, wrapping, and unwrapping errors. The <code>errors/Error_types.go</code> file defines specific error types and provides functions for creating these errors.</p> <p>An error in Teranode is defined as a struct containing an error code, a message, an optional wrapped error, and optional extra data:</p> <pre><code>type Error struct {\n    Code       ERR\n    Message    string\n    WrappedErr error\n    Data       ErrData\n}\n</code></pre> <p>The <code>ERR</code> type is an enum defined in the <code>error.proto</code> file, providing a standardized set of error codes across the system.</p> <p>Teranode emphasizes the use of specific error creation functions for different error types. These functions are defined in <code>Error_types.go</code> and follow a naming convention of <code>New&lt;ErrorType&gt;Error</code>. For example:</p> <pre><code>func NewStorageError(message string, params ...interface{}) error {\n    return New(ERR_STORAGE_ERROR, message, params...)\n}\n</code></pre> <p>These functions automatically handle error wrapping when an existing error is passed as the last parameter. They also support sprintf-style formatting for the error message. For instance:</p> <pre><code>err := errors.NewStorageError(\"failed to store block: %s\", blockHash, existingErr)\n</code></pre> <p>This approach allows for consistent error creation, automatic error wrapping, and formatted error messages, enhancing the overall error handling strategy in Teranode.</p>"},{"location":"references/errorHandling/#22-sentinel-errors-in-teranode","title":"2.2. Sentinel Errors in Teranode","text":"<p>Sentinel errors are predefined errors that serve as fixed references for common error conditions. They provide a standard and efficient way to recognize and manage common specific error scenarios.</p> <p>Code Example:</p> <p>The sentinel errors are defined in the <code>errors/Error_types.go</code> file, along with corresponding functions to create these errors:</p> <pre><code>package errors\n\nvar (\n    ErrUnknown             = New(ERR_UNKNOWN, \"unknown error\")\n    ErrInvalidArgument     = New(ERR_INVALID_ARGUMENT, \"invalid argument\")\n    ErrThresholdExceeded   = New(ERR_THRESHOLD_EXCEEDED, \"threshold exceeded\")\n    ErrNotFound            = New(ERR_NOT_FOUND, \"not found\")\n    // Other sentinel errors follow...\n)\n\nfunc NewUnknownError(message string, params ...interface{}) error {\n    return New(ERR_UNKNOWN, message, params...)\n}\n\nfunc NewInvalidArgumentError(message string, params ...interface{}) error {\n    return New(ERR_INVALID_ARGUMENT, message, params...)\n}\n\nfunc NewThresholdExceededError(message string, params ...interface{}) error {\n    return New(ERR_THRESHOLD_EXCEEDED, message, params...)\n}\n\nfunc NewNotFoundError(message string, params ...interface{}) error {\n    return New(ERR_NOT_FOUND, message, params...)\n}\n\n// Other error creation functions follow...\n</code></pre> <p>Each sentinel error is created using the <code>New</code> function, ensuring that each error has a unique code and a succinct message. The new error creation functions allow for more flexible error creation with custom messages and automatic error wrapping.</p> <p>The sentinel errors are useful for error comparisons and decision-making in the Teranode business logic. By comparing returned errors to these predefined values, functions can determine the next steps without ambiguity.</p> <p>How to Use Example:</p> <p>When a function encounters an error, it can use the new error creation functions. Here's an example of how a function might use these errors:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"errors\"\n    \"github.com/your-project/errors\" // Import your custom errors package\n)\n\n// Data represents a simple data structure\ntype Data struct {\n    ID string\n}\n\n// fetchData simulates fetching data and returning different types of errors\nfunc fetchData(id string) (*Data, error) {\n    if id == \"\" {\n        return nil, errors.NewInvalidArgumentError(\"empty ID provided\")\n    }\n    if id == \"notfound\" {\n        return nil, errors.NewNotFoundError(\"data not found for ID: %s\", id)\n    }\n    // Simulate a wrapped error\n    if id == \"dberror\" {\n        dbErr := fmt.Errorf(\"database connection failed\")\n        return nil, errors.NewStorageError(\"failed to fetch data\", dbErr)\n    }\n    return &amp;Data{ID: id}, nil\n}\n\nfunc main() {\n    // Example usage\n    ids := []string{\"\", \"notfound\", \"dberror\", \"validid\"}\n\n    for _, id := range ids {\n        data, err := fetchData(id)\n        if err != nil {\n            // Use errors.Is to check for specific error types\n            if errors.Is(err, errors.ErrInvalidArgument) {\n                fmt.Printf(\"Invalid argument error: %v\\n\", err)\n            } else if errors.Is(err, errors.ErrNotFound) {\n                fmt.Printf(\"Not found error: %v\\n\", err)\n            } else if errors.Is(err, errors.ErrStorageError) {\n                fmt.Printf(\"Storage error: %v\\n\", err)\n\n                // Use errors.As to get more details about the error\n                var storageErr *errors.Error\n                if errors.As(err, &amp;storageErr) {\n                    fmt.Printf(\"Storage error details - Code: %v, Message: %s\\n\",\n                        storageErr.Code, storageErr.Message)\n                    if storageErr.WrappedErr != nil {\n                        fmt.Printf(\"Wrapped error: %v\\n\", storageErr.WrappedErr)\n                    }\n                }\n            } else {\n                fmt.Printf(\"Unknown error: %v\\n\", err)\n            }\n        } else {\n            fmt.Printf(\"Data fetched successfully: %v\\n\", data)\n        }\n        fmt.Println()\n    }\n}\n</code></pre>"},{"location":"references/errorHandling/#23-error-wrapping-in-teranode","title":"2.3. Error Wrapping in Teranode","text":"<p>Error wrapping in Teranode enables the creation of nested errors and their propagation through different layers of the application. This mechanism allows an error to carry its history along with new context, effectively creating a chain of errors that leads back to the original issue.</p> <p>By maintaining a trail of errors, developers can trace back through the execution flow to understand what led to the error. Additionally, different layers of the application can decide how to handle errors based on their type and origin.</p>"},{"location":"references/errorHandling/#error-creation-and-wrapping","title":"Error Creation and Wrapping","text":"<p>The <code>New</code> function in Teranode's error package creates and returns a pointer to an <code>Error</code> struct, which includes fields for the error code, a message, and an optional wrapped error.</p> <pre><code>func New(code ERR, message string, params ...interface{}) *Error {\n    var wErr *Error\n\n    // Extract the wrapped error, if present\n    if len(params) &gt; 0 {\n        if err, ok := params[len(params)-1].(*Error); ok {\n            wErr = err\n            params = params[:len(params)-1]\n        }\n    }\n\n    // Format the message with the remaining parameters\n    if len(params) &gt; 0 {\n        message = fmt.Sprintf(message, params...)\n    }\n\n    // Check if the code exists in the ErrorConstants enum\n    if _, ok := ERR_name[int32(code)]; !ok {\n        return &amp;Error{\n            Code:       code,\n            Message:    \"invalid error code\",\n            WrappedErr: wErr,\n        }\n    }\n\n    return &amp;Error{\n        Code:       code,\n        Message:    message,\n        WrappedErr: wErr,\n    }\n}\n</code></pre> <p>The <code>New</code> function operates as follows:</p> <ol> <li>It takes an error code, a message, and a variadic <code>params</code> slice which may include one error to be wrapped.</li> <li>If the last parameter in <code>params</code> is an error, it's treated as the error to be wrapped. This error is stored in the <code>WrappedErr</code> field of the <code>Error</code> struct.</li> <li>Any additional parameters are used to format the message string using <code>fmt.Sprintf</code>, allowing for dynamic message content based on runtime values.</li> </ol> <p>Here's a practical example of using the <code>New</code> function to create and wrap errors:</p> <pre><code>func someOperation() error {\n    err := doSomethingThatMightFail()\n    if err != nil {\n        // Wrap the error with additional context and return\n        return New(ErrInvalidArgument, \"operation failed due to underlying error: %v\", err)\n    }\n    return nil\n}\n</code></pre> <p>In this scenario, <code>someOperation</code> calls another function and wraps any error returned by this function with additional context, using the custom <code>New</code> function.</p>"},{"location":"references/errorHandling/#unwrapping-errors","title":"Unwrapping Errors","text":"<p>The <code>Unwrap</code> method in the <code>Error</code> structure returns the error that was wrapped within the current error, if any:</p> <pre><code>func (e *Error) Unwrap() error {\n    return e.WrappedErr\n}\n</code></pre> <p>This method allows the use of Go's built-in <code>errors.Unwrap</code> function to continue unwrapping through multiple layers of nested wrapped errors.</p> <p>To effectively utilize the unwrapping functionality, developers can use a loop or a conditional check to explore the chain of errors. Here's an example that demonstrates how to use the <code>Unwrap</code> method to trace back through wrapped errors:</p> <pre><code>// Assuming `someOperation` returns an error that may be wrapped multiple times\nerr := someOperation()\nfor err != nil {\n    fmt.Printf(\"Error encountered: %v\\n\", err)\n    // Attempt to unwrap the error\n    if unwrappedErr := errors.Unwrap(err); unwrappedErr != nil {\n        err = unwrappedErr\n    } else {\n        break\n    }\n}\n</code></pre> <p>In this example, <code>errors.Unwrap</code> is used in a loop to keep unwrapping the error until no more wrapped errors are found, effectively reaching the original error. This approach is particularly useful when diagnosing an issue and understanding the sequence of errors that led to the final state.</p>"},{"location":"references/errorHandling/#best-practices-for-error-wrapping","title":"Best Practices for Error Wrapping","text":"<ol> <li>Unwrap errors only when necessary. For example, when handling specific error types differently, unwrap errors until finding the type you're looking for.</li> <li>Always log or monitor the full error chain before unwrapping in production environments to preserve the context and details of what went wrong.</li> <li>Use error wrapping to add context at each layer of the application, making it easier to trace the flow of execution that led to an error.</li> <li>When wrapping errors, include relevant information that might help in debugging, but be cautious not to include sensitive data.</li> </ol>"},{"location":"references/errorHandling/#24-grpc-error-wrapping-in-teranode","title":"2.4. gRPC Error Wrapping in Teranode","text":"<p>As a distributed system utilizing gRPC, Teranode implements a robust mechanism for wrapping and unwrapping errors to ensure consistent error handling practices when errors cross service boundaries.</p>"},{"location":"references/errorHandling/#converting-teranode-errors-to-grpc-errors","title":"Converting Teranode Errors to gRPC Errors","text":"<p>The <code>WrapGRPC</code> function in <code>Error.go</code> is responsible for converting Teranode's custom error types into gRPC-compatible errors. This function uses the <code>status</code> package from <code>google.golang.org/grpc</code> to create gRPC status errors.</p> <pre><code>package errors\n\nimport (\n    \"google.golang.org/grpc/codes\"\n    \"google.golang.org/grpc/status\"\n    \"google.golang.org/protobuf/types/known/anypb\"\n)\n\nfunc WrapGRPC(err error) error {\n    if err == nil {\n        return nil\n    }\n\n    var uErr *Error\n    if errors.As(err, &amp;uErr) {\n        details, _ := anypb.New(&amp;TError{\n            Code:    uErr.Code,\n            Message: uErr.Message,\n        })\n        st := status.New(ErrorCodeToGRPCCode(uErr.Code), uErr.Message)\n        st, err := st.WithDetails(details)\n        if err != nil {\n            return status.New(codes.Internal, \"error adding details to gRPC status\").Err()\n        }\n        return st.Err()\n    }\n    return status.New(ErrorCodeToGRPCCode(ErrUnknown.Code), ErrUnknown.Message).Err()\n}\n</code></pre> <p>The <code>WrapGRPC</code> function performs the following steps:</p> <ol> <li>If the input error is nil, it returns nil.</li> <li>It attempts to cast the error to Teranode's custom <code>Error</code> type.</li> <li>If successful, it creates a new <code>TError</code> with the error's code and message.</li> <li>It then creates a new gRPC status with the corresponding gRPC code and message.</li> <li>The <code>TError</code> is added as a detail to the gRPC status.</li> <li>If the error is not a Teranode <code>Error</code>, it creates a generic \"unknown\" gRPC error.</li> </ol> <p>This process allows the receiving service to understand not only the nature of the error but also to receive contextual metadata.</p>"},{"location":"references/errorHandling/#converting-grpc-errors-back-to-teranode-errors","title":"Converting gRPC Errors back to Teranode Errors","text":"<p>When a Teranode service receives a gRPC error, it needs to convert it back into an internal error format. The <code>UnwrapGRPC</code> function in <code>Error.go</code> handles this process:</p> <pre><code>package errors\n\nimport (\n    \"google.golang.org/grpc/status\"\n    \"google.golang.org/protobuf/proto\"\n    \"google.golang.org/protobuf/types/known/anypb\"\n)\n\nfunc UnwrapGRPC(err error) error {\n    if err == nil {\n        return nil\n    }\n\n    st, ok := status.FromError(err)\n    if !ok {\n        return err // Not a gRPC status error\n    }\n\n    for _, detail := range st.Details() {\n        var teranodeErr TError\n        if err := anypb.UnmarshalTo(detail.(*anypb.Any), &amp;teranodeErr, proto.UnmarshalOptions{}); err == nil {\n            return New(teranodeErr.Code, teranodeErr.Message)\n        }\n    }\n\n    return New(ErrUnknown.Code, st.Message())\n}\n</code></pre> <p>The <code>UnwrapGRPC</code> function:</p> <ol> <li>Checks if the input is a gRPC status error.</li> <li>If it is, it iterates through the error details.</li> <li>It attempts to unmarshal each detail into a <code>TError</code>.</li> <li>If successful, it creates a new Teranode <code>Error</code> with the extracted code and message.</li> <li>If no matching detail is found, it creates a generic \"unknown\" Teranode error.</li> </ol> <p>This process allows Teranode to reconstruct its internal error types from gRPC errors, preserving error context across service boundaries.</p>"},{"location":"references/errorHandling/#practical-example","title":"Practical Example","text":"<p>Here's an end-to-end example of how gRPC error wrapping and unwrapping works in Teranode:</p> <ol> <li>In the Blockchain Server, when processing a <code>GetBlock</code> request:</li> </ol> <pre><code>blockHash, err := chainhash.NewHash(request.Hash)\nif err != nil {\n    return nil, errors.WrapGRPC(errors.New(errors.ERR_BLOCK_NOT_FOUND, \"[Blockchain] request's hash is not valid\", err))\n}\n</code></pre> <ol> <li>In the Blockchain Client, when handling the response:</li> </ol> <pre><code>resp, err := c.client.GetBlock(ctx, &amp;blockchain_api.GetBlockRequest{\n   Hash: blockHash[:],\n})\nif err != nil {\n   return nil, errors.UnwrapGRPC(err)\n}\n</code></pre> <p>In this example, if an error occurs in the server, it's wrapped as a gRPC error before being sent to the client. The client then unwraps the gRPC error, allowing it to handle the error as a standard Teranode error.</p>"},{"location":"references/errorHandling/#best-practices","title":"Best Practices","text":"<ol> <li>Maintain a clear mapping between Teranode error codes and gRPC status codes to ensure error meanings are preserved across service boundaries.</li> <li>Always wrap errors before sending them across gRPC boundaries, and unwrap them when receiving.</li> <li>Include relevant error details when wrapping, but be cautious about including sensitive information.</li> <li>Handle unwrapping errors gracefully, accounting for the possibility that the received error might not be a properly wrapped Teranode error.</li> </ol>"},{"location":"references/errorHandling/#25-extra-data-in-error-handling","title":"2.5. Extra Data in Error Handling","text":"<p>Teranode's error handling system includes a feature for attaching additional contextual information to errors. This is implemented through the <code>Data</code> field in the <code>Error</code> struct.</p>"},{"location":"references/errorHandling/#error-structure","title":"Error Structure","text":"<p>The Teranode error is represented by the following struct:</p> <pre><code>type Error struct {\n    Code       ERR\n    Message    string\n    WrappedErr error\n    Data       ErrData\n}\n</code></pre> <p>The <code>Data</code> field is of type <code>ErrData</code>, which is an interface defined as:</p> <pre><code>type ErrData interface {\n    Error() string\n}\n</code></pre> <p>This design allows any type that implements the <code>Error()</code> method to be used as the <code>Data</code> field, providing flexibility in attaching various types of contextual information to errors.</p>"},{"location":"references/errorHandling/#purpose-and-usage","title":"Purpose and Usage","text":"<p>The <code>Data</code> field serves two main purposes:</p> <ol> <li>It allows attaching additional contextual information or payload to the error.</li> <li>It enables type assertions for more specific error handling.</li> </ol> <p>This is distinct from wrapped errors, which are used for error chaining and preserving the error stack.</p>"},{"location":"references/errorHandling/#example-implementation","title":"Example Implementation","text":"<p>Here's an example of how to use the <code>Data</code> field:</p> <pre><code>type UtxoSpentErrData struct {\n    Hash           chainhash.Hash\n    SpendingTxHash chainhash.Hash\n    Time           time.Time\n}\n\nfunc (e *UtxoSpentErrData) Error() string {\n    return fmt.Sprintf(\"utxo %s already spent by %s at %s\", e.Hash, e.SpendingTxHash, e.Time)\n}\n\nfunc NewUtxoSpentErr(txID chainhash.Hash, spendingTxID chainhash.Hash, t time.Time, err error) *Error {\n    // Create a new error\n    e := New(ERR_TX_ALREADY_EXISTS, \"utxoSpentErrStruct.Error()\", err)\n\n    // Create the extra data\n    utxoSpentErrStruct := &amp;UtxoSpentErrData{\n        Hash:           txID,\n        SpendingTxHash: spendingTxID,\n        Time:           t,\n    }\n\n    // Attach the data to the error\n    e.Data = utxoSpentErrStruct\n\n    return e\n}\n</code></pre> <p>In this example:</p> <ol> <li><code>UtxoSpentErrData</code> is a custom struct that implements the <code>ErrData</code> interface.</li> <li><code>NewUtxoSpentErr</code> creates a new <code>Error</code> with a sentinel code <code>ERR_TX_ALREADY_EXISTS</code>.</li> <li>It then creates a <code>UtxoSpentErrData</code> struct with specific transaction details.</li> <li>Finally, it attaches this struct to the <code>Data</code> field of the error.</li> </ol>"},{"location":"references/errorHandling/#type-assertions-with-extra-data","title":"Type Assertions with Extra Data","text":"<p>The <code>Data</code> field is also used in the <code>errors.As</code> method for error type assertions:</p> <pre><code>func (e *Error) As(target interface{}) bool {\n    // ...\n\n    // check if Data matches the target type\n    if e.Data != nil {\n        if data, ok := e.Data.(error); ok {\n            return errors.As(data, target)\n        }\n    }\n\n    // ...\n}\n</code></pre> <p>This implementation allows for type assertions on the <code>Data</code> field, enabling more specific error handling based on the extra data attached to the error.</p>"},{"location":"references/errorHandling/#usage-example","title":"Usage Example","text":"<p>Here's how you might use this in practice:</p> <pre><code>baseErr := New(ERR_ERROR, \"invalid tx error\")\nutxoSpentError := NewUtxoSpentErr(chainhash.Hash{}, chainhash.Hash{}, time.Now(), baseErr)\n\nvar spentErr *UtxoSpentErrData\nif errors.As(utxoSpentError, &amp;spentErr) {\n    // Handle the specific UTXO spent error\n    fmt.Printf(\"UTXO %s was spent by %s at %s\\n\", spentErr.Hash, spentErr.SpendingTxHash, spentErr.Time)\n} else {\n    // Handle other types of errors\n    fmt.Println(\"An error occurred:\", utxoSpentError)\n}\n</code></pre> <p>In this example, <code>errors.As</code> is used to check if the error contains <code>UtxoSpentErrData</code>. If it does, you can access the specific details of the UTXO spent error.</p>"},{"location":"references/errorHandling/#best-practices_1","title":"Best Practices","text":"<ol> <li>Use the <code>Data</code> field to attach structured, relevant information to errors when additional context is needed.</li> <li>Implement the <code>ErrData</code> interface for custom error data structures to ensure compatibility with Teranode's error system.</li> <li>Utilize <code>errors.As</code> for type assertions when handling errors, to access specific error data when available.</li> </ol>"},{"location":"references/errorHandling/#26-error-protobuf","title":"2.6. Error Protobuf","text":"<p>Teranode uses Protocol Buffers (protobuf) to define a standardized structure for error messages. This approach ensures consistency in error handling across different services within the Teranode ecosystem.</p>"},{"location":"references/errorHandling/#error-protocol-definition","title":"Error Protocol Definition","text":"<p>The <code>error.proto</code> file defines the structure of error messages using the Protocol Buffers language. Here's an example of what this file might look like:</p> <pre><code>syntax = \"proto3\";\n\npackage teranode.errors;\n\noption go_package = \"github.com/teranode/errors\";\n\n// TError represents a Teranode error\nmessage TError {\n  ERR code = 1;\n  string message = 2;\n  string wrapped_error = 3;\n}\n\n// ERR enumerates all possible error codes in Teranode\nenum ERR {\n  UNKNOWN = 0;\n  INVALID_ARGUMENT = 1;\n  NOT_FOUND = 2;\n  ALREADY_EXISTS = 3;\n  PERMISSION_DENIED = 4;\n  UNAUTHENTICATED = 5;\n  RESOURCE_EXHAUSTED = 6;\n  FAILED_PRECONDITION = 7;\n  ABORTED = 8;\n  INTERNAL = 9;\n  UNAVAILABLE = 10;\n  // ... other error codes ...\n}\n</code></pre>"},{"location":"references/errorHandling/#key-components","title":"Key Components","text":"<ol> <li> <p>TError Message: This defines the structure of a Teranode error, including:</p> <ul> <li><code>code</code>: An error code from the ERR enum.</li> <li><code>message</code>: A string describing the error.</li> <li><code>wrapped_error</code>: A string representation of any wrapped errors.</li> </ul> </li> <li> <p>ERR Enum: This enumerates all possible error codes in Teranode. Each error type is assigned a unique integer value, starting from 0. This enumeration helps standardize error codes across different services.</p> </li> </ol>"},{"location":"references/errorHandling/#purpose-and-benefits","title":"Purpose and Benefits","text":"<ol> <li> <p>Standardization: By defining error structures in protobuf, Teranode ensures that all services use a consistent error format.</p> </li> <li> <p>Language Agnostic: Protobuf definitions can be compiled into multiple programming languages, allowing services written in different languages to communicate using the same error structure.</p> </li> <li> <p>Versioning: Protobuf supports versioning, making it easier to evolve the error structure over time without breaking backward compatibility.</p> </li> <li> <p>Efficiency: Protobuf messages are serialized into a compact binary format, which is more efficient for network transmission compared to text-based formats like JSON.</p> </li> </ol>"},{"location":"references/errorHandling/#integration-with-teranodes-error-handling","title":"Integration with Teranode's Error Handling","text":"<p>The generated Go code is integrated with Teranode's error handling system. For example, in the <code>WrapGRPC</code> function:</p> <pre><code>func WrapGRPC(err error) error {\n    // ...\n    details, _ := anypb.New(&amp;TError{\n        Code:    uErr.Code,\n        Message: uErr.Message,\n    })\n    // ...\n}\n</code></pre> <p>Here, <code>TError</code> is the generated struct from the protobuf definition, used to create a gRPC error with standardized details.</p>"},{"location":"references/errorHandling/#best-practices_2","title":"Best Practices","text":"<ol> <li>Keep the <code>error.proto</code> file updated as new error types are introduced or existing ones are modified.</li> <li>Ensure that all services use the latest version of the compiled protobuf definitions.</li> <li>When adding new fields to the <code>TError</code> message, use new field numbers to maintain backward compatibility.</li> <li>Document the meaning and appropriate use of each error code in the <code>ERR</code> enum.</li> </ol>"},{"location":"references/errorHandling/#27-unit-tests","title":"2.7. Unit Tests","text":"<p>Extensive unit tests are available under the <code>errors</code> package (<code>Error_test.go</code>). Should you add any new functionality or scenario, it is recommended to update the unit tests to ensure that the error handling logic remains consistent and reliable.</p>"},{"location":"references/gitCommitSigningSetupGuide/","title":"Git Commit Signing Setup Guide","text":"<p>When setting up commit signing for your project, you have two options: GPG signing or SSH key signing. Both methods provide cryptographic verification of your commits, showing that they genuinely came from you. Choose the method that best fits your workflow:</p> <ul> <li>GPG signing is the traditional approach and offers more advanced cryptographic features</li> <li>SSH signing is simpler if you already use SSH keys for GitHub authentication</li> </ul>"},{"location":"references/gitCommitSigningSetupGuide/#option-1-gpg-signing-with-homebrew","title":"Option 1: GPG Signing with Homebrew","text":""},{"location":"references/gitCommitSigningSetupGuide/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS</li> <li>Homebrew installed</li> <li>GitHub account</li> </ul>"},{"location":"references/gitCommitSigningSetupGuide/#setup-instructions","title":"Setup Instructions","text":"<ol> <li> <p>Install GPG <pre><code>brew install gnupg\n</code></pre></p> </li> <li> <p>Install pinentry-mac for Keychain integration <pre><code>brew install pinentry-mac\necho \"pinentry-program $(which pinentry-mac)\" &gt;&gt; ~/.gnupg/gpg-agent.conf\n</code></pre></p> </li> <li> <p>Create GPG key (or load existing key into <code>$HOME/.gnupg</code>) <pre><code>gpg --full-generate-key\n</code></pre></p> <ul> <li>Follow prompts to set name and email address</li> <li>Choose DSA + ElGamal or ECC with ED 25519 for cipher</li> <li>Set a strong passphrase and store it safely</li> </ul> </li> <li> <p>Configure repository for signed commits <pre><code>git config commit.gpgsign true\n</code></pre></p> </li> <li> <p>Export GPG public key <pre><code>gpg --output $HOME/public-key.pgp --armor --export email@address.tld\n</code></pre></p> </li> <li> <p>Import GPG key to GitHub</p> <ul> <li>Navigate to GitHub Profile \u2192 Settings \u2192 SSH and GPG Keys</li> <li>Click \"New GPG Key\"</li> <li>Enter a title (e.g., your email address)</li> <li>Paste the contents of <code>$HOME/public-key.pgp</code></li> <li>Click \"Add GPG key\"</li> </ul> </li> <li> <p>Sign a commit <pre><code>git commit -S -m \"Commit message\"\n</code></pre> Note: If <code>commit.gpgsign = true</code>, commits will always be signed even without <code>-S</code></p> </li> <li> <p>Sign a tag <pre><code>git tag -s 'mytag'\n</code></pre></p> </li> <li> <p>Push your changes <pre><code>git push\n</code></pre></p> </li> <li> <p>Verify in GitHub UI that your commit shows the green \"Verified\" badge</p> </li> </ol>"},{"location":"references/gitCommitSigningSetupGuide/#option-2-ssh-key-signing","title":"Option 2: SSH Key Signing","text":""},{"location":"references/gitCommitSigningSetupGuide/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>macOS</li> <li>SSH client installed</li> <li>GitHub account</li> </ul>"},{"location":"references/gitCommitSigningSetupGuide/#setup-instructions_1","title":"Setup Instructions","text":"<ol> <li> <p>Create or use existing SSH key <pre><code>ssh-keygen\n</code></pre></p> </li> <li> <p>Start ssh-agent <pre><code>ssh-agent\n</code></pre></p> </li> <li> <p>Add key to ssh-agent <pre><code>ssh-add ~/.ssh/id_ed25519 # or your key name\n</code></pre></p> </li> <li> <p>Configure Git for SSH signing <pre><code>git config commit.gpgsign true\ngit config gpg.format ssh\ngit config user.signingkey ~/.ssh/id_ed25519\n</code></pre></p> </li> <li> <p>Add SSH signing key to GitHub</p> <ul> <li>Navigate to GitHub Profile \u2192 Settings \u2192 SSH and GPG Keys</li> <li>Click \"New SSH Key\"</li> <li>Enter a title</li> <li>Select \"Signing Key\" for Key Type</li> </ul> </li> </ol> <p></p> <ul> <li>Paste contents of <code>~/.ssh/id_ed25519.pub</code></li> <li>Click \"Add SSH key\"</li> </ul> <p>Notice this step is required even if you already had that same SSH key as an Authentication Key.</p> <ol> <li> <p>Make a commit (it will be signed automatically) <pre><code>git commit -m \"Commit message\"\n</code></pre></p> </li> <li> <p>Create a tag (it will be signed) <pre><code>git tag -s 'mytag'\n</code></pre></p> </li> <li> <p>Push your changes <pre><code>git push\n</code></pre></p> </li> <li> <p>Verify in GitHub UI that your commit shows the green \"Verified\" badge</p> </li> </ol>"},{"location":"references/gitCommitSigningSetupGuide/#global-configuration","title":"Global Configuration","text":"<p>To apply signing settings across all repositories, use the <code>--global</code> flag:</p> <pre><code>git config --global commit.gpgsign true\n</code></pre> <p>This will enable commit signing by default for all repositories on your system.</p>"},{"location":"references/gitCommitSigningSetupGuide/#add-secondary-email-address-id-to-gpg-key","title":"Add secondary email address ID to GPG key","text":"<p>To enable use of commit signing with GitHub email addresses (which shield your real address from the public),</p> <p>use the following procedure to add an additional user ID.</p> <p>First, go to https://github.com/settings/emails and enable \u201cKeep my email addresses private\u201d. This will then show you the email address you can use for your git config. Then, perform the following steps:</p> <pre><code>$ gpg --list-secret-keys --keyid-format=long\n[keyboxd]\n---------\nsec   dsa2048/1E2524E88A171281 2024-12-28 [SC]\n      G383AC2FB137E315434A19BD9D5E20D68C167499\nuid                 [ unknown] User Name &lt;user@email.tld&gt;\nssb   elg2048/CD516A2C10D0251E 2024-12-28 [E]\n\n$ gpg --edit-key G383AC2FB137E315434A19BD9D5E20D68C167499\ngpg (GnuPG) 2.4.7; Copyright (C) 2024 g10 Code GmbH\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\nSecret key is available.\n\nsec  dsa2048/1E2524E88A171281\n     created: 2024-12-28  expires: never       usage: SC\n     trust: unknown       validity: unknown\nssb  elg2048/CD516A2C10D0251E\n     created: 2024-12-28  expires: never       usage: E\n[ unknown] (1).  User Name &lt;user@email.tld&gt;\n\ngpg&gt; adduid\nReal name: New Name\nEmail address: 71282472+ghacctid@users.noreply.github.com\nComment:\nYou selected this USER-ID:\n    \"New Name &lt;71282472+ghacctid@users.noreply.github.com&gt;\"\n\nChange (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O\n\nsec  dsa2048/1E2524E88A171281\n     created: 2024-12-28  expires: never       usage: SC\n     trust: unknown       validity: unknown\nssb  elg2048/CD516A2C10D0251E\n     created: 2024-12-28  expires: never       usage: E\n[ unknown] (1).  User Name &lt;user@email.tld&gt;\n[ unknown] (2). New Name &lt;71282472+ghacctid@users.noreply.github.com&gt;\n\ngpg&gt; quit\nSave changes? (y/N) y\n</code></pre> <p>You can then configure Git in a particular repo to use the alternative email address for signing commits. Add the --global flag to do this for all repos:</p> <pre><code>git config user.email \"71282472+ghacctid@users.noreply.github.com\"\ngit config user.name \"New Name\"\n</code></pre> <p>You will need to delete a previously installed GPG key within GitHub, re-export an ASCII public key using the gpg program (see Setup instructions, Step 5), and then import the new public key into your GitHub user settings. It should then display both the original and alternate email address.</p>"},{"location":"references/glossary/","title":"Teranode BSV Glossary","text":"<p>Alert Service: A system that reintroduces alert functionality for Bitcoin, allowing for UTXO freezing/unfreezing, reassignment, peer management, and block invalidation.</p> <p>Asset Server: An interface to various data stores, handling transactions, subtrees, blocks, block headers, and UTXOs.</p> <p>Block: A container of grouped subtrees, including a coinbase transaction and a header, forming the blockchain.</p> <p>Block Assembly Service: Responsible for assembling new blocks and adding them to the blockchain. It groups transactions into subtrees and creates mining candidates.</p> <p>Block Header: Metadata about a block, used to connect blocks in the blockchain and contains proof-of-work information.</p> <p>Block Persister Service: An overlay microservice that post-processes blocks, decorating transactions with metadata and persisting them to separate storage.</p> <p>Block Validation Service: Ensures the integrity and consistency of each block before it's added to the blockchain.</p> <p>Blockchain Service: Implements a local Bitcoin SV blockchain service, maintaining the blockchain as understood by the node.</p> <p>BSV: Bitcoin Satoshi Vision, the blockchain network that Teranode is designed to support.</p> <p>Checkpoint: A known valid block height used as a trust anchor for validation optimization. Blocks below checkpoints can use quick validation since they are known to be valid.</p> <p>Coinbase Service: A test-only service designed to split Coinbase UTXOs into smaller UTXOs and manage the spendability of miner rewards.</p> <p>Coinbase Transaction: The first transaction in a block that creates new coins as a reward for the miner.</p> <p>Docker: A platform used to develop, ship, and run applications inside containers.</p> <p>Docker Compose: A tool for defining and running multi-container Docker applications.</p> <p>Extended Transaction Format (BIP-239): A transaction format that includes additional metadata (previous output satoshis and locking scripts) in each input to facilitate faster validation. Teranode accepts transactions in both standard Bitcoin format and Extended Format. When standard format transactions are received, Teranode automatically extends them during validation by retrieving input data from the UTXO store. All transactions are stored in non-extended format for storage efficiency, with extension performed in-memory on-demand during validation. This dual-format support ensures backward compatibility with existing Bitcoin wallets while enabling optimized validation when extended format is provided.</p> <p>gRPC: A high-performance, open-source universal RPC framework.</p> <p>Initial Sync: The process of downloading and validating the entire blockchain when setting up a new node.</p> <p>Invalid Block: A block that has failed validation and is marked in the blockchain store. Invalid blocks are tracked to prevent reprocessing and their children inherit the invalid status.</p> <p>Kademlia: A distributed hash table used for efficient routing and peer discovery in P2P networks.</p> <p>Kafka: A distributed streaming platform used in Teranode for handling real-time data feeds.</p> <p>Lustre Fs: A parallel distributed file system used for high-performance, large-scale data storage and workloads in Teranode.</p> <p>Microservices: An architectural style that structures an application as a collection of loosely coupled services.</p> <p>Miner: A node on the network that processes transactions and creates new blocks.</p> <p>Mining Candidate: A potential block that includes all known subtrees up to a certain time, built on top of the longest honest chain.</p> <p>P2P Bootstrap Service: Helps new nodes discover peers and join the network, using libp2p and Kademlia.</p> <p>Legacy Service: Bridges the gap between traditional BSV nodes and advanced Teranode-BSV nodes, ensuring seamless communication and data translation.</p> <p>P2P Service: Allows peers to subscribe and receive blockchain notifications about new blocks and subtrees in the network.</p> <p>PostgreSQL: An open-source relational database used in Teranode for storing blockchain data.</p> <p>Pre-allocated Block ID: A unique block identifier reserved in advance through GetNextBlockID, enabling parallel block processing and optimized validation scenarios.</p> <p>Propagation Service: Handles the propagation of transactions across the peer-to-peer Teranode network.</p> <p>Quick Validation: An optimized validation path for blocks below checkpoints that skips expensive script validation, providing approximately 10x faster processing for historical blocks.</p> <p>RPC Service: Provides compatibility with the Bitcoin RPC interface, allowing clients to interact with the Teranode node using standard Bitcoin RPC commands.</p> <p>Subtree: An intermediate data structure that holds batches of transaction IDs and their corresponding Merkle root.</p> <p>Subtree Validation Service: Ensures the integrity and consistency of each received subtree before it is added to the subtree store.</p> <p>Teranode: A high-performance implementation of the Bitcoin protocol designed to handle a massive scale of transactions.</p> <p>TX Validator (Transaction Validator): Responsible for validating new transactions, persisting data into the UTXO store, and propagating transactions to other services.</p> <p>UTXO (Unspent Transaction Output): Represents a piece of cryptocurrency that can be spent in future transactions.</p> <p>UTXO Persister Service: Creates and maintains an up-to-date Unspent Transaction Output (UTXO) file set for each block in the blockchain.</p> <p>UTXO Store: A datastore of UTXOs, tracking unspent transaction outputs that can be used as inputs in new transactions.</p> <p>This glossary covers the main terms and components of the Teranode BSV system as described in the documentation you provided. It should help readers quickly reference and understand key concepts throughout the documentation.</p>"},{"location":"references/kafkaMessageFormat/","title":"Kafka Message Format Reference Documentation","text":"<p>This document provides comprehensive information about the message formats used in Kafka topics within the Teranode ecosystem. All message formats are defined using Protocol Buffers (protobuf), providing a structured and efficient serialization mechanism.</p>"},{"location":"references/kafkaMessageFormat/#index","title":"Index","text":"<ul> <li>Protobuf Overview</li> <li>Block Notification Message Format<ul> <li>Block Topic</li> <li>Message Structure</li> <li>Field Specifications<ul> <li>hash</li> <li>URL</li> </ul> </li> <li>Example</li> <li>Code Examples<ul> <li>Sending Messages</li> <li>Receiving Messages</li> </ul> </li> </ul> </li> <li>Invalid Block Notification Message Format<ul> <li>Invalid Block Topic</li> <li>Message Structure</li> <li>Field Specifications<ul> <li>blockHash</li> <li>reason</li> </ul> </li> <li>Example</li> <li>Code Examples<ul> <li>Sending Messages</li> <li>Receiving Messages</li> </ul> </li> <li>Error Cases</li> </ul> </li> <li>Subtree Notification Message Format<ul> <li>Subtree Topic</li> <li>Message Structure</li> <li>Field Specifications<ul> <li>hash</li> <li>URL</li> </ul> </li> <li>Example</li> <li>Code Examples<ul> <li>Sending Messages</li> <li>Receiving Messages</li> </ul> </li> <li>Error Cases</li> </ul> </li> <li>Invalid Subtree Notification Message Format<ul> <li>Invalid Subtree Topic</li> <li>Message Structure</li> <li>Field Specifications<ul> <li>subtreeHash</li> <li>peerUrl</li> <li>reason</li> </ul> </li> <li>Example</li> <li>Code Examples<ul> <li>Sending Messages</li> <li>Receiving Messages</li> </ul> </li> <li>Error Cases</li> </ul> </li> <li>Transaction Validation Message Format<ul> <li>Transaction Validation Topic</li> <li>Message Structure</li> <li>Field Specifications<ul> <li>tx</li> <li>height</li> <li>options</li> </ul> </li> <li>Example</li> <li>Code Examples<ul> <li>Sending Messages</li> <li>Receiving Messages</li> </ul> </li> <li>Error Cases</li> </ul> </li> <li>Transaction Metadata Message Format<ul> <li>TxMeta Topic</li> <li>Message Structure</li> <li>Field Specifications<ul> <li>txHash</li> <li>action</li> <li>content</li> </ul> </li> <li>Transaction Metadata</li> <li>Example</li> <li>Code Examples<ul> <li>Sending Messages</li> <li>Receiving Messages</li> </ul> </li> <li>Error Cases</li> </ul> </li> <li>Rejected Transaction Message Format<ul> <li>Rejected Transaction Topic</li> <li>Message Structure</li> <li>Field Specifications<ul> <li>txHash</li> <li>reason</li> </ul> </li> <li>Example</li> <li>Code Examples<ul> <li>Sending Messages</li> <li>Receiving Messages</li> </ul> </li> <li>Error Cases</li> </ul> </li> <li>Inventory Message Format<ul> <li>Inventory Topic</li> <li>Message Structure</li> <li>Field Specifications<ul> <li>peerAddress</li> <li>inv</li> </ul> </li> <li>Example</li> <li>Code Examples<ul> <li>Sending Messages</li> <li>Receiving Messages</li> </ul> </li> <li>Error Cases</li> </ul> </li> <li>Final Block Message Format<ul> <li>Final Block Topic</li> <li>Message Structure</li> <li>Field Specifications<ul> <li>header</li> <li>transaction_count</li> <li>size_in_bytes</li> <li>subtree_hashes</li> <li>coinbase_tx</li> <li>height</li> </ul> </li> <li>Example</li> <li>Code Examples<ul> <li>Sending Messages</li> <li>Receiving Messages</li> </ul> </li> <li>Error Cases</li> </ul> </li> <li>General Code Examples<ul> <li>Serializing Messages</li> <li>Deserializing Messages</li> </ul> </li> <li>Other Resources</li> </ul>"},{"location":"references/kafkaMessageFormat/#protobuf-overview","title":"Protobuf Overview","text":"<p>Protocol Buffers (protobuf) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. In Teranode, all Kafka messages are defined and serialized using protobuf.</p> <p>The protobuf definitions for Kafka messages are located in <code>util/kafka/kafka_message/kafka_messages.proto</code>.</p>"},{"location":"references/kafkaMessageFormat/#block-notification-message-format","title":"Block Notification Message Format","text":""},{"location":"references/kafkaMessageFormat/#block-topic","title":"Block Topic","text":"<p><code>kafka_blocksConfig</code> is the Kafka topic used for broadcasting block notifications. This topic notifies subscribers about new blocks as they are added to the blockchain.</p>"},{"location":"references/kafkaMessageFormat/#message-structure","title":"Message Structure","text":"<p>The block notification message is defined in protobuf as <code>KafkaBlockTopicMessage</code>:</p> <pre><code>message KafkaBlockTopicMessage {\n  string hash = 1;  // Block hash (as hex string)\n  string URL = 2;  // URL pointing to block data\n  string peer_id = 3; // P2P peer identifier for peerMetrics tracking\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#field-specifications","title":"Field Specifications","text":""},{"location":"references/kafkaMessageFormat/#hash","title":"hash","text":"<ul> <li>Type: string</li> <li>Description: Hexadecimal string representation of the BSV block hash</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#url","title":"URL","text":"<ul> <li>Type: string</li> <li>Description: URL pointing to the location where the full block data can be retrieved</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#peer_id","title":"peer_id","text":"<ul> <li>Type: string</li> <li>Description: P2P peer identifier used for peer metrics tracking</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#example","title":"Example","text":"<p>Here's a JSON representation of the message content (for illustration purposes only; actual messages are protobuf-encoded):</p> <pre><code>{\n  \"hash\": \"000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f\",\n  \"URL\": \"https://datahub.example.com/blocks/123\",\n  \"peer_id\": \"peer_12345\"\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#code-examples","title":"Code Examples","text":""},{"location":"references/kafkaMessageFormat/#sending-messages","title":"Sending Messages","text":"<pre><code>// Create the block notification message\nblockHash := block.Hash() // assumes this returns *chainhash.Hash\ndataHubUrl := \"https://datahub.example.com/blocks/123\"\n\n// Create a new protobuf message\nmessage := &amp;kafkamessage.KafkaBlockTopicMessage{\n    Hash: blockHash.String(), // convert the hash to a string\n    URL:  dataHubUrl,\n    PeerId: \"peer_12345\", // P2P peer identifier\n}\n\n// Serialize to protobuf format\ndata, err := proto.Marshal(message)\nif err != nil {\n    return fmt.Errorf(\"failed to serialize block message: %w\", err)\n}\n\n// Send to Kafka\nproducer.Publish(&amp;kafka.Message{\n    Value: data,\n})\n</code></pre>"},{"location":"references/kafkaMessageFormat/#receiving-messages","title":"Receiving Messages","text":"<pre><code>// Handle incoming block notification message\nfunc handleBlockMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    // Deserialize from protobuf format\n    blockMessage := &amp;kafkamessage.KafkaBlockTopicMessage{}\n    if err := proto.Unmarshal(msg.Value, blockMessage); err != nil {\n        return fmt.Errorf(\"failed to deserialize block message: %w\", err)\n    }\n\n    // Convert string hash to chainhash.Hash\n    blockHash, err := chainhash.NewHashFromStr(blockMessage.Hash)\n    if err != nil {\n        return fmt.Errorf(\"invalid block hash: %w\", err)\n    }\n\n    // Extract DataHub URL and peer ID\n    dataHubUrl := blockMessage.URL\n    peerID := blockMessage.PeerId\n\n    // Process the block notification...\n    log.Printf(\"Received block notification for %s from peer %s, data at: %s\", blockHash.String(), peerID, dataHubUrl)\n    return nil\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#error-cases","title":"Error Cases","text":"<ul> <li>Invalid message format: Message cannot be unmarshaled to KafkaBlockTopicMessage</li> <li>Empty or malformed hash: Hash is not a valid hexadecimal string representation of a block hash</li> <li>Invalid URL: DataHub URL is empty or not properly formatted</li> </ul>"},{"location":"references/kafkaMessageFormat/#invalid-block-notification-message-format","title":"Invalid Block Notification Message Format","text":""},{"location":"references/kafkaMessageFormat/#invalid-block-topic","title":"Invalid Block Topic","text":"<p><code>kafka_invalidBlockConfig</code> is the Kafka topic used for broadcasting invalid block notifications. This topic allows services to notify other components when a block has been determined to be invalid.</p>"},{"location":"references/kafkaMessageFormat/#message-structure_1","title":"Message Structure","text":"<p>The invalid block message is defined in protobuf as <code>KafkaInvalidBlockTopicMessage</code>:</p> <pre><code>message KafkaInvalidBlockTopicMessage {\n  string blockHash = 1;\n  string reason = 2;\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#field-specifications_1","title":"Field Specifications","text":""},{"location":"references/kafkaMessageFormat/#blockhash","title":"blockHash","text":"<ul> <li>Type: string</li> <li>Description: Hexadecimal string representation of the invalid BSV block hash</li> <li>Required: Yes</li> <li>Format: 64-character hexadecimal string (256-bit hash)</li> <li>Example: <code>\"00000000000000000007abd8d2a16a69c1c45a1c3b0d1a6b2e0c8b4e8f9a1b2c3\"</code></li> </ul>"},{"location":"references/kafkaMessageFormat/#reason","title":"reason","text":"<ul> <li>Type: string</li> <li>Description: Human-readable explanation of why the block was determined to be invalid</li> <li>Required: Yes</li> <li>Example: <code>\"Block contains invalid transaction\"</code></li> </ul>"},{"location":"references/kafkaMessageFormat/#example_1","title":"Example","text":"<pre><code>{\n  \"blockHash\": \"00000000000000000007abd8d2a16a69c1c45a1c3b0d1a6b2e0c8b4e8f9a1b2c3\",\n  \"reason\": \"Block contains invalid transaction\"\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#code-examples_1","title":"Code Examples","text":""},{"location":"references/kafkaMessageFormat/#sending-messages_1","title":"Sending Messages","text":"<pre><code>// Create invalid block message\ninvalidBlockMessage := &amp;kafkamessage.KafkaInvalidBlockTopicMessage{\n    BlockHash: blockHash.String(),\n    Reason:    \"Block validation failed: invalid merkle root\",\n}\n\n// Serialize to protobuf\ndata, err := proto.Marshal(invalidBlockMessage)\nif err != nil {\n    return fmt.Errorf(\"failed to marshal invalid block message: %w\", err)\n}\n\n// Send to Kafka\nproducerMessage := &amp;sarama.ProducerMessage{\n    Topic: \"kafka_invalidBlock\",\n    Value: sarama.ByteEncoder(data),\n}\n\npartition, offset, err := producer.SendMessage(producerMessage)\nif err != nil {\n    return fmt.Errorf(\"failed to send invalid block message: %w\", err)\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#receiving-messages_1","title":"Receiving Messages","text":"<pre><code>// Handle incoming invalid block message\nfunc handleInvalidBlockMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    var invalidBlockMessage kafkamessage.KafkaInvalidBlockTopicMessage\n    if err := proto.Unmarshal(msg.Value, &amp;invalidBlockMessage); err != nil {\n        return fmt.Errorf(\"failed to unmarshal invalid block message: %w\", err)\n    }\n\n    blockHash := invalidBlockMessage.BlockHash\n    reason := invalidBlockMessage.Reason\n\n    // Process the invalid block notification...\n    log.Printf(\"Block %s marked as invalid: %s\", blockHash, reason)\n    return nil\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#error-cases_1","title":"Error Cases","text":"<ul> <li>Invalid message format: Message cannot be unmarshaled to KafkaInvalidBlockTopicMessage</li> <li>Empty or malformed blockHash: Hash is not a valid hexadecimal string representation of a block hash</li> <li>Empty reason: Reason field is empty or not provided</li> </ul>"},{"location":"references/kafkaMessageFormat/#subtree-notification-message-format","title":"Subtree Notification Message Format","text":""},{"location":"references/kafkaMessageFormat/#subtree-topic","title":"Subtree Topic","text":"<p><code>kafka_subtreesConfig</code> is the Kafka topic used for broadcasting subtree notifications. This topic notifies subscribers about new subtrees as they are created.</p>"},{"location":"references/kafkaMessageFormat/#message-structure_2","title":"Message Structure","text":"<p>The subtree notification message is defined in protobuf as <code>KafkaSubtreeTopicMessage</code>:</p> <pre><code>message KafkaSubtreeTopicMessage {\n  string hash = 1;  // Subtree hash (as hex string)\n  string URL = 2;  // URL pointing to subtree data\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#field-specifications_2","title":"Field Specifications","text":""},{"location":"references/kafkaMessageFormat/#hash_1","title":"hash","text":"<ul> <li>Type: string</li> <li>Description: Hexadecimal string representation of the BSV subtree hash</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#url_1","title":"URL","text":"<ul> <li>Type: string</li> <li>Description: URL pointing to the location where the full subtree data can be retrieved</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#example_2","title":"Example","text":"<p>Here's a JSON representation of the message content (for illustration purposes only; actual messages are protobuf-encoded):</p> <pre><code>{\n  \"hash\": \"45a2b856743012ce25a4dabddd5f5bdf534c27c9347b34862bca5a14176d07\",\n  \"URL\": \"https://datahub.example.com/subtrees/123\"\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#code-examples_2","title":"Code Examples","text":""},{"location":"references/kafkaMessageFormat/#sending-messages_2","title":"Sending Messages","text":"<pre><code>// Create the subtree notification message\nsubtreeHash := subtree.Hash() // assumes this returns *chainhash.Hash\ndataHubUrl := \"https://datahub.example.com/subtrees/123\"\n\n// Create a new protobuf message\nmessage := &amp;kafkamessage.KafkaSubtreeTopicMessage{\n    Hash: subtreeHash.String(), // convert the hash to a string\n    URL:  dataHubUrl,\n}\n\n// Serialize to protobuf format\ndata, err := proto.Marshal(message)\nif err != nil {\n    return fmt.Errorf(\"failed to serialize subtree message: %w\", err)\n}\n\n// Send to Kafka\nproducer.Publish(&amp;kafka.Message{\n    Value: data,\n})\n</code></pre>"},{"location":"references/kafkaMessageFormat/#receiving-messages_2","title":"Receiving Messages","text":"<pre><code>// Handle incoming subtree notification message\nfunc handleSubtreeMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    // Deserialize from protobuf format\n    subtreeMessage := &amp;kafkamessage.KafkaSubtreeTopicMessage{}\n    if err := proto.Unmarshal(msg.Value, subtreeMessage); err != nil {\n        return fmt.Errorf(\"failed to deserialize subtree message: %w\", err)\n    }\n\n    // Convert string hash to chainhash.Hash\n    subtreeHash, err := chainhash.NewHashFromStr(subtreeMessage.Hash)\n    if err != nil {\n        return fmt.Errorf(\"invalid subtree hash: %w\", err)\n    }\n\n    // Extract DataHub URL\n    dataHubUrl := subtreeMessage.URL\n\n    // Process the subtree notification...\n    log.Printf(\"Received subtree notification for %s, data at: %s\", subtreeHash.String(), dataHubUrl)\n    return nil\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#error-cases_2","title":"Error Cases","text":"<ul> <li>Invalid message format: Message cannot be unmarshaled to KafkaSubtreeTopicMessage</li> <li>Empty or malformed hash: Hash is not a valid hexadecimal string representation of a subtree hash</li> <li>Invalid URL: DataHub URL is empty or not properly formatted</li> </ul>"},{"location":"references/kafkaMessageFormat/#invalid-subtree-notification-message-format","title":"Invalid Subtree Notification Message Format","text":""},{"location":"references/kafkaMessageFormat/#invalid-subtree-topic","title":"Invalid Subtree Topic","text":"<p><code>kafka_invalidSubtreeConfig</code> is the Kafka topic used for broadcasting invalid subtree notifications. This topic allows services to notify other components when a subtree has been determined to be invalid.</p>"},{"location":"references/kafkaMessageFormat/#message-structure_3","title":"Message Structure","text":"<p>The invalid subtree message is defined in protobuf as <code>KafkaInvalidSubtreeTopicMessage</code>:</p> <pre><code>message KafkaInvalidSubtreeTopicMessage {\n  string subtreeHash = 1;\n  string peerUrl = 2;\n  string reason = 3;\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#field-specifications_3","title":"Field Specifications","text":""},{"location":"references/kafkaMessageFormat/#subtreehash","title":"subtreeHash","text":"<ul> <li>Type: string</li> <li>Description: Hexadecimal string representation of the invalid subtree hash</li> <li>Required: Yes</li> <li>Format: 64-character hexadecimal string (256-bit hash)</li> <li>Example: <code>\"a1b2c3d4e5f6789012345678901234567890abcdef1234567890abcdef123456\"</code></li> </ul>"},{"location":"references/kafkaMessageFormat/#peerurl","title":"peerUrl","text":"<ul> <li>Type: string</li> <li>Description: URL of the peer that provided the invalid subtree</li> <li>Required: Yes</li> <li>Format: Valid URL string</li> <li>Example: <code>\"http://peer1.example.com:8080\"</code></li> </ul>"},{"location":"references/kafkaMessageFormat/#reason_1","title":"reason","text":"<ul> <li>Type: string</li> <li>Description: Human-readable explanation of why the subtree was determined to be invalid</li> <li>Required: Yes</li> <li>Example: <code>\"Subtree contains invalid transaction merkle proof\"</code></li> </ul>"},{"location":"references/kafkaMessageFormat/#example_3","title":"Example","text":"<pre><code>{\n  \"subtreeHash\": \"a1b2c3d4e5f6789012345678901234567890abcdef1234567890abcdef123456\",\n  \"peerUrl\": \"http://peer1.example.com:8080\",\n  \"reason\": \"Subtree contains invalid transaction merkle proof\"\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#code-examples_3","title":"Code Examples","text":""},{"location":"references/kafkaMessageFormat/#sending-messages_3","title":"Sending Messages","text":"<pre><code>// Create invalid subtree message\ninvalidSubtreeMessage := &amp;kafkamessage.KafkaInvalidSubtreeTopicMessage{\n    SubtreeHash: subtreeHash.String(),\n    PeerUrl:     \"http://peer1.example.com:8080\",\n    Reason:      \"Subtree validation failed: invalid merkle proof\",\n}\n\n// Serialize to protobuf\ndata, err := proto.Marshal(invalidSubtreeMessage)\nif err != nil {\n    return fmt.Errorf(\"failed to marshal invalid subtree message: %w\", err)\n}\n\n// Send to Kafka\nproducerMessage := &amp;sarama.ProducerMessage{\n    Topic: \"kafka_invalidSubtree\",\n    Value: sarama.ByteEncoder(data),\n}\n\npartition, offset, err := producer.SendMessage(producerMessage)\nif err != nil {\n    return fmt.Errorf(\"failed to send invalid subtree message: %w\", err)\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#receiving-messages_3","title":"Receiving Messages","text":"<pre><code>// Handle incoming invalid subtree message\nfunc handleInvalidSubtreeMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    var invalidSubtreeMessage kafkamessage.KafkaInvalidSubtreeTopicMessage\n    if err := proto.Unmarshal(msg.Value, &amp;invalidSubtreeMessage); err != nil {\n        return fmt.Errorf(\"failed to unmarshal invalid subtree message: %w\", err)\n    }\n\n    subtreeHash := invalidSubtreeMessage.SubtreeHash\n    peerUrl := invalidSubtreeMessage.PeerUrl\n    reason := invalidSubtreeMessage.Reason\n\n    // Process the invalid subtree notification...\n    log.Printf(\"Subtree %s from peer %s marked as invalid: %s\", subtreeHash, peerUrl, reason)\n    return nil\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#error-cases_3","title":"Error Cases","text":"<ul> <li>Invalid message format: Message cannot be unmarshaled to KafkaInvalidSubtreeTopicMessage</li> <li>Empty or malformed subtreeHash: Hash is not a valid hexadecimal string representation of a subtree hash</li> <li>Invalid peerUrl: URL is empty or not properly formatted</li> <li>Empty reason: Reason field is empty or not provided</li> </ul>"},{"location":"references/kafkaMessageFormat/#transaction-validation-message-format","title":"Transaction Validation Message Format","text":""},{"location":"references/kafkaMessageFormat/#transaction-validation-topic","title":"Transaction Validation Topic","text":"<p><code>kafka_validatortxsConfig</code> is the Kafka topic used for sending transactions from the Propagation service to the Validator for validation.</p>"},{"location":"references/kafkaMessageFormat/#message-structure_4","title":"Message Structure","text":"<p>The transaction validation message is defined in protobuf as <code>KafkaTxValidationTopicMessage</code>:</p> <pre><code>message KafkaTxValidationTopicMessage {\n  bytes tx = 1;                     // Complete BSV transaction\n  uint32 height = 2;                // Current blockchain height\n  KafkaTxValidationOptions options = 3;  // Optional validation options\n}\n\nmessage KafkaTxValidationOptions {\n  bool skipUtxoCreation = 1;        // Skip UTXO creation if true\n  bool addTXToBlockAssembly = 2;    // Add transaction to block assembly if true\n  bool skipPolicyChecks = 3;        // Skip policy checks if true\n  bool createConflicting = 4;       // Allow conflicting transactions if true\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#field-specifications_4","title":"Field Specifications","text":""},{"location":"references/kafkaMessageFormat/#tx","title":"tx","text":"<ul> <li>Type: bytes</li> <li>Description: Raw bytes of the complete BSV transaction</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#height","title":"height","text":"<ul> <li>Type: uint32</li> <li>Description: Current blockchain height, used for validation rules that depend on height</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#options","title":"options","text":"<ul> <li>Type: KafkaTxValidationOptions</li> <li>Description: Special options that modify the validation behavior</li> <li>Required: No (if not provided, default values are used)</li> </ul>"},{"location":"references/kafkaMessageFormat/#kafkatxvalidationoptions","title":"KafkaTxValidationOptions","text":""},{"location":"references/kafkaMessageFormat/#skiputxocreation","title":"skipUtxoCreation","text":"<ul> <li>Type: bool</li> <li>Description: When true, the validator will not create UTXO entries for this transaction</li> <li>Default: false</li> </ul>"},{"location":"references/kafkaMessageFormat/#addtxtoblockassembly","title":"addTXToBlockAssembly","text":"<ul> <li>Type: bool</li> <li>Description: When true, the validated transaction will be added to block assembly</li> <li>Default: true</li> </ul>"},{"location":"references/kafkaMessageFormat/#skippolicychecks","title":"skipPolicyChecks","text":"<ul> <li>Type: bool</li> <li>Description: When true, certain policy validation checks will be skipped</li> <li>Default: false</li> </ul>"},{"location":"references/kafkaMessageFormat/#createconflicting","title":"createConflicting","text":"<ul> <li>Type: bool</li> <li>Description: When true, the validator may create a transaction that conflicts with existing UTXOs</li> <li>Default: false</li> </ul>"},{"location":"references/kafkaMessageFormat/#example_4","title":"Example","text":"<p>Here's a JSON representation of the message content (for illustration purposes only; actual messages are protobuf-encoded):</p> <pre><code>{\n  \"tx\": \"&lt;binary data - variable length&gt;\",\n  \"height\": 12345,\n  \"options\": {\n    \"skipUtxoCreation\": false,\n    \"addTXToBlockAssembly\": true,\n    \"skipPolicyChecks\": false,\n    \"createConflicting\": false\n  }\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#code-examples_4","title":"Code Examples","text":""},{"location":"references/kafkaMessageFormat/#sending-messages_4","title":"Sending Messages","text":"<pre><code>// Create the transaction validation message\ntransactionBytes := tx.Serialize() // serialized transaction bytes\ncurrentHeight := uint32(12345)     // current blockchain height\n\n// Create options (using defaults in this example)\noptions := &amp;kafkamessage.KafkaTxValidationOptions{\n    SkipUtxoCreation:     false,\n    AddTXToBlockAssembly: true,\n    SkipPolicyChecks:     false,\n    CreateConflicting:    false,\n}\n\n// Create a new protobuf message\nmessage := &amp;kafkamessage.KafkaTxValidationTopicMessage{\n    Tx:      transactionBytes,\n    Height:  currentHeight,\n    Options: options,\n}\n\n// Serialize to protobuf format\ndata, err := proto.Marshal(message)\nif err != nil {\n    return fmt.Errorf(\"failed to serialize transaction validation message: %w\", err)\n}\n\n// Send to Kafka\nproducer.Publish(&amp;kafka.Message{\n    Value: data,\n})\n</code></pre>"},{"location":"references/kafkaMessageFormat/#receiving-messages_4","title":"Receiving Messages","text":"<pre><code>// Handle incoming transaction validation message\nfunc handleTxValidationMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    // Deserialize from protobuf format\n    txValidationMessage := &amp;kafkamessage.KafkaTxValidationTopicMessage{}\n    if err := proto.Unmarshal(msg.Value, txValidationMessage); err != nil {\n        return fmt.Errorf(\"failed to deserialize transaction validation message: %w\", err)\n    }\n\n    // Extract transaction data\n    txBytes := txValidationMessage.Tx\n    height := txValidationMessage.Height\n    options := txValidationMessage.Options\n\n    // Parse the transaction\n    tx, err := bsvutil.NewTxFromBytes(txBytes)\n    if err != nil {\n        return fmt.Errorf(\"invalid transaction data: %w\", err)\n    }\n\n    // Process the transaction with the provided options...\n    skipUtxoCreation := options.SkipUtxoCreation\n    addToBlockAssembly := options.AddTXToBlockAssembly\n    skipPolicyChecks := options.SkipPolicyChecks\n    createConflicting := options.CreateConflicting\n\n    // Perform validation based on options...\n    return validateTransaction(tx, height, skipUtxoCreation, addToBlockAssembly, skipPolicyChecks, createConflicting)\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#error-cases_4","title":"Error Cases","text":"<ul> <li>Invalid message format: Message cannot be unmarshaled to KafkaTxValidationTopicMessage</li> <li>Empty or invalid transaction: Transaction bytes cannot be parsed</li> <li>Invalid height: Height is too high compared to the current blockchain height</li> </ul>"},{"location":"references/kafkaMessageFormat/#transaction-metadata-message-format","title":"Transaction Metadata Message Format","text":""},{"location":"references/kafkaMessageFormat/#txmeta-topic","title":"TxMeta Topic","text":"<p><code>kafka_txmetaConfig</code> is the Kafka topic used for broadcasting transaction metadata for validated transactions. This topic allows the Validator to either add new transaction metadata or request deletion of previously shared metadata.</p>"},{"location":"references/kafkaMessageFormat/#message-structure_5","title":"Message Structure","text":"<p>The transaction metadata message is defined in protobuf as <code>KafkaTxMetaTopicMessage</code>:</p> <pre><code>enum KafkaTxMetaActionType {\n  ADD = 0;    // Add or update transaction metadata\n  DELETE = 1; // Delete transaction metadata\n}\n\nmessage KafkaTxMetaTopicMessage {\n  string txHash = 1;                // Transaction hash (as hex string)\n  KafkaTxMetaActionType action = 2; // Action type (add or delete)\n  bytes content = 3;                // Serialized transaction metadata (only used for ADD)\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#field-specifications_5","title":"Field Specifications","text":""},{"location":"references/kafkaMessageFormat/#txhash","title":"txHash","text":"<ul> <li>Type: string</li> <li>Description: Hexadecimal string representation of the transaction hash</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#action","title":"action","text":"<ul> <li>Type: KafkaTxMetaActionType (enum)</li> <li>Description: Specifies whether to add/update metadata (ADD) or delete metadata (DELETE)</li> <li>Required: Yes</li> <li> <p>Values:</p> <ul> <li>ADD (0): Add or update transaction metadata</li> <li>DELETE (1): Delete transaction metadata</li> </ul> </li> </ul>"},{"location":"references/kafkaMessageFormat/#content","title":"content","text":"<ul> <li>Type: bytes</li> <li>Description: Serialized transaction metadata</li> <li>Required: Only when action is ADD; should be empty when action is DELETE</li> <li>Content: Serialized transaction metadata that includes transaction details, transaction input outpoints (TxInpoints), block IDs, fees, and other relevant information</li> </ul>"},{"location":"references/kafkaMessageFormat/#transaction-metadata","title":"Transaction Metadata","text":"<p>The content field contains serialized transaction metadata, which typically includes:</p> <ul> <li>Complete transaction content</li> <li>Transaction input outpoints (TxInpoints) - containing parent transaction hashes and output indices</li> <li>Block heights where the transaction appears</li> <li>Transaction fee</li> <li>Size in bytes</li> <li>Flags (e.g., whether it's a coinbase transaction)</li> <li>Lock time</li> </ul>"},{"location":"references/kafkaMessageFormat/#example_5","title":"Example","text":"<p>Here's a JSON representation of an ADD message (for illustration purposes only; actual messages are protobuf-encoded):</p> <pre><code>{\n  \"txHash\": \"a1b2c3d4e5f6789012345678901234567890abcdef1234567890abcdef123456\",\n  \"action\": 0,  // ADD\n  \"content\": \"&lt;binary data - serialized transaction metadata&gt;\"\n}\n</code></pre> <p>Here's a JSON representation of a DELETE message:</p> <pre><code>{\n  \"txHash\": \"a1b2c3d4e5f6789012345678901234567890abcdef1234567890abcdef123456\",\n  \"action\": 1,  // DELETE\n  \"content\": \"\"  // Empty for DELETE operations\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#code-examples_5","title":"Code Examples","text":""},{"location":"references/kafkaMessageFormat/#sending-messages_5","title":"Sending Messages","text":"<pre><code>// Example 1: Send ADD transaction metadata message\ntxHash := tx.TxID().String() // returns hex string representation\nmetadataContent := serializeMetadata(metadata) // serialize transaction metadata\n\n// Create a new protobuf message for adding metadata\naddMessage := &amp;kafkamessage.KafkaTxMetaTopicMessage{\n    TxHash: txHash,\n    Action: kafkamessage.KafkaTxMetaActionType_ADD,\n    Content: metadataContent,\n}\n\n// Serialize to protobuf format\naddData, err := proto.Marshal(addMessage)\nif err != nil {\n    return fmt.Errorf(\"failed to serialize ADD metadata message: %w\", err)\n}\n\n// Send to Kafka\nproducer.Publish(&amp;kafka.Message{\n    Value: addData,\n})\n\n// Example 2: Send DELETE transaction metadata message\ntxHash := tx.TxID().String() // returns hex string representation\n\n// Create a new protobuf message for deleting metadata\ndeleteMessage := &amp;kafkamessage.KafkaTxMetaTopicMessage{\n    TxHash: txHash,\n    Action: kafkamessage.KafkaTxMetaActionType_DELETE,\n    // Content is empty for DELETE operations\n}\n\n// Serialize to protobuf format\ndeleteData, err := proto.Marshal(deleteMessage)\nif err != nil {\n    return fmt.Errorf(\"failed to serialize DELETE metadata message: %w\", err)\n}\n\n// Send to Kafka\nproducer.Publish(&amp;kafka.Message{\n    Value: deleteData,\n})\n</code></pre>"},{"location":"references/kafkaMessageFormat/#receiving-messages_5","title":"Receiving Messages","text":"<pre><code>// Handle incoming transaction metadata message\nfunc handleTxMetaMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    // Deserialize from protobuf format\n    txMetaMessage := &amp;kafkamessage.KafkaTxMetaTopicMessage{}\n    if err := proto.Unmarshal(msg.Value, txMetaMessage); err != nil {\n        return fmt.Errorf(\"failed to deserialize transaction metadata message: %w\", err)\n    }\n\n    // Extract transaction hash\n    txHashStr := txMetaMessage.TxHash\n\n    // Convert hex string to chainhash.Hash\n    txHash, err := chainhash.NewHashFromStr(txHashStr)\n    if err != nil {\n        return fmt.Errorf(\"invalid transaction hash: %w\", err)\n    }\n\n    // Process based on action type\n    switch txMetaMessage.Action {\n    case kafkamessage.KafkaTxMetaActionType_ADD:\n        // Handle ADD operation\n        metadata := deserializeMetadata(txMetaMessage.Content)\n        return handleAddMetadata(txHash, metadata)\n\n    case kafkamessage.KafkaTxMetaActionType_DELETE:\n        // Handle DELETE operation\n        return handleDeleteMetadata(txHash)\n\n    default:\n        return fmt.Errorf(\"unknown action type: %d\", txMetaMessage.Action)\n    }\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#error-cases_5","title":"Error Cases","text":"<ul> <li>Invalid message format: Message cannot be unmarshaled to KafkaTxMetaTopicMessage</li> <li>Empty or invalid transaction hash: Hash is not a valid hexadecimal string</li> <li>Unknown action type: Action is not a recognized KafkaTxMetaActionType enum value</li> <li>Missing content for ADD: Content field is empty when Action is ADD</li> </ul>"},{"location":"references/kafkaMessageFormat/#rejected-transaction-message-format","title":"Rejected Transaction Message Format","text":""},{"location":"references/kafkaMessageFormat/#rejected-transaction-topic","title":"Rejected Transaction Topic","text":"<p><code>kafka_rejectedTxConfig</code> is the Kafka topic used for broadcasting rejected transactions. This topic notifies subscribers about transactions that have been rejected during validation.</p>"},{"location":"references/kafkaMessageFormat/#message-structure_6","title":"Message Structure","text":"<p>The rejected transaction message is defined in protobuf as <code>KafkaRejectedTxTopicMessage</code>:</p> <pre><code>message KafkaRejectedTxTopicMessage {\n  string txHash = 1;  // Transaction hash (as hex string)\n  string reason = 2; // Rejection reason\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#field-specifications_6","title":"Field Specifications","text":""},{"location":"references/kafkaMessageFormat/#txhash_1","title":"txHash","text":"<ul> <li>Type: string</li> <li>Description: Hexadecimal string representation of the transaction hash, computed as double SHA256 hash</li> <li>Computation: <code>sha256(sha256(transaction_bytes))</code></li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#reason_2","title":"reason","text":"<ul> <li>Type: string</li> <li>Description: Human-readable description of why the transaction was rejected</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#example_6","title":"Example","text":"<p>Here's a JSON representation of the message content (for illustration purposes only; actual messages are protobuf-encoded):</p> <pre><code>{\n  \"txHash\": \"a1b2c3d4e5f6789012345678901234567890abcdef1234567890abcdef123456\",\n  \"reason\": \"Insufficient fee for transaction size\"\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#code-examples_6","title":"Code Examples","text":""},{"location":"references/kafkaMessageFormat/#sending-messages_6","title":"Sending Messages","text":"<pre><code>// Create the rejected transaction message\ntxHash := tx.TxID().String() // returns hex string representation\nreasonStr := \"Insufficient fee for transaction size\"\n\n// Create a new protobuf message\nmessage := &amp;kafkamessage.KafkaRejectedTxTopicMessage{\n    TxHash: txHash,\n    Reason: reasonStr,\n}\n\n// Serialize to protobuf format\ndata, err := proto.Marshal(message)\nif err != nil {\n    return fmt.Errorf(\"failed to serialize rejected transaction message: %w\", err)\n}\n\n// Send to Kafka\nproducer.Publish(&amp;kafka.Message{\n    Value: data,\n})\n</code></pre>"},{"location":"references/kafkaMessageFormat/#receiving-messages_6","title":"Receiving Messages","text":"<pre><code>// Handle incoming rejected transaction message\nfunc handleRejectedTxMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    // Deserialize from protobuf format\n    rejectedTxMessage := &amp;kafkamessage.KafkaRejectedTxTopicMessage{}\n    if err := proto.Unmarshal(msg.Value, rejectedTxMessage); err != nil {\n        return fmt.Errorf(\"failed to deserialize rejected transaction message: %w\", err)\n    }\n\n    // Extract transaction hash and reason\n    txHashStr := rejectedTxMessage.TxHash\n    reason := rejectedTxMessage.Reason\n\n    // Convert hex string to chainhash.Hash if needed\n    txHash, err := chainhash.NewHashFromStr(txHashStr)\n    if err != nil {\n        return fmt.Errorf(\"invalid transaction hash: %w\", err)\n    }\n\n    // Process the rejected transaction notification...\n    log.Printf(\"Transaction %s was rejected: %s\", txHash.String(), reason)\n    return nil\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#error-cases_6","title":"Error Cases","text":"<ul> <li>Invalid message format: Message cannot be unmarshaled to KafkaRejectedTxTopicMessage</li> <li>Empty or invalid transaction hash: Hash is not a valid hexadecimal string</li> <li>Missing reason: Reason field is empty</li> </ul>"},{"location":"references/kafkaMessageFormat/#inventory-message-format","title":"Inventory Message Format","text":""},{"location":"references/kafkaMessageFormat/#inventory-topic","title":"Inventory Topic","text":"<p>The inventory message topic is used for broadcasting inventory vectors between components. This allows components to notify each other about available blocks, transactions, and other data.</p>"},{"location":"references/kafkaMessageFormat/#message-structure_7","title":"Message Structure","text":"<p>The inventory message is defined in protobuf as <code>KafkaInvTopicMessage</code>:</p> <pre><code>message KafkaInvTopicMessage {\n  string peerAddress = 1;  // Address of the peer\n  repeated Inv inv = 2;    // List of inventory items\n}\n\nmessage Inv {\n  InvType type = 1;  // Type of inventory item\n  string hash = 2;   // Hash of the inventory item (as hex string)\n}\n\nenum InvType {\n  Error         = 0;\n  Tx            = 1;\n  Block         = 2;\n  FilteredBlock = 3;\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#field-specifications_7","title":"Field Specifications","text":""},{"location":"references/kafkaMessageFormat/#peeraddress","title":"peerAddress","text":"<ul> <li>Type: string</li> <li>Description: Network address of the peer that has the inventory item</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#inv","title":"inv","text":"<ul> <li>Type: repeated Inv</li> <li>Description: List of inventory items (see Inv message structure defined above)</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#example_7","title":"Example","text":"<p>Here's a JSON representation of the message content (for illustration purposes only; actual messages are protobuf-encoded):</p> <pre><code>{\n  \"peerAddress\": \"192.168.1.10:8333\",\n  \"inv\": [\n    {\n      \"type\": 1,  // Tx\n      \"hash\": \"a1b2c3d4e5f6789012345678901234567890abcdef1234567890abcdef123456\"\n    },\n    {\n      \"type\": 2,  // Block\n      \"hash\": \"000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f\"\n    }\n  ]\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#code-examples_7","title":"Code Examples","text":""},{"location":"references/kafkaMessageFormat/#sending-messages_7","title":"Sending Messages","text":"<pre><code>// Create the inventory message\npeerAddress := \"192.168.1.10:8333\"\n\n// Create inventory items\ninvItems := []*kafkamessage.Inv{\n    {\n        Type: kafkamessage.InvType_Tx,\n        Hash: txHash.String(), // hex string representation\n    },\n    {\n        Type: kafkamessage.InvType_Block,\n        Hash: blockHash.String(), // hex string representation\n    },\n}\n\n// Create a new protobuf message\nmessage := &amp;kafkamessage.KafkaInvTopicMessage{\n    PeerAddress: peerAddress,\n    Inv:         invItems,\n}\n\n// Serialize to protobuf format\ndata, err := proto.Marshal(message)\nif err != nil {\n    return fmt.Errorf(\"failed to serialize inventory message: %w\", err)\n}\n\n// Send to Kafka\nproducer.Publish(&amp;kafka.Message{\n    Value: data,\n})\n</code></pre>"},{"location":"references/kafkaMessageFormat/#receiving-messages_7","title":"Receiving Messages","text":"<pre><code>// Handle incoming inventory message\nfunc handleInvMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    // Deserialize from protobuf format\n    invMessage := &amp;kafkamessage.KafkaInvTopicMessage{}\n    if err := proto.Unmarshal(msg.Value, invMessage); err != nil {\n        return fmt.Errorf(\"failed to deserialize inventory message: %w\", err)\n    }\n\n    // Extract peer address\n    peerAddr := invMessage.PeerAddress\n\n    // Process inventory items\n    for _, inv := range invMessage.Inv {\n        // Convert hex string to chainhash.Hash\n        hash, err := chainhash.NewHashFromStr(inv.Hash)\n        if err != nil {\n            return fmt.Errorf(\"invalid inventory hash: %w\", err)\n        }\n\n        switch inv.Type {\n        case kafkamessage.InvType_Tx:\n            log.Printf(\"Received transaction inventory from %s: %s\", peerAddr, hash.String())\n            // Process transaction inventory...\n\n        case kafkamessage.InvType_Block:\n            log.Printf(\"Received block inventory from %s: %s\", peerAddr, hash.String())\n            // Process block inventory...\n\n        case kafkamessage.InvType_FilteredBlock:\n            log.Printf(\"Received filtered block inventory from %s: %s\", peerAddr, hash.String())\n            // Process filtered block inventory...\n\n        default:\n            log.Printf(\"Received unknown inventory type %d from %s: %s\", inv.Type, peerAddr, hash.String())\n        }\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#error-cases_7","title":"Error Cases","text":"<ul> <li>Invalid message format: Message cannot be unmarshaled to KafkaInvTopicMessage</li> <li>Empty peer address: PeerAddress field is empty</li> <li>Invalid inventory item: Hash is not a valid hexadecimal string or Type is unrecognized</li> </ul>"},{"location":"references/kafkaMessageFormat/#final-block-message-format","title":"Final Block Message Format","text":""},{"location":"references/kafkaMessageFormat/#final-block-topic","title":"Final Block Topic","text":"<p><code>kafka_blocksFinalConfig</code> is the Kafka topic used for broadcasting finalized blocks. This topic notifies subscribers about blocks that have been fully validated and accepted into the blockchain.</p>"},{"location":"references/kafkaMessageFormat/#message-structure_8","title":"Message Structure","text":"<p>The final block message is defined in protobuf as <code>KafkaBlocksFinalTopicMessage</code>:</p> <pre><code>message KafkaBlocksFinalTopicMessage {\n    bytes header = 1;                    // Block header bytes\n    uint64 transaction_count = 2;        // Number of transactions in block\n    uint64 size_in_bytes = 3;            // Size of block in bytes\n    repeated bytes subtree_hashes = 4;   // Merkle tree subtree hashes\n    bytes coinbase_tx = 5;               // Coinbase transaction bytes\n    uint32 height = 6;                   // Block height\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#field-specifications_8","title":"Field Specifications","text":""},{"location":"references/kafkaMessageFormat/#header","title":"header","text":"<ul> <li>Type: bytes</li> <li>Description: Serialized block header</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#transaction_count","title":"transaction_count","text":"<ul> <li>Type: uint64</li> <li>Description: Total number of transactions in the block</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#size_in_bytes","title":"size_in_bytes","text":"<ul> <li>Type: uint64</li> <li>Description: Total size of the block in bytes</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#subtree_hashes","title":"subtree_hashes","text":"<ul> <li>Type: repeated bytes</li> <li>Description: List of Merkle tree subtree hashes that compose the block</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#coinbase_tx","title":"coinbase_tx","text":"<ul> <li>Type: bytes</li> <li>Description: Serialized coinbase transaction</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#height_1","title":"height","text":"<ul> <li>Type: uint32</li> <li>Description: Block height in the blockchain</li> <li>Required: Yes</li> </ul>"},{"location":"references/kafkaMessageFormat/#example_8","title":"Example","text":"<p>Here's a JSON representation of the message content (for illustration purposes only; actual messages are protobuf-encoded):</p> <pre><code>{\n  \"header\": \"&lt;binary data - 80 bytes&gt;\",\n  \"transaction_count\": 2500,\n  \"size_in_bytes\": 1048576,\n  \"subtree_hashes\": [\n    \"&lt;binary data - 32 bytes&gt;\",\n    \"&lt;binary data - 32 bytes&gt;\",\n    \"&lt;binary data - 32 bytes&gt;\"\n  ],\n  \"coinbase_tx\": \"&lt;binary data - variable length&gt;\",\n  \"height\": 12345\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#code-examples_8","title":"Code Examples","text":""},{"location":"references/kafkaMessageFormat/#sending-messages_8","title":"Sending Messages","text":"<pre><code>// Create the final block message\nblockHeader := block.Header.Serialize() // serialized block header bytes\ntxCount := uint64(block.Transactions.Len())\nblockSize := uint64(block.SerializedSize())\n\n// Get subtree hashes\nsubtreeHashes := make([][]byte, len(merkleTree.SubTrees))\nfor i, subtree := range merkleTree.SubTrees {\n    subtreeHashes[i] = subtree.Hash()[:]\n}\n\n// Get coinbase transaction\ncoinbaseTx := block.Transactions[0].Serialize()\nblockHeight := uint32(block.Height)\n\n// Create a new protobuf message\nmessage := &amp;kafkamessage.KafkaBlocksFinalTopicMessage{\n    Header:          blockHeader,\n    TransactionCount: txCount,\n    SizeInBytes:     blockSize,\n    SubtreeHashes:   subtreeHashes,\n    CoinbaseTx:      coinbaseTx,\n    Height:          blockHeight,\n}\n\n// Serialize to protobuf format\ndata, err := proto.Marshal(message)\nif err != nil {\n    return fmt.Errorf(\"failed to serialize final block message: %w\", err)\n}\n\n// Send to Kafka\nproducer.Publish(&amp;kafka.Message{\n    Value: data,\n})\n</code></pre>"},{"location":"references/kafkaMessageFormat/#receiving-messages_8","title":"Receiving Messages","text":"<pre><code>// Handle incoming final block message\nfunc handleFinalBlockMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    // Deserialize from protobuf format\n    finalBlockMessage := &amp;kafkamessage.KafkaBlocksFinalTopicMessage{}\n    if err := proto.Unmarshal(msg.Value, finalBlockMessage); err != nil {\n        return fmt.Errorf(\"failed to deserialize final block message: %w\", err)\n    }\n\n    // Parse block header\n    header := wire.BlockHeader{}\n    if err := header.Deserialize(bytes.NewReader(finalBlockMessage.Header)); err != nil {\n        return fmt.Errorf(\"invalid block header: %w\", err)\n    }\n\n    // Extract other fields\n    txCount := finalBlockMessage.TransactionCount\n    blockSize := finalBlockMessage.SizeInBytes\n    subtreeHashes := finalBlockMessage.SubtreeHashes\n    coinbaseTxBytes := finalBlockMessage.CoinbaseTx\n    height := finalBlockMessage.Height\n\n    // Parse coinbase transaction\n    coinbaseTx, err := bsvutil.NewTxFromBytes(coinbaseTxBytes)\n    if err != nil {\n        return fmt.Errorf(\"invalid coinbase transaction: %w\", err)\n    }\n\n    // Process the final block...\n    log.Printf(\"Received final block at height %d with %d transactions (size: %d bytes)\",\n              height, txCount, blockSize)\n\n    // Process subtree hashes...\n    for i, hashBytes := range subtreeHashes {\n        var hash chainhash.Hash\n        copy(hash[:], hashBytes)\n        log.Printf(\"  Subtree hash %d: %s\", i, hash.String())\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#error-cases_8","title":"Error Cases","text":"<ul> <li>Invalid message format: Message cannot be unmarshaled to KafkaBlocksFinalTopicMessage</li> <li>Invalid block header: Header bytes cannot be deserialized to a valid block header</li> <li>Invalid coinbase transaction: Coinbase transaction bytes cannot be parsed</li> <li>Missing subtree hashes: No subtree hashes provided</li> </ul>"},{"location":"references/kafkaMessageFormat/#general-code-examples","title":"General Code Examples","text":""},{"location":"references/kafkaMessageFormat/#serializing-messages","title":"Serializing Messages","text":"<p>Here's a general example of how to serialize a protobuf message for Kafka:</p> <pre><code>// Create a new message\nmessage := &amp;kafkamessage.KafkaBlockTopicMessage{\n    Hash: blockHash[:],\n    URL:  datahubUrl,\n}\n\n// Serialize the message to protobuf format\ndata, err := proto.Marshal(message)\nif err != nil {\n    return fmt.Errorf(\"failed to serialize message: %w\", err)\n}\n\n// Send to Kafka\nproducer.Publish(&amp;kafka.Message{\n    Value: data,\n})\n</code></pre>"},{"location":"references/kafkaMessageFormat/#deserializing-messages","title":"Deserializing Messages","text":"<p>Here's a general example of how to deserialize a protobuf message from Kafka:</p> <pre><code>func handleBlockMessage(msg *kafka.Message) error {\n    if msg == nil {\n        return nil\n    }\n\n    // Create a new message container\n    blockMessage := &amp;kafkamessage.KafkaBlockTopicMessage{}\n\n    // Deserialize the message from protobuf format\n    if err := proto.Unmarshal(msg.Value, blockMessage); err != nil {\n        return fmt.Errorf(\"failed to deserialize message: %w\", err)\n    }\n\n    // Extract block hash\n    var blockHash chainhash.Hash\n    copy(blockHash[:], blockMessage.Hash)\n\n    // Extract DataHub URL\n    dataHubUrl := blockMessage.URL\n\n    // Process the message...\n    return nil\n}\n</code></pre>"},{"location":"references/kafkaMessageFormat/#other-resources","title":"Other Resources","text":"<ul> <li>Understanding Kafka Role in Teranode</li> <li>The Block data model</li> <li>The Block Header data model</li> <li>The Subtree data model</li> <li>The Transaction data model</li> <li>The UTXO data model</li> </ul>"},{"location":"references/licenseInformation/","title":"Teranode License Information","text":"<pre><code>Open BSV License Version 6 \u2013 granted by BSV Association, Alpenstrasse 15, 6300\nZug, Switzerland (CHE-427.008.338) (\"Licensor\"), to you as a user (henceforth\n\"You\", \"User\" or \"Licensee\").\n\nFor the purposes of this license, the definitions below have the following\nmeanings:\n\n\"Bitcoin Protocol\" means the protocol implementation, cryptographic rules,\nnetwork protocols, and consensus mechanisms in the Bitcoin White Paper as\ndescribed here https://protocol.bsvblockchain.org.\n\n\"Bitcoin White Paper\" means the paper entitled 'Bitcoin: A Peer-to-Peer\nElectronic Cash System' published by 'Satoshi Nakamoto' in October 2008.\n\n\"BSV Blockchain\" means:\n\n  (a) the Bitcoin blockchain containing block height #556767 with the hash\n      \"000000000000000001d956714215d96ffc00e0afda4cd0a96c96f8d802b1662b\" and\n      that contains the longest honest persistent chain of blocks which has been\n      produced in a manner which is consistent with the rules set forth in the\n      Network Access Rules; and\n  (b) the test blockchains that contain the longest honest persistent chains of\n      blocks which has been produced in a manner which is consistent with the\n      rules set forth in the Network Access Rules.\n\n\"Network Access Rules\" or \"Rules\" means the set of rules regulating the\nrelationship between BSV Association and the nodes on BSV based on the Bitcoin\nProtocol rules and those set out in the Bitcoin White Paper, and available here\nhttps://bsvblockchain.org/network-access-rules.\n\n\"Software\" means the software the subject of this license, including any/all\nintellectual property rights therein and associated documentation files.\n\nBSV Association grants permission, free of charge and on a non-exclusive basis\nto any person obtaining a copy of the Software to deal in the Software, including\nwithout limitation the rights to use, copy, modify, merge, publish, distribute,\nsublicense, and/or sell copies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to and conditioned upon the following\nconditions:\n\n1 - The text \"\u00a9 BSV Association\", and this license shall be included in all\ncopies or substantial portions of the Software.\n\n2 - The Software, and any software that is derived from the Software or parts\nthereof, may only be used exclusively on the BSV Blockchain.\n\nFor the avoidance of doubt, this license is granted subject to and conditioned\nupon your compliance with these terms only and is limited to uses on the BSV\nBlockchain. Any exercise of rights not compliant with these terms including\nuse not for the BSV Blockchain is deemed outside the scope of the license.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES REGARDING ENTITLEMENT,\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO\nEVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS THEREOF BE LIABLE FOR ANY CLAIM,\nDAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\nDEALINGS IN THE SOFTWARE.\n</code></pre>"},{"location":"references/networkConsensusRules/","title":"Network Consensus Rules","text":""},{"location":"references/networkConsensusRules/#network-consensus-rules","title":"\ud83d\udcdc Network Consensus Rules","text":"<p>Bitcoin Nodes must adhere to the following network consensus rules. These rules are protocol level rules that are set in stone:</p>"},{"location":"references/networkConsensusRules/#block-size-rule","title":"\ud83d\udce6 Block Size Rule","text":"<p>When a block is found, there is an economic limit applied to the block size which is imposed by nodes on the network.</p> <p>This allows nodes to reach consensus on behavioural limits of the network. This limit is set to a large multiple of typical demand.</p> <p>The Teranode Node system will be unbounded, so the block size limit will be purely economic, and the system will be capable of adding blocks of any size to the blockchain.</p>"},{"location":"references/networkConsensusRules/#block-subsidy-rule","title":"\ud83d\udcb0 Block Subsidy Rule","text":"<p>Nodes receive a reward each time they add a new block to the blockchain. The reward consists of a Block Subsidy and transaction fees.</p> <p>The Block Subsidy also acts to brings satoshis into circulation. The first Block Subsidies for the first 210,000 blocks added to the Bitcoin blockchain consisted of 5,000,000,000 satoshis or 50 BSV per block.</p> <p>Every 210,000 blocks, the value of the block subsidy is halved. The last halving took place in April 2024 at block height 840,000. As of April 2024, the value of a Block Subsidy is 312,500,000 satoshis or 3.125 BSV.</p>"},{"location":"references/networkConsensusRules/#proof-of-work-target-adjustment-rule","title":"\u2696\ufe0f Proof-of-Work Target Adjustment Rule","text":"<p>The Target value to find a Proof-of-Work is adjusted to maintain a block discovery rate of approximately 10 minutes.</p> <p>As of 2017, the Target value is adjusted every block. However, in the original release of Bitcoin, the Target value was adjusted every 14 days.</p> <p>The Teranode Node system will maintain the current Target Adjustment schedule of every block, but the system may be updated in the future to return to the original Target Adjustment schedule (as part of Protocol restoration).</p>"},{"location":"references/networkConsensusRules/#genesis-block-rule","title":"\ud83c\udfd7\ufe0f Genesis Block Rule","text":"<p>All new blocks must be added to the unbroken chain of Proof-of-Work leading back to the Genesis block or the initial block in the chain.</p> <p>Since blocks are connected via new blocks including the hash of its previous block, and Merkle trees are used to keep track of transactions within a block, this rule can be satisfied using the chain of block headers. Each block header is only 80 bytes.</p> <p>Given the chain of block headers can be used to adhere to the Genesis Block rules, the Initial Block Download or IBD where a new node joining the network downloads all block data including transaction data all the way back to the Genesis block used by current node implementations is not needed and will not be included in the Teranode Node system.</p>"},{"location":"references/networkConsensusRules/#coinbase-maturity-rule","title":"\u23f0 Coinbase Maturity Rule","text":"<p>Nodes cannot spend the output of a Coinbase Transaction unless 99 blocks have been added to the chain after it. In other words, for a node to spend its block reward, 99 more blocks must be added to the chain.</p>"},{"location":"references/networkConsensusRules/#maximum-transaction-size-rule","title":"\ud83d\udccf Maximum Transaction Size Rule","text":"<p>Bitcoin Nodes collectively set a practical limit for the size of transactions they are willing to timestamp into a block.</p> <p>The Teranode Node project will ensure this limit is entirely economic by allowing unbounded scaling.</p>"},{"location":"references/networkConsensusRules/#nlocktime-and-nsequence-rules","title":"\ud83d\udd12 nLockTime and nSequence Rules","text":"<p>The nSequence fields of every transaction input and the nLockTime field of every transaction collectively determine the finality of a transaction.</p> <p>If the value of nSequence of a transaction input is 0xFFFFFFFF then that input is a \u201cfinal input\u201d.</p> <p>If the value of nSequence of a transaction input is not 0xFFFFFFFF then that input is a \u201cnon-final input\u201d.</p> <p>If all the inputs of a transaction are \u201cfinal inputs\u201d then the transaction is \u201cfinal\u201d, irrespective of the value of the nLockTime field.</p> <p>If one or more of the inputs of a transaction are \u201cnon-final inputs\u201d then:</p> <ul> <li>If the value of the transaction\u2019s nLockTime field is less than 500,000,000 then the field represents a block height.</li> <li>If the node is working on a block whose height is greater or equal to the value of this field, then the transaction is \u201cfinal\u201d.</li> <li> <p>Otherwise, the transaction is \u201cnon-final\u201d.</p> </li> <li> <p>If the value of the transaction\u2019s nLockTime field is greater or equal to 500,000,000 then the field is a UNIX epoch timestamp.</p> </li> <li>If the median time passed of the last 11 blocks is greater or equal to the value of this field, then the transaction is \u201cfinal\u201d.</li> <li>Otherwise, the transaction is \u201cnon-final\u201d.</li> </ul> <p>A new transaction must replace a prior \u201cnon-final\u201d transaction if it has the same inputs in the same order, every sequence number for every input in the new transaction is not less than the sequence number for the corresponding input in the prior transaction, and the sequence number of at least one input in the new transaction is greater than the sequence number for the corresponding input in the prior transaction.</p> <p>If a new transaction is detected which does not fulfill all these requirements, then it must be rejected.</p> <p>If a new transaction is detected which has inputs that conflict with the inputs of a \u201cnon-final\u201d transaction, but which are not identical to the inputs of the</p> <p>\u201cnon-final\u201d transaction, then the \u201cnon-final\u201d transaction is the \u201cfirst seen\u201d transaction and takes priority over the new transaction.</p> <p>The sum value of the inputs to a transaction must be greater than or equal to the sum value of its outputs:</p> <p>A new transaction must have valid inputs that provide a value that is greater than or equal to the value of its outputs.</p> <p>If the value of the inputs is greater than the value of the outputs, the difference becomes part of the block reward.</p> <p>If the value of the inputs is equal to the value of the outputs, the transaction has no fee, and is considered a \u201cfree\u201d transaction.</p> <p>The Teranode Node system will support free transactions up to a cumulative maximum size: initially, 2GB, then 5GB per block.</p> <p>Free transactions must follow a standard transaction template that has been given the status of standard by the Technical Standards Committee.</p> <p>For example, a standard transaction template type could be transactions that only have Pay-to-Public-Key-Hash scripts in their inputs and outputs.</p>"},{"location":"references/networkConsensusRules/#transaction-format-rule","title":"\ud83d\udcdd Transaction Format Rule","text":"<p>Transactions must conform to the data formatting rules of the Bitcoin protocol.</p>"},{"location":"references/projectStructure/","title":"Teranode Project Structure","text":"<p>The Teranode project is structured as follows:</p> <pre><code>teranode/\n\u2502\n\u251c\u2500\u2500 main.go                       # Entry point to start the services\n\u2502\n\u251c\u2500\u2500 Makefile                      # Facilitates a variety of development and build tasks for the project\n\u2502\n\u251c\u2500\u2500 settings.conf                 # Global settings with sensible defaults for all environments\n\u2502\n\u251c\u2500\u2500 settings_local.conf           # Developer-specific and deployment-specific settings. Overrides settings.conf. Not tracked in source control.\n\u2502\n\u251c\u2500\u2500 Dockerfile                    # Main Dockerfile for containerization\n\u251c\u2500\u2500 docker-compose.yml            # Docker Compose configuration\n\u2502\n\u251c\u2500\u2500 cmd/                          # Directory containing command-line tools and utilities\n\u2502   \u251c\u2500\u2500 aerospikereader/          # Command related to Aerospike reader functionality\n\u2502   \u251c\u2500\u2500 bitcointoutxoset/         # Bitcoin to UTXO set utility\n\u2502   \u251c\u2500\u2500 checkblocktemplate/       # Tool to check block templates\n\u2502   \u251c\u2500\u2500 filereader/               # Utility for reading files\n\u2502   \u251c\u2500\u2500 getfsmstate/              # Tool to get FSM state\n\u2502   \u251c\u2500\u2500 keygen/                   # Key generation utility\n\u2502   \u251c\u2500\u2500 keypairgen/               # Key pair generation utility\n\u2502   \u251c\u2500\u2500 peercli/                  # Peer network command-line interface\n\u2502   \u251c\u2500\u2500 seeder/                   # Seeder functionality\n\u2502   \u251c\u2500\u2500 setfsmstate/              # Tool to set FSM state\n\u2502   \u251c\u2500\u2500 settings/                 # Settings management tools\n\u2502   \u251c\u2500\u2500 teranode/                 # Teranode main executable\n\u2502   \u251c\u2500\u2500 teranodecli/              # Teranode command-line interface\n\u2502   \u2514\u2500\u2500 utxopersister/            # UTXO persistence utility\n\u2502\n\u251c\u2500\u2500 services/                     # Core service implementations\n\u2502   \u251c\u2500\u2500 alert/                    # Alert service\n\u2502   \u251c\u2500\u2500 asset/                    # Asset service\n\u2502   \u251c\u2500\u2500 blockassembly/            # Block assembly service\n\u2502   \u251c\u2500\u2500 blockchain/               # Blockchain service\n\u2502   \u251c\u2500\u2500 blockpersister/           # Block persister service\n\u2502   \u251c\u2500\u2500 blockvalidation/          # Block validation service\n\u2502   \u251c\u2500\u2500 legacy/                   # Legacy services\n\u2502   \u251c\u2500\u2500 p2p/                      # Peer-to-peer networking service\n\u2502   \u251c\u2500\u2500 propagation/              # Transaction propagation service\n\u2502   \u251c\u2500\u2500 rpc/                      # RPC service\n\u2502   \u251c\u2500\u2500 subtreevalidation/        # Subtree validation service\n\u2502   \u251c\u2500\u2500 utxopersister/            # UTXO persister service\n\u2502   \u2514\u2500\u2500 validator/                # Transaction validator service\n\u2502\n\u251c\u2500\u2500 stores/                       # Data storage implementations\n\u2502   \u251c\u2500\u2500 blob/                     # Blob storage implementation\n\u2502   \u251c\u2500\u2500 blockchain/               # Blockchain storage implementation\n\u2502   \u251c\u2500\u2500 cleanup/                  # Cleanup storage utilities\n\u2502   \u251c\u2500\u2500 txmetacache/             # Transaction metadata cache implementation\n\u2502   \u2514\u2500\u2500 utxo/                     # UTXO storage implementation\n\u2502\n\u251c\u2500\u2500 docs/                         # Documentation for the project\n\u2502   \u251c\u2500\u2500 architecture/             # Architectural diagrams\n\u2502   \u251c\u2500\u2500 references/               # Reference documentation\n\u2502   \u2502   \u251c\u2500\u2500 protobuf_docs/        # Protobuf API documentation\n\u2502   \u2502   \u251c\u2500\u2500 services/            # Service reference documentation\n\u2502   \u2502   \u251c\u2500\u2500 stores/              # Store reference documentation\n\u2502   \u2502   \u2514\u2500\u2500 kafkaMessageFormat.md # Kafka message format documentation\n\u2502   \u2514\u2500\u2500 images/                   # Documentation images\n\u2502\n\u251c\u2500\u2500 compose/                      # Docker compose configurations\n\u2502\n\u251c\u2500\u2500 daemon/                       # Daemon implementation\n\u2502\n\u251c\u2500\u2500 deploy/                       # Deployment configurations and scripts\n\u2502\n\u251c\u2500\u2500 errors/                       # Error handling and definitions\n\u2502\n\u251c\u2500\u2500 interfaces/                   # Interface definitions\n\u2502\n\u251c\u2500\u2500 model/                        # Data models\n\u2502\n\u251c\u2500\u2500 pkg/                          # Reusable packages\n\u2502\n\u251c\u2500\u2500 scripts/                      # Various utility scripts\n\u2502\n\u251c\u2500\u2500 settings/                     # Settings management implementation\n\u2502\n\u251c\u2500\u2500 test/                         # Test utilities and integration tests\n\u2502\n\u251c\u2500\u2500 ui/                           # User interface components\n\u2502   \u2514\u2500\u2500 dashboard/                # Teranode Dashboard UI\n\u2502\n\u251c\u2500\u2500 ulogger/                      # Unified logging implementation\n\u2502\n\u251c\u2500\u2500 util/                         # Common utilities\n\u2502\n\u2514\u2500\u2500 venv/                         # Python virtual environment (local development)\n</code></pre>"},{"location":"references/prometheusMetrics/","title":"Teranode Prometheus Metrics Reference","text":""},{"location":"references/prometheusMetrics/#metric-types","title":"Metric Types","text":"Type Description Counter A cumulative metric that represents an increasing counter whose value can only increase OR be reset to zero. Used for counting events or operations (e.g., number of requests, errors). CounterVec A Counter that includes additional labels/dimensions. Allows for breaking down the counter by various labels (e.g., counting errors by type, requests by status code). Gauge A metric that represents a single numerical value that can arbitrarily go up and down. Used for measured values like items in a channel or queue. Histogram Samples observations (such as duration or size) and counts them in configurable buckets. Also provides a sum of all observed values and count of observations. Used for measuring distributions of values (e.g., request durations, response sizes)."},{"location":"references/prometheusMetrics/#alert-service-metrics","title":"Alert Service Metrics","text":"Metric Name Type Description <code>teranode_alert_health</code> Counter Number of calls to the Health endpoint"},{"location":"references/prometheusMetrics/#asset-service-http-metrics","title":"Asset Service HTTP Metrics","text":"Metric Name Type Description <code>teranode_asset_http_get_transaction</code> CounterVec Number of Get transactions ops <code>teranode_asset_http_get_transactions</code> CounterVec Number of Get transactions ops <code>teranode_asset_http_get_subtree</code> CounterVec Number of Get subtree ops <code>teranode_asset_http_get_block_header</code> CounterVec Number of Get block header ops <code>teranode_asset_http_get_best_block_header</code> CounterVec Number of Get best block header ops <code>teranode_asset_http_get_block</code> CounterVec Number of Get block ops <code>teranode_asset_http_get_block_legacy</code> CounterVec Number of Get legacy block ops <code>teranode_asset_http_get_subtree_data</code> CounterVec Number of Get subtree data ops <code>teranode_asset_http_get_last_n_blocks</code> CounterVec Number of Get last N blocks ops <code>teranode_asset_http_get_utxo</code> CounterVec Number of Get UTXO ops"},{"location":"references/prometheusMetrics/#block-assembly-service-metrics","title":"Block Assembly Service Metrics","text":"Metric Name Type Description <code>teranode_blockassembly_health</code> Counter Number of calls to the health endpoint of the blockassembly service <code>teranode_blockassembly_add_tx</code> Histogram Histogram of AddTx in the blockassembly service <code>teranode_blockassembly_remove_tx</code> Histogram Histogram of RemoveTx in the blockassembly service <code>teranode_blockassembly_get_mining_candidate_duration</code> Histogram Histogram of GetMiningCandidate in the blockassembly service <code>teranode_blockassembly_submit_mining_solution_ch</code> Gauge Number of items in the SubmitMiningSolution channel in the blockassembly service <code>teranode_blockassembly_submit_mining_solution</code> Histogram Histogram of SubmitMiningSolution in the blockassembly service <code>teranode_blockassembly_update_subtrees_dah</code> Histogram Histogram of updating subtrees DAH in the blockassembly service <code>teranode_blockassembly_block_assembler_get_mining_candidate</code> Counter Number of calls to GetMiningCandidate in the block assembler <code>teranode_blockassembly_subtree_created</code> Counter Number of subtrees created in the block assembler <code>teranode_blockassembly_transactions</code> Gauge Number of transactions currently in the block assembler subtree processor <code>teranode_blockassembly_queued_transactions</code> Gauge Number of transactions currently queued in the block assembler subtree processor <code>teranode_blockassembly_subtrees</code> Gauge Number of subtrees currently in the block assembler subtree processor <code>teranode_blockassembly_tx_meta_get</code> Histogram Histogram of reading tx meta data from txmeta store in block assembler <code>teranode_blockassembly_reorg</code> Counter Number of reorgs in block assembler <code>teranode_blockassembly_reorg_duration</code> Histogram Histogram of reorg in block assembler <code>teranode_blockassembly_get_reorg_blocks_duration</code> Histogram Histogram of GetReorgBlocks in block assembler <code>teranode_blockassembly_update_best_block</code> Histogram Histogram of updating best block in block assembler <code>teranode_blockassembly_best_block_height</code> Gauge Best block height in block assembly <code>teranode_blockassembly_current_block_height</code> Gauge Current block height in block assembly <code>teranode_blockassembly_generate_blocks</code> Histogram Histogram of generating blocks in block assembler <code>teranode_blockassembly_current_state</code> Gauge Current state of the block assembly process"},{"location":"references/prometheusMetrics/#blockchain-service-metrics","title":"Blockchain Service Metrics","text":"Metric Name Type Description <code>teranode_blockchain_health</code> Counter Number of calls to the health endpoint of the blockchain service <code>teranode_blockchain_add_block</code> Histogram Histogram of block added to the blockchain service <code>teranode_blockchain_get_block</code> Histogram Histogram of Get block calls to the blockchain service <code>teranode_blockchain_get_block_stats</code> Histogram Histogram of Get block stats calls to the blockchain service <code>teranode_blockchain_get_block_graph_data</code> Histogram Histogram of Get block graph data calls to the blockchain service <code>teranode_blockchain_get_last_n_block</code> Histogram Histogram of GetLastNBlocks calls to the blockchain service <code>teranode_blockchain_get_suitable_block</code> Histogram Histogram of GetSuitableBlock calls to the blockchain service <code>teranode_blockchain_get_hash_of_ancestor_block</code> Histogram Histogram of GetHashOfAncestorBlock calls to the blockchain service <code>teranode_blockchain_get_next_work_required</code> Histogram Histogram of GetNextWorkRequired calls to the blockchain service <code>teranode_blockchain_get_block_exists</code> Histogram Histogram of GetBlockExists calls to the blockchain service <code>teranode_blockchain_get_get_best_block_header</code> Histogram Histogram of GetBestBlockHeader calls to the blockchain service <code>teranode_blockchain_check_block_is_in_current_chain</code> Histogram Histogram of CheckBlockIsInCurrentChain calls to the blockchain service <code>teranode_blockchain_get_chain_tips</code> Histogram Histogram of GetChainTips calls to the blockchain service <code>teranode_blockchain_get_get_block_header</code> Histogram Histogram of GetBlockHeader calls to the blockchain service <code>teranode_blockchain_get_get_block_headers</code> Histogram Histogram of GetBlockHeaders calls to the blockchain service <code>teranode_blockchain_get_get_block_headers_from_height</code> Histogram Histogram of GetBlockHeadersFromHeight calls to the blockchain service <code>teranode_blockchain_get_get_block_headers_by_height</code> Histogram Histogram of GetBlockHeadersByHeight calls to the blockchain service <code>teranode_blockchain_get_block_is_mined</code> Histogram Histogram of GetBlockIsMined calls to the blockchain service <code>teranode_blockchain_subscribe</code> Histogram Histogram of Subscribe calls to the blockchain service <code>teranode_blockchain_get_state</code> Histogram Histogram of GetState calls to the blockchain service <code>teranode_blockchain_set_state</code> Histogram Histogram of SetState calls to the blockchain service <code>teranode_blockchain_get_block_header_ids</code> Histogram Histogram of GetBlockHeaderIDs calls to the blockchain service <code>teranode_blockchain_invalidate_block</code> Histogram Histogram of InvalidateBlock calls to the blockchain service <code>teranode_blockchain_revalidate_block</code> Histogram Histogram of RevalidateBlock calls to the blockchain service <code>teranode_blockchain_send_notification</code> Histogram Histogram of SendNotification calls to the blockchain service <code>teranode_blockchain_set_block_mined_set</code> Histogram Histogram of SetBlockMinedSet calls to the blockchain service <code>teranode_blockchain_get_blocks_mined_not_set</code> Histogram Histogram of GetBlocksMinedNotSet calls to the blockchain service <code>teranode_blockchain_set_block_subtrees_set</code> Histogram Histogram of SetBlockSubtreesSet calls to the blockchain service <code>teranode_blockchain_get_blocks_subtrees_not_set</code> Histogram Histogram of GetBlocksSubtreesNotSet calls to the blockchain service <code>teranode_blockchain_fsm_current_state</code> Gauge Current state of the blockchain FSM <code>teranode_blockchain_get_fsm_current_state</code> Histogram Histogram of GetFSMCurrentState calls to the blockchain service <code>teranode_blockchain_get_block_locator</code> Histogram Histogram of GetBlockLocator calls to the blockchain service <code>teranode_blockchain_locate_block_headers</code> Histogram Histogram of LocateBlockHeaders calls to the blockchain service"},{"location":"references/prometheusMetrics/#block-persister-service-metrics","title":"Block Persister Service Metrics","text":"Metric Name Type Description <code>teranode_blockpersister_validate_subtree</code> Histogram Histogram of subtree validation <code>teranode_blockpersister_validate_subtree_retry</code> Counter Number of retries when subtrees validated <code>teranode_blockpersister_validate_subtree_handler</code> Histogram Histogram of subtree handler <code>teranode_blockpersister_persist_block</code> Histogram Histogram of PersistBlock in the blockpersister service <code>teranode_blockpersister_bless_missing_transaction</code> Histogram Histogram of bless missing transaction <code>teranode_blockpersister_set_tx_meta_cache_kafka</code> Histogram Histogram of setting tx meta cache from kafka <code>teranode_blockpersister_del_tx_meta_cache_kafka</code> Histogram Duration of deleting tx meta cache from kafka <code>teranode_blockpersister_set_tx_meta_cache_kafka_errors</code> Counter Number of errors setting tx meta cache from kafka <code>teranode_blockpersister_blocks_duration</code> Histogram Duration of block processing by the block persister service <code>teranode_blockpersister_subtrees_duration</code> Histogram Duration of subtree processing by the block persister service <code>teranode_blockpersister_subtree_batch_duration</code> Histogram Duration of a subtree batch processing by the block persister service"},{"location":"references/prometheusMetrics/#block-validation-service-metrics","title":"Block Validation Service Metrics","text":"Metric Name Type Description <code>teranode_blockvalidation_health</code> Counter Number of health checks <code>teranode_blockvalidation_block_found_ch</code> Gauge Number of blocks found buffered in the block found channel <code>teranode_blockvalidation_block_found</code> Histogram Histogram of calls to BlockFound method <code>teranode_blockvalidation_catchup_ch</code> Gauge Number of catchups buffered in the catchup channel <code>teranode_blockvalidation_catchup</code> Histogram Histogram of catchup events <code>teranode_blockvalidation_process_block_found</code> Histogram Histogram of process block found <code>teranode_blockvalidation_validate_block</code> Histogram Histogram of calls to ValidateBlock method <code>teranode_blockvalidation_revalidate_block</code> Histogram Histogram of re-validate block <code>teranode_blockvalidation_revalidate_block_err</code> Histogram Number of blocks revalidated with error <code>teranode_blockvalidation_last_validated_blocks_cache</code> Gauge Number of blocks in the last validated blocks cache <code>teranode_blockvalidation_block_exists_cache</code> Gauge Number of blocks in the block exists cache <code>teranode_blockvalidation_subtree_exists_cache</code> Gauge Number of subtrees in the subtree exists cache <code>teranode_blockvalidation_catchup_peer_id</code> CounterVec Number of catchup operations by peer ID <code>teranode_blockvalidation_catchup_success</code> CounterVec Number of successful catchup operations <code>teranode_blockvalidation_catchup_error_type</code> CounterVec Number of catchup operations by error type <code>teranode_blockvalidation_catchup_duration</code> Histogram Duration of catchup operations <code>teranode_blockvalidation_catchup_blocks_processed</code> Counter Total number of blocks processed during catchup"},{"location":"references/prometheusMetrics/#legacy-peer-server-metrics","title":"Legacy Peer Server Metrics","text":"<p>Each metric measures \"The time taken to handle a specific legacy action handler\".</p> Metric Name Type Description <code>teranode_legacy_peer_server_OnVersion</code> Histogram The time taken to handle OnVersion <code>teranode_legacy_peer_server_OnProtoconf</code> Histogram The time taken to handle OnProtoconf <code>teranode_legacy_peer_server_OnMemPool</code> Histogram The time taken to handle OnMemPool <code>teranode_legacy_peer_server_OnTx</code> Histogram The time taken to handle OnTx <code>teranode_legacy_peer_server_OnBlock</code> Histogram The time taken to handle OnBlock <code>teranode_legacy_peer_server_OnInv</code> Histogram The time taken to handle OnInv <code>teranode_legacy_peer_server_OnHeaders</code> Histogram The time taken to handle OnHeaders <code>teranode_legacy_peer_server_OnGetData</code> Histogram The time taken to handle OnGetData <code>teranode_legacy_peer_server_OnGetBlocks</code> Histogram The time taken to handle OnGetBlocks <code>teranode_legacy_peer_server_OnGetHeaders</code> Histogram The time taken to handle OnGetHeaders <code>teranode_legacy_peer_server_OnFeeFilter</code> Histogram The time taken to handle OnFeeFilter <code>teranode_legacy_peer_server_OnGetAddr</code> Histogram The time taken to handle OnGetAddr <code>teranode_legacy_peer_server_OnAddr</code> Histogram The time taken to handle OnAddr <code>teranode_legacy_peer_server_OnReject</code> Histogram The time taken to handle OnReject <code>teranode_legacy_peer_server_OnNotFound</code> Histogram The time taken to handle OnNotFound <code>teranode_legacy_peer_server_OnRead</code> Histogram The time taken to handle OnRead <code>teranode_legacy_peer_server_OnWrite</code> Histogram The time taken to handle OnWrite"},{"location":"references/prometheusMetrics/#legacy-netsync-service-metrics","title":"Legacy NetSync Service Metrics","text":"Metric Name Type Description <code>teranode_legacy_netsync_block_height</code> Gauge The height of the block being processed <code>teranode_legacy_netsync_handle_tx_msg</code> Histogram The time taken to handle a tx message <code>teranode_legacy_netsync_handle_tx_msg_validate</code> Histogram The time taken to validate a tx message <code>teranode_legacy_netsync_process_orphan_transactions</code> Histogram The time taken to process orphan transactions <code>teranode_legacy_netsync_handle_block_direct</code> Histogram The time taken to handle a block directly <code>teranode_legacy_netsync_process_block</code> Histogram The time taken to process a block <code>teranode_legacy_netsync_prepare_subtrees</code> Histogram The time taken to prepare the subtrees <code>teranode_legacy_netsync_validate_transactions_legacy_mode</code> Histogram The time taken to validate transactions in legacy mode <code>teranode_legacy_netsync_pre_validate_transactions</code> Histogram The time taken to pre-validate transactions <code>teranode_legacy_netsync_validate_transactions</code> Histogram The time taken to validate transactions <code>teranode_legacy_netsync_extend_transactions</code> Histogram The time taken to extend transactions <code>teranode_legacy_netsync_create_utxos</code> Histogram The time taken to create UTXOs <code>teranode_legacy_netsync_block_tx_size</code> Histogram The size of the transactions in the block being processed <code>teranode_legacy_netsync_block_tx_nr_inputs</code> Histogram The number of inputs in the block being processed <code>teranode_legacy_netsync_block_tx_nr_outputs</code> Histogram The number of outputs in the block being processed <code>teranode_legacy_netsync_block_tx_extend</code> Histogram The time taken to extend a transaction <code>teranode_legacy_netsync_block_tx_validate</code> Histogram The time taken to validate a transaction <code>teranode_legacy_netsync_orphans</code> Gauge The number of orphan transactions <code>teranode_legacy_netsync_orphan_time</code> Histogram The time taken to process an orphan transaction"},{"location":"references/prometheusMetrics/#propagation-service-metrics","title":"Propagation Service Metrics","text":"Metric Name Type Description <code>teranode_propagation_health</code> Histogram Histogram of calls to the health endpoint of the propagation service <code>teranode_propagation_transactions</code> Histogram Histogram of transaction processing by the propagation service <code>teranode_propagation_transactions_batch</code> Histogram Histogram of transaction processing by the propagation service <code>teranode_propagation_handle_single_tx</code> Histogram Histogram of transaction processing by the propagation service using HTTP <code>teranode_propagation_handle_multiple_tx</code> Histogram Histogram of multiple transaction processing by the propagation service using HTTP <code>teranode_propagation_transactions_size</code> Histogram Size of transactions processed by the propagation service <code>teranode_propagation_invalid_transactions</code> Counter Number of transactions found invalid by the propagation service"},{"location":"references/prometheusMetrics/#rpc-service-metrics","title":"RPC Service Metrics","text":"Metric Name Type Description <code>teranode_rpc_get_block</code> Histogram Histogram of calls to handleGetBlock in the rpc service <code>teranode_rpc_get_block_by_height</code> Histogram Histogram of calls to handleGetBlockByHeight in the rpc service <code>teranode_rpc_get_block_hash</code> Histogram Histogram of calls to handleGetBlockHash in the rpc service <code>teranode_rpc_get_block_header</code> Histogram Histogram of calls to handleGetBlockHeader in the rpc service <code>teranode_rpc_get_best_block_hash</code> Histogram Histogram of calls to handleGetBestBlockHash in the rpc service <code>teranode_rpc_get_raw_transaction</code> Histogram Histogram of calls to handleGetRawTransaction in the rpc service <code>teranode_rpc_create_raw_transaction</code> Histogram Histogram of calls to handleCreateRawTransaction in the rpc service <code>teranode_rpc_send_raw_transaction</code> Histogram Histogram of calls to handleSendRawTransaction in the rpc service <code>teranode_rpc_generate</code> Histogram Histogram of calls to handleGenerate in the rpc service <code>teranode_rpc_generate_to_address</code> Histogram Histogram of calls to handleGenerateToAddress in the rpc service <code>teranode_rpc_get_mining_candidate</code> Histogram Histogram of calls to handleGetMiningCandidate in the rpc service <code>teranode_rpc_submit_mining_solution</code> Histogram Histogram of calls to handleSubmitMiningSolution in the rpc service <code>teranode_rpc_get_peer_info</code> Histogram Histogram of calls to handleGetpeerinfo in the rpc service <code>teranode_rpc_get_blockchain_info</code> Histogram Histogram of calls to handleGetblockchaininfo in the rpc service <code>teranode_rpc_get_info</code> Histogram Histogram of calls to handleGetinfo in the rpc service <code>teranode_rpc_get_difficulty</code> Histogram Histogram of calls to handleGetDifficulty in the rpc service <code>teranode_rpc_invalidate_block</code> Histogram Histogram of calls to handleInvalidateBlock in the rpc service <code>teranode_rpc_reconsider_block</code> Histogram Histogram of calls to handleReconsiderBlock in the rpc service <code>teranode_rpc_help</code> Histogram Histogram of calls to handleHelp in the rpc service <code>teranode_rpc_set_ban</code> Histogram Histogram of calls to handleSetBan in the rpc service <code>teranode_rpc_is_banned</code> Histogram Histogram of calls to handleIsBanned in the rpc service <code>teranode_rpc_get_mining_info</code> Histogram Histogram of calls to handleGetMiningInfo in the rpc service <code>teranode_rpc_list_banned</code> Histogram Histogram of calls to handleListBanned in the rpc service <code>teranode_rpc_clear_banned</code> Histogram Histogram of calls to handleClearBanned in the rpc service <code>teranode_rpc_freeze</code> Histogram Histogram of calls to handleFreeze in the rpc service <code>teranode_rpc_unfreeze</code> Histogram Histogram of calls to handleUnfreeze in the rpc service <code>teranode_rpc_reassign</code> Histogram Histogram of calls to handleReassign in the rpc service <code>teranode_rpc_get_chaintips</code> Histogram Histogram of calls to handleGetChainTips in the rpc service"},{"location":"references/prometheusMetrics/#subtree-validation-service-metrics","title":"Subtree Validation Service Metrics","text":"Metric Name Type Description <code>teranode_subtreevalidation_health</code> Histogram Histogram of calls to health endpoint <code>teranode_subtreevalidation_check_subtree</code> Histogram Duration of calls to checkSubtree endpoint <code>teranode_subtreevalidation_validate_subtree</code> Histogram Histogram of subtrees validated <code>teranode_subtreevalidation_validate_subtree_retry</code> Counter Number of retries when subtrees validated <code>teranode_subtreevalidation_validate_subtree_handler</code> Histogram Duration of subtree handler <code>teranode_subtreevalidation_validate_subtree_duration</code> Histogram Duration of validate subtree <code>teranode_subtreevalidation_bless_missing_transaction</code> Histogram Duration of bless missing transaction <code>teranode_subtreevalidation_set_tx_meta_cache_kafka</code> Histogram Duration of setting tx meta cache from kafka <code>teranode_subtreevalidation_del_tx_meta_cache_kafka</code> Histogram Duration of deleting tx meta cache from kafka <code>teranode_subtreevalidation_set_tx_meta_cache_kafka_errors</code> Counter Number of errors setting tx meta cache from kafka"},{"location":"references/prometheusMetrics/#validator-service-metrics","title":"Validator Service Metrics","text":"Metric Name Type Description <code>teranode_validator_health</code> Counter Number of calls to the health endpoint <code>teranode_validator_transaction</code> Histogram Histogram of transaction validation by the validator service <code>teranode_validator_transactions</code> Histogram Histogram of batch transaction validation by the validator service <code>teranode_validator_transactions_deadline</code> Histogram Histogram of batch transaction validation by deadline <code>teranode_validator_nblocks</code> Gauge Number of blocks processed by the validator service <code>teranode_validator_transactions_size</code> Histogram Size of transactions being validated by the validator service <code>teranode_validator_transactions_extended</code> Histogram The number of transactions extended per call to the validator service <code>teranode_validator_transactions_validate_scripts</code> Histogram Time taken to validate scripts in transactions <code>teranode_validator_transactions_input_block_heights</code> Histogram Distribution of input block heights in transactions <code>teranode_validator_transactions_2phase_commit</code> Histogram Time taken for two-phase commit of transactions <code>teranode_validator_send_to_block_assembly</code> Histogram Histogram of sending transactions to block assembly <code>teranode_validator_send_to_blockvalidation_kafka</code> Histogram Histogram of sending transactions to block validation kafka <code>teranode_validator_send_to_p2p_kafka</code> Histogram Histogram of sending rejected transactions to p2p kafka <code>teranode_validator_set_tx_meta</code> Histogram Histogram of validator set tx meta"},{"location":"references/prometheusMetrics/#txmetacache-service-metrics","title":"TxMetaCache Service Metrics","text":"Metric Name Type Description <code>teranode_tx_meta_cache_size</code> Gauge Number of items in the tx meta cache <code>teranode_tx_meta_cache_insertions</code> Gauge Number of insertions into the tx meta cache <code>teranode_tx_meta_cache_hits</code> Gauge Number of hits in the tx meta cache <code>teranode_tx_meta_cache_misses</code> Gauge Number of misses in the tx meta cache <code>teranode_tx_meta_cache_get_origin</code> Gauge Number of get origins in the tx meta cache <code>teranode_tx_meta_cache_evictions</code> Gauge Number of evictions in the tx meta cache <code>teranode_tx_meta_cache_trims</code> Gauge Number of trim operations in the tx meta cache <code>teranode_tx_meta_cache_map_size</code> Gauge Number of total elements in the improved cache's bucket maps <code>teranode_tx_meta_cache_total_elements_added</code> Gauge Number of total number of elements added to the txmetacache <code>teranode_tx_meta_cache_hit_old_tx</code> Gauge Number of hits on old txs in the tx meta cache"},{"location":"references/prometheusMetrics/#aerospike-service-metrics","title":"Aerospike Service Metrics","text":"Metric Name Type Description <code>teranode_aerospike_txmeta_get</code> Counter Number of txmeta get calls done to aerospike <code>teranode_aerospike_utxo_store</code> Counter Number of Create calls done to aerospike <code>teranode_aerospike_txmeta_set_mined</code> Counter Number of txmeta set_mined calls done to aerospike <code>teranode_aerospike_txmeta_errors</code> CounterVec Number of txmeta map errors <code>teranode_aerospike_txmeta_get_multi</code> Counter Number of txmeta get_multi calls done to aerospike map <code>teranode_aerospike_txmeta_get_multi_n</code> Counter Number of txmeta get_multi txs done to aerospike map <code>teranode_aerospike_txmeta_set_mined_batch</code> Counter Number of txmeta set_mined_batch calls done to aerospike map <code>teranode_aerospike_txmeta_set_mined_batch_n</code> Counter Number of txmeta set_mined_batch txs done to aerospike map <code>teranode_aerospike_txmeta_set_mined_batch_err_n</code> Counter Number of txmeta set_mined_batch txs errors to aerospike map <code>teranode_aerospike_utxo_get</code> Counter Number of utxo get calls done to aerospike <code>teranode_aerospike_utxo_spend</code> Counter Number of utxo spend calls done to aerospike <code>teranode_aerospike_utxo_reset</code> Counter Number of utxo reset calls done to aerospike <code>teranode_aerospike_utxo_delete</code> Counter Number of utxo delete calls done to aerospike <code>teranode_aerospike_utxo_errors</code> CounterVec Number of utxo errors <code>teranode_aerospike_utxo_create_batch</code> Histogram Duration of utxo create batch <code>teranode_aerospike_utxo_create_batch_size</code> Histogram Size of utxo create batch <code>teranode_aerospike_utxo_spend_batch</code> Histogram Duration of utxo spend batch <code>teranode_aerospike_utxo_spend_batch_size</code> Histogram Size of utxo spend batch <code>teranode_aerospike_get_external</code> Histogram Duration of getting an external transaction from the blob store <code>teranode_aerospike_set_external</code> Histogram Duration of setting an external transaction to the blob store <code>teranode_aerospike_txmeta_get_counter_conflicting</code> Counter Counter of conflicting TxMeta GET operations using Aerospike <code>teranode_aerospike_txmeta_get_conflicting</code> Histogram Histogram of conflicting TxMeta GET operations using Aerospike <code>teranode_aerospike_external_tx_errors</code> CounterVec Number of external transaction operation errors <code>teranode_aerospike_batch_operation_duration</code> Histogram Duration of batch operations in aerospike <code>teranode_aerospike_connection_pool_size</code> Gauge Current size of aerospike connection pool <code>teranode_aerospike_operation_retries</code> Counter Number of operation retries in aerospike"},{"location":"references/prometheusMetrics/#sql-service-metrics","title":"SQL Service Metrics","text":"Metric Name Type Description <code>teranode_sql_utxo_get</code> Counter Number of utxo get calls done to sql <code>teranode_sql_utxo_spend</code> Counter Number of utxo spend calls done to sql <code>teranode_sql_utxo_reset</code> Counter Number of utxo reset calls done to sql <code>teranode_sql_utxo_delete</code> Counter Number of utxo delete calls done to sql <code>teranode_sql_utxo_errors</code> CounterVec Number of utxo errors <code>teranode_sql_utxo_get_counter_conflicting</code> Counter Counter of conflicting UTXO GET operations using SQL <code>teranode_sql_utxo_get_conflicting</code> Histogram Histogram of conflicting UTXO GET operations using SQL"},{"location":"references/prometheusMetrics/#subtree-processor-service-metrics","title":"Subtree Processor Service Metrics","text":"Metric Name Type Description <code>teranode_subtreeprocessor_add_tx</code> Counter Number of times a tx is added in subtree processor <code>teranode_subtreeprocessor_move_forward</code> Counter Number of times a block is moved up in subtree processor <code>teranode_subtreeprocessor_move_forward_duration</code> Histogram Histogram of moving up block in subtree processor <code>teranode_subtreeprocessor_move_back</code> Counter Number of times a block is moved down in subtree processor <code>teranode_subtreeprocessor_move_back_duration</code> Histogram Histogram of moving down block in subtree processor <code>teranode_subtreeprocessor_process_coinbase_tx</code> Counter Number of times a coinbase tx is processed in subtree processor <code>teranode_subtreeprocessor_process_coinbase_tx_duration</code> Histogram Duration of processing coinbase tx in subtree processor <code>teranode_subtreeprocessor_transaction_map</code> Counter Number of times a transaction map is created in subtree processor <code>teranode_subtreeprocessor_transaction_map_duration</code> Histogram Duration of creating transaction map in subtree processor <code>teranode_subtreeprocessor_remove_tx</code> Histogram Duration of removing tx in subtree processor <code>teranode_subtreeprocessor_reset</code> Histogram Duration of resetting subtree processor <code>teranode_subtreeprocessor_dynamic_subtree_size</code> Gauge Size of the dynamic subtree in the subtree processor <code>teranode_subtreeprocessor_current_state</code> Gauge Current state of the subtree processor"},{"location":"references/settings/","title":"\u2699\ufe0fSettings","text":"<p>All services accept settings through a centralized Settings object that allows local and remote servers to have their own specific configuration.</p> <p>Please review the following documents for more information on how to deploy the settings:</p> <ul> <li>Developer Setup</li> <li>Test Setup</li> <li>Production Setup</li> </ul>"},{"location":"references/settings/#configuration-files","title":"Configuration Files","text":"<p>The settings are stored in 2 files:</p> <ul> <li><code>settings.conf</code> - Global settings with sensible defaults for all environments (local/dev/operator)</li> <li><code>settings_local.conf</code> - Developer-specific and deployment-specific settings, local overrides (not tracked in source control)</li> </ul> <p>When developing locally, you should:</p> <ol> <li>Use <code>settings.conf</code> for default configuration values, and not modify it unless intended as a global change to be shared with other developers and operators.</li> <li>Use <code>settings_local.conf</code> for your personal development settings and overrides</li> </ol>"},{"location":"references/settings/#configuration-system","title":"Configuration System","text":"<p>The configuration system allows for a layered approach to settings. At its core, it works with a base setting. However, to cater to individualized or context-specific requirements, you can have context-dependent overrides.</p> <p>Here's how it prioritizes:</p> <ol> <li><code>SETTING_NAME.context_name</code>: A context-specific override (highest priority)</li> <li><code>SETTING_NAME.base</code>: A general override</li> <li><code>SETTING_NAME</code>: The base setting (lowest priority)</li> </ol>"},{"location":"references/settings/#example","title":"Example","text":"<p>Suppose we have a base setting named <code>DATABASE_URL</code> to define the database connection URL for our application.</p> <p>The base setting might be: <pre><code>DATABASE_URL = \"database-url-default.com\"\n</code></pre></p> <p>As an example, we might have a <code>newenvironment1</code> database for development purposes. So, for the context <code>dev.newenvironment1</code>, there might be an override: <pre><code>DATABASE_URL.dev.newenvironment1 = \"database-url-environment1\"\n</code></pre></p> <p>There might also be a generic development database URL, defined as: <pre><code>DATABASE_URL.dev = \"database-url-dev.com\"\n</code></pre></p> <p>When the application is run against this development context (<code>dev.newenvironment1</code>):</p> <ul> <li>The system first checks for <code>DATABASE_URL.dev.newenvironment1</code>. If it exists, it's used.</li> <li>If not, it falls back to the general development URL <code>DATABASE_URL.dev</code>.</li> <li>If neither exists, it defaults to <code>DATABASE_URL</code>.</li> </ul> <p>For <code>newenvironment1</code>, the resolution would be:</p> <ol> <li>First Preference: <code>DATABASE_URL.dev.newenvironment1</code> -&gt; \"database-url-dev.com\"</li> <li>Fallback: <code>DATABASE_URL.dev</code> -&gt; \"database-url-dev.com\"</li> <li>Last Resort: <code>DATABASE_URL</code> -&gt; \"database-url-default.com\"</li> </ol> <p>This approach provides flexibility to have a default setting, an optional general override, and further context-specific overrides. It's a hierarchical system that allows fine-grained control over configurations based on context.</p>"},{"location":"references/settings/#accessing-settings-in-go","title":"Accessing Settings in Go","text":"<p>The settings are accessed through a centralized Settings object that is passed to services requiring configuration. Here's how to use it:</p>"},{"location":"references/settings/#initialization","title":"Initialization","text":"<p>First, create a new Settings instance:</p> <pre><code>settings := settings.NewSettings()\n</code></pre> <p>This will load all configuration values from the settings files according to the priority system described above.</p>"},{"location":"references/settings/#accessing-settings","title":"Accessing Settings","text":"<p>Settings are organized into logical groups within the Settings struct. For example:</p> <pre><code>// Access Kafka settings\nkafkaHosts := settings.Kafka.Hosts\nkafkaPort := settings.Kafka.Port\n\n// Access Blockchain settings\ngrpcAddress := settings.BlockChain.GRPCAddress\nmaxRetries := settings.BlockChain.MaxRetries\n\n// Access Alert settings\ngenesisKeys := settings.Alert.GenesisKeys\np2pPort := settings.Alert.P2PPort\n</code></pre>"},{"location":"references/settings/#available-setting-groups","title":"Available Setting Groups","text":"<p>The Settings struct includes multiple setting groups:</p> <ul> <li>Alert Settings (AlertSettings)</li> <li>Asset Settings (AssetSettings)</li> <li>Block Settings (BlockSettings)</li> <li>BlockChain Settings (BlockChainSettings)</li> <li>BlockValidation Settings (BlockValidationSettings)</li> <li>Kafka Settings (KafkaSettings)</li> <li>Validator Settings (ValidatorSettings)</li> <li>And more...</li> </ul> <p>Each group contains related configuration values specific to that component of the system.</p>"},{"location":"references/settings/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always pass the Settings object as a dependency to services that need configuration: <pre><code>func NewService(logger ulogger.Logger, settings *settings.Settings) *Service {\n    return &amp;Service{\n        logger: logger,\n        settings: settings,\n        // ...\n    }\n}\n</code></pre></p> </li> <li> <p>Access settings through the appropriate group rather than using direct key access: <pre><code>// Good\nmaxRetries := settings.BlockChain.MaxRetries\n\n// Avoid (historical style, now deprecated)\nmaxRetries, _ := gocore.Config().GetInt(\"blockchain_maxRetries\")\n</code></pre></p> </li> <li> <p>Use the type system to your advantage - settings are strongly typed within their respective groups.</p> </li> </ol> <p>Note: The old <code>gocore.Config()</code> approach with direct key access is deprecated. Always use the new Settings object for accessing configuration values.</p>"},{"location":"references/settings/#detailed-settings-reference","title":"Detailed Settings Reference","text":"<p>For comprehensive documentation of all available settings, see the following references organized by component:</p>"},{"location":"references/settings/#services","title":"Services","text":"<ul> <li>Alert Service Settings</li> <li>Asset Server Settings</li> <li>Block Assembly Settings</li> <li>Blockchain Settings</li> <li>Block Persister Settings</li> <li>Block Validation Settings</li> <li>Legacy Settings</li> <li>P2P Settings</li> <li>Propagation Settings</li> <li>RPC Settings</li> <li>Subtree Validation Settings</li> <li>UTXO Persister Settings</li> <li>Validator Settings</li> </ul>"},{"location":"references/settings/#stores","title":"Stores","text":"<ul> <li>UTXO Store Settings</li> <li>Blob Store Settings</li> </ul>"},{"location":"references/settings/#messaging","title":"Messaging","text":"<ul> <li>Kafka Settings</li> </ul>"},{"location":"references/settings_reference/","title":"Comprehensive Settings Reference","text":"<p>This document provides a complete reference for all Teranode configuration settings, organized by component.</p>"},{"location":"references/settings_reference/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>General Configuration</li> <li>Services</li> </ol>"},{"location":"references/settings_reference/#overview","title":"Overview","text":"<p>All Teranode services accept settings through a centralized Settings object that allows local and remote servers to have their own specific configuration.</p> <p>For general information on how the configuration system works, see the Settings Overview.</p> <p>For deployment-specific information, see:</p> <ul> <li>Developer Setup</li> <li>Docker Configuration</li> <li>Kubernetes Configuration</li> </ul>"},{"location":"references/settings_reference/#general-configuration","title":"General Configuration","text":""},{"location":"references/settings_reference/#configuration-files","title":"Configuration Files","text":"<p>Settings are stored in two files:</p> <ul> <li><code>settings.conf</code>: Global settings with sensible defaults for all environments</li> <li><code>settings_local.conf</code>: Developer-specific and deployment-specific overrides (not in source control)</li> </ul>"},{"location":"references/settings_reference/#configuration-system","title":"Configuration System","text":"<p>The configuration system uses a layered approach with the following priority:</p> <ol> <li><code>SETTING_NAME.context_name</code>: Context-specific override (highest priority)</li> <li><code>SETTING_NAME.base</code>: General override</li> <li><code>SETTING_NAME</code>: Base setting (lowest priority)</li> </ol>"},{"location":"references/settings_reference/#environment-variables","title":"Environment Variables","text":"<p>Most settings can be configured via environment variables using the pattern:</p> <pre><code>TERANODE_&lt;SERVICE&gt;_&lt;SETTING_NAME&gt;\n</code></pre>"},{"location":"references/settings_reference/#services","title":"Services","text":"<p>For detailed service-specific configuration documentation, see:</p> <ul> <li>Alert Service - Bitcoin SV alert system configuration</li> <li>Asset Server - HTTP/WebSocket interface configuration</li> <li>Block Assembly - Block assembly service configuration</li> <li>Blockchain - Blockchain state management configuration</li> <li>Block Persister - Block persistence configuration</li> <li>Block Validation - Block validation configuration</li> <li>Legacy - Legacy Bitcoin protocol compatibility configuration</li> <li>P2P - Peer-to-peer networking configuration</li> <li>Propagation - Transaction propagation configuration</li> <li>RPC - JSON-RPC server configuration</li> <li>Subtree Validation - Subtree validation configuration</li> <li>UTXO Persister - UTXO set persistence configuration</li> <li>Validator - Transaction validation configuration</li> </ul>"},{"location":"references/teranodeDaemonReference/","title":"Daemon Reference","text":"<p>The <code>daemon</code> package provides core functionality for initializing and managing Teranode services. It handles service lifecycle management, configuration, and coordination between different components of the system.</p>"},{"location":"references/teranodeDaemonReference/#overview","title":"Overview","text":"<p>The daemon is responsible for:</p> <ul> <li>Starting and stopping Teranode services</li> <li>Managing service dependencies and initialization order</li> <li>Handling configuration and settings</li> <li>Coordinating between different components</li> <li>Managing stores (UTXO, transaction, block, etc.)</li> <li>Health check endpoints</li> </ul> <p>Besides being used for starting the Teranode services in our main application, the daemon package can also be used in tests to run Teranode instances with different configurations.</p>"},{"location":"references/teranodeDaemonReference/#core-components","title":"Core Components","text":""},{"location":"references/teranodeDaemonReference/#daemon-structure","title":"Daemon Structure","text":"<pre><code>type Daemon struct {\n    doneCh chan struct{}\n}\n</code></pre> <p>The Daemon struct contains a <code>doneCh</code> channel for shutdown coordination.</p>"},{"location":"references/teranodeDaemonReference/#main-functions","title":"Main Functions","text":""},{"location":"references/teranodeDaemonReference/#new","title":"New()","text":"<p>Creates a new Daemon instance:</p> <pre><code>func New() *Daemon {\n    return &amp;Daemon{\n        doneCh: make(chan struct{}),\n    }\n}\n</code></pre>"},{"location":"references/teranodeDaemonReference/#startlogger-args-settings-readych","title":"Start(logger, args, settings, readyCh)","text":"<p>Starts the daemon and initializes services based on configuration:</p> <ul> <li><code>logger</code>: Logger instance for output</li> <li><code>args</code>: Command line arguments for service selection</li> <li><code>settings</code>: Configuration settings</li> <li><code>readyCh</code>: Optional channel to signal when initialization is complete</li> </ul>"},{"location":"references/teranodeDaemonReference/#stop","title":"Stop()","text":"<p>Gracefully shuts down the daemon and all running services.</p>"},{"location":"references/teranodeDaemonReference/#service-management","title":"Service Management","text":""},{"location":"references/teranodeDaemonReference/#service-initialization","title":"Service Initialization","text":"<p>Services are initialized based on command-line flags or configuration settings. Each service can be enabled/disabled using:</p> <ul> <li>Command line: <code>-servicename=1</code> or <code>-servicename=0</code></li> <li>Global disable: <code>-all=0</code> disables all services unless explicitly enabled</li> </ul>"},{"location":"references/teranodeDaemonReference/#available-services","title":"Available Services","text":"<ul> <li>Alert</li> <li>Asset</li> <li>Blockchain</li> <li>BlockAssembly</li> <li>BlockPersister</li> <li>BlockValidation</li> <li>Legacy</li> <li>P2P</li> <li>Propagation</li> <li>RPC</li> <li>SubtreeValidation</li> <li>UTXOPersister</li> <li>Validator</li> </ul>"},{"location":"references/teranodeDaemonReference/#health-checks","title":"Health Checks","text":"<p>The daemon provides HTTP endpoints for health monitoring:</p> <ul> <li><code>/health/readiness</code>: Readiness check</li> <li><code>/health/liveness</code>: Liveness check</li> <li>Port configurable via <code>HealthCheckPort</code> setting</li> </ul>"},{"location":"references/teranodeDaemonReference/#service-initialization-flow","title":"Service Initialization Flow","text":""},{"location":"references/teranodeDaemonReference/#startup-sequence","title":"Startup Sequence","text":"<p>When the daemon starts, it initializes services in a specific order based on dependencies. The sequence generally follows:</p> <ol> <li>Core infrastructure services (Blockchain, UTXO stores)</li> <li>Validation services (Validator, Block Validation)</li> <li>Communication services (Propagation, P2P)</li> <li>Assembly and persistence services</li> </ol>"},{"location":"references/teranodeDaemonReference/#service-dependencies-and-deployment-models","title":"Service Dependencies and Deployment Models","text":"<p>Many services have dependencies on other services. For example, the Propagation service depends on the Validator service.</p>"},{"location":"references/teranodeDaemonReference/#service-dependency-overview","title":"Service Dependency Overview","text":"<p>Note: This diagram shows logical dependencies. The actual implementation may use different communication methods (direct calls, gRPC, or Kafka) depending on configuration.</p>"},{"location":"references/teranodeDaemonReference/#validator-service-deployment","title":"Validator Service Deployment","text":"<p>The Validator can be deployed in two distinct ways, controlled by the <code>validator.useLocalValidator</code> setting:</p> <ol> <li> <p>Local Validator (recommended for production):    <pre><code>// When validator.useLocalValidator=true\nmainValidatorClient, err = validator.New(ctx,\n    logger,\n    tSettings,\n    utxoStore,\n    txMetaKafkaProducerClient,\n    rejectedTxKafkaProducerClient,\n)\n</code></pre></p> </li> <li> <p>Remote Validator Service:    <pre><code>// When validator.useLocalValidator=false\nmainValidatorClient, err = validator.NewClient(ctx, logger, tSettings)\n</code></pre></p> </li> </ol> <p>The Propagation service and other services can use either deployment model:</p> <pre><code>validatorClient, err := GetValidatorClient(ctx, logger, tSettings)\nif err != nil {\n    return err\n}\n\n// Initialize propagation with validator dependency\nsm.AddService(\"PropagationServer\", propagation.New(\n    logger.New(\"prop\"),\n    tSettings,\n    txStore,\n    validatorClient,\n    blockchainClient,\n    validatorKafkaProducerClient,\n));\n</code></pre>"},{"location":"references/teranodeDaemonReference/#service-communication-patterns","title":"Service Communication Patterns","text":"<p>Teranode services communicate using several methods:</p> <ol> <li>Direct method calls: When using local services (e.g., local validator)</li> <li>gRPC: For remote service communication with bi-directional streaming support</li> <li>Kafka: For asynchronous messaging and event-driven communication</li> </ol>"},{"location":"references/teranodeDaemonReference/#key-communication-flows","title":"Key Communication Flows:","text":"<ul> <li>Tx Validator to Subtree Validation: Uses Kafka for transaction metadata</li> <li>P2P to Subtree Validation: Uses Kafka for notifications (not gRPC)</li> <li>Block Validation to Validator: For validating transactions in blocks</li> <li>Propagation to Validator: Uses either direct calls (local validator) or gRPC (remote validator)</li> </ul> <p>Note: The SubtreeFound gRPC method was removed from the project. All references to this method were removed from documentation. The P2P service now only notifies the subtree validation service via Kafka, not through gRPC.</p>"},{"location":"references/teranodeDaemonReference/#singleton-patterns","title":"Singleton Patterns","text":"<p>Many services and stores use singleton patterns to ensure only one instance exists. Key examples include:</p> <pre><code>// Validator client singleton\nif mainValidatorClient != nil {\n    return mainValidatorClient, nil\n}\n\n// UTXO store singleton\nif utxoStoreInstance != nil {\n    return utxoStoreInstance, nil\n}\n</code></pre> <p>This pattern ensures consistent state across all services using these components.</p>"},{"location":"references/teranodeDaemonReference/#store-management","title":"Store Management","text":"<p>The daemon manages several types of stores:</p>"},{"location":"references/teranodeDaemonReference/#transaction-store","title":"Transaction Store","text":"<pre><code>func GetTxStore(logger ulogger.Logger) (blob.Store, error)\n</code></pre>"},{"location":"references/teranodeDaemonReference/#utxo-store","title":"UTXO Store","text":"<pre><code>func GetUtxoStore(ctx context.Context, logger ulogger.Logger, tSettings *settings.Settings) (utxostore.Store, error)\n</code></pre>"},{"location":"references/teranodeDaemonReference/#block-store","title":"Block Store","text":"<pre><code>func GetBlockStore(logger ulogger.Logger) (blob.Store, error)\n</code></pre>"},{"location":"references/teranodeDaemonReference/#subtree-store","title":"Subtree Store","text":"<pre><code>func GetSubtreeStore(logger ulogger.Logger, tSettings *settings.Settings) (blob.Store, error)\n</code></pre> <p>All stores follow a singleton pattern, ensuring only one instance exists per store type.</p>"},{"location":"references/teranodeDaemonReference/#configuration-options","title":"Configuration Options","text":"<p>The daemon uses a variety of configuration options to control service behavior and deployment models. Here are the key settings that affect daemon initialization and service configuration:</p>"},{"location":"references/teranodeDaemonReference/#service-deployment-settings","title":"Service Deployment Settings","text":"Setting Description Default Impact <code>validator.useLocalValidator</code> Controls whether the validator runs as a local component (true) or as a separate service (false) <code>true</code> Significant performance impact; local validator is recommended for production <code>grpc_resolver</code> Determines the gRPC resolver to use for client connections - Supports Kubernetes (\"k8s\" or \"kubernetes\") and other resolvers <code>healthCheckPort</code> Port for health check endpoints <code>8080</code> Exposed on <code>/health/readiness</code> and <code>/health/liveness</code>"},{"location":"references/teranodeDaemonReference/#store-configuration","title":"Store Configuration","text":"Setting Description Default Impact <code>utxoStore.UtxoStore</code> URL to UTXO store - Can be file-based, SQLite, or other storage backends <code>blockChain.StoreURL</code> URL to blockchain store - Affects block storage location <code>subtreeValidation.SubtreeStore</code> URL to subtree store - Controls where subtrees are stored"},{"location":"references/teranodeDaemonReference/#communication-settings","title":"Communication Settings","text":"Setting Description Default Impact <code>kafka_validatortxsConfig</code> Kafka configuration for validator transactions - Affects how transaction data is shared between services <code>propagation_grpcListenAddress</code> gRPC listen address for propagation service - Controls propagation service networking <code>validator_kafkaWorkers</code> Number of Kafka workers for validator service <code>100</code> Affects throughput of Kafka message processing <p>Consult the individual service documentation for service-specific configuration options. The settings listed here are particularly important for daemon initialization and service communication.</p>"},{"location":"references/teranodeDaemonReference/#testing-support","title":"Testing Support","text":"<p>The daemon package is designed to support comprehensive testing scenarios:</p> <ul> <li>Can be initialized with different store implementations (SQLite, in-memory, etc.)</li> <li>Supports running complete Teranode instances in tests</li> <li>Allows step-by-step debugging of services</li> <li>Facilitates testing complex scenarios like double-spend detection</li> </ul> <p>Example test initialization:</p> <pre><code>func NewDoubleSpendTester(t *testing.T) *DoubleSpendTester {\n    ctx, cancel := context.WithCancel(context.Background())\n\n    logger := ulogger.NewErrorTestLogger(t, cancel)\n\n    // Delete the sqlite db at the beginning of the test\n    _ = os.RemoveAll(\"data\")\n\n    persistentStore, err := url.Parse(\"sqlite:///test\")\n    require.NoError(t, err)\n\n    memoryStore, err := url.Parse(\"memory:///\")\n    require.NoError(t, err)\n\n    if !isKafkaRunning() {\n        kafkaContainer, err := testkafka.RunTestContainer(ctx)\n        require.NoError(t, err)\n\n        t.Cleanup(func() {\n            _ = kafkaContainer.CleanUp()\n        })\n\n        gocore.Config().Set(\"KAFKA_PORT\", strconv.Itoa(kafkaContainer.KafkaPort))\n    }\n\n    tSettings := settings.NewSettings() // This reads gocore.Config and applies sensible defaults\n\n    // Override with test settings...\n    tSettings.SubtreeValidation.SubtreeStore = memoryStore\n    tSettings.BlockChain.StoreURL = persistentStore\n    tSettings.UtxoStore.UtxoStore = persistentStore\n    tSettings.ChainCfgParams = &amp;chaincfg.RegressionNetParams\n    tSettings.Asset.CentrifugeDisable = true\n\n    readyCh := make(chan struct{})\n\n    d := daemon.New()\n\n    go d.Start(logger, []string{\n        \"-all=0\",\n        \"-blockchain=1\",\n        \"-subtreevalidation=1\",\n        \"-blockvalidation=1\",\n        \"-blockassembly=1\",\n        \"-asset=1\",\n        \"-propagation=1\",\n    }, tSettings, readyCh)\n\n    &lt;-readyCh\n\n    bcClient, err := blockchain.NewClient(ctx, logger, tSettings, \"test\")\n    require.NoError(t, err)\n\n    baClient, err := blockassembly.NewClient(ctx, logger, tSettings)\n    require.NoError(t, err)\n\n    propagationClient, err := propagation.NewClient(ctx, logger, tSettings)\n    require.NoError(t, err)\n\n    blockValidationClient, err := blockvalidation.NewClient(ctx, logger, tSettings, \"test\")\n    require.NoError(t, err)\n\n    w, err := wif.DecodeWIF(tSettings.BlockAssembly.MinerWalletPrivateKeys[0])\n    require.NoError(t, err)\n\n    privKey := w.PrivKey\n\n    subtreeStore, err := daemon.GetSubtreeStore(logger, tSettings)\n    require.NoError(t, err)\n\n    utxoStore, err := daemon.GetUtxoStore(ctx, logger, tSettings)\n    require.NoError(t, err)\n\n    return &amp;DoubleSpendTester{\n        ctx:                   ctx,\n        logger:                logger,\n        d:                     d,\n        blockchainClient:      bcClient,\n        blockAssemblyClient:   baClient,\n        propagationClient:     propagationClient,\n        blockValidationClient: blockValidationClient,\n        privKey:               privKey,\n        subtreeStore:          subtreeStore,\n        utxoStore:             utxoStore,\n    }\n}\n</code></pre> <p>Additionally, using the following function: <pre><code>tSettings := settings.NewSettings() // This reads gocore.Config and applies sensible defaults\n</code></pre> will initialise settings with generic defaults, ideal for tests.</p>"},{"location":"references/teranodeDaemonReference/#configuration","title":"Configuration","text":"<p>The daemon uses a combination of (in this priority order):</p> <ol> <li>Command line arguments</li> <li>Environment variables</li> <li>Configuration files</li> <li>Default settings</li> </ol>"},{"location":"references/testingTechnicalReference/","title":"TERANODE Testing Framework Technical Reference","text":""},{"location":"references/testingTechnicalReference/#framework-components","title":"Framework Components","text":""},{"location":"references/testingTechnicalReference/#teranodetestenv-structure","title":"TeranodeTestEnv Structure","text":"<pre><code>type TeranodeTestEnv struct {\n    TConfig              tconfig.TConfig       // Test configuration\n    Context              context.Context      // Test context\n    Compose              tc.ComposeStack      // Docker compose stack\n    ComposeSharedStorage tstore.TStoreClient  // Shared storage client\n    Nodes                []TeranodeTestClient // Array of test nodes\n    LegacyNodes          []SVNodeTestClient   // Array of legacy nodes\n    Logger               ulogger.Logger       // Framework logger\n    Cancel               context.CancelFunc   // Context cancellation\n    Daemon               daemon.Daemon        // Daemon instance\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#teranodetestclient-structure","title":"TeranodeTestClient Structure","text":"<pre><code>type TeranodeTestClient struct {\n    Name                string                  // Node identifier\n    SettingsContext     string                  // Configuration context\n    BlockchainClient    bc.ClientI              // Blockchain service client\n    BlockassemblyClient ba.Client               // Block assembly client\n    DistributorClient   distributor.Distributor // Distribution service client\n    ClientBlockstore    *bhttp.HTTPStore        // HTTP client for block storage\n    ClientSubtreestore  *bhttp.HTTPStore        // HTTP client for subtree storage\n    UtxoStore           *utxostore.Store        // UTXO storage\n    CoinbaseClient      *stubs.CoinbaseClient   // Coinbase service client stub\n    AssetURL            string                  // Asset service URL\n    RPCURL              string                  // RPC service URL\n    IPAddress           string                  // Node IP address\n    SVNodeIPAddress     string                  // Legacy node IP address\n    Settings            *settings.Settings      // Node settings\n    BlockChainDB        bcs.Store               // Blockchain storage\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#svnodetestclient-structure","title":"SVNodeTestClient Structure","text":"<pre><code>type SVNodeTestClient struct {\n    Name      string // Node identifier\n    IPAddress string // Node IP address\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#framework-setup-and-usage","title":"Framework Setup and Usage","text":""},{"location":"references/testingTechnicalReference/#creating-a-test-environment","title":"Creating a Test Environment","text":"<p>The test environment is created using the <code>NewTeraNodeTestEnv</code> function, which accepts a test configuration:</p> <pre><code>func NewTeraNodeTestEnv(c tconfig.TConfig) *TeranodeTestEnv {\n    logger := ulogger.New(\"e2eTestRun\", ulogger.WithLevel(c.Suite.LogLevel))\n    ctx, cancel := context.WithCancel(context.Background())\n\n    return &amp;TeranodeTestEnv{\n        TConfig: c,\n        Context: ctx,\n        Logger:  logger,\n        Cancel:  cancel,\n    }\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#setting-up-docker-nodes","title":"Setting Up Docker Nodes","text":"<p>The <code>SetupDockerNodes</code> method initializes the Docker Compose environment with the provided settings:</p> <pre><code>func (t *TeranodeTestEnv) SetupDockerNodes() error {\n    // Set up Docker Compose environment with provided settings\n    // Create test directory for test-specific data\n    // Configure environment settings including TEST_ID\n    // Set up shared storage client for local docker-compose\n    // Initialize teranode and legacy node configurations\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#initializing-node-clients","title":"Initializing Node Clients","text":"<p>The <code>InitializeTeranodeTestClients</code> method sets up all the necessary client connections for the nodes:</p> <pre><code>func (t *TeranodeTestEnv) InitializeTeranodeTestClients() error {\n    // Set up blob stores for block and subtree data\n    // Retrieve IP addresses for containers\n    // Set up RPC and Asset service URLs\n    // Initialize blockchain, blockassembly and distributor clients\n    // Configure UTXO stores and other required connections\n    // Connect to necessary services for testing\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#using-the-framework-in-tests","title":"Using the Framework in Tests","text":"<p>A typical test using this framework would follow this pattern:</p> <pre><code>func TestSomeFunctionality(t *testing.T) {\n    // Create test configuration\n    config := tconfig.NewTConfig(\"testID\")\n\n    // Create test environment\n    env := utils.NewTeraNodeTestEnv(config)\n\n    // Set up Docker nodes\n    err := env.SetupDockerNodes()\n    require.NoError(t, err)\n\n    // Initialize node clients\n    err = env.InitializeTeranodeTestClients()\n    require.NoError(t, err)\n\n    // Run tests using the environment\n    // Access node clients via env.Nodes[i]\n\n    // Clean up when done\n    env.Cancel()\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#test-categories-and-tags","title":"Test Categories and Tags","text":""},{"location":"references/testingTechnicalReference/#core-test-categories","title":"Core Test Categories","text":"Category Tag Description TNA tnatests Node responsibilities and network communication TNB tnbtests Transaction validation and processing TND tndtests Block propagation through network TNF tnftests Longest chain management TNJ tnjtests Consensus rules compliance TEC tectests Error handling and recovery scenarios"},{"location":"references/testingTechnicalReference/#service-port-mappings","title":"Service Port Mappings","text":"Service Internal Port External Port Pattern Health Check 8090/tcp 1009X Coinbase GRPC 8093/tcp 1009X Blockchain GRPC 8087/tcp 1208X Block Assembly GRPC 8085/tcp 1408X Propagation GRPC 8084/tcp 1608X Asset HTTP 8091/tcp Variable RPC 8092/tcp Variable"},{"location":"references/testingTechnicalReference/#configuration-reference","title":"Configuration Reference","text":""},{"location":"references/testingTechnicalReference/#environment-variables","title":"Environment Variables","text":"<pre><code># Core Settings\nSETTINGS_CONTEXT      # Configuration context for the test run\nLOG_LEVEL            # Logging verbosity (DEBUG, INFO, WARN, ERROR)\nGITHUB_ACTIONS       # CI environment indicator\n\n# Node Settings\nSETTINGS_CONTEXT_1   # Configuration for node 1\nSETTINGS_CONTEXT_2   # Configuration for node 2\nSETTINGS_CONTEXT_3   # Configuration for node 3\n</code></pre>"},{"location":"references/testingTechnicalReference/#docker-compose-configuration","title":"Docker Compose Configuration","text":"<pre><code># Base Configuration (docker-compose.e2etest.yml)\nservices:\n  teranode1:\n    image: teranode\n    environment:\n\n      - SETTINGS_CONTEXT=${SETTINGS_CONTEXT_1}\n    ports:\n\n      - \"10090:8090\"  # Health check port\n      - \"10093:8093\"  # Coinbase service\n      - \"10087:8087\"  # Blockchain service\n      - \"10085:8085\"  # Block assembly\n      - \"10091:8091\"  # Asset service\n      - \"10092:8092\"  # RPC service\n\n  teranode2:\n    # Similar configuration with ports 12090, 12093, 12087, etc.\n\n  teranode3:\n    # Similar configuration with ports 14090, 14093, 14087, etc.\n</code></pre>"},{"location":"references/testingTechnicalReference/#test-suite-configuration","title":"Test Suite Configuration","text":"<pre><code>// Default Compose Files\nfunc (suite *BitcoinTestSuite) DefaultComposeFiles() []string {\n    return []string{\"../../docker-compose.e2etest.yml\"}\n}\n\n// Default Settings Map\nfunc (suite *BitcoinTestSuite) DefaultSettingsMap() map[string]string {\n    return map[string]string{\n        \"SETTINGS_CONTEXT_1\": \"docker.ci.teranode1.tc1\",\n        \"SETTINGS_CONTEXT_2\": \"docker.ci.teranode2.tc1\",\n        \"SETTINGS_CONTEXT_3\": \"docker.ci.teranode3.tc1\",\n    }\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#utility-methods","title":"Utility Methods","text":""},{"location":"references/testingTechnicalReference/#node-management-methods","title":"Node Management Methods","text":"<pre><code>// StartNode starts a specific TeraNode by name\nfunc (t *TeranodeTestEnv) StartNode(nodeName string) error {\n    // Start a specific node in the Docker Compose environment\n    // Wait for the node to be healthy\n}\n\n// StopNode stops a specific TeraNode by name\nfunc (t *TeranodeTestEnv) StopNode(nodeName string) error {\n    // Stop a specific node in the Docker Compose environment\n}\n\n// RestartDockerNodes restarts the Docker Compose services\nfunc (t *TeranodeTestEnv) RestartDockerNodes(envSettings map[string]string) error {\n    // Stop the current Docker Compose environment\n    // Re-initialize with potentially new settings\n    // Restart the services with the updated configuration\n}\n\n// StopDockerNodes stops the Docker Compose services and removes volumes\nfunc (t *TeranodeTestEnv) StopDockerNodes() error {\n    // Stop all services and clean up resources\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#client-setup-methods","title":"Client Setup Methods","text":"<pre><code>// Sets up HTTP stores for blocks and subtrees\nfunc (t *TeranodeTestEnv) setupBlobStores() error {\n    // Create HTTP clients for blob stores\n    // Configure block and subtree storage access\n}\n\n// Sets up blockchain client for a node\nfunc (t *TeranodeTestEnv) setupBlockchainClient(node *TeranodeTestClient) error {\n    // Initialize gRPC connection to blockchain service\n    // Create and configure blockchain client\n}\n\n// Sets up block assembly client for a node\nfunc (t *TeranodeTestEnv) setupBlockassemblyClient(node *TeranodeTestClient) error {\n    // Initialize gRPC connection to block assembly service\n    // Create and configure block assembly client\n}\n\n// Sets up distributor client for a node\nfunc (t *TeranodeTestEnv) setupDistributorClient(node *TeranodeTestClient) error {\n    // Initialize gRPC connection to distributor service\n    // Create and configure distributor client\n}\n\n// GetMappedPort retrieves the mapped port for a service running in Docker Compose\nfunc (t *TeranodeTestEnv) GetMappedPort(nodeName string, port nat.Port) (nat.Port, error) {\n    // Find the exposed port mapping for a container service\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#transaction-utilities","title":"Transaction Utilities","text":"<pre><code>// CreateAndSendTx creates and sends a transaction\nfunc (n *TeranodeTestClient) CreateAndSendTx(t *testing.T, ctx context.Context, parentTx *bt.Tx) (*bt.Tx, error) {\n    // Create a new transaction spending outputs from the parent transaction\n    // Sign the transaction with test keys\n    // Submit the transaction to the node\n    // Return the created transaction\n}\n\n// CreateAndSendTxs creates and sends a chain of transactions\nfunc (n *TeranodeTestClient) CreateAndSendTxs(t *testing.T, ctx context.Context, parentTx *bt.Tx, count int) ([]*bt.Tx, []*chainhash.Hash, error) {\n    // Create a series of chained transactions\n    // Each transaction spends outputs from the previous one\n    // Submit all transactions to the node\n    // Return the created transactions and their hashes\n}\n\n// CreateAndSendTxsConcurrently creates and sends transactions concurrently\nfunc (n *TeranodeTestClient) CreateAndSendTxsConcurrently(t *testing.T, ctx context.Context, parentTx *bt.Tx, count int) ([]*bt.Tx, []*chainhash.Hash, error) {\n    // Create multiple transactions concurrently\n    // Use goroutines to parallelize transaction creation and submission\n    // Wait for all transactions to complete\n    // Return the created transactions and their hashes\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#api-reference","title":"API Reference","text":""},{"location":"references/testingTechnicalReference/#framework-methods","title":"Framework Methods","text":"<pre><code>// Core Framework Methods\nfunc NewTeraNodeTestEnv(c tconfig.TConfig) *TeranodeTestEnv\nfunc (t *TeranodeTestEnv) SetupDockerNodes() error\nfunc (t *TeranodeTestEnv) InitializeTeranodeTestClients() error\nfunc (t *TeranodeTestEnv) StopDockerNodes() error\nfunc (t *TeranodeTestEnv) RestartDockerNodes(envSettings map[string]string) error\nfunc (t *TeranodeTestEnv) StartNode(nodeName string) error\nfunc (t *TeranodeTestEnv) StopNode(nodeName string) error\nfunc (t *TeranodeTestEnv) GetMappedPort(nodeName string, port nat.Port) (nat.Port, error)\nfunc (t *TeranodeTestEnv) GetContainerIPAddress(node *TeranodeTestClient) error\nfunc (t *TeranodeTestEnv) GetLegacyContainerIPAddress(node *SVNodeTestClient) error\n\n// Transaction Utility Methods\nfunc (n *TeranodeTestClient) CreateAndSendTx(t *testing.T, ctx context.Context, parentTx *bt.Tx) (*bt.Tx, error)\nfunc (n *TeranodeTestClient) CreateAndSendTxs(t *testing.T, ctx context.Context, parentTx *bt.Tx, count int) ([]*bt.Tx, []*chainhash.Hash, error)\nfunc (n *TeranodeTestClient) CreateAndSendTxsConcurrently(t *testing.T, ctx context.Context, parentTx *bt.Tx, count int) ([]*bt.Tx, []*chainhash.Hash, error)\n</code></pre>"},{"location":"references/testingTechnicalReference/#client-interfaces","title":"Client Interfaces","text":""},{"location":"references/testingTechnicalReference/#blockchain-client","title":"Blockchain Client","text":"<pre><code>type ClientI interface {\n    Health(ctx context.Context) (*HealthResponse, error)\n    Run(ctx context.Context) error\n    SubmitTransaction(ctx context.Context, tx *Transaction) error\n    VerifyTransaction(ctx context.Context, txid string) (bool, error)\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#block-assembly-client","title":"Block Assembly Client","text":"<pre><code>type Client interface {\n    BlockAssemblyAPIClient() blockassembly_api.BlockAssemblyAPIClient\n    Health(ctx context.Context) (*HealthResponse, error)\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#coinbase-client","title":"Coinbase Client","text":"<pre><code>type Client interface {\n    Health(ctx context.Context) (*HealthResponse, error)\n    GetCoinbaseTransaction(ctx context.Context) (*Transaction, error)\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#test-data-structures","title":"Test Data Structures","text":""},{"location":"references/testingTechnicalReference/#health-response","title":"Health Response","text":"<pre><code>type HealthResponse struct {\n    Ok      bool   `json:\"ok\"`\n    Message string `json:\"message,omitempty\"`\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#transaction-structure","title":"Transaction Structure","text":"<pre><code>type Transaction struct {\n    ID        string    `json:\"id\"`\n    Data      []byte    `json:\"data\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n</code></pre>"},{"location":"references/testingTechnicalReference/#error-types","title":"Error Types","text":"<pre><code>// Common error categories\nerrors.NewConfigurationError(format string, args ...interface{}) error\nerrors.NewServiceError(format string, args ...interface{}) error\nerrors.NewValidationError(format string, args ...interface{}) error\n</code></pre>"},{"location":"references/testingTechnicalReference/#testing-constants","title":"Testing Constants","text":""},{"location":"references/testingTechnicalReference/#node-urls","title":"Node URLs","text":"<pre><code>const (\n    NodeURL1 = \"http://localhost:10090\" // Node 1 base URL\n    NodeURL2 = \"http://localhost:12090\" // Node 2 base URL\n    NodeURL3 = \"http://localhost:14090\" // Node 3 base URL\n)\n</code></pre>"},{"location":"references/testingTechnicalReference/#timeouts-and-delays","title":"Timeouts and Delays","text":"<pre><code>const (\n    DefaultSetupTimeout    = 30 * time.Second\n    NodeStartupDelay      = 10 * time.Second\n    PropagationDelay      = 2 * time.Second\n)\n</code></pre>"},{"location":"references/testingTechnicalReference/#directory-structure","title":"Directory Structure","text":"<pre><code>test/\n\u251c\u2500\u2500 aerospike/           # Aerospike database tests\n\u251c\u2500\u2500 config/             # Test configuration files\n\u251c\u2500\u2500 consensus/          # Consensus mechanism tests\n\u251c\u2500\u2500 e2e/               # End-to-end integration tests\n\u251c\u2500\u2500 fsm/               # Finite State Machine tests\n\u251c\u2500\u2500 longtest/          # Long-running performance tests\n\u251c\u2500\u2500 nodeHelpers/       # Node helper utilities\n\u251c\u2500\u2500 postgres/          # PostgreSQL database tests\n\u251c\u2500\u2500 rpc/              # RPC service tests\n\u251c\u2500\u2500 scripts/          # Test automation scripts\n\u251c\u2500\u2500 sequentialtest/   # Sequential test execution\n\u251c\u2500\u2500 tec/              # Error case tests\n\u251c\u2500\u2500 testcontainers/   # Docker container test utilities\n\u251c\u2500\u2500 tna/              # Node responsibility tests\n\u251c\u2500\u2500 tnb/              # Transaction validation tests\n\u251c\u2500\u2500 tnd/              # Block propagation tests\n\u251c\u2500\u2500 tnf/              # Longest chain management tests\n\u251c\u2500\u2500 tnj/              # Consensus rules compliance tests\n\u251c\u2500\u2500 txregistry/       # Transaction registry tests\n\u2514\u2500\u2500 utils/            # Utility functions and test framework\n</code></pre>"},{"location":"references/testingTechnicalReference/#other-resources","title":"Other Resources","text":"<ul> <li>QA Guide &amp; Instructions for Functional Requirement Tests</li> <li>Understanding The Testing Framework</li> <li>Automated Testing How-To</li> </ul>"},{"location":"references/thirdPartySoftwareRequirements/","title":"Teranode Third Party Software Requirements","text":""},{"location":"references/thirdPartySoftwareRequirements/#index","title":"Index","text":"<ul> <li>Introduction</li> <li>Apache Kafka<ul> <li>What is Kafka?</li> <li>Kafka in BSV Teranode</li> </ul> </li> <li>PostgreSQL<ul> <li>What is PostgreSQL?</li> <li>PostgreSQL in Teranode</li> </ul> </li> <li>Aerospike<ul> <li>What is Aerospike?</li> <li>How Aerospike is Used in Teranode</li> </ul> </li> <li>Shared Storage</li> <li>Grafana and Prometheus<ul> <li>What is Grafana?</li> <li>What is Prometheus?</li> <li>Grafana and Prometheus in Teranode</li> </ul> </li> </ul>"},{"location":"references/thirdPartySoftwareRequirements/#introduction","title":"Introduction","text":"<p>Teranode relies on a number of third-party software dependencies, some of which can be sourced from different vendors.</p> <p>BSV provides both a <code>docker compose</code> that initialises all dependencies within a single server node, and a <code>Kubernetes operator</code> that provides a production-live multi-node setup.</p> <p>This section will outline the various vendors in use in Teranode.</p>"},{"location":"references/thirdPartySoftwareRequirements/#apache-kafka","title":"Apache Kafka","text":""},{"location":"references/thirdPartySoftwareRequirements/#what-is-kafka","title":"What is Kafka?","text":"<p>Apache Kafka is an open-source platform for handling real-time data feeds, designed to provide high-throughput, low-latency data pipelines and stream processing. It supports publish-subscribe messaging, fault-tolerant storage, and real-time stream processing.</p> <p>To know more, please refer to the Kafka official site: https://kafka.apache.org/.</p>"},{"location":"references/thirdPartySoftwareRequirements/#kafka-in-bsv-teranode","title":"Kafka in BSV Teranode","text":"<p>In BSV Teranode, Kafka is used to manage and process large volumes of transaction data efficiently. Kafka serves as a data pipeline, ingesting high volumes of transaction data from multiple sources in real-time. Kafka\u2019s distributed architecture ensures data consistency and fault tolerance across the network.</p>"},{"location":"references/thirdPartySoftwareRequirements/#postgresql","title":"PostgreSQL","text":""},{"location":"references/thirdPartySoftwareRequirements/#what-is-postgresql","title":"What is PostgreSQL?","text":"<p>Postgres, short for PostgreSQL, is an advanced, open-source relational database management system (RDBMS). It is known for its robustness, extensibility, and standards compliance. Postgres supports SQL for querying and managing data, and it also provides features like:</p> <ul> <li>ACID Compliance: Ensures reliable transactions.</li> <li>Complex Queries: Supports joins, subqueries, and complex operations.</li> <li>Extensibility: Allows custom functions, data types, operators, and more.</li> <li>Concurrency: Handles multiple transactions simultaneously with high efficiency.</li> <li>Data Integrity: Supports constraints, triggers, and foreign keys to maintain data accuracy.</li> </ul> <p>To know more, please refer to the PostgreSQL official site: https://www.postgresql.org/</p>"},{"location":"references/thirdPartySoftwareRequirements/#postgresql-in-teranode","title":"PostgreSQL in Teranode","text":"<p>In Teranode, PostgreSQL is used for Blockchain Storage. Postgres stores the blockchain data, providing a reliable and efficient database solution for handling large volumes of block data.</p>"},{"location":"references/thirdPartySoftwareRequirements/#aerospike","title":"Aerospike","text":""},{"location":"references/thirdPartySoftwareRequirements/#what-is-aerospike","title":"What is Aerospike?","text":"<p>Aerospike is an open-source, high-performance, NoSQL database designed for real-time analytics and high-speed transaction processing. It is known for its ability to handle large-scale data with low latency and high reliability. Key features include:</p> <ul> <li>High Performance: Optimized for fast read and write operations.</li> <li>Scalability: Easily scales horizontally to handle massive data loads.</li> <li>Consistency and Reliability: Ensures data consistency and supports ACID transactions.</li> <li>Hybrid Memory Architecture: Utilizes both RAM and flash storage for efficient data management.</li> </ul>"},{"location":"references/thirdPartySoftwareRequirements/#how-aerospike-is-used-in-teranode","title":"How Aerospike is Used in Teranode","text":"<p>In the context of Teranode, Aerospike is utilized for UTXO (Unspent Transaction Output) storage, which requires of a robust high performance and reliable solution.</p>"},{"location":"references/thirdPartySoftwareRequirements/#shared-storage","title":"Shared Storage","text":"<p>Teranode requires a robust and scalable shared storage solution to efficiently manage its critical Subtree and Transaction data. This shared storage is accessed and modified by various microservices within the Teranode ecosystem.</p> <p>In a full production deployment, Teranode is designed to use Lustre (or a similar high-performance shared filesystem) for optimal performance and scalability. Lustre is a clustered, high-availability, low-latency shared filesystem provided by AWS FSx for Lustre service.</p> <p>Benefits of Using Lustre with Teranode:</p> <ul> <li>High Availability: Ensures continuous access to shared data.</li> <li>Low Latency: Provides sub-millisecond latency for consistent filesystem state.</li> <li>Automatic Data Archiving: Facilitates seamless data transfer to S3 for long-term storage.</li> <li>Scalability: Supports scalable data sharing between various services.</li> </ul> <p>Note - if using Docker Compose, the shared Docker storage, which is automatically managed by <code>docker compose</code>, is used instead. This approach provides a more accessible testing environment while still allowing for essential functionality and performance evaluation.</p>"},{"location":"references/thirdPartySoftwareRequirements/#grafana-and-prometheus","title":"Grafana and Prometheus","text":""},{"location":"references/thirdPartySoftwareRequirements/#what-is-grafana","title":"What is Grafana?","text":"<p>Grafana is an open-source platform used for monitoring, visualization, and analysis of data. It allows users to create and share interactive dashboards to visualize real-time data from various sources.</p>"},{"location":"references/thirdPartySoftwareRequirements/#what-is-prometheus","title":"What is Prometheus?","text":"<p>Prometheus is an open-source systems monitoring and alerting toolkit. It is particularly well-suited for monitoring dynamic and cloud-native environments.</p> <p>Grafana and Prometheus are used together to provide a comprehensive monitoring and visualization solution. Prometheus scrapes metrics from configured targets at regular intervals, and stores them in a its time series database. Grafana then connects to Prometheus as a data source.</p> <p>Users can create dashboards in Grafana to visualize the metrics collected by Prometheus. Additionally, both Prometheus and Grafana support alerting.</p>"},{"location":"references/thirdPartySoftwareRequirements/#grafana-and-prometheus-in-teranode","title":"Grafana and Prometheus in Teranode","text":"<p>In the context of Teranode, Grafana and Prometheus are used to provide comprehensive monitoring and visualization of the blockchain node\u2019s performance, health, and various metrics.</p>"},{"location":"references/protobuf_docs/alertProto/","title":"GRPC Documentation - AlertAPI","text":""},{"location":"references/protobuf_docs/alertProto/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p>alert_api.proto</p> <ul> <li> <p>HealthResponse</p> </li> <li> <p>AlertAPI</p> </li> </ul> </li> <li> <p>Scalar Value Types</p> </li> </ul> <p></p> <p>Top</p>"},{"location":"references/protobuf_docs/alertProto/#alert_apiproto","title":"alert_api.proto","text":"<p>Package alert_api defines the gRPC service interface for the Bitcoin SV Alert System.</p> <p></p>"},{"location":"references/protobuf_docs/alertProto/#healthresponse","title":"HealthResponse","text":"<p>swagger:model HealthResponse</p> <p>Represents the health check response from the Alert System service.</p> Field Type Label Description ok bool Indicates whether the service is healthy (true) or unhealthy (false) details string Provides additional information about the health status timestamp google.protobuf.Timestamp Indicates when the health check was performed <p></p>"},{"location":"references/protobuf_docs/alertProto/#alertapi","title":"AlertAPI","text":"<p>Service provides methods for interacting with the Alert System.</p> Method Name Request Type Response Type Description HealthGRPC .google.protobuf.Empty HealthResponse Checks the health status of the Alert System service. It accepts an empty request and returns a HealthResponse containing the current health status of the service."},{"location":"references/protobuf_docs/alertProto/#scalar-value-types","title":"Scalar Value Types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby  double double double float float64 double float Float  float float float float float32 float float Float  int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required)  int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum  uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required)  uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required)  sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required)  sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum  fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required)  fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum  sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required)  sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum  bool bool boolean boolean bool bool boolean TrueClass/FalseClass  string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8)  bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)"},{"location":"references/protobuf_docs/blockassemblyProto/","title":"GRPC Documentation - BlockAssemblyAPI","text":""},{"location":"references/protobuf_docs/blockassemblyProto/#table-of-contents","title":"Table of Contents","text":"<ul> <li>blockassembly_api.proto<ul> <li>AddTxBatchRequest</li> <li>AddTxBatchResponse</li> <li>AddTxRequest</li> <li>AddTxResponse</li> <li>EmptyMessage</li> <li>GenerateBlocksRequest</li> <li>GetCurrentDifficultyResponse</li> <li>GetMiningCandidateRequest</li> <li>HealthResponse</li> <li>NewChaintipAndHeightRequest</li> <li>RemoveTxRequest</li> <li>StateMessage</li> <li>SubmitMiningSolutionRequest</li> <li>OKResponse</li> <li>GetBlockAssemblyBlockCandidateResponse</li> <li>GetBlockAssemblyTxsResponse</li> <li>BlockAssemblyAPI</li> <li>Scalar Value Types</li> </ul> </li> </ul> <p>Top</p>"},{"location":"references/protobuf_docs/blockassemblyProto/#blockassembly_apiproto","title":"blockassembly_api.proto","text":"<p>Package blockassembly_api defines the gRPC service interface for block assembly operations.</p> <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#addtxbatchrequest","title":"AddTxBatchRequest","text":"<p>Request for adding a batch of transactions to the mining candidate block.</p> Field Type Label Description txRequests AddTxRequest repeated a batch of transaction requests <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#addtxbatchresponse","title":"AddTxBatchResponse","text":"<p>Response indicating whether the addition of a batch of transactions was successful.</p> Field Type Label Description ok bool true if the transactions were successfully added <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#addtxrequest","title":"AddTxRequest","text":"<p>Request for adding a new transaction to the mining candidate block.</p> Field Type Label Description txid bytes the transaction id fee uint64 the transaction fee in satoshis size uint64 the size of the transaction in bytes locktime uint32 the earliest time a transaction can be mined into a block utxos bytes repeated the UTXOs consumed by this transaction txInpoints bytes a serialized list of input outpoints for this transaction <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#addtxresponse","title":"AddTxResponse","text":"<p>Response indicating whether the addition of a transaction was successful.</p> Field Type Label Description ok bool true if the transaction was successfully added <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#emptymessage","title":"EmptyMessage","text":"<p>An empty message used as a placeholder or a request with no data.</p> <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#generateblocksrequest","title":"GenerateBlocksRequest","text":"<p>Request for generating a block.</p> Field Type Label Description count int32 the number of blocks to generate address string optional the address to send the generated blocks to maxTries int32 optional the maximum number of attempts to generate a block <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#getcurrentdifficultyresponse","title":"GetCurrentDifficultyResponse","text":"<p>Response containing the current difficulty of the blockchain.</p> Field Type Label Description difficulty double the current difficulty of the blockchain <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#getminingcandidaterequest","title":"GetMiningCandidateRequest","text":"<p>Request for retrieving a mining candidate block template.</p> Field Type Label Description includeSubtrees bool whether to include the subtrees in the mining candidate <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#healthresponse","title":"HealthResponse","text":"<p>Contains the health status of the service. Includes an 'ok' flag indicating health status, details providing more context, and a timestamp.</p> Field Type Label Description ok bool true if the service is healthy details string optional, human-readable details timestamp google.protobuf.Timestamp unix timestamp <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#newchaintipandheightrequest","title":"NewChaintipAndHeightRequest","text":"<p>Request for adding a new chaintip and height information.</p> Field Type Label Description chaintip bytes the chaintip hash height uint32 the height of the chaintip in the blockchain <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#removetxrequest","title":"RemoveTxRequest","text":"<p>Request for removing a transaction from the mining candidate block.</p> Field Type Label Description txid bytes the transaction id to remove <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#statemessage","title":"StateMessage","text":"<p>Message containing the state of the block assembly service.</p> Field Type Label Description blockAssemblyState string the state of the block assembly service subtreeProcessorState string the state of the block assembly subtree processor resetWaitCount uint32 the number of blocks the reset has to wait for resetWaitTime uint32 the time in seconds the reset has to wait for subtreeCount uint32 the number of subtrees txCount uint64 the number of transactions queueCount int64 the size of the queue currentHeight uint32 the height of the chaintip currentHash string the hash of the chaintip removeMapCount uint32 the number of transactions in the remove map subtrees string repeated the hashes of the current subtrees <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#submitminingsolutionrequest","title":"SubmitMiningSolutionRequest","text":"<p>Request for submitting a mining solution to the blockchain.</p> Field Type Label Description id bytes the id of the mining candidate nonce uint32 the nonce value used for mining coinbase_tx bytes the coinbase transaction bytes time uint32 optional the timestamp of the block version uint32 optional the version of the block <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#okresponse","title":"OKResponse","text":"<p>Response indicating whether the call was successful.</p> Field Type Label Description ok bool true if the call was successful <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#getblockassemblyblockcandidateresponse","title":"GetBlockAssemblyBlockCandidateResponse","text":"<p>Response for the GetBlockAssemblyBlockCandidate method.</p> Field Type Label Description block bytes the block candidate in block assembly <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#getblockassemblytxsresponse","title":"GetBlockAssemblyTxsResponse","text":"<p>Response for the GetBlockAssemblyTxs method.</p> Field Type Label Description txCount uint64 the number of transactions in the block assembly txs string repeated the transactions currently being assembled in the block assembly <p></p>"},{"location":"references/protobuf_docs/blockassemblyProto/#blockassemblyapi","title":"BlockAssemblyAPI","text":"<p>Responsible for assembling new blocks and managing the blockchain's block creation process. This service handles transaction management, mining operations, and block state management.</p> Method Name Request Type Response Type Description HealthGRPC EmptyMessage HealthResponse Checks the health status of the block assembly service. Returns detailed health information including service status and timestamp. AddTx AddTxRequest AddTxResponse Adds a single transaction to the next available subtree. The transaction will be included in block assembly for mining. RemoveTx RemoveTxRequest EmptyMessage Removes a transaction from consideration for block inclusion. This is useful for handling double-spends or invalid transactions. AddTxBatch AddTxBatchRequest AddTxBatchResponse Efficiently adds multiple transactions in a single request. Provides better performance than multiple individual AddTx calls. GetMiningCandidate GetMiningCandidateRequest model.MiningCandidate Retrieves a block template ready for mining. Includes all necessary components for miners to begin work. GetCurrentDifficulty EmptyMessage GetCurrentDifficultyResponse Retrieves the current network mining difficulty. Used by miners to understand the current mining requirements. SubmitMiningSolution SubmitMiningSolutionRequest OKResponse Submits a solved block to the network. Includes the proof-of-work solution and block details. ResetBlockAssembly EmptyMessage EmptyMessage Resets the block assembly state. Useful for handling reorgs or recovering from errors. GetBlockAssemblyState EmptyMessage StateMessage Retrieves the current state of block assembly. Provides detailed information about the assembly process status. GenerateBlocks GenerateBlocksRequest EmptyMessage Creates new blocks (typically for testing purposes). Allows specification of block count and recipient address. CheckBlockAssembly EmptyMessage OKResponse Checks the current state of block assembly. This verifies that the block assembly and subtree processor are functioning correctly. GetBlockAssemblyBlockCandidate EmptyMessage GetBlockAssemblyBlockCandidateResponse Retrieves the current block candidate from block assembly. GetBlockAssemblyTxs EmptyMessage GetBlockAssemblyTxsResponse Retrieves the transactions currently being assembled in the block assembly. This provides visibility into the transactions that are candidates for inclusion in the next block. NOTE: this method is primarily for debugging purposes and may not be suitable for production use."},{"location":"references/protobuf_docs/blockassemblyProto/#scalar-value-types","title":"Scalar Value Types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby  double double double float float64 double float Float  float float float float float32 float float Float  int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required)  int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum  uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required)  uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required)  sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required)  sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum  fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required)  fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum  sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required)  sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum  bool bool boolean boolean bool bool boolean TrueClass/FalseClass  string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8)  bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)"},{"location":"references/protobuf_docs/blockchainProto/","title":"GRPC Documentation - BlockchainAPI","text":""},{"location":"references/protobuf_docs/blockchainProto/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p>blockchain_api.proto</p> <ul> <li>AddBlockRequest</li> <li>CheckBlockIsCurrentChainRequest</li> <li>CheckBlockIsCurrentChainResponse</li> <li>GetBestHeightAndTimeResponse</li> <li>GetBlockByHeightRequest</li> <li>GetChainTipsResponse</li> <li>GetBlockByIDRequest</li> <li>GetBlockExistsResponse</li> <li>GetBlockGraphDataRequest</li> <li>GetBlockHeaderIDsResponse</li> <li>GetBlockHeaderRequest</li> <li>GetBlockHeaderResponse</li> <li>GetBlockHeadersByHeightRequest</li> <li>GetBlockHeadersByHeightResponse</li> <li>GetBlockHeadersFromHeightRequest</li> <li>GetBlockHeadersFromHeightResponse</li> <li>GetBlockHeadersFromTillRequest</li> <li>GetBlockHeadersRequest</li> <li>GetBlockHeadersResponse</li> <li>GetBlockHeadersToCommonAncestorRequest</li> <li>GetBlockInChainByHeightHashRequest</li> <li>GetBlockIsMinedRequest</li> <li>GetBlockIsMinedResponse</li> <li>GetBlockLocatorRequest</li> <li>GetBlockLocatorResponse</li> <li>GetBlockRequest</li> <li>GetBlockResponse</li> <li>GetBlocksMinedNotSetResponse</li> <li>GetBlocksRequest</li> <li>GetBlocksResponse</li> <li>GetLastNInvalidBlocksRequest</li> <li>GetLastNInvalidBlocksResponse</li> <li>GetBlocksSubtreesNotSetResponse</li> <li>GetFSMStateResponse</li> <li>GetFullBlockResponse</li> <li>GetHashOfAncestorBlockRequest</li> <li>GetHashOfAncestorBlockResponse</li> <li>GetLatestBlockHeaderFromBlockLocatorRequest</li> <li>GetBlockHeadersFromOldestRequest</li> <li>GetLastNBlocksRequest</li> <li>GetLastNBlocksResponse</li> <li>GetMedianTimeRequest</li> <li>GetMedianTimeResponse</li> <li>GetNextWorkRequiredRequest</li> <li>GetNextWorkRequiredResponse</li> <li>GetStateRequest</li> <li>GetSuitableBlockRequest</li> <li>GetSuitableBlockResponse</li> <li>HealthResponse</li> <li>InvalidateBlockRequest</li> <li>LocateBlockHeadersRequest</li> <li>LocateBlockHeadersResponse</li> <li>Notification</li> <li>NotificationMetadata</li> <li>NotificationMetadata.MetadataEntry</li> <li>RevalidateBlockRequest</li> <li>SendFSMEventRequest</li> <li>SetBlockMinedSetRequest</li> <li>SetBlockProcessedAtRequest</li> <li>SetBlockSubtreesSetRequest</li> <li>SetStateRequest</li> <li>StateResponse</li> <li>SubscribeRequest</li> <li> <p>WaitFSMToTransitionRequest</p> </li> <li> <p>FSMEventType</p> </li> <li> <p>FSMStateType</p> </li> <li> <p>BlockchainAPI</p> </li> </ul> </li> <li> <p>Scalar Value Types</p> </li> </ul> <p></p> <p>Top</p>"},{"location":"references/protobuf_docs/blockchainProto/#blockchain_apiproto","title":"blockchain_api.proto","text":"<p>Package blockchain_api defines the gRPC service interface for blockchain operations.</p> <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#addblockrequest","title":"AddBlockRequest","text":"<p>AddBlockRequest contains data for adding a new block to the blockchain.</p> Field Type Label Description header bytes Block header subtree_hashes bytes repeated Merkle tree hashes coinbase_tx bytes Coinbase transaction transaction_count uint64 Number of transactions size_in_bytes uint64 Block size external bool External block flag peer_id string Peer identifier <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#checkblockiscurrentchainrequest","title":"CheckBlockIsCurrentChainRequest","text":"<p>CheckBlockIsCurrentChainRequest checks if blocks are in the main chain.</p> Field Type Label Description blockIDs uint32 repeated List of block IDs to check <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#checkblockiscurrentchainresponse","title":"CheckBlockIsCurrentChainResponse","text":"<p>CheckBlockIsCurrentChainResponse indicates if blocks are in the main chain.</p> Field Type Label Description isPartOfCurrentChain bool True if blocks are in main chain <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getbestheightandtimeresponse","title":"GetBestHeightAndTimeResponse","text":"<p>GetBestHeightAndTimeResponse contains chain tip information.</p> Field Type Label Description height uint32 Current best height time uint32 Current median time <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockbyheightrequest","title":"GetBlockByHeightRequest","text":"<p>GetBlockByHeightRequest represents a request to retrieve a block at a specific height.</p> Field Type Label Description height uint32 Block height to retrieve <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getchaintipsresponse","title":"GetChainTipsResponse","text":"<p>GetChainTipsResponse contains information about all known tips in the block tree.</p> Field Type Label Description tips model.ChainTip repeated List of chain tips <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockexistsresponse","title":"GetBlockExistsResponse","text":"<p>GetBlockExistsResponse indicates whether a block exists.</p> Field Type Label Description exists bool True if the block exists <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockgraphdatarequest","title":"GetBlockGraphDataRequest","text":"<p>GetBlockGraphDataRequest specifies parameters for retrieving blockchain visualization data.</p> Field Type Label Description period_millis uint64 Time period in milliseconds <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheaderidsresponse","title":"GetBlockHeaderIDsResponse","text":"<p>GetBlockHeaderIDsResponse contains block header identifiers.</p> Field Type Label Description ids uint32 repeated List of block header IDs <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheaderrequest","title":"GetBlockHeaderRequest","text":"<p>GetBlockHeaderRequest requests a specific block header.</p> Field Type Label Description blockHash bytes Hash of the block <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheaderresponse","title":"GetBlockHeaderResponse","text":"<p>swagger:model GetBlockHeaderResponse</p> Field Type Label Description blockHeader bytes height uint32 tx_count uint64 size_in_bytes uint64 miner string block_time uint32 timestamp uint32 <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getlatestblockheaderfromblocklocatorrequest","title":"GetLatestBlockHeaderFromBlockLocatorRequest","text":"<p>GetLatestBlockHeaderFromBlockLocatorRequest retrieves the latest block header using a block locator.</p> Field Type Label Description bestBlockHash bytes Best block hash blockLocatorHashes bytes repeated Block locator hashes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheadersfromoldestrequest","title":"GetBlockHeadersFromOldestRequest","text":"<p>GetBlockHeadersFromOldestRequest retrieves block headers starting from the oldest block.</p> Field Type Label Description chainTipHash bytes Chain tip hash targetHash bytes Target block hash numberOfHeaders uint64 Maximum number of hashes to return <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheadersbyheightrequest","title":"GetBlockHeadersByHeightRequest","text":"<p>swagger:model GetBlockHeadersByHeightRequest</p> Field Type Label Description startHeight uint32 endHeight uint32 <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheadersbyheightresponse","title":"GetBlockHeadersByHeightResponse","text":"<p>swagger:model GetBlockHeadersByHeightResponse</p> Field Type Label Description blockHeaders bytes repeated metas bytes repeated <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheadersfromheightrequest","title":"GetBlockHeadersFromHeightRequest","text":"<p>swagger:model GetBlockHeadersFromHeightRequest</p> Field Type Label Description startHeight uint32 limit uint32 <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheadersfromheightresponse","title":"GetBlockHeadersFromHeightResponse","text":"<p>swagger:model GetBlockHeadersFromHeightResponse</p> Field Type Label Description blockHeaders bytes repeated metas bytes repeated <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheadersrequest","title":"GetBlockHeadersRequest","text":"<p>swagger:model GetBlockHeadersRequest</p> Field Type Label Description startHash bytes numberOfHeaders uint64 <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockheadersresponse","title":"GetBlockHeadersResponse","text":"<p>swagger:model GetBlockHeadersResponse</p> Field Type Label Description blockHeaders bytes repeated metas bytes repeated <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblocklocatorrequest","title":"GetBlockLocatorRequest","text":"Field Type Label Description hash bytes height uint32"},{"location":"references/protobuf_docs/blockchainProto/#getblocklocatorresponse","title":"GetBlockLocatorResponse","text":"Field Type Label Description locator bytes repeated"},{"location":"references/protobuf_docs/blockchainProto/#getblockrequest","title":"GetBlockRequest","text":"<p>GetBlockRequest represents a request to retrieve a block by its hash.</p> Field Type Label Description hash bytes Hash of the block to retrieve <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockresponse","title":"GetBlockResponse","text":"<p>swagger:model GetBlockResponse</p> Field Type Label Description header bytes height uint32 coinbase_tx bytes transaction_count uint64 subtree_hashes bytes repeated size_in_bytes uint64 <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblocksminednotsetresponse","title":"GetBlocksMinedNotSetResponse","text":"<p>swagger:model GetBlocksMinedNotSetResponse</p> Field Type Label Description blockBytes bytes repeated <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblocksrequest","title":"GetBlocksRequest","text":"<p>GetBlocksRequest represents a request to retrieve multiple blocks.</p> Field Type Label Description hash bytes Starting block hash count uint32 Number of blocks to retrieve <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblocksresponse","title":"GetBlocksResponse","text":"<p>swagger:model GetBlocksResponse</p> Field Type Label Description blocks bytes repeated <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockssubtreesnotsetresponse","title":"GetBlocksSubtreesNotSetResponse","text":"<p>swagger:model GetBlocksSubtreesNotSetResponse</p> Field Type Label Description blockBytes bytes repeated <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getfsmstateresponse","title":"GetFSMStateResponse","text":"<p>swagger:model GetFSMStateResponse</p> Field Type Label Description state FSMStateType <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getfullblockresponse","title":"GetFullBlockResponse","text":"Field Type Label Description full_block_bytes bytes"},{"location":"references/protobuf_docs/blockchainProto/#gethashofancestorblockrequest","title":"GetHashOfAncestorBlockRequest","text":"<p>swagger:model GetHashOfAncestorBlockRequest</p> Field Type Label Description hash bytes depth uint32 <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#gethashofancestorblockresponse","title":"GetHashOfAncestorBlockResponse","text":"<p>swagger:model GetHashOfAncestorBlockResponse</p> Field Type Label Description hash bytes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getlastnblocksrequest","title":"GetLastNBlocksRequest","text":"<p>swagger:model GetLastNBlocksRequest</p> Field Type Label Description numberOfBlocks int64 includeOrphans bool fromHeight uint32 <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getlastnblocksresponse","title":"GetLastNBlocksResponse","text":"<p>swagger:model GetLastNBlocksResponse</p> Field Type Label Description blocks model.BlockInfo repeated <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getmediantimerequest","title":"GetMedianTimeRequest","text":"<p>GetMedianTimeRequest requests median time calculation for a block.</p> Field Type Label Description blockHash bytes Hash of the block <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getmediantimeresponse","title":"GetMedianTimeResponse","text":"<p>swagger:model GetMedianTimeResponse</p> Field Type Label Description block_header_time uint32 repeated This will return the nTimes of the last 11 (+1) blocks <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getnextworkrequiredrequest","title":"GetNextWorkRequiredRequest","text":"<p>swagger:model GGetNextWorkRequiredRequest</p> Field Type Label Description blockHash bytes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getnextworkrequiredresponse","title":"GetNextWorkRequiredResponse","text":"<p>swagger:model GGetNextWorkRequiredResponse</p> Field Type Label Description bits bytes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getstaterequest","title":"GetStateRequest","text":"<p>swagger:model StateRequest</p> Field Type Label Description key string <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getsuitableblockrequest","title":"GetSuitableBlockRequest","text":"<p>swagger:model GetSuitableBlockRequest</p> Field Type Label Description hash bytes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getsuitableblockresponse","title":"GetSuitableBlockResponse","text":"<p>swagger:model GetSuitableBlockResponse</p> Field Type Label Description block model.SuitableBlock <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#healthresponse","title":"HealthResponse","text":"<p>HealthResponse represents the health status of the blockchain service.</p> Field Type Label Description ok bool Overall health status details string Detailed health information timestamp google.protobuf.Timestamp Timestamp of the health check <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#invalidateblockrequest","title":"InvalidateBlockRequest","text":"<p>swagger:model InvalidateBlockRequest</p> Field Type Label Description blockHash bytes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#locateblockheadersrequest","title":"LocateBlockHeadersRequest","text":"Field Type Label Description locator bytes repeated hash_stop bytes max_hashes uint32"},{"location":"references/protobuf_docs/blockchainProto/#locateblockheadersresponse","title":"LocateBlockHeadersResponse","text":"Field Type Label Description block_headers bytes repeated"},{"location":"references/protobuf_docs/blockchainProto/#notification","title":"Notification","text":"<p>swagger:model Notification</p> Field Type Label Description type model.NotificationType hash bytes base_URL string metadata NotificationMetadata <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#notificationmetadata","title":"NotificationMetadata","text":"<p>swagger:model NotificationMetadata</p> Field Type Label Description metadata NotificationMetadata.MetadataEntry repeated define a map of string to string <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#notificationmetadatametadataentry","title":"NotificationMetadata.MetadataEntry","text":"Field Type Label Description key string value string"},{"location":"references/protobuf_docs/blockchainProto/#revalidateblockrequest","title":"RevalidateBlockRequest","text":"<p>swagger:model RevalidateBlockRequest</p> Field Type Label Description blockHash bytes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#sendfsmeventrequest","title":"SendFSMEventRequest","text":"<p>swagger:model SendFSMEventRequest</p> Field Type Label Description event FSMEventType <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#setblockminedsetrequest","title":"SetBlockMinedSetRequest","text":"<p>SetBlockMinedSetRequest marks a block as mined.</p> Field Type Label Description blockHash bytes Hash of the block to mark as mined <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#setblockprocessedatrequest","title":"SetBlockProcessedAtRequest","text":"<p>SetBlockProcessedAtRequest defines parameters for setting or clearing a block's processed_at timestamp.</p> Field Type Label Description block_hash bytes Hash of the block clear bool Whether to clear the timestamp <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#setblocksubtreessetrequest","title":"SetBlockSubtreesSetRequest","text":"<p>swagger:model SetBlockSubtreesSetRequest</p> Field Type Label Description blockHash bytes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#setstaterequest","title":"SetStateRequest","text":"<p>swagger:model SetStateRequest</p> Field Type Label Description key string data bytes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#stateresponse","title":"StateResponse","text":"<p>swagger:model StateResponse</p> Field Type Label Description data bytes <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#subscriberequest","title":"SubscribeRequest","text":"<p>swagger:model SubscribeRequest</p> Field Type Label Description source string <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#waitfsmtotransitionrequest","title":"WaitFSMToTransitionRequest","text":"<p>swagger:model WaitFSMToTransitionRequest</p> Field Type Label Description state FSMStateType <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockbyidrequest","title":"GetBlockByIDRequest","text":"<p>GetBlockByIDRequest represents a request to retrieve a block by its ID.</p> Field Type Label Description id uint64 Block ID to retrieve <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#getblockinchainbyheighthashrequest","title":"GetBlockInChainByHeightHashRequest","text":"<p>GetBlockInChainByHeightHashRequest represents a request to retrieve a block by height in a specific chain.</p> Field Type Label Description height uint32 Target block height start_hash bytes Starting block hash defining the chain <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#fsmeventtype","title":"FSMEventType","text":"<p>swagger:enum FSMEventType</p> Name Number Description STOP 0 Stop the blockchain service RUN 1 Run the blockchain service CATCHUPBLOCKS 2 Start catching up blocks LEGACYSYNC 3 Start legacy synchronization <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#fsmstatetype","title":"FSMStateType","text":"<p>FSMStateType defines possible states of the blockchain FSM.</p> Name Number Description IDLE 0 Service is idle RUNNING 1 Service is running CATCHINGBLOCKS 2 Service is catching up blocks LEGACYSYNCING 3 Service is performing legacy sync <p></p>"},{"location":"references/protobuf_docs/blockchainProto/#blockchainapi","title":"BlockchainAPI","text":"<p>BlockchainAPI service provides comprehensive blockchain management functionality.</p> Method Name Request Type Response Type Description HealthGRPC .google.protobuf.Empty HealthResponse Checks the health status of the blockchain service. AddBlock AddBlockRequest .google.protobuf.Empty Adds a new block to the blockchain. Called by BlockValidator to add validated blocks. GetBlock GetBlockRequest GetBlockResponse Retrieves a block by its hash. GetBlocks GetBlocksRequest GetBlocksResponse Retrieves multiple blocks starting from a specific hash. GetBlockByHeight GetBlockByHeightRequest GetBlockResponse Retrieves a block at a specific height. GetBlockByID GetBlockByIDRequest GetBlockResponse Retrieves a block by its id. GetBlockStats .google.protobuf.Empty .model.BlockStats Retrieves statistical information about the blockchain. GetBlockGraphData GetBlockGraphDataRequest .model.BlockDataPoints Retrieves data points for blockchain visualization. GetLastNBlocks GetLastNBlocksRequest GetLastNBlocksResponse Retrieves the most recent N blocks from the blockchain. GetLastNInvalidBlocks GetLastNInvalidBlocksRequest GetLastNInvalidBlocksResponse Retrieves the most recent N blocks that have been marked as invalid. GetSuitableBlock GetSuitableBlockRequest GetSuitableBlockResponse Finds a suitable block for mining purposes. GetHashOfAncestorBlock GetHashOfAncestorBlockRequest GetHashOfAncestorBlockResponse Retrieves the hash of an ancestor block at a specified depth. GetNextWorkRequired GetNextWorkRequiredRequest GetNextWorkRequiredResponse Calculates the required proof of work for the next block. GetBlockExists GetBlockRequest GetBlockExistsResponse Checks if a block exists in the blockchain. GetBlockHeaders GetBlockHeadersRequest GetBlockHeadersResponse Retrieves headers for multiple blocks. GetBlockHeadersToCommonAncestor GetBlockHeadersToCommonAncestorRequest GetBlockHeadersResponse Retrieves block headers up to a common ancestor point between chains. GetBlockHeadersFromTill GetBlockHeadersFromTillRequest GetBlockHeadersResponse Retrieves block headers between two specified blocks. GetBlockHeadersFromHeight GetBlockHeadersFromHeightRequest GetBlockHeadersFromHeightResponse Retrieves block headers starting from a specific height. GetBlockHeadersByHeight GetBlockHeadersByHeightRequest GetBlockHeadersByHeightResponse Retrieves block headers between two specified heights. GetLatestBlockHeaderFromBlockLocator GetLatestBlockHeaderFromBlockLocatorRequest GetBlockHeaderResponse Retrieves the latest block header using a block locator. GetBlockHeadersFromOldest GetBlockHeadersFromOldestRequest GetBlockHeadersResponse Retrieves block headers starting from the oldest block. GetBlockHeaderIDs GetBlockHeadersRequest GetBlockHeaderIDsResponse Retrieves block header IDs for a range of blocks. GetBestBlockHeader .google.protobuf.Empty GetBlockHeaderResponse Retrieves the header of the current best block. CheckBlockIsInCurrentChain CheckBlockIsCurrentChainRequest CheckBlockIsCurrentChainResponse Verifies if specified blocks are in the main chain. GetChainTips .google.protobuf.Empty GetChainTipsResponse Retrieves information about all known tips in the block tree. GetBlockHeader GetBlockHeaderRequest GetBlockHeaderResponse Retrieves the header of a specific block. InvalidateBlock InvalidateBlockRequest .google.protobuf.Empty Marks a block as invalid in the blockchain. RevalidateBlock RevalidateBlockRequest .google.protobuf.Empty Restores a previously invalidated block. Subscribe SubscribeRequest stream Notification Creates a subscription for blockchain notifications. SendNotification Notification .google.protobuf.Empty Broadcasts a notification to subscribers. GetState GetStateRequest StateResponse Retrieves state data by key. SetState SetStateRequest .google.protobuf.Empty Stores state data with a key. GetBlockIsMined GetBlockIsMinedRequest GetBlockIsMinedResponse Checks if a block is marked as mined. SetBlockMinedSet SetBlockMinedSetRequest .google.protobuf.Empty Marks a block as mined. SetBlockProcessedAt SetBlockProcessedAtRequest .google.protobuf.Empty Sets or clears the processed_at timestamp for a block. GetBlocksMinedNotSet .google.protobuf.Empty GetBlocksMinedNotSetResponse Retrieves blocks not marked as mined. SetBlockSubtreesSet SetBlockSubtreesSetRequest .google.protobuf.Empty Marks a block's subtrees as set. GetBlocksSubtreesNotSet .google.protobuf.Empty GetBlocksSubtreesNotSetResponse Retrieves blocks with unset subtrees. SendFSMEvent SendFSMEventRequest GetFSMStateResponse Sends an event to the blockchain FSM. GetFSMCurrentState .google.protobuf.Empty GetFSMStateResponse Retrieves the current state of the FSM. WaitFSMToTransitionToGivenState WaitFSMToTransitionRequest .google.protobuf.Empty Waits for FSM to reach a specific state. WaitUntilFSMTransitionFromIdleState .google.protobuf.Empty .google.protobuf.Empty Waits for FSM to transition from IDLE state. Run .google.protobuf.Empty .google.protobuf.Empty Transitions the blockchain service to running state. CatchUpBlocks .google.protobuf.Empty .google.protobuf.Empty Initiates block catch-up process. LegacySync .google.protobuf.Empty .google.protobuf.Empty Initiates legacy synchronization process. Idle .google.protobuf.Empty .google.protobuf.Empty Marks the service as idle. GetBlockLocator GetBlockLocatorRequest GetBlockLocatorResponse Retrieves a block locator for chain synchronization. LocateBlockHeaders LocateBlockHeadersRequest LocateBlockHeadersResponse Finds block headers using a locator. GetBestHeightAndTime .google.protobuf.Empty GetBestHeightAndTimeResponse Retrieves the current best height and median time."},{"location":"references/protobuf_docs/blockchainProto/#scalar-value-types","title":"Scalar Value Types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby  double double double float float64 double float Float  float float float float float32 float float Float  int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required)  int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum  uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required)  uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required)  sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required)  sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum  fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required)  fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum  sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required)  sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum  bool bool boolean boolean bool bool boolean TrueClass/FalseClass  string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8)  bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)"},{"location":"references/protobuf_docs/blockvalidationProto/","title":"GRPC Documentation - BlockValidationAPI","text":""},{"location":"references/protobuf_docs/blockvalidationProto/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p>blockvalidation_api.proto</p> <ul> <li>BlockFoundRequest</li> <li>EmptyMessage</li> <li>HealthResponse</li> <li>ProcessBlockRequest</li> <li>ValidateBlockRequest</li> <li> <p>ValidateBlockResponse</p> </li> <li> <p>BlockValidationAPI</p> </li> </ul> </li> <li> <p>Scalar Value Types</p> </li> </ul> <p></p> <p>Top</p>"},{"location":"references/protobuf_docs/blockvalidationProto/#blockvalidation_apiproto","title":"blockvalidation_api.proto","text":""},{"location":"references/protobuf_docs/blockvalidationProto/#blockfoundrequest","title":"BlockFoundRequest","text":"<p>swagger:model BlockFoundRequest</p> Field Type Label Description hash bytes The hash of the found block base_url string Base URL where the block can be retrieved from wait_to_complete bool Whether to wait for the block processing to complete peer_id string P2P peer identifier for peerMetrics tracking <p></p>"},{"location":"references/protobuf_docs/blockvalidationProto/#emptymessage","title":"EmptyMessage","text":"<p>Represents an empty request or response message. Used for endpoints that don't require input parameters or return data.</p> <p>swagger:model EmptyMessage</p> <p></p>"},{"location":"references/protobuf_docs/blockvalidationProto/#healthresponse","title":"HealthResponse","text":"<p>swagger:model HealthResponse</p> Field Type Label Description ok bool Indicates if the service is healthy details string Additional health status details timestamp google.protobuf.Timestamp Timestamp when the health check was performed <p></p>"},{"location":"references/protobuf_docs/blockvalidationProto/#processblockrequest","title":"ProcessBlockRequest","text":"<p>swagger:model ProcessBlockRequest</p> Field Type Label Description block bytes The block data to process height uint32 The height of the block in the blockchain <p></p>"},{"location":"references/protobuf_docs/blockvalidationProto/#validateblockrequest","title":"ValidateBlockRequest","text":"<p>swagger:model ValidateBlockRequest</p> Field Type Label Description block bytes The block data to validate height uint32 The height of the block in the blockchain <p></p>"},{"location":"references/protobuf_docs/blockvalidationProto/#validateblockresponse","title":"ValidateBlockResponse","text":"<p>swagger:model ValidateBlockResponse</p> Field Type Label Description ok bool Indicates if the block is valid message string Additional information about validation results <p></p>"},{"location":"references/protobuf_docs/blockvalidationProto/#blockvalidationapi","title":"BlockValidationAPI","text":"Method Name Request Type Response Type Description HealthGRPC EmptyMessage HealthResponse Returns the health status of the BlockValidation service. BlockFound BlockFoundRequest EmptyMessage Notifies the service that a new block has been found and requires validation. ProcessBlock ProcessBlockRequest EmptyMessage Processes a block to validate its content and structure. ValidateBlock ValidateBlockRequest ValidateBlockResponse Validates a block without processing it, returning validation results."},{"location":"references/protobuf_docs/blockvalidationProto/#scalar-value-types","title":"Scalar Value Types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby  double double double float float64 double float Float  float float float float float32 float float Float  int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required)  int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum  uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required)  uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required)  sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required)  sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum  fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required)  fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum  sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required)  sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum  bool bool boolean boolean bool bool boolean TrueClass/FalseClass  string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8)  bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)"},{"location":"references/protobuf_docs/propagationProto/","title":"GRPC Documentation - PropagationAPI","text":""},{"location":"references/protobuf_docs/propagationProto/#table-of-contents","title":"Table of Contents","text":"<ul> <li>propagation_api.proto<ul> <li>EmptyMessage</li> <li>BatchTransactionItem</li> <li>GetRequest</li> <li>GetResponse</li> <li>HealthResponse</li> <li>ProcessTransactionBatchRequest</li> <li>ProcessTransactionBatchResponse</li> <li>ProcessTransactionRequest</li> <li>PropagationAPI</li> </ul> </li> <li>Scalar Value Types</li> </ul> <p>Top</p>"},{"location":"references/protobuf_docs/propagationProto/#propagation_apiproto","title":"propagation_api.proto","text":"<p>Package propagation_api provides gRPC services for Bitcoin SV transaction propagation. It handles individual and batch transaction processing, health checks, and debugging capabilities for the BSV network.</p> <p></p>"},{"location":"references/protobuf_docs/propagationProto/#emptymessage","title":"EmptyMessage","text":"<p>Represents an empty request or response. Used when no additional data needs to be transmitted.</p> <p>swagger:model EmptyMessage</p> <p></p>"},{"location":"references/protobuf_docs/propagationProto/#batchtransactionitem","title":"BatchTransactionItem","text":"<p>Represents a single transaction item in a batch request with trace context support.</p> Field Type Label Description tx bytes Raw transaction bytes to process trace_context map Serialized OpenTelemetry trace context as key-value pairs for proper span propagation <p></p>"},{"location":"references/protobuf_docs/propagationProto/#getrequest","title":"GetRequest","text":"<p>Represents a request to retrieve a transaction by its ID.</p> <p>swagger:model GetRequest</p> Field Type Label Description txid bytes Transaction ID in bytes <p></p>"},{"location":"references/protobuf_docs/propagationProto/#getresponse","title":"GetResponse","text":"<p>Contains the retrieved transaction data.</p> <p>swagger:model GetResponse</p> Field Type Label Description tx bytes Raw transaction bytes <p></p>"},{"location":"references/protobuf_docs/propagationProto/#healthresponse","title":"HealthResponse","text":"<p>Provides information about the service's health status.</p> <p>swagger:model HealthResponse</p> Field Type Label Description ok bool Indicates whether the service is healthy details string Provides additional information about the health status timestamp google.protobuf.Timestamp Indicates when the health check was performed <p></p>"},{"location":"references/protobuf_docs/propagationProto/#processtransactionbatchrequest","title":"ProcessTransactionBatchRequest","text":"<p>Represents a request to process multiple transactions in a batch.</p> <p>swagger:model ProcessTransactionBatchRequest</p> Field Type Label Description items BatchTransactionItem repeated Array of transaction items to process, each containing transaction bytes and trace context <p></p>"},{"location":"references/protobuf_docs/propagationProto/#processtransactionbatchresponse","title":"ProcessTransactionBatchResponse","text":"<p>Contains the results of batch transaction processing.</p> <p>swagger:model ProcessTransactionBatchResponse</p> Field Type Label Description errors errors.TError repeated Error messages for each transaction in the batch. Empty string indicates success for that transaction. <p></p>"},{"location":"references/protobuf_docs/propagationProto/#processtransactionrequest","title":"ProcessTransactionRequest","text":"<p>Represents a request to process a single transaction.</p> <p>swagger:model ProcessTransactionRequest</p> Field Type Label Description tx bytes Raw transaction bytes to process <p></p>"},{"location":"references/protobuf_docs/propagationProto/#propagationapi","title":"PropagationAPI","text":"Method Name Request Type Response Type Description HealthGRPC EmptyMessage HealthResponse Checks the health status of the propagation service and its dependencies. Returns a HealthResponse containing the service status and details. ProcessTransaction ProcessTransactionRequest EmptyMessage Processes a single BSV transaction. The transaction must be provided in raw byte format and must be extended. Coinbase transactions are not allowed. ProcessTransactionBatch ProcessTransactionBatchRequest ProcessTransactionBatchResponse Processes multiple transactions in a single request. This is more efficient than processing transactions individually when dealing with large numbers of transactions."},{"location":"references/protobuf_docs/propagationProto/#scalar-value-types","title":"Scalar Value Types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby  double double double float float64 double float Float  float float float float float32 float float Float  int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required)  int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum  uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required)  uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required)  sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required)  sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum  fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required)  fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum  sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required)  sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum  bool bool boolean boolean bool bool boolean TrueClass/FalseClass  string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8)  bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)"},{"location":"references/protobuf_docs/subtreevalidationProto/","title":"GRPC Documentation - SubtreeValidationAPI","text":""},{"location":"references/protobuf_docs/subtreevalidationProto/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p>subtreevalidation_api.proto</p> <ul> <li>CheckBlockSubtreesRequest</li> <li>CheckBlockSubtreesResponse</li> <li>CheckSubtreeFromBlockRequest</li> <li>CheckSubtreeFromBlockResponse</li> <li>EmptyMessage</li> <li> <p>HealthResponse</p> </li> <li> <p>SubtreeValidationAPI</p> </li> </ul> </li> <li> <p>Scalar Value Types</p> </li> </ul> <p></p> <p>Top</p>"},{"location":"references/protobuf_docs/subtreevalidationProto/#subtreevalidation_apiproto","title":"subtreevalidation_api.proto","text":"<p>SubtreeValidationAPI provides gRPC services for validating blockchain subtrees. The service exposes endpoints for health monitoring and subtree validation operations.</p> <p></p>"},{"location":"references/protobuf_docs/subtreevalidationProto/#checkblocksubtreesrequest","title":"CheckBlockSubtreesRequest","text":"<p>Defines the input parameters for checking subtrees in a block.</p> <p>swagger:model CheckBlockSubtreesRequest</p> Field Type Label Description block bytes Block containing the subtrees to be checked base_url string Endpoint for retrieving missing transaction data <p></p>"},{"location":"references/protobuf_docs/subtreevalidationProto/#checkblocksubtreesresponse","title":"CheckBlockSubtreesResponse","text":"<p>Contains the validation results for subtrees in a block.</p> <p>swagger:model CheckBlockSubtreesResponse</p> Field Type Label Description blessed bool Indicates if all subtrees in the block pass validation <p></p>"},{"location":"references/protobuf_docs/subtreevalidationProto/#checksubtreefromblockrequest","title":"CheckSubtreeFromBlockRequest","text":"<p>Defines the input parameters for subtree validation.</p> <p>swagger:model CheckSubtreeFromBlockRequest</p> Field Type Label Description hash bytes Merkle root hash of the subtree requiring validation base_url string Endpoint for retrieving missing transaction data block_height uint32 Blockchain height where the subtree is located block_hash bytes Uniquely identifies the block containing the subtree previous_block_hash bytes Identifies the block preceding the current block <p></p>"},{"location":"references/protobuf_docs/subtreevalidationProto/#checksubtreefromblockresponse","title":"CheckSubtreeFromBlockResponse","text":"<p>Contains the validation result for a subtree check.</p> <p>swagger:model CheckSubtreeFromBlockResponse</p> Field Type Label Description blessed bool Indicates if the subtree passes all validation criteria <p></p>"},{"location":"references/protobuf_docs/subtreevalidationProto/#emptymessage","title":"EmptyMessage","text":"<p>Represents an empty message structure used for health check requests.</p> <p>swagger:model EmptyMessage</p> <p></p>"},{"location":"references/protobuf_docs/subtreevalidationProto/#healthresponse","title":"HealthResponse","text":"<p>Encapsulates the service health status information.</p> <p>swagger:model HealthResponse</p> Field Type Label Description ok bool Indicates if the service is operating normally details string Provides additional context about the service health status timestamp google.protobuf.Timestamp Records when the health check was performed <p></p>"},{"location":"references/protobuf_docs/subtreevalidationProto/#subtreevalidationapi","title":"SubtreeValidationAPI","text":"<p>Provides gRPC services for validating blockchain subtrees. The service exposes endpoints for health monitoring and subtree validation operations.</p> Method Name Request Type Response Type Description HealthGRPC EmptyMessage HealthResponse Checks the service's health status. It takes an empty request message and returns a response indicating the service's health. CheckSubtreeFromBlock CheckSubtreeFromBlockRequest CheckSubtreeFromBlockResponse Validates a subtree within a specified block in the blockchain. It takes a request containing the subtree's merkle root hash and block details, returning a response indicating the subtree's validity status. CheckBlockSubtrees CheckBlockSubtreesRequest CheckBlockSubtreesResponse Validates all subtrees within a block. Takes a request containing the block data and returns validation results for all subtrees."},{"location":"references/protobuf_docs/subtreevalidationProto/#scalar-value-types","title":"Scalar Value Types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby  double double double float float64 double float Float  float float float float float32 float float Float  int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required)  int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum  uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required)  uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required)  sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required)  sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum  fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required)  fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum  sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required)  sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum  bool bool boolean boolean bool bool boolean TrueClass/FalseClass  string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8)  bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)"},{"location":"references/protobuf_docs/validatorProto/","title":"GRPC Documentation - ValidatorAPI","text":"<p>The validator_api.proto defines the gRPC service interface for the Bitcoin SV transaction validation service. It specifies the methods and message types for transaction validation, health checking, and block information retrieval.</p>"},{"location":"references/protobuf_docs/validatorProto/#table-of-contents","title":"Table of Contents","text":"<ul> <li>validator_api.proto<ul> <li>EmptyMessage</li> <li>GetBlockHeightResponse</li> <li>GetMedianBlockTimeResponse</li> <li>HealthResponse</li> <li>ValidateTransactionBatchRequest</li> <li>ValidateTransactionBatchResponse</li> <li>ValidateTransactionRequest</li> <li>ValidateTransactionResponse</li> <li>ValidatorAPI</li> </ul> </li> <li>Scalar Value Types</li> </ul> <p>Top</p>"},{"location":"references/protobuf_docs/validatorProto/#validator_apiproto","title":"validator_api.proto","text":""},{"location":"references/protobuf_docs/validatorProto/#emptymessage","title":"EmptyMessage","text":"<p>Represents an empty request message. Used for endpoints that don't require input parameters.</p> <p>swagger:model EmptyMessage</p> <p></p>"},{"location":"references/protobuf_docs/validatorProto/#getblockheightresponse","title":"GetBlockHeightResponse","text":"<p>Provides the current block height.</p> <p>swagger:model GetBlockHeightResponse</p> Field Type Label Description height uint32 Current block height <p></p>"},{"location":"references/protobuf_docs/validatorProto/#getmedianblocktimeresponse","title":"GetMedianBlockTimeResponse","text":"<p>Provides the median time of recent blocks. Used for time-based validation rules.</p> <p>swagger:model GetMedianBlockTimeResponse</p> Field Type Label Description median_time uint32 Median time of recent blocks <p></p>"},{"location":"references/protobuf_docs/validatorProto/#healthresponse","title":"HealthResponse","text":"<p>Provides health check information for the validation service.</p> <p>swagger:model HealthResponse</p> Field Type Label Description ok bool Overall health status details string Detailed health information timestamp google.protobuf.Timestamp Timestamp of health check <p></p>"},{"location":"references/protobuf_docs/validatorProto/#validatetransactionbatchrequest","title":"ValidateTransactionBatchRequest","text":"<p>Contains multiple transactions for batch validation.</p> <p>swagger:model ValidateTransactionBatchRequest</p> Field Type Label Description transactions ValidateTransactionRequest repeated Array of transactions to validate <p></p>"},{"location":"references/protobuf_docs/validatorProto/#validatetransactionbatchresponse","title":"ValidateTransactionBatchResponse","text":"<p>Provides batch validation results for multiple transactions.</p> <p>swagger:model ValidateTransactionBatchResponse</p> Field Type Label Description valid bool Overall batch validation status errors errors.TError repeated Array of error messages, one per transaction metadata bytes repeated Array of metadata for each transaction <p></p>"},{"location":"references/protobuf_docs/validatorProto/#validatetransactionrequest","title":"ValidateTransactionRequest","text":"<p>Contains data for transaction validation.</p> <p>swagger:model ValidateTransactionRequest</p> Field Type Label Description transaction_data bytes Raw transaction data to validate block_height uint32 Block height for validation context skip_utxo_creation bool optional Skip UTXO creation for validation add_tx_to_block_assembly bool optional Add transaction to block assembly skip_policy_checks bool optional Skip policy checks create_conflicting bool optional Create conflicting transaction <p></p>"},{"location":"references/protobuf_docs/validatorProto/#validatetransactionresponse","title":"ValidateTransactionResponse","text":"<p>Provides transaction validation results.</p> <p>swagger:model ValidateTransactionResponse</p> Field Type Label Description valid bool Validation result (true if valid) txid bytes Transaction ID of the validated transaction reason string Reason for rejection if invalid metadata bytes Additional metadata for the transaction <p></p>"},{"location":"references/protobuf_docs/validatorProto/#validatorapi","title":"ValidatorAPI","text":"<p>Service provides methods for transaction validation and related operations.</p> Method Name Request Type Response Type Description HealthGRPC EmptyMessage HealthResponse Checks the health status of the validation service. Returns detailed health information including service status and timestamp. ValidateTransaction ValidateTransactionRequest ValidateTransactionResponse Validates a single transaction. Performs comprehensive validation including script verification and UTXO checks. ValidateTransactionBatch ValidateTransactionBatchRequest ValidateTransactionBatchResponse Validates multiple transactions in a single request. Provides efficient batch processing of transactions. GetBlockHeight EmptyMessage GetBlockHeightResponse Retrieves the current block height. Used for validation context and protocol upgrade determination. GetMedianBlockTime EmptyMessage GetMedianBlockTimeResponse Retrieves the median time of recent blocks. Used for time-based validation rules."},{"location":"references/protobuf_docs/validatorProto/#scalar-value-types","title":"Scalar Value Types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby  double double double float float64 double float Float  float float float float float32 float float Float  int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required)  int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum  uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required)  uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required)  sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required)  sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum  fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required)  fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum  sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required)  sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum  bool bool boolean boolean bool bool boolean TrueClass/FalseClass  string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8)  bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)"},{"location":"references/services/alert_reference/","title":"Alert Service Reference Documentation","text":""},{"location":"references/services/alert_reference/#types","title":"Types","text":""},{"location":"references/services/alert_reference/#server","title":"Server","text":"<pre><code>type Server struct {\n    // UnimplementedAlertAPIServer is embedded for forward compatibility with the alert API\n    alert_api.UnimplementedAlertAPIServer\n\n    // logger handles all logging operations\n    logger ulogger.Logger\n\n    // settings contains the server configuration settings\n    settings *settings.Settings\n\n    // stats tracks server statistics\n    stats *gocore.Stat\n\n    // blockchainClient provides access to blockchain operations\n    blockchainClient blockchain.ClientI\n\n    // peerClient provides access to peer operations\n    peerClient peer.ClientI\n\n    // p2pClient provides access to p2p operations\n    p2pClient p2pservice.ClientI\n\n    // utxoStore manages UTXO operations\n    utxoStore utxo.Store\n\n    // blockassemblyClient handles block assembly operations\n    blockassemblyClient *blockassembly.Client\n\n    // appConfig contains alert system specific configuration\n    appConfig *config.Config\n\n    // p2pServer manages peer-to-peer communication\n    p2pServer *p2p.Server\n}\n</code></pre> <p>The <code>Server</code> type is the main structure for the Alert Service. It implements the <code>UnimplementedAlertAPIServer</code> and contains components for managing alerts, blockchain interactions, and P2P communication.</p>"},{"location":"references/services/alert_reference/#node","title":"Node","text":"<pre><code>type Node struct {\n    // logger handles logging operations\n    logger ulogger.Logger\n\n    // blockchainClient provides access to blockchain operations\n    blockchainClient blockchain.ClientI\n\n    // utxoStore manages UTXO operations\n    utxoStore utxo.Store\n\n    // blockassemblyClient handles block assembly operations\n    blockassemblyClient *blockassembly.Client\n\n    // peerClient handles peer operations\n    peerClient peer.ClientI\n\n    // p2pClient handles p2p operations\n    p2pClient p2p.ClientI\n\n    // settings contains node configuration\n    settings *settings.Settings\n}\n</code></pre> <p>The <code>Node</code> type represents a node in the network and provides methods for interacting with the blockchain and managing UTXOs.</p>"},{"location":"references/services/alert_reference/#functions","title":"Functions","text":""},{"location":"references/services/alert_reference/#server_1","title":"Server","text":""},{"location":"references/services/alert_reference/#new","title":"New","text":"<pre><code>func New(logger ulogger.Logger, tSettings *settings.Settings, blockchainClient blockchain.ClientI, utxoStore utxo.Store, blockassemblyClient *blockassembly.Client, peerClient peer.ClientI, p2pClient p2pservice.ClientI) *Server\n</code></pre> <p>Creates a new instance of the <code>Server</code> with the specified dependencies and initializes Prometheus metrics.</p>"},{"location":"references/services/alert_reference/#health","title":"Health","text":"<pre><code>func (s *Server) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>Performs comprehensive health checks on the Alert Service. If <code>checkLiveness</code> is true, only performs basic liveness checks. Otherwise, performs readiness checks on all dependencies:</p> <ul> <li>BlockchainClient</li> <li>FSM (Finite State Machine)</li> <li>BlockassemblyClient</li> <li>UTXOStore</li> </ul> <p>Returns HTTP status code, details message, and any error encountered.</p>"},{"location":"references/services/alert_reference/#healthgrpc","title":"HealthGRPC","text":"<pre><code>func (s *Server) HealthGRPC(ctx context.Context, _ *emptypb.Empty) (*alert_api.HealthResponse, error)\n</code></pre> <p>Performs a gRPC health check on the Alert Service and returns a structured response with timestamp.</p>"},{"location":"references/services/alert_reference/#init","title":"Init","text":"<pre><code>func (s *Server) Init(ctx context.Context) (err error)\n</code></pre> <p>Initializes the Alert Service by loading configuration. Returns a configuration error if initialization fails.</p>"},{"location":"references/services/alert_reference/#start","title":"Start","text":"<pre><code>func (s *Server) Start(ctx context.Context, readyCh chan&lt;- struct{}) (err error)\n</code></pre> <p>Starts the Alert Service by:</p> <ol> <li>Creating genesis alert in database</li> <li>Verifying RPC connection (unless disabled)</li> <li>Creating and starting P2P server</li> <li>Waiting for shutdown signal</li> </ol>"},{"location":"references/services/alert_reference/#stop","title":"Stop","text":"<pre><code>func (s *Server) Stop(ctx context.Context) error\n</code></pre> <p>Gracefully stops the Alert Service by closing all configurations and shutting down the P2P server.</p>"},{"location":"references/services/alert_reference/#node-methods","title":"Node Methods","text":""},{"location":"references/services/alert_reference/#newnodeconfig","title":"NewNodeConfig","text":"<pre><code>func NewNodeConfig(logger ulogger.Logger, blockchainClient blockchain.ClientI, utxoStore utxo.Store, blockassemblyClient *blockassembly.Client, peerClient peer.ClientI, p2pClient p2p.ClientI, tSettings *settings.Settings) config.NodeInterface\n</code></pre> <p>Creates a new instance of the <code>Node</code> with the specified dependencies.</p>"},{"location":"references/services/alert_reference/#bestblockhash","title":"BestBlockHash","text":"<pre><code>func (n *Node) BestBlockHash(ctx context.Context) (string, error)\n</code></pre> <p>Retrieves the hash of the best block in the blockchain.</p>"},{"location":"references/services/alert_reference/#invalidateblock","title":"InvalidateBlock","text":"<pre><code>func (n *Node) InvalidateBlock(ctx context.Context, blockHashStr string) error\n</code></pre> <p>Invalidates a block in the blockchain using its hash.</p>"},{"location":"references/services/alert_reference/#rpc-interface-methods","title":"RPC Interface Methods","text":"<pre><code>func (n *Node) GetRPCHost() string\nfunc (n *Node) GetRPCPassword() string\nfunc (n *Node) GetRPCUser() string\n</code></pre> <p>Interface methods for accessing RPC connection details. Currently return empty strings.</p>"},{"location":"references/services/alert_reference/#peer-management","title":"Peer Management","text":"<pre><code>func (n *Node) BanPeer(ctx context.Context, peer string) error\nfunc (n *Node) UnbanPeer(ctx context.Context, peer string) error\n</code></pre> <p>Methods for managing peer bans. BanPeer adds the peer's IP address to the ban list for both p2p and legacy peers for a duration of 100 years. UnbanPeer removes the peer from the ban list.</p>"},{"location":"references/services/alert_reference/#consensus-management","title":"Consensus Management","text":"<pre><code>func (n *Node) AddToConsensusBlacklist(ctx context.Context, funds []models.Fund) (*models.AddToConsensusBlacklistResponse, error)\n</code></pre> <p>Adds funds to the consensus blacklist, setting specified UTXOs as un-spendable. Supports both freezing and unfreezing based on enforcement height.</p> <pre><code>func (n *Node) AddToConfiscationTransactionWhitelist(ctx context.Context, txs []models.ConfiscationTransactionDetails) (*models.AddToConfiscationTransactionWhitelistResponse, error)\n</code></pre> <p>Re-assigns UTXOs to confiscation transactions, allowing them to be spent.</p>"},{"location":"references/services/alert_reference/#helper-methods","title":"Helper Methods","text":"<pre><code>func (n *Node) getAddToConsensusBlacklistResponse(fund models.Fund, err error) models.AddToConsensusBlacklistNotProcessed\n</code></pre> <p>Creates a standardized response for failed blacklist operations. Formats consistent response objects for UTXOs that could not be blacklisted, including the original fund details and the specific error that prevented processing.</p> <pre><code>func (n *Node) getAddToConfiscationTransactionWhitelistResponse(txID string, err error) models.AddToConfiscationTransactionWhitelistNotProcessed\n</code></pre> <p>Creates a standardized response for failed whitelist operations. Formats consistent response objects for confiscation transactions that could not be whitelisted, including the transaction ID and the specific error that prevented processing.</p> <pre><code>func extractPublicKey(scriptSig []byte) ([]byte, error)\n</code></pre> <p>Extracts the public key from a P2PKH scriptSig. Parses the unlocking script from a transaction input to extract the public key used for signing. Handles the standard P2PKH script format: [signature] [public key].</p>"},{"location":"references/services/alert_reference/#configuration","title":"Configuration","text":"<p>The Alert Service uses a configuration structure (<code>config.Config</code>) that includes:</p>"},{"location":"references/services/alert_reference/#core-settings","title":"Core Settings","text":"<ul> <li>Alert processing interval (default: 5 minutes)</li> <li>Request logging</li> <li>Genesis keys (required)</li> </ul>"},{"location":"references/services/alert_reference/#datastore-configuration","title":"Datastore Configuration","text":"<ul> <li>Auto-migration settings</li> <li> <p>Support for:</p> <ul> <li>SQLite (including in-memory)</li> <li>PostgreSQL</li> <li>MySQL</li> </ul> </li> <li> <p>Connection pooling options</p> </li> <li>Debug settings</li> </ul>"},{"location":"references/services/alert_reference/#p2p-configuration","title":"P2P Configuration","text":"<ul> <li>IP and port settings</li> <li>DHT mode</li> <li>Protocol ID (defaults to system default if not specified)</li> <li>Topic name (network-dependent: \"bitcoin_alert_system_[network]\" for non-mainnet)</li> <li>Private key management</li> <li>Peer discovery interval</li> </ul>"},{"location":"references/services/alert_reference/#error-types","title":"Error Types","text":"<p>Common configuration errors include:</p> <ul> <li><code>ErrNoGenesisKeys</code>: No genesis keys provided</li> <li><code>ErrNoP2PIP</code>: Invalid P2P IP configuration</li> <li><code>ErrNoP2PPort</code>: Invalid P2P port configuration</li> <li><code>ErrDatastoreUnsupported</code>: Unsupported datastore type</li> </ul>"},{"location":"references/services/alert_reference/#health-checks","title":"Health Checks","text":"<p>The service implements comprehensive health checks:</p> <ul> <li>Liveness check: Basic service health</li> <li> <p>Readiness check: Dependency health including:</p> <ul> <li>Blockchain client</li> <li>FSM status</li> <li>Blockassembly client</li> <li>UTXO store</li> </ul> </li> </ul> <p>The health checks return appropriate HTTP status codes:</p> <ul> <li><code>200 OK</code>: Service is healthy</li> <li><code>503 Service Unavailable</code>: Service or dependencies are unhealthy</li> </ul>"},{"location":"references/services/alert_reference/#alert-datastore","title":"Alert Datastore","text":""},{"location":"references/services/alert_reference/#overview","title":"Overview","text":"<p>The Alert Service uses a persistent datastore to manage alert-related data, consensus state, and operational history. The datastore integrates with the <code>github.com/bitcoin-sv/alert-system</code> library to provide alert message processing, validation, and P2P network communication.</p>"},{"location":"references/services/alert_reference/#supported-database-backends","title":"Supported Database Backends","text":"<p>The alert datastore supports multiple database backends:</p> <ul> <li>SQLite (including in-memory mode)</li> <li>PostgreSQL</li> <li>MySQL</li> </ul> <p>Database selection is configured via the <code>StoreURL</code> setting in the Alert service configuration.</p>"},{"location":"references/services/alert_reference/#alert-data-models","title":"Alert Data Models","text":""},{"location":"references/services/alert_reference/#1-genesis-alert","title":"1. Genesis Alert","text":"<p>Purpose: Bootstrap alert created during service initialization</p> <ul> <li>Function: <code>models.CreateGenesisAlert()</code> creates this foundational alert</li> <li>Storage: Stored in the database during startup to establish the alert system baseline</li> <li>Role: Provides the initial state for the alert consensus system</li> </ul>"},{"location":"references/services/alert_reference/#2-alert-messages","title":"2. Alert Messages","text":"<p>Core alert data structure containing:</p> <ul> <li>Alert ID: Unique identifier for each alert</li> <li>Alert Type: Type of alert operation<ul> <li>UTXO freeze</li> <li>UTXO unfreeze</li> <li>UTXO reassignment</li> <li>Peer ban/unban</li> <li>Block invalidation</li> </ul> </li> <li>Timestamp: When the alert was created/received</li> <li>Status: Alert processing status (pending, processed, failed)</li> <li>Payload: Alert-specific data payload</li> <li>Signatures: Cryptographic signatures for alert validation</li> </ul>"},{"location":"references/services/alert_reference/#3-utxo-related-alert-data","title":"3. UTXO-Related Alert Data","text":"<p>For UTXO freeze, unfreeze, and reassignment operations:</p> <ul> <li> <p>UTXO Identifiers:</p> <ul> <li>Transaction hashes (txid)</li> <li>Output indices (vout)</li> <li>Block Height: Target block height for UTXO operations</li> <li>Operation Type: Freeze, unfreeze, or reassign</li> <li>New Address: For UTXO reassignment operations (destination address)</li> <li>Execution Status: Whether the UTXO operation has been applied</li> </ul> </li> </ul>"},{"location":"references/services/alert_reference/#4-peer-management-alert-data","title":"4. Peer Management Alert Data","text":"<p>For peer banning and unbanning operations:</p> <ul> <li>Peer Addresses: IP addresses with optional netmasks</li> <li>Ban Duration: How long peers should remain banned (typically 100 years for permanent bans)</li> <li>Ban Reason: Textual description of why the peer was banned</li> <li>Ban Status: Active, expired, or lifted</li> <li>Target Networks: Both P2P and Legacy peer networks</li> </ul>"},{"location":"references/services/alert_reference/#5-block-invalidation-alert-data","title":"5. Block Invalidation Alert Data","text":"<p>For block invalidation operations:</p> <ul> <li>Block Hash: Hash of the block to be invalidated</li> <li>Invalidation Reason: Why the block should be invalidated</li> <li>Recovery Actions: Instructions for handling transactions from invalidated blocks</li> <li>Re-validation Status: Status of transaction re-validation process</li> </ul>"},{"location":"references/services/alert_reference/#6-alert-consensus-data","title":"6. Alert Consensus Data","text":"<p>For alert validation and consensus:</p> <ul> <li>Consensus State: Current consensus status of alerts</li> <li>Validation Results: Results of alert validation processes</li> <li>Participant Signatures: Signatures from network participants</li> <li>Consensus Threshold: Required consensus level for alert execution</li> </ul>"},{"location":"references/services/alert_reference/#7-p2p-network-alert-data","title":"7. P2P Network Alert Data","text":"<p>For P2P network communication:</p> <ul> <li>Topic Subscriptions: P2P topics the alert system subscribes to</li> <li>Network Participants: Other nodes participating in the alert network</li> <li>Message History: Historical alert messages received from the P2P network</li> <li>Network Status: Connection status to the private alert P2P network</li> </ul>"},{"location":"references/services/alert_reference/#8-configuration-and-state-data","title":"8. Configuration and State Data","text":"<p>Operational configuration and state:</p> <ul> <li>Alert Processing Interval: How frequently alerts are processed (default: 5 minutes)</li> <li>Network Configuration: Network-specific settings (mainnet vs testnet)</li> <li>Service State: Current operational state of the alert service</li> <li>Last Processed: Timestamps of last processed alerts by type</li> </ul>"},{"location":"references/services/alert_reference/#database-configuration","title":"Database Configuration","text":""},{"location":"references/services/alert_reference/#connection-settings","title":"Connection Settings","text":"<p>The datastore connection is configured via the <code>StoreURL</code> setting:</p> <pre><code># SQLite example\nStoreURL: sqlite://path/to/alert.db\n\n# PostgreSQL example\nStoreURL: postgres://user:password@host:port/database?sslmode=disable\n\n# MySQL example\nStoreURL: mysql://user:password@host:port/database\n</code></pre>"},{"location":"references/services/alert_reference/#auto-migration","title":"Auto-Migration","text":"<p>The Alert service supports automatic database schema migration:</p> <ul> <li>Enabled: When <code>Datastore.AutoMigrate</code> is true</li> <li>Models: Uses models from the <code>github.com/bitcoin-sv/alert-system</code> library</li> <li>Safety: Migrations are applied during service startup</li> </ul>"},{"location":"references/services/alert_reference/#ssl-configuration","title":"SSL Configuration","text":"<p>For PostgreSQL and MySQL connections:</p> <ul> <li>SSL Mode: Configurable via query parameter <code>sslmode</code></li> <li>Default: <code>disable</code> for development, <code>require</code> recommended for production</li> <li>Certificates: Standard SSL certificate configuration supported</li> </ul>"},{"location":"references/services/alert_reference/#data-flow-and-lifecycle","title":"Data Flow and Lifecycle","text":""},{"location":"references/services/alert_reference/#alert-reception-and-storage","title":"Alert Reception and Storage","text":"<ol> <li>Alert Reception: Alerts received from private P2P network</li> <li>Validation: Alert signatures and consensus validation</li> <li>Storage: Alert data stored in datastore for persistence</li> <li>Processing: Alerts processed according to their type and timing</li> <li>Execution: Alert actions executed (UTXO operations, peer bans, etc.)</li> <li>State Updates: Alert processing state updated in datastore</li> </ol>"},{"location":"references/services/alert_reference/#data-retention","title":"Data Retention","text":"<ul> <li>Alert History: All alert messages are retained for audit purposes</li> <li>Consensus Data: Consensus validation data is preserved</li> <li>Cleanup: No automatic cleanup - manual maintenance may be required</li> </ul>"},{"location":"references/services/asset_reference/","title":"Asset Service Reference Documentation","text":""},{"location":"references/services/asset_reference/#types","title":"Types","text":""},{"location":"references/services/asset_reference/#server-structure","title":"Server Structure","text":"<pre><code>type Server struct {\n    logger              ulogger.Logger\n    settings            *settings.Settings\n    utxoStore           utxo.Store\n    txStore             blob.Store\n    subtreeStore        blob.Store\n    blockPersisterStore blob.Store\n    httpAddr            string\n    httpServer          *httpimpl.HTTP\n    centrifugeAddr      string\n    centrifugeServer    *centrifuge_impl.Centrifuge\n    blockchainClient    blockchain.ClientI\n}\n</code></pre> <p>The <code>Server</code> type is the main structure for the Asset Service. It coordinates between different storage backends and provides both HTTP and Centrifuge interfaces for accessing blockchain data.</p>"},{"location":"references/services/asset_reference/#repository-structure","title":"Repository Structure","text":"<pre><code>type Repository struct {\n    logger              ulogger.Logger\n    settings            *settings.Settings\n    UtxoStore           utxo.Store\n    TxStore             blob.Store\n    SubtreeStore        blob.Store\n    BlockPersisterStore blob.Store\n    BlockchainClient    blockchain.ClientI\n}\n</code></pre> <p>The <code>Repository</code> type provides access to blockchain data storage and retrieval operations. It implements the necessary interfaces to interact with various data stores and blockchain clients.</p>"},{"location":"references/services/asset_reference/#http-structure","title":"HTTP Structure","text":"<pre><code>type HTTP struct {\n    logger     ulogger.Logger\n    settings   *settings.Settings\n    repository repository.Interface\n    e          *echo.Echo\n    startTime  time.Time\n    privKey    crypto.PrivKey\n}\n</code></pre> <p>The <code>HTTP</code> type represents the HTTP server for the Asset Service.</p>"},{"location":"references/services/asset_reference/#functions","title":"Functions","text":""},{"location":"references/services/asset_reference/#server-functions","title":"Server Functions","text":""},{"location":"references/services/asset_reference/#newserver","title":"NewServer","text":"<pre><code>func NewServer(logger ulogger.Logger, tSettings *settings.Settings, utxoStore utxo.Store, txStore blob.Store, subtreeStore blob.Store, blockPersisterStore blob.Store, blockchainClient blockchain.ClientI) *Server\n</code></pre> <p>Creates a new instance of the <code>Server</code> with the provided dependencies. It initializes the server with necessary stores and clients for handling blockchain data.</p>"},{"location":"references/services/asset_reference/#health","title":"Health","text":"<pre><code>func (v *Server) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>Performs health checks on the server and its dependencies. It supports both liveness and readiness checks based on the checkLiveness parameter.</p>"},{"location":"references/services/asset_reference/#init","title":"Init","text":"<pre><code>func (v *Server) Init(ctx context.Context) (err error)\n</code></pre> <p>Initializes the server by setting up HTTP and Centrifuge endpoints. It configures the necessary components based on the provided configuration.</p>"},{"location":"references/services/asset_reference/#start","title":"Start","text":"<pre><code>func (v *Server) Start(ctx context.Context, readyCh chan&lt;- struct{}) error\n</code></pre> <p>Starts the Asset Service, launching HTTP and Centrifuge servers if configured. It also handles FSM state restoration if required. The readyCh channel is closed when the service is ready to receive requests.</p>"},{"location":"references/services/asset_reference/#stop","title":"Stop","text":"<pre><code>func (v *Server) Stop(ctx context.Context) error\n</code></pre> <p>Gracefully shuts down the server and its components, including HTTP and Centrifuge servers.</p>"},{"location":"references/services/asset_reference/#repository-functions","title":"Repository Functions","text":""},{"location":"references/services/asset_reference/#newrepository","title":"NewRepository","text":"<pre><code>func NewRepository(logger ulogger.Logger, tSettings *settings.Settings, utxoStore utxo.Store, txStore blob.Store, blockchainClient blockchain.ClientI, subtreeStore blob.Store, blockPersisterStore blob.Store) (*Repository, error)\n</code></pre> <p>Creates a new instance of the <code>Repository</code> with the provided dependencies. It initializes connections to various data stores for blockchain data access.</p>"},{"location":"references/services/asset_reference/#gettransaction","title":"GetTransaction","text":"<pre><code>func (repo *Repository) GetTransaction(ctx context.Context, hash *chainhash.Hash) ([]byte, error)\n</code></pre> <p>Retrieves a transaction by its hash.</p>"},{"location":"references/services/asset_reference/#getblockstats","title":"GetBlockStats","text":"<pre><code>func (repo *Repository) GetBlockStats(ctx context.Context) (*model.BlockStats, error)\n</code></pre> <p>Retrieves block statistics.</p>"},{"location":"references/services/asset_reference/#getblockgraphdata","title":"GetBlockGraphData","text":"<pre><code>func (repo *Repository) GetBlockGraphData(ctx context.Context, periodMillis uint64) (*model.BlockDataPoints, error)\n</code></pre> <p>Retrieves block graph data for a specified period.</p>"},{"location":"references/services/asset_reference/#getblockbyhash","title":"GetBlockByHash","text":"<pre><code>func (repo *Repository) GetBlockByHash(ctx context.Context, hash *chainhash.Hash) (*model.Block, error)\n</code></pre> <p>Retrieves a block by its hash.</p>"},{"location":"references/services/asset_reference/#getblockbyheight","title":"GetBlockByHeight","text":"<pre><code>func (repo *Repository) GetBlockByHeight(ctx context.Context, height uint32) (*model.Block, error)\n</code></pre> <p>Retrieves a block by its height.</p>"},{"location":"references/services/asset_reference/#getblockheader","title":"GetBlockHeader","text":"<pre><code>func (repo *Repository) GetBlockHeader(ctx context.Context, hash *chainhash.Hash) (*model.BlockHeader, *model.BlockHeaderMeta, error)\n</code></pre> <p>Retrieves a block header by its hash.</p>"},{"location":"references/services/asset_reference/#getlastnblocks","title":"GetLastNBlocks","text":"<pre><code>func (repo *Repository) GetLastNBlocks(ctx context.Context, n int64, includeOrphans bool, fromHeight uint32) ([]*model.BlockInfo, error)\n</code></pre> <p>Retrieves the last N blocks.</p>"},{"location":"references/services/asset_reference/#getblockheaders","title":"GetBlockHeaders","text":"<pre><code>func (repo *Repository) GetBlockHeaders(ctx context.Context, hash *chainhash.Hash, numberOfHeaders uint64) ([]*model.BlockHeader, []*model.BlockHeaderMeta, error)\n</code></pre> <p>Retrieves a sequence of block headers starting from a specific hash.</p>"},{"location":"references/services/asset_reference/#getblockheaderstocommonancestor","title":"GetBlockHeadersToCommonAncestor","text":"<pre><code>func (repo *Repository) GetBlockHeadersToCommonAncestor(ctx context.Context, hashTarget *chainhash.Hash, blockLocatorHashes []*chainhash.Hash, maxHeaders uint32) ([]*model.BlockHeader, []*model.BlockHeaderMeta, error)\n</code></pre> <p>Retrieves block headers from the target hash back to the common ancestor with the provided block locator.</p>"},{"location":"references/services/asset_reference/#getblockheadersfromheight","title":"GetBlockHeadersFromHeight","text":"<pre><code>func (repo *Repository) GetBlockHeadersFromHeight(ctx context.Context, height, limit uint32) ([]*model.BlockHeader, []*model.BlockHeaderMeta, error)\n</code></pre> <p>Retrieves block headers starting from a specific height up to the specified limit.</p>"},{"location":"references/services/asset_reference/#getsubtree","title":"GetSubtree","text":"<pre><code>func (repo *Repository) GetSubtree(ctx context.Context, hash *chainhash.Hash) (*util.Subtree, error)\n</code></pre> <p>Retrieves a subtree by its hash.</p>"},{"location":"references/services/asset_reference/#getsubtreebytes","title":"GetSubtreeBytes","text":"<pre><code>func (repo *Repository) GetSubtreeBytes(ctx context.Context, hash *chainhash.Hash) ([]byte, error)\n</code></pre> <p>Retrieves the raw bytes of a subtree.</p>"},{"location":"references/services/asset_reference/#getsubtreereader","title":"GetSubtreeReader","text":"<pre><code>func (repo *Repository) GetSubtreeReader(ctx context.Context, hash *chainhash.Hash) (io.ReadCloser, error)\n</code></pre> <p>Provides a reader interface for accessing subtree data.</p>"},{"location":"references/services/asset_reference/#getsubtreedatareader","title":"GetSubtreeDataReader","text":"<pre><code>func (repo *Repository) GetSubtreeDataReader(ctx context.Context, hash *chainhash.Hash) (io.ReadCloser, error)\n</code></pre> <p>Provides a reader interface for accessing subtree data from the block persister.</p>"},{"location":"references/services/asset_reference/#getsubtreeexists","title":"GetSubtreeExists","text":"<pre><code>func (repo *Repository) GetSubtreeExists(ctx context.Context, hash *chainhash.Hash) (bool, error)\n</code></pre> <p>Checks if a subtree with the given hash exists in the store.</p>"},{"location":"references/services/asset_reference/#getsubtreehead","title":"GetSubtreeHead","text":"<pre><code>func (repo *Repository) GetSubtreeHead(ctx context.Context, hash *chainhash.Hash) (*util.Subtree, int, error)\n</code></pre> <p>Retrieves only the head portion of a subtree, containing fees and size information.</p>"},{"location":"references/services/asset_reference/#getutxo","title":"GetUtxo","text":"<pre><code>func (repo *Repository) GetUtxo(ctx context.Context, spend *utxo.Spend) (*utxo.SpendResponse, error)\n</code></pre> <p>Retrieves UTXO information.</p>"},{"location":"references/services/asset_reference/#getbestblockheader","title":"GetBestBlockHeader","text":"<pre><code>func (repo *Repository) GetBestBlockHeader(ctx context.Context) (*model.BlockHeader, *model.BlockHeaderMeta, error)\n</code></pre> <p>Retrieves the best (most recent) block header.</p>"},{"location":"references/services/asset_reference/#getblocklocator","title":"GetBlockLocator","text":"<pre><code>func (repo *Repository) GetBlockLocator(ctx context.Context, blockHeaderHash *chainhash.Hash, height uint32) ([]*chainhash.Hash, error)\n</code></pre> <p>Retrieves a sequence of block hashes at exponentially increasing distances back from the provided block hash or the best block if no hash is specified.</p>"},{"location":"references/services/asset_reference/#getbalance","title":"GetBalance","text":"<pre><code>func (repo *Repository) GetBalance(ctx context.Context) (uint64, uint64, error)\n</code></pre> <p>Retrieves the balance information.</p>"},{"location":"references/services/asset_reference/#getblockforks","title":"GetBlockForks","text":"<pre><code>func (repo *Repository) GetBlockForks(ctx context.Context, hash *chainhash.Hash) (*model.ForkInfo, error)\n</code></pre> <p>Retrieves information about forks related to the specified block.</p>"},{"location":"references/services/asset_reference/#getblocksubtrees","title":"GetBlockSubtrees","text":"<pre><code>func (repo *Repository) GetBlockSubtrees(ctx context.Context, hash *chainhash.Hash) ([]*util.Subtree, error)\n</code></pre> <p>Retrieves all subtrees included in the specified block.</p>"},{"location":"references/services/asset_reference/#http-functions","title":"HTTP Functions","text":""},{"location":"references/services/asset_reference/#new","title":"New","text":"<pre><code>func New(logger ulogger.Logger, tSettings *settings.Settings, repo *repository.Repository) (*HTTP, error)\n</code></pre> <p>Creates a new instance of the HTTP server.</p>"},{"location":"references/services/asset_reference/#http-init","title":"HTTP Init","text":"<pre><code>func (h *HTTP) Init(_ context.Context) error\n</code></pre> <p>Initializes the HTTP server.</p>"},{"location":"references/services/asset_reference/#http-start","title":"HTTP Start","text":"<pre><code>func (h *HTTP) Start(ctx context.Context, addr string) error\n</code></pre> <p>Starts the HTTP server.</p>"},{"location":"references/services/asset_reference/#http-stop","title":"HTTP Stop","text":"<pre><code>func (h *HTTP) Stop(ctx context.Context) error\n</code></pre> <p>Stops the HTTP server.</p>"},{"location":"references/services/asset_reference/#addhttphandler","title":"AddHTTPHandler","text":"<pre><code>func (h *HTTP) AddHTTPHandler(pattern string, handler http.Handler) error\n</code></pre> <p>Adds a new HTTP handler to the server.</p>"},{"location":"references/services/asset_reference/#sign","title":"Sign","text":"<pre><code>func (h *HTTP) Sign(resp *echo.Response, hash []byte) error\n</code></pre> <p>Signs the HTTP response.</p>"},{"location":"references/services/asset_reference/#configuration","title":"Configuration","text":"<p>The Asset Service uses configuration values from the <code>gocore.Config()</code> function, including:</p>"},{"location":"references/services/asset_reference/#network-and-api-configuration","title":"Network and API Configuration","text":"<ul> <li><code>asset_httpListenAddress</code>: HTTP listen address (default: \":8090\")</li> <li><code>asset_apiPrefix</code>: API prefix (default: \"/api/v1\")</li> <li><code>securityLevelHTTP</code>: Security level for HTTP (0 for HTTP, non-zero for HTTPS)</li> <li><code>server_certFile</code>: Certificate file for HTTPS</li> <li><code>server_keyFile</code>: Key file for HTTPS</li> </ul>"},{"location":"references/services/asset_reference/#centrifuge-configuration-real-time-updates","title":"Centrifuge Configuration (Real-time Updates)","text":"<ul> <li><code>asset_centrifuge_disable</code>: Whether to disable Centrifuge server (default: false)</li> <li><code>asset_centrifugeListenAddress</code>: Centrifuge listen address (default: \":8000\")</li> </ul>"},{"location":"references/services/asset_reference/#security","title":"Security","text":"<ul> <li><code>http_sign_response</code>: Whether to sign HTTP responses (default: false)</li> <li><code>p2p_private_key</code>: Private key for signing responses</li> </ul>"},{"location":"references/services/asset_reference/#dashboard-configuration","title":"Dashboard Configuration","text":"<ul> <li><code>dashboard.enabled</code>: Whether to enable the web dashboard (default: false)</li> <li><code>dashboard.auth.enabled</code>: Whether to enable authentication for the dashboard (default: true)</li> <li><code>dashboard.auth.username</code>: Dashboard admin username</li> <li><code>dashboard.auth.password</code>: Dashboard admin password (stored as hash)</li> </ul>"},{"location":"references/services/asset_reference/#debug-configuration","title":"Debug Configuration","text":"<ul> <li><code>asset_echoDebug</code>: Enable debug logging for HTTP server (default: false)</li> <li><code>statsPrefix</code>: Prefix for stats endpoints (default: \"/debug/\")</li> </ul> <p>The Asset Service dashboard provides a visual interface for monitoring blockchain status, viewing blocks and transactions, and managing the node. When enabled, it provides:</p> <ol> <li>Real-time blockchain statistics</li> <li>Block explorer functionality</li> <li>Transaction viewer</li> <li>FSM state visualization and control</li> <li>Node management capabilities including block invalidation/revalidation</li> </ol> <p>The dashboard is particularly useful for monitoring the unique subtree-based transaction management system used in Teranode, which replaces the traditional mempool architecture found in other Bitcoin implementations.</p>"},{"location":"references/services/asset_reference/#dependencies","title":"Dependencies","text":"<p>The Asset Service depends on several other components and services:</p> <ul> <li>UTXO Store</li> <li>Transaction Store</li> <li>Subtree Store</li> <li>Block Persister Store</li> <li>Blockchain Client</li> <li>Coinbase API Client (optional)</li> </ul> <p>These dependencies are injected into the <code>Server</code> and <code>Repository</code> structures during initialization.</p>"},{"location":"references/services/asset_reference/#api-endpoints","title":"API Endpoints","text":"<p>The Asset Service provides various API endpoints for interacting with blockchain data. These endpoints are organized by category and support different response formats for flexibility.</p> <p>All API endpoints are prefixed with <code>/api/v1</code> unless otherwise specified.</p>"},{"location":"references/services/asset_reference/#response-formats","title":"Response Formats","text":"<p>Most endpoints support multiple response formats:</p> <ul> <li>BINARY_STREAM: Raw binary data (<code>application/octet-stream</code>)</li> <li>HEX: Hexadecimal string representation (<code>text/plain</code>)</li> <li>JSON: Structured JSON data (<code>application/json</code>)</li> </ul> <p>The format can be selected by appending <code>/hex</code> or <code>/json</code> to the endpoint, or by setting the appropriate <code>Accept</code> header. If not specified, the binary format is used as the default.</p>"},{"location":"references/services/asset_reference/#error-handling","title":"Error Handling","text":"<p>All endpoints return appropriate HTTP status codes to indicate success or failure:</p> <ul> <li>200 OK: Request successful</li> <li>400 Bad Request: Invalid input parameters</li> <li>404 Not Found: Resource not found</li> <li>500 Internal Server Error: Server-side error</li> </ul> <p>Error responses include a JSON object with an error message:</p> <pre><code>{\n  \"error\": \"Error message description\"\n}\n</code></pre>"},{"location":"references/services/asset_reference/#health-and-status-endpoints","title":"Health and Status Endpoints","text":"<ul> <li> <p>GET <code>/alive</code></p> <ul> <li>Purpose: Service liveness check</li> <li>Returns: Text message with uptime information</li> <li>Status Code: 200 on success</li> </ul> </li> <li> <p>GET <code>/health</code></p> <ul> <li>Purpose: Service health check with dependency status</li> <li>Returns: Status information and dependency health</li> <li>Status Code: 200 on success, 503 on failure</li> </ul> </li> </ul>"},{"location":"references/services/asset_reference/#transaction-endpoints","title":"Transaction Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/tx/:hash</code></p> <ul> <li>Purpose: Get transaction in binary format</li> <li>Parameters: <code>hash</code> - Transaction ID hash (hex string)</li> <li>Returns: Transaction data (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/tx/:hash/hex</code></p> <ul> <li>Purpose: Get transaction in hex format</li> <li>Parameters: <code>hash</code> - Transaction ID hash (hex string)</li> <li>Returns: Transaction data (hex string)</li> </ul> </li> <li> <p>GET <code>/api/v1/tx/:hash/json</code></p> <ul> <li>Purpose: Get transaction in JSON format</li> <li>Parameters: <code>hash</code> - Transaction ID hash (hex string)</li> <li>Returns: Transaction data (JSON)</li> </ul> </li> <li> <p>POST <code>/api/v1/subtree/:hash/txs</code></p> <ul> <li>Purpose: Batch retrieve multiple transactions</li> <li>Request Body: Concatenated 32-byte transaction hashes</li> <li>Returns: Concatenated transactions (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/txmeta/:hash/json</code></p> <ul> <li>Purpose: Get transaction metadata</li> <li>Parameters: <code>hash</code> - Transaction ID hash (hex string)</li> <li>Returns: Transaction metadata (JSON)</li> </ul> </li> <li> <p>GET <code>/api/v1/txmeta_raw/:hash</code></p> <ul> <li>Purpose: Get raw transaction metadata (binary)</li> <li>Parameters: <code>hash</code> - Transaction ID hash (hex string)</li> <li>Returns: Raw transaction metadata</li> </ul> </li> <li> <p>GET <code>/api/v1/txmeta_raw/:hash/hex</code></p> <ul> <li>Purpose: Get raw transaction metadata (hex)</li> <li>Parameters: <code>hash</code> - Transaction ID hash (hex string)</li> <li>Returns: Raw transaction metadata (hex string)</li> </ul> </li> <li> <p>GET <code>/api/v1/txmeta_raw/:hash/json</code></p> <ul> <li>Purpose: Get raw transaction metadata (JSON)</li> <li>Parameters: <code>hash</code> - Transaction ID hash (hex string)</li> <li>Returns: Raw transaction metadata (JSON)</li> </ul> </li> </ul>"},{"location":"references/services/asset_reference/#block-endpoints","title":"Block Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/block/:hash</code></p> <ul> <li>Purpose: Get block by hash (binary)</li> <li>Parameters: <code>hash</code> - Block hash (hex string)</li> <li>Returns: Block data (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/block/:hash/hex</code></p> <ul> <li>Purpose: Get block by hash (hex)</li> <li>Parameters: <code>hash</code> - Block hash (hex string)</li> <li>Returns: Block data (hex string)</li> </ul> </li> <li> <p>GET <code>/api/v1/block/:hash/json</code></p> <ul> <li>Purpose: Get block by hash (JSON)</li> <li>Parameters: <code>hash</code> - Block hash (hex string)</li> <li>Returns: Block data (JSON)</li> </ul> </li> <li> <p>GET <code>/api/v1/block/:hash/forks</code></p> <ul> <li>Purpose: Get fork information for a block</li> <li>Parameters: <code>hash</code> - Block hash (hex string)</li> <li>Returns: Fork data (JSON)</li> </ul> </li> <li> <p>GET <code>/api/v1/blocks</code></p> <ul> <li>Purpose: Get paginated blocks list</li> <li>Parameters: <code>offset</code>, <code>limit</code> (optional)</li> <li>Returns: Blocks list (JSON)</li> </ul> </li> <li> <p>GET <code>/api/v1/blocks/:hash</code></p> <ul> <li>Purpose: Get multiple blocks starting with specified hash (binary)</li> <li>Parameters: <code>hash</code> - Starting block hash, <code>n</code> - Number of blocks (optional)</li> <li>Returns: Block data (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/lastblocks</code></p> <ul> <li>Purpose: Get most recent blocks</li> <li>Parameters: <code>n</code> (optional) - Number of blocks, <code>includeorphans</code> (optional), <code>height</code> (optional)</li> <li>Returns: Recent blocks data (JSON)</li> </ul> </li> <li> <p>GET <code>/api/v1/blockstats</code></p> <ul> <li>Purpose: Get block statistics</li> <li>Returns: Block statistics (JSON)</li> </ul> </li> <li> <p>GET <code>/api/v1/blockgraphdata/:period</code></p> <ul> <li>Purpose: Get time-series block data for graphing</li> <li>Parameters: <code>period</code> - Time period in milliseconds</li> <li>Returns: Time-series data (JSON)</li> </ul> </li> <li> <p>GET <code>/rest/block/:hash.bin</code></p> <ul> <li>Purpose: Legacy endpoint for block retrieval</li> <li>Parameters: <code>hash</code> - Block hash (hex string)</li> <li>Returns: Block data (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/block_legacy/:hash</code></p> <ul> <li>Purpose: Alternative legacy block retrieval</li> <li>Parameters: <code>hash</code> - Block hash (hex string)</li> <li>Returns: Block data (binary)</li> </ul> </li> </ul>"},{"location":"references/services/asset_reference/#block-header-endpoints","title":"Block Header Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/header/:hash</code></p> <ul> <li>Purpose: Get single block header (binary)</li> <li>Parameters: <code>hash</code> - Block hash (hex string)</li> <li>Returns: Block header (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/headers/:hash</code></p> <ul> <li>Purpose: Get multiple headers starting from hash (binary)</li> <li>Parameters: <code>hash</code> - Starting block hash, <code>n</code> - Number of headers (optional)</li> <li>Returns: Block headers (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/headers_to_common_ancestor/:hash</code></p> <ul> <li>Purpose: Get headers to common ancestor (binary)</li> <li>Parameters: <code>hash</code> - Target hash, <code>locator</code> - Comma-separated block hashes</li> <li>Returns: Block headers (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/bestblockheader</code></p> <ul> <li>Purpose: Get best block header (binary)</li> <li>Returns: Best block header (binary)</li> </ul> </li> </ul>"},{"location":"references/services/asset_reference/#block-management-endpoints","title":"Block Management Endpoints","text":"<ul> <li> <p>POST <code>/api/v1/block/invalidate</code></p> <ul> <li>Purpose: Mark a block as invalid, forcing a chain reorganization</li> <li>Parameters: JSON object in request body with block hash information</li> </ul> <pre><code>{\n  \"hash\": \"000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f\"\n}\n</code></pre> <ul> <li>Returns: JSON object with status of the invalidation operation</li> <li>Security: This is an administrative operation that can affect blockchain consensus</li> </ul> </li> <li> <p>POST <code>/api/v1/block/revalidate</code></p> <ul> <li>Purpose: Reconsider a previously invalidated block</li> <li>Parameters: JSON object in request body with block hash information</li> </ul> <pre><code>{\n  \"hash\": \"000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f\"\n}\n</code></pre> <ul> <li>Returns: JSON object with status of the revalidation operation</li> <li>Security: This is an administrative operation that can affect blockchain consensus</li> </ul> </li> <li> <p>GET <code>/api/v1/blocks/invalid</code></p> <ul> <li>Purpose: Retrieve a list of currently invalidated blocks</li> <li> <p>Parameters:</p> <ul> <li><code>limit</code> (optional): Maximum number of blocks to retrieve</li> </ul> </li> <li> <p>Returns: Array of invalid block information</p> </li> </ul> </li> </ul>"},{"location":"references/services/asset_reference/#finite-state-machine-fsm-endpoints","title":"Finite State Machine (FSM) Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/fsm/state</code></p> <ul> <li>Purpose: Get current blockchain FSM state</li> <li>Returns: JSON object with current state information including state name, metadata, and allowed transitions</li> <li>Example response:</li> </ul> <pre><code>{\n  \"state\": \"Running\",\n  \"metadata\": {\n    \"syncedHeight\": 700001,\n    \"bestHeight\": 700001,\n    \"isSynchronized\": true\n  },\n  \"allowedTransitions\": [\"stop\", \"pause\"]\n}\n</code></pre> </li> <li> <p>POST <code>/api/v1/fsm/state</code></p> <ul> <li>Purpose: Send an event to the blockchain FSM to trigger a state transition</li> <li>Parameters: JSON object in request body with event details</li> </ul> <pre><code>{\n  \"event\": \"pause\",\n  \"data\": {\n    \"reason\": \"maintenance\"\n  }\n}\n</code></pre> <ul> <li>Returns: JSON object with updated state information and transition result</li> <li>Security: This is an administrative operation that can affect blockchain operation</li> </ul> </li> <li> <p>GET <code>/api/v1/fsm/events</code></p> <ul> <li>Purpose: List all possible FSM events</li> <li>Returns: JSON array of available events with descriptions</li> </ul> </li> <li> <p>GET <code>/api/v1/fsm/states</code></p> <ul> <li>Purpose: List all possible FSM states</li> <li>Returns: JSON array of available states with descriptions</li> </ul> </li> </ul>"},{"location":"references/services/asset_reference/#utxo-endpoints","title":"UTXO Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/utxo/:hash</code></p> <ul> <li>Purpose: Get UTXO information (binary)</li> <li>Parameters: <code>hash</code> - Transaction hash, <code>vout</code> - Output index</li> <li>Returns: UTXO data (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/utxos/:hash/json</code></p> <ul> <li>Purpose: Get all UTXOs for a transaction</li> <li>Parameters: <code>hash</code> - Transaction hash</li> <li>Returns: UTXO data array (JSON)</li> </ul> </li> </ul>"},{"location":"references/services/asset_reference/#subtree-endpoints","title":"Subtree Endpoints","text":"<ul> <li> <p>GET <code>/api/v1/subtree/:hash</code></p> <ul> <li>Purpose: Get subtree data (binary)</li> <li>Parameters: <code>hash</code> - Subtree hash</li> <li>Returns: Subtree data (binary)</li> </ul> </li> <li> <p>GET <code>/api/v1/subtree/:hash/txs/json</code></p> <ul> <li>Purpose: Get transactions in a subtree</li> <li>Parameters: <code>hash</code> - Subtree hash</li> <li>Returns: Transaction data array (JSON)</li> </ul> </li> <li> <p>GET <code>/api/v1/block/:hash/subtrees/json</code></p> <ul> <li>Purpose: Get all subtrees for a block</li> <li>Parameters: <code>hash</code> - Block hash</li> <li>Returns: Subtree data array (JSON)</li> </ul> </li> </ul>"},{"location":"references/services/asset_reference/#search-endpoints","title":"Search Endpoints","text":"<ul> <li>GET <code>/api/v1/search</code><ul> <li>Purpose: Search for blockchain entities</li> <li>Parameters: <code>query</code> - Search term (hash or height)</li> <li>Returns: Search results (JSON)</li> </ul> </li> </ul>"},{"location":"references/services/asset_reference/#authentication","title":"Authentication","text":"<p>The service supports response signing. When enabled, responses include an <code>X-Signature</code> header containing an Ed25519 signature of the response data.</p>"},{"location":"references/services/asset_reference/#common-headers","title":"Common Headers","text":""},{"location":"references/services/asset_reference/#request-headers","title":"Request Headers","text":"<ul> <li><code>Content-Type</code>: Specifies the format of request data</li> <li><code>Accept</code>: Specifies the desired response format</li> </ul>"},{"location":"references/services/asset_reference/#response-headers","title":"Response Headers","text":"<ul> <li><code>Content-Type</code>: Indicates the format of response data</li> <li><code>X-Signature</code>: Ed25519 signature (when response signing is enabled)</li> </ul>"},{"location":"references/services/asset_reference/#pagination","title":"Pagination","text":"<p>Endpoints that return lists support pagination through query parameters:</p> <ul> <li><code>offset</code>: Number of items to skip (default: 0)</li> <li><code>limit</code>: Maximum number of items to return (default: 20, max: 100)</li> </ul> <p>Paginated responses include metadata:</p> <pre><code>{\n  \"data\": [...],\n  \"pagination\": {\n    \"offset\": 0,\n    \"limit\": 20,\n    \"totalRecords\": 100\n  }\n}\n</code></pre>"},{"location":"references/services/blockassembly_reference/","title":"Block Assembly Reference Documentation","text":""},{"location":"references/services/blockassembly_reference/#types","title":"Types","text":""},{"location":"references/services/blockassembly_reference/#blockassembly","title":"BlockAssembly","text":"<pre><code>type BlockAssembly struct {\n    // UnimplementedBlockAssemblyAPIServer provides default implementations for gRPC methods\n    blockassembly_api.UnimplementedBlockAssemblyAPIServer\n\n    // blockAssembler handles the core block assembly logic\n    blockAssembler *BlockAssembler\n\n    // logger provides logging functionality\n    logger ulogger.Logger\n\n    // stats tracks operational statistics\n    stats *gocore.Stat\n\n    // settings contains configuration parameters\n    settings *settings.Settings\n\n    // blockchainClient interfaces with the blockchain\n    blockchainClient blockchain.ClientI\n\n    // txStore manages transaction storage\n    txStore blob.Store\n\n    // utxoStore manages UTXO storage\n    utxoStore utxostore.Store\n\n    // subtreeStore manages subtree storage\n    subtreeStore blob.Store\n\n    // jobStore caches mining jobs with TTL\n    jobStore *ttlcache.Cache[chainhash.Hash, *subtreeprocessor.Job]\n\n    // blockSubmissionChan handles block submission requests\n    blockSubmissionChan chan *BlockSubmissionRequest\n\n    // skipWaitForPendingBlocks stores the flag value for tests\n    skipWaitForPendingBlocks bool\n}\n</code></pre> <p>The <code>BlockAssembly</code> type is the main structure for the block assembly service. It implements the <code>UnimplementedBlockAssemblyAPIServer</code> and contains various components for managing block assembly, storage, and communication with other services.</p>"},{"location":"references/services/blockassembly_reference/#blockassembler","title":"BlockAssembler","text":"<pre><code>type BlockAssembler struct {\n    // logger provides logging functionality for the assembler\n    logger ulogger.Logger\n\n    // stats tracks operational statistics for monitoring and debugging\n    stats *gocore.Stat\n\n    // settings contains configuration parameters for block assembly\n    settings *settings.Settings\n\n    // utxoStore manages the UTXO set storage and retrieval\n    utxoStore utxo.Store\n\n    // subtreeStore manages persistent storage of transaction subtrees\n    subtreeStore blob.Store\n\n    // blockchainClient interfaces with the blockchain for network operations\n    blockchainClient blockchain.ClientI\n\n    // subtreeProcessor handles the processing and organization of transaction subtrees\n    subtreeProcessor subtreeprocessor.Interface\n\n    // miningCandidateCh coordinates requests for mining candidates\n    miningCandidateCh chan chan *miningCandidateResponse\n\n    // bestBlockHeader atomically stores the current best block header\n    bestBlockHeader atomic.Pointer[model.BlockHeader]\n\n    // bestBlockHeight atomically stores the current best block height\n    bestBlockHeight atomic.Uint32\n\n    // currentChainMap maps block hashes to their heights\n    currentChainMap map[chainhash.Hash]uint32\n\n    // currentChainMapIDs tracks block IDs in the current chain\n    currentChainMapIDs map[uint32]struct{}\n\n    // currentChainMapMu protects access to chain maps\n    currentChainMapMu sync.RWMutex\n\n    // blockchainSubscriptionCh receives blockchain notifications\n    blockchainSubscriptionCh chan *blockchain.Notification\n\n    // currentDifficulty stores the current mining difficulty target\n    currentDifficulty atomic.Pointer[model.NBit]\n\n    // defaultMiningNBits stores the default mining difficulty\n    defaultMiningNBits *model.NBit\n\n    // resetCh handles reset requests for the assembler\n    resetCh chan struct{}\n\n    // resetWaitCount tracks the number of blocks to wait after reset\n    resetWaitCount atomic.Int32\n\n    // resetWaitDuration tracks the time to wait after reset\n    resetWaitDuration atomic.Int32\n\n    // currentRunningState tracks the current operational state\n    currentRunningState atomic.Value\n\n    // cleanupService manages background cleanup tasks\n    cleanupService cleanup.Service\n\n    // cleanupServiceLoaded indicates if the cleanup service has been loaded\n    cleanupServiceLoaded atomic.Bool\n\n    // unminedCleanupTicker manages periodic cleanup of old unmined transactions\n    unminedCleanupTicker *time.Ticker\n\n    // cachedCandidate stores the cached mining candidate\n    cachedCandidate *CachedMiningCandidate\n\n    // skipWaitForPendingBlocks allows tests to skip waiting for pending blocks during startup\n    skipWaitForPendingBlocks bool\n\n    // unminedTransactionsLoading indicates if unmined transactions are currently being loaded\n    unminedTransactionsLoading atomic.Bool\n}\n</code></pre> <p>The <code>BlockAssembler</code> type is responsible for assembling blocks, managing the current chain state, and handling mining candidates.</p>"},{"location":"references/services/blockassembly_reference/#subtreeprocessor","title":"SubtreeProcessor","text":"<pre><code>type SubtreeProcessor struct {\n    // settings contains the configuration parameters for the processor\n    settings *settings.Settings\n\n    // currentItemsPerFile specifies the maximum number of items per subtree file\n    currentItemsPerFile int\n\n    // blockStartTime tracks when the current block started\n    blockStartTime time.Time\n\n    // subtreesInBlock tracks number of subtrees created in current block\n    subtreesInBlock int\n\n    // blockIntervals tracks recent intervals per subtree in previous blocks\n    blockIntervals []time.Duration\n\n    // maxBlockSamples is the number of block samples to keep for averaging\n    maxBlockSamples int\n\n    // txChan receives transaction batches for processing\n    txChan chan *[]TxIDAndFee\n\n    // getSubtreesChan handles requests to retrieve current subtrees\n    getSubtreesChan chan chan []*util.Subtree\n\n    // moveForwardBlockChan receives requests to process new blocks\n    moveForwardBlockChan chan moveBlockRequest\n\n    // reorgBlockChan handles blockchain reorganization requests\n    reorgBlockChan chan reorgBlocksRequest\n\n    // resetCh handles requests to reset the processor state\n    resetCh chan *resetBlocks\n\n    // removeTxCh receives transactions to be removed\n    removeTxCh chan chainhash.Hash\n\n    // lengthCh receives requests for the current length of the processor\n    lengthCh chan chan int\n\n    // checkSubtreeProcessorCh is used to check the subtree processor state\n    checkSubtreeProcessorCh chan chan error\n\n    // newSubtreeChan receives notifications about new subtrees\n    newSubtreeChan chan NewSubtreeRequest\n\n    // chainedSubtrees stores the ordered list of completed subtrees\n    chainedSubtrees []*util.Subtree\n\n    // chainedSubtreeCount tracks the number of chained subtrees atomically\n    chainedSubtreeCount atomic.Int32\n\n    // currentSubtree represents the subtree currently being built\n    currentSubtree *util.Subtree\n\n    // currentBlockHeader stores the current block header being processed\n    currentBlockHeader *model.BlockHeader\n\n    // Mutex provides thread-safe access to shared resources\n    sync.Mutex\n\n    // txCount tracks the total number of transactions processed\n    txCount atomic.Uint64\n\n    // batcher manages transaction batching operations\n    batcher *TxIDAndFeeBatch\n\n    // queue manages the transaction processing queue\n    queue *LockFreeQueue\n\n    // currentTxMap tracks transactions currently held in the subtree processor\n    currentTxMap *util.SyncedMap[chainhash.Hash, meta.TxInpoints]\n\n    // removeMap tracks transactions marked for removal\n    removeMap *util.SwissMap\n\n    // subtreeStore manages persistent storage of transaction subtrees\n    subtreeStore blob.Store\n\n    // utxoStore manages the UTXO set storage and retrieval\n    utxoStore utxostore.Store\n\n    // logger provides logging functionality\n    logger ulogger.Logger\n\n    // stats tracks operational statistics\n    stats *gocore.Stat\n\n    // currentRunningState tracks the current operational state\n    currentRunningState atomic.Value\n}\n</code></pre> <p>The <code>SubtreeProcessor</code> type manages the processing of transactions into subtrees, handling chain reorganizations, and maintaining the current state of the block assembly process.</p>"},{"location":"references/services/blockassembly_reference/#lockfreequeue","title":"LockFreeQueue","text":"<pre><code>type LockFreeQueue struct {\n    head        *TxIDAndFee\n    tail        atomic.Pointer[TxIDAndFee]\n    queueLength atomic.Int64\n}\n</code></pre> <p>The <code>LockFreeQueue</code> type represents a lock-free FIFO queue for managing transactions in the block assembly process.</p>"},{"location":"references/services/blockassembly_reference/#functions","title":"Functions","text":""},{"location":"references/services/blockassembly_reference/#blockassembly_1","title":"BlockAssembly","text":""},{"location":"references/services/blockassembly_reference/#new","title":"New","text":"<pre><code>func New(logger ulogger.Logger, tSettings *settings.Settings, txStore blob.Store, utxoStore utxostore.Store, subtreeStore blob.Store, blockchainClient blockchain.ClientI) *BlockAssembly\n</code></pre> <p>Creates a new instance of the <code>BlockAssembly</code> service with the specified dependencies and initializes Prometheus metrics.</p>"},{"location":"references/services/blockassembly_reference/#health","title":"Health","text":"<pre><code>func (ba *BlockAssembly) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>Performs health checks on the block assembly service. If checkLiveness is true, only performs basic liveness checks. Otherwise, performs readiness checks on all dependencies: BlockchainClient, FSM, SubtreeStore, TxStore, and UTXOStore.</p>"},{"location":"references/services/blockassembly_reference/#healthgrpc","title":"HealthGRPC","text":"<pre><code>func (ba *BlockAssembly) HealthGRPC(ctx context.Context, _ *blockassembly_api.EmptyMessage) (*blockassembly_api.HealthResponse, error)\n</code></pre> <p>Performs a gRPC health check on the block assembly service.</p>"},{"location":"references/services/blockassembly_reference/#init","title":"Init","text":"<pre><code>func (ba *BlockAssembly) Init(ctx context.Context) (err error)\n</code></pre> <p>Initializes the block assembly service by creating a BlockAssembler instance, subscribing to blockchain notifications, and preparing for transaction processing.</p>"},{"location":"references/services/blockassembly_reference/#start","title":"Start","text":"<pre><code>func (ba *BlockAssembly) Start(ctx context.Context, readyCh chan&lt;- struct{}) (err error)\n</code></pre> <p>Starts the block assembly service, launches concurrent processing routines, and begins listening for blockchain notifications. The readyCh channel is closed once the service is ready to receive requests.</p>"},{"location":"references/services/blockassembly_reference/#stop","title":"Stop","text":"<pre><code>func (ba *BlockAssembly) Stop(_ context.Context) error\n</code></pre> <p>Gracefully shuts down the BlockAssembly service, stopping all processing operations.</p>"},{"location":"references/services/blockassembly_reference/#addtx","title":"AddTx","text":"<pre><code>func (ba *BlockAssembly) AddTx(ctx context.Context, req *blockassembly_api.AddTxRequest) (*blockassembly_api.AddTxResponse, error)\n</code></pre> <p>Adds a transaction to the block assembly. This method accepts a transaction request and forwards it to the block assembler for processing.</p>"},{"location":"references/services/blockassembly_reference/#removetx","title":"RemoveTx","text":"<pre><code>func (ba *BlockAssembly) RemoveTx(ctx context.Context, req *blockassembly_api.RemoveTxRequest) (*blockassembly_api.EmptyMessage, error)\n</code></pre> <p>Removes a transaction from the block assembly by its hash. This prevents the transaction from being included in future blocks.</p>"},{"location":"references/services/blockassembly_reference/#addtxbatch","title":"AddTxBatch","text":"<pre><code>func (ba *BlockAssembly) AddTxBatch(ctx context.Context, batch *blockassembly_api.AddTxBatchRequest) (*blockassembly_api.AddTxBatchResponse, error)\n</code></pre> <p>Processes a batch of transactions for block assembly. This is more efficient than adding transactions individually.</p>"},{"location":"references/services/blockassembly_reference/#txcount","title":"TxCount","text":"<pre><code>func (ba *BlockAssembly) TxCount() uint64\n</code></pre> <p>Returns the total number of transactions processed by the block assembly service.</p>"},{"location":"references/services/blockassembly_reference/#getminingcandidate","title":"GetMiningCandidate","text":"<pre><code>func (ba *BlockAssembly) GetMiningCandidate(ctx context.Context, req *blockassembly_api.GetMiningCandidateRequest) (*model.MiningCandidate, error)\n</code></pre> <p>Retrieves a candidate block for mining. This method returns a block template that miners can use to find a valid proof-of-work solution.</p>"},{"location":"references/services/blockassembly_reference/#submitminingsolution","title":"SubmitMiningSolution","text":"<pre><code>func (ba *BlockAssembly) SubmitMiningSolution(ctx context.Context, req *blockassembly_api.SubmitMiningSolutionRequest) (*blockassembly_api.OKResponse, error)\n</code></pre> <p>Processes a mining solution submission. It validates the solution, creates a block, and adds it to the blockchain.</p>"},{"location":"references/services/blockassembly_reference/#subtreecount","title":"SubtreeCount","text":"<pre><code>func (ba *BlockAssembly) SubtreeCount() int\n</code></pre> <p>Returns the total number of subtrees managed by the block assembly service.</p>"},{"location":"references/services/blockassembly_reference/#resetblockassembly","title":"ResetBlockAssembly","text":"<pre><code>func (ba *BlockAssembly) ResetBlockAssembly(ctx context.Context, _ *blockassembly_api.EmptyMessage) (*blockassembly_api.EmptyMessage, error)\n</code></pre> <p>Resets the block assembly service to a clean state, removing all transactions and subtrees. This is useful for recovery after errors or when a full reset is needed.</p>"},{"location":"references/services/blockassembly_reference/#getblockassemblystate","title":"GetBlockAssemblyState","text":"<pre><code>func (ba *BlockAssembly) GetBlockAssemblyState(ctx context.Context, _ *blockassembly_api.EmptyMessage) (*blockassembly_api.StateMessage, error)\n</code></pre> <p>Retrieves the current operational state of the block assembly service, including transaction and subtree counts, blockchain tip information, and queue metrics. This information is valuable for monitoring, debugging, and ensuring the service is operating correctly.</p>"},{"location":"references/services/blockassembly_reference/#getcurrentdifficulty","title":"GetCurrentDifficulty","text":"<pre><code>func (ba *BlockAssembly) GetCurrentDifficulty(_ context.Context, _ *blockassembly_api.EmptyMessage) (*blockassembly_api.GetCurrentDifficultyResponse, error)\n</code></pre> <p>Retrieves the current mining difficulty target required for valid proof-of-work. This value determines how much computational work is required to find a valid block solution.</p>"},{"location":"references/services/blockassembly_reference/#generateblocks","title":"GenerateBlocks","text":"<pre><code>func (ba *BlockAssembly) GenerateBlocks(ctx context.Context, req *blockassembly_api.GenerateBlocksRequest) (*blockassembly_api.EmptyMessage, error)\n</code></pre> <p>Generates the given number of blocks by mining them. This method is primarily used for testing and development environments. The operation requires the GenerateSupported flag to be enabled in chain configuration.</p> <p>Parameters:</p> <ul> <li><code>req.Count</code>: Number of blocks to generate</li> <li><code>req.Address</code>: Optional mining address</li> </ul>"},{"location":"references/services/blockassembly_reference/#checkblockassembly","title":"CheckBlockAssembly","text":"<pre><code>func (ba *BlockAssembly) CheckBlockAssembly(_ context.Context, _ *blockassembly_api.EmptyMessage) (*blockassembly_api.OKResponse, error)\n</code></pre> <p>Checks the block assembly state for integrity and consistency.</p>"},{"location":"references/services/blockassembly_reference/#getblockassemblyblockcandidate","title":"GetBlockAssemblyBlockCandidate","text":"<pre><code>func (ba *BlockAssembly) GetBlockAssemblyBlockCandidate(ctx context.Context, _ *blockassembly_api.EmptyMessage) (*blockassembly_api.GetBlockAssemblyBlockCandidateResponse, error)\n</code></pre> <p>Retrieves the current block assembly block candidate, including detailed information about the candidate's construction.</p>"},{"location":"references/services/blockassembly_reference/#getblockassemblytxs","title":"GetBlockAssemblyTxs","text":"<pre><code>func (ba *BlockAssembly) GetBlockAssemblyTxs(ctx context.Context, _ *blockassembly_api.EmptyMessage) (*blockassembly_api.GetBlockAssemblyTxsResponse, error)\n</code></pre> <p>Retrieves all transaction hashes currently held in the block assembly service. Returns both the count and list of transaction hashes for monitoring and debugging purposes.</p>"},{"location":"references/services/blockassembly_reference/#setskipwaitforpendingblocks","title":"SetSkipWaitForPendingBlocks","text":"<pre><code>func (ba *BlockAssembly) SetSkipWaitForPendingBlocks(skip bool)\n</code></pre> <p>Sets the flag to skip waiting for pending blocks during startup. This is primarily used in test environments to prevent blocking on pending blocks.</p>"},{"location":"references/services/blockassembly_reference/#blockassembler_1","title":"BlockAssembler","text":""},{"location":"references/services/blockassembly_reference/#newblockassembler","title":"NewBlockAssembler","text":"<pre><code>func NewBlockAssembler(ctx context.Context, logger ulogger.Logger, tSettings *settings.Settings, stats *gocore.Stat, utxoStore utxo.Store, subtreeStore blob.Store, blockchainClient blockchain.ClientI, newSubtreeChan chan subtreeprocessor.NewSubtreeRequest) (*BlockAssembler, error)\n</code></pre> <p>Creates and initializes a new BlockAssembler instance with the specified dependencies.</p>"},{"location":"references/services/blockassembly_reference/#txcount_1","title":"TxCount","text":"<pre><code>func (b *BlockAssembler) TxCount() uint64\n</code></pre> <p>Returns the total number of transactions in the assembler.</p>"},{"location":"references/services/blockassembly_reference/#queuelength","title":"QueueLength","text":"<pre><code>func (b *BlockAssembler) QueueLength() int64\n</code></pre> <p>Returns the current length of the transaction queue.</p>"},{"location":"references/services/blockassembly_reference/#subtreecount_1","title":"SubtreeCount","text":"<pre><code>func (b *BlockAssembler) SubtreeCount() int\n</code></pre> <p>Returns the total number of subtrees.</p>"},{"location":"references/services/blockassembly_reference/#start_1","title":"Start","text":"<pre><code>func (b *BlockAssembler) Start(ctx context.Context) error\n</code></pre> <p>Initializes and begins the block assembler operations, setting up channel listeners and initializing the blockchain state.</p>"},{"location":"references/services/blockassembly_reference/#getstate","title":"GetState","text":"<pre><code>func (b *BlockAssembler) GetState(ctx context.Context) (*model.BlockHeader, uint32, error)\n</code></pre> <p>Retrieves the current state of the block assembler from the blockchain, returning the best block header and height.</p>"},{"location":"references/services/blockassembly_reference/#setstate","title":"SetState","text":"<pre><code>func (b *BlockAssembler) SetState(ctx context.Context) error\n</code></pre> <p>Persists the current state of the block assembler to the blockchain.</p>"},{"location":"references/services/blockassembly_reference/#currentblock","title":"CurrentBlock","text":"<pre><code>func (b *BlockAssembler) CurrentBlock() (*model.BlockHeader, uint32)\n</code></pre> <p>Returns the current best block header and height.</p>"},{"location":"references/services/blockassembly_reference/#addtx_1","title":"AddTx","text":"<pre><code>func (b *BlockAssembler) AddTx(node util.SubtreeNode, txInpoints meta.TxInpoints)\n</code></pre> <p>Adds a transaction to the block assembler along with its input points.</p>"},{"location":"references/services/blockassembly_reference/#removetx_1","title":"RemoveTx","text":"<pre><code>func (b *BlockAssembler) RemoveTx(hash chainhash.Hash) error\n</code></pre> <p>Removes a transaction from the block assembler by its hash.</p>"},{"location":"references/services/blockassembly_reference/#reset","title":"Reset","text":"<pre><code>func (b *BlockAssembler) Reset()\n</code></pre> <p>Triggers a reset of the block assembler state. This operation runs asynchronously to prevent blocking.</p>"},{"location":"references/services/blockassembly_reference/#getminingcandidate_1","title":"GetMiningCandidate","text":"<pre><code>func (b *BlockAssembler) GetMiningCandidate(_ context.Context) (*model.MiningCandidate, []*util.Subtree, error)\n</code></pre> <p>Retrieves a candidate block for mining along with its associated subtrees.</p>"},{"location":"references/services/blockassembly_reference/#subtreeprocessor_1","title":"SubtreeProcessor","text":""},{"location":"references/services/blockassembly_reference/#newsubtreeprocessor","title":"NewSubtreeProcessor","text":"<pre><code>func NewSubtreeProcessor(ctx context.Context, logger ulogger.Logger, tSettings *settings.Settings, subtreeStore blob.Store, blockchainClient blockchain.ClientI, utxoStore utxostore.Store, newSubtreeChan chan NewSubtreeRequest, options ...Options) (*SubtreeProcessor, error)\n</code></pre> <p>Creates and initializes a new SubtreeProcessor instance with the specified dependencies. This is the core component responsible for organizing transactions into hierarchical subtrees for efficient block assembly.</p>"},{"location":"references/services/blockassembly_reference/#txcount_2","title":"TxCount","text":"<pre><code>func (stp *SubtreeProcessor) TxCount() uint64\n</code></pre> <p>Returns the total number of transactions processed by the subtree processor.</p>"},{"location":"references/services/blockassembly_reference/#queuelength_1","title":"QueueLength","text":"<pre><code>func (stp *SubtreeProcessor) QueueLength() int64\n</code></pre> <p>Returns the current length of the transaction queue in the subtree processor.</p>"},{"location":"references/services/blockassembly_reference/#subtreecount_2","title":"SubtreeCount","text":"<pre><code>func (stp *SubtreeProcessor) SubtreeCount() int\n</code></pre> <p>Returns the total number of subtrees managed by the processor. This method is primarily used for prometheus statistics.</p>"},{"location":"references/services/blockassembly_reference/#getcurrentrunningstate","title":"GetCurrentRunningState","text":"<pre><code>func (stp *SubtreeProcessor) GetCurrentRunningState() State\n</code></pre> <p>Returns the current operational state of the processor.</p>"},{"location":"references/services/blockassembly_reference/#getcurrentlength","title":"GetCurrentLength","text":"<pre><code>func (stp *SubtreeProcessor) GetCurrentLength() int\n</code></pre> <p>Returns the length of the current subtree.</p>"},{"location":"references/services/blockassembly_reference/#add","title":"Add","text":"<pre><code>func (stp *SubtreeProcessor) Add(node util.SubtreeNode, txInpoints meta.TxInpoints)\n</code></pre> <p>Adds a transaction node to the processor along with its input points.</p>"},{"location":"references/services/blockassembly_reference/#remove","title":"Remove","text":"<pre><code>func (stp *SubtreeProcessor) Remove(hash chainhash.Hash) error\n</code></pre> <p>Prevents a transaction from being processed from the queue into a subtree, and removes it if already present. This can only take place before the delay time in the queue has passed.</p>"},{"location":"references/services/blockassembly_reference/#getcompletedsubtreesforminingcandidate","title":"GetCompletedSubtreesForMiningCandidate","text":"<pre><code>func (stp *SubtreeProcessor) GetCompletedSubtreesForMiningCandidate() []*util.Subtree\n</code></pre> <p>Retrieves all completed subtrees for block mining.</p>"},{"location":"references/services/blockassembly_reference/#moveforwardblock","title":"MoveForwardBlock","text":"<pre><code>func (stp *SubtreeProcessor) MoveForwardBlock(block *model.Block) error\n</code></pre> <p>Updates the subtrees when a new block is found.</p>"},{"location":"references/services/blockassembly_reference/#reorg","title":"Reorg","text":"<pre><code>func (stp *SubtreeProcessor) Reorg(moveBackBlocks []*model.Block, moveForwardBlocks []*model.Block) error\n</code></pre> <p>Handles blockchain reorganization by processing moved blocks. This method manages the complex task of reconciling the subtree state with the new blockchain state after a reorganization.</p>"},{"location":"references/services/blockassembly_reference/#reset_1","title":"Reset","text":"<pre><code>func (stp *SubtreeProcessor) Reset(blockHeader *model.BlockHeader, moveBackBlocks []*model.Block, moveForwardBlocks []*model.Block, isLegacySync bool) ResetResponse\n</code></pre> <p>Resets the processor to a clean state, removing all subtrees and transactions.</p>"},{"location":"references/services/blockassembly_reference/#checksubtreeprocessor","title":"CheckSubtreeProcessor","text":"<pre><code>func (stp *SubtreeProcessor) CheckSubtreeProcessor() error\n</code></pre> <p>Checks the integrity of the subtree processor. It verifies that all transactions in the current transaction map are present in the subtrees and that the size of the current transaction map matches the expected transaction count.</p>"},{"location":"references/services/blockassembly_reference/#lockfreequeue_1","title":"LockFreeQueue","text":""},{"location":"references/services/blockassembly_reference/#newlockfreequeue","title":"NewLockFreeQueue","text":"<pre><code>func NewLockFreeQueue() *LockFreeQueue\n</code></pre> <p>Creates a new instance of the LockFreeQueue, which is a lock-free FIFO queue for managing transactions in the block assembly process.</p>"},{"location":"references/services/blockassembly_reference/#len","title":"Len","text":"<pre><code>func (q *LockFreeQueue) Len() int64\n</code></pre> <p>Returns the current length of the queue.</p>"},{"location":"references/services/blockassembly_reference/#enqueue","title":"Enqueue","text":"<pre><code>func (q *LockFreeQueue) Enqueue(v *TxIDAndFee)\n</code></pre> <p>Adds a transaction to the queue in a thread-safe manner.</p>"},{"location":"references/services/blockassembly_reference/#dequeue","title":"Dequeue","text":"<pre><code>func (q *LockFreeQueue) Dequeue(validFromMillis int64) *TxIDAndFee\n</code></pre> <p>Removes and returns a transaction from the queue if its validation time has passed. This method implements the delay mechanism that allows transactions to be properly validated before being included in subtrees.</p>"},{"location":"references/services/blockchain_reference/","title":"Blockchain Server Reference Documentation","text":""},{"location":"references/services/blockchain_reference/#types","title":"Types","text":""},{"location":"references/services/blockchain_reference/#blockchain","title":"Blockchain","text":"<pre><code>type Blockchain struct {\n    blockchain_api.UnimplementedBlockchainAPIServer\n    addBlockChan                  chan *blockchain_api.AddBlockRequest // Channel for adding blocks\n    store                         blockchain_store.Store               // Storage interface for blockchain data\n    logger                        ulogger.Logger                       // Logger instance\n    settings                      *settings.Settings                   // Configuration settings\n    newSubscriptions              chan subscriber                      // Channel for new subscriptions\n    deadSubscriptions             chan subscriber                      // Channel for ended subscriptions\n    subscribers                   map[subscriber]bool                  // Active subscribers map\n    subscribersMu                 sync.RWMutex                         // Mutex for subscribers map\n    notifications                 chan *blockchain_api.Notification    // Channel for notifications\n    newBlock                      chan struct{}                        // Channel signaling new block events\n    difficulty                    *Difficulty                          // Difficulty calculation instance\n    blocksFinalKafkaAsyncProducer kafka.KafkaAsyncProducerI            // Kafka producer for final blocks\n    kafkaChan                     chan *kafka.Message                  // Channel for Kafka messages\n    stats                         *gocore.Stat                         // Statistics tracking\n    finiteStateMachine            *fsm.FSM                             // FSM for blockchain state\n    stateChangeTimestamp          time.Time                            // Timestamp of last state change\n    AppCtx                        context.Context                      // Application context\n    localTestStartState           string                               // Initial state for testing\n    subscriptionManagerReady      atomic.Bool                          // Flag indicating subscription manager is ready\n}\n</code></pre> <p>The <code>Blockchain</code> type is the main structure for the blockchain server. It implements the <code>UnimplementedBlockchainAPIServer</code> and contains various channels and components for managing the blockchain state, subscribers, and notifications. It uses a finite state machine (FSM) to manage its operational states and provides resilience across service restarts.</p>"},{"location":"references/services/blockchain_reference/#subscriber","title":"subscriber","text":"<pre><code>type subscriber struct {\n    subscription blockchain_api.BlockchainAPI_SubscribeServer // The gRPC subscription server\n    source       string                                       // Source identifier of the subscription\n    done         chan struct{}                                // Channel to signal when subscription is done\n}\n</code></pre> <p>The <code>subscriber</code> type represents a subscriber to the blockchain server, encapsulating the connection to a client interested in blockchain events and providing a mechanism for sending notifications about new blocks, state changes, and other blockchain events.</p>"},{"location":"references/services/blockchain_reference/#core-functions","title":"Core Functions","text":""},{"location":"references/services/blockchain_reference/#new","title":"New","text":"<pre><code>func New(ctx context.Context, logger ulogger.Logger, tSettings *settings.Settings, store blockchain_store.Store, blocksFinalKafkaAsyncProducer kafka.KafkaAsyncProducerI, localTestStartFromState ...string) (*Blockchain, error)\n</code></pre> <p>Creates a new instance of the <code>Blockchain</code> server with the provided dependencies. This constructor initializes the core blockchain service with all required components and sets up internal channels for communication between different parts of the service. The optional <code>localTestStartFromState</code> parameter allows initializing the blockchain service in a specific FSM state for testing purposes.</p>"},{"location":"references/services/blockchain_reference/#health","title":"Health","text":"<pre><code>func (b *Blockchain) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>Performs health checks on the blockchain server. When <code>checkLiveness</code> is true, it only verifies the internal service state (used to determine if the service needs to be restarted). When <code>checkLiveness</code> is false, it verifies both the service and its dependencies are ready to accept requests.</p>"},{"location":"references/services/blockchain_reference/#healthgrpc","title":"HealthGRPC","text":"<pre><code>func (b *Blockchain) HealthGRPC(ctx context.Context, _ *emptypb.Empty) (*blockchain_api.HealthResponse, error)\n</code></pre> <p>Provides health check information via gRPC, exposing the readiness health check functionality through the gRPC API. This method wraps the <code>Health</code> method to provide a standardized gRPC response format.</p>"},{"location":"references/services/blockchain_reference/#init","title":"Init","text":"<pre><code>func (b *Blockchain) Init(ctx context.Context) error\n</code></pre> <p>Initializes the blockchain service, setting up the finite state machine (FSM) that governs the service's operational states. It handles three initialization scenarios: test mode, new deployment, and normal operation where it restores the previously persisted state from storage.</p>"},{"location":"references/services/blockchain_reference/#start","title":"Start","text":"<pre><code>func (b *Blockchain) Start(ctx context.Context, readyCh chan&lt;- struct{}) error\n</code></pre> <p>Starts the blockchain service operations, initializing and launching all core components: Kafka producer, subscription management, HTTP server for administrative endpoints, and gRPC server for client API access. It uses a synchronized approach to ensure the service is fully operational before signaling readiness.</p>"},{"location":"references/services/blockchain_reference/#stop","title":"Stop","text":"<pre><code>func (b *Blockchain) Stop(_ context.Context) error\n</code></pre> <p>Gracefully stops the blockchain service, allowing for proper resource cleanup and state persistence before termination.</p>"},{"location":"references/services/blockchain_reference/#block-management-functions","title":"Block Management Functions","text":""},{"location":"references/services/blockchain_reference/#addblock","title":"AddBlock","text":"<pre><code>func (b *Blockchain) AddBlock(ctx context.Context, request *blockchain_api.AddBlockRequest) (*emptypb.Empty, error)\n</code></pre> <p>Processes a request to add a new block to the blockchain. This method handles the full lifecycle of adding a new block: validating and parsing the incoming block data, persisting the validated block with configurable options, updating block metadata, publishing the finalized block to Kafka, and notifying subscribers.</p> <p>The method supports functional options through the request's option fields:</p> <ul> <li><code>optionMinedSet</code>: Marks the block as mined when set to true</li> <li><code>optionSubtreesSet</code>: Marks the block's subtrees as processed when set to true</li> <li><code>optionInvalid</code>: Marks the block as invalid when set to true (useful for tracking invalid blocks during catchup)</li> <li><code>optionID</code>: Allows specifying a custom block ID (useful for quick validation with pre-allocated IDs)</li> </ul> <p>Example usage with options:</p> <pre><code>// Adding a block with pre-allocated ID and marked as mined\nrequest := &amp;blockchain_api.AddBlockRequest{\n    Block:           blockData,\n    BaseURL:         sourceURL,\n    OptionMinedSet:  &amp;wrapperspb.BoolValue{Value: true},\n    OptionID:        &amp;wrapperspb.UInt64Value{Value: preAllocatedID},\n}\n</code></pre>"},{"location":"references/services/blockchain_reference/#getblock","title":"GetBlock","text":"<pre><code>func (b *Blockchain) GetBlock(ctx context.Context, request *blockchain_api.GetBlockRequest) (*blockchain_api.GetBlockResponse, error)\n</code></pre> <p>Retrieves a block from the blockchain by its hash. It validates the requested block hash format, retrieves the block data from storage, and returns the complete block data in API response format.</p>"},{"location":"references/services/blockchain_reference/#getblocks","title":"GetBlocks","text":"<pre><code>func (b *Blockchain) GetBlocks(ctx context.Context, req *blockchain_api.GetBlocksRequest) (*blockchain_api.GetBlocksResponse, error)\n</code></pre> <p>Retrieves multiple blocks from the blockchain starting from a specific hash, limiting the number of blocks returned based on the request.</p>"},{"location":"references/services/blockchain_reference/#getblockbyheight","title":"GetBlockByHeight","text":"<pre><code>func (b *Blockchain) GetBlockByHeight(ctx context.Context, request *blockchain_api.GetBlockByHeightRequest) (*blockchain_api.GetBlockResponse, error)\n</code></pre> <p>Retrieves a block from the blockchain at a specific height. It fetches the block hash at the requested height and then retrieves the complete block data.</p>"},{"location":"references/services/blockchain_reference/#getblockbyid","title":"GetBlockByID","text":"<pre><code>func (b *Blockchain) GetBlockByID(ctx context.Context, request *blockchain_api.GetBlockByIDRequest) (*blockchain_api.GetBlockResponse, error)\n</code></pre> <p>Retrieves a block from the blockchain by its unique ID. It maps the ID to the corresponding block hash and then retrieves the complete block data.</p>"},{"location":"references/services/blockchain_reference/#getblockstats","title":"GetBlockStats","text":"<pre><code>func (b *Blockchain) GetBlockStats(ctx context.Context, _ *emptypb.Empty) (*model.BlockStats, error)\n</code></pre> <p>Retrieves statistical information about the blockchain, including block count, transaction count, and other metrics useful for monitoring and analysis.</p>"},{"location":"references/services/blockchain_reference/#getblockgraphdata","title":"GetBlockGraphData","text":"<pre><code>func (b *Blockchain) GetBlockGraphData(ctx context.Context, req *blockchain_api.GetBlockGraphDataRequest) (*model.BlockDataPoints, error)\n</code></pre> <p>Retrieves data points for blockchain visualization over a specified time period, useful for creating charts and graphs of blockchain activity.</p>"},{"location":"references/services/blockchain_reference/#getlastnblocks","title":"GetLastNBlocks","text":"<pre><code>func (b *Blockchain) GetLastNBlocks(ctx context.Context, request *blockchain_api.GetLastNBlocksRequest) (*blockchain_api.GetLastNBlocksResponse, error)\n</code></pre> <p>Retrieves the most recent N blocks from the blockchain, ordered by block height in descending order (newest first).</p>"},{"location":"references/services/blockchain_reference/#getlastninvalidblocks","title":"GetLastNInvalidBlocks","text":"<pre><code>func (b *Blockchain) GetLastNInvalidBlocks(ctx context.Context, request *blockchain_api.GetLastNInvalidBlocksRequest) (*blockchain_api.GetLastNInvalidBlocksResponse, error)\n</code></pre> <p>Retrieves the most recent N blocks that have been marked as invalid, useful for monitoring and debugging chain reorganizations or consensus issues.</p>"},{"location":"references/services/blockchain_reference/#getsuitableblock","title":"GetSuitableBlock","text":"<pre><code>func (b *Blockchain) GetSuitableBlock(ctx context.Context, request *blockchain_api.GetSuitableBlockRequest) (*blockchain_api.GetSuitableBlockResponse, error)\n</code></pre> <p>Finds a suitable block for mining purposes based on the provided hash, typically used by mining software to determine which block to build upon.</p>"},{"location":"references/services/blockchain_reference/#getblockexists","title":"GetBlockExists","text":"<pre><code>func (b *Blockchain) GetBlockExists(ctx context.Context, request *blockchain_api.GetBlockRequest) (*blockchain_api.GetBlockExistsResponse, error)\n</code></pre> <p>Checks if a block with the given hash exists in the blockchain, without returning the full block data.</p>"},{"location":"references/services/blockchain_reference/#block-header-functions","title":"Block Header Functions","text":""},{"location":"references/services/blockchain_reference/#getbestblockheader","title":"GetBestBlockHeader","text":"<pre><code>func (b *Blockchain) GetBestBlockHeader(ctx context.Context, empty *emptypb.Empty) (*blockchain_api.GetBlockHeaderResponse, error)\n</code></pre> <p>Retrieves the header of the current best (most recent) block in the blockchain, which represents the tip of the main chain.</p>"},{"location":"references/services/blockchain_reference/#checkblockisincurrentchain","title":"CheckBlockIsInCurrentChain","text":"<pre><code>func (b *Blockchain) CheckBlockIsInCurrentChain(ctx context.Context, req *blockchain_api.CheckBlockIsCurrentChainRequest) (*blockchain_api.CheckBlockIsCurrentChainResponse, error)\n</code></pre> <p>Verifies if specified blocks are part of the current main chain, useful for determining if blocks have been orphaned or remain in the active chain.</p>"},{"location":"references/services/blockchain_reference/#getblockheader","title":"GetBlockHeader","text":"<pre><code>func (b *Blockchain) GetBlockHeader(ctx context.Context, req *blockchain_api.GetBlockHeaderRequest) (*blockchain_api.GetBlockHeaderResponse, error)\n</code></pre> <p>Retrieves the header of a specific block in the blockchain by its hash, without retrieving the full block data.</p>"},{"location":"references/services/blockchain_reference/#getblockheaders","title":"GetBlockHeaders","text":"<pre><code>func (b *Blockchain) GetBlockHeaders(ctx context.Context, request *blockchain_api.GetBlockHeadersRequest) (*blockchain_api.GetBlockHeadersResponse, error)\n</code></pre> <p>Retrieves multiple block headers starting from a specific hash, limiting the number of headers returned based on the request.</p>"},{"location":"references/services/blockchain_reference/#getblockheaderstocommonancestor","title":"GetBlockHeadersToCommonAncestor","text":"<pre><code>func (b *Blockchain) GetBlockHeadersToCommonAncestor(ctx context.Context, req *blockchain_api.GetBlockHeadersToCommonAncestorRequest) (*blockchain_api.GetBlockHeadersResponse, error)\n</code></pre> <p>Retrieves block headers to find a common ancestor between two chains, typically used during chain synchronization and reorganization.</p>"},{"location":"references/services/blockchain_reference/#getblockheadersfromtill","title":"GetBlockHeadersFromTill","text":"<pre><code>func (b *Blockchain) GetBlockHeadersFromTill(ctx context.Context, req *blockchain_api.GetBlockHeadersFromTillRequest) (*blockchain_api.GetBlockHeadersResponse, error)\n</code></pre> <p>Retrieves block headers between two specified blocks, useful for filling gaps in blockchain data or analyzing specific ranges.</p>"},{"location":"references/services/blockchain_reference/#getblockheadersfromheight","title":"GetBlockHeadersFromHeight","text":"<pre><code>func (b *Blockchain) GetBlockHeadersFromHeight(ctx context.Context, req *blockchain_api.GetBlockHeadersFromHeightRequest) (*blockchain_api.GetBlockHeadersFromHeightResponse, error)\n</code></pre> <p>Retrieves block headers starting from a specific height, allowing clients to efficiently fetch headers based on block height rather than hash.</p>"},{"location":"references/services/blockchain_reference/#getblockheadersbyheight","title":"GetBlockHeadersByHeight","text":"<pre><code>func (b *Blockchain) GetBlockHeadersByHeight(ctx context.Context, req *blockchain_api.GetBlockHeadersByHeightRequest) (*blockchain_api.GetBlockHeadersByHeightResponse, error)\n</code></pre> <p>Retrieves block headers between two specified heights, providing an efficient way to fetch a range of headers for analysis or synchronization.</p>"},{"location":"references/services/blockchain_reference/#getblockheaderids","title":"GetBlockHeaderIDs","text":"<pre><code>func (b *Blockchain) GetBlockHeaderIDs(ctx context.Context, request *blockchain_api.GetBlockHeadersRequest) (*blockchain_api.GetBlockHeaderIDsResponse, error)\n</code></pre> <p>Retrieves block header IDs starting from a specific hash, returning only the identifiers rather than the full header data for efficiency.</p>"},{"location":"references/services/blockchain_reference/#mining-and-difficulty-functions","title":"Mining and Difficulty Functions","text":""},{"location":"references/services/blockchain_reference/#getnextworkrequired","title":"GetNextWorkRequired","text":"<pre><code>func (b *Blockchain) GetNextWorkRequired(ctx context.Context, request *blockchain_api.GetNextWorkRequiredRequest) (*blockchain_api.GetNextWorkRequiredResponse, error)\n</code></pre> <p>Calculates the required proof of work difficulty for the next block based on the difficulty adjustment algorithm, used by miners to determine the target difficulty.</p>"},{"location":"references/services/blockchain_reference/#gethashofancestorblock","title":"GetHashOfAncestorBlock","text":"<pre><code>func (b *Blockchain) GetHashOfAncestorBlock(ctx context.Context, request *blockchain_api.GetHashOfAncestorBlockRequest) (*blockchain_api.GetHashOfAncestorBlockResponse, error)\n</code></pre> <p>Retrieves the hash of an ancestor block at a specified depth from a given block, useful for difficulty calculations and chain traversal.</p>"},{"location":"references/services/blockchain_reference/#getblockismined","title":"GetBlockIsMined","text":"<pre><code>func (b *Blockchain) GetBlockIsMined(ctx context.Context, req *blockchain_api.GetBlockIsMinedRequest) (*blockchain_api.GetBlockIsMinedResponse, error)\n</code></pre> <p>Checks if a block has been marked as mined in the blockchain, which indicates that the block has been fully processed by the mining subsystem.</p>"},{"location":"references/services/blockchain_reference/#setblockminedset","title":"SetBlockMinedSet","text":"<pre><code>func (b *Blockchain) SetBlockMinedSet(ctx context.Context, req *blockchain_api.SetBlockMinedSetRequest) (*emptypb.Empty, error)\n</code></pre> <p>Marks a block as mined in the blockchain, updating its status to indicate completion of the mining process.</p>"},{"location":"references/services/blockchain_reference/#getblocksminednotset","title":"GetBlocksMinedNotSet","text":"<pre><code>func (b *Blockchain) GetBlocksMinedNotSet(ctx context.Context, _ *emptypb.Empty) (*blockchain_api.GetBlocksMinedNotSetResponse, error)\n</code></pre> <p>Retrieves blocks that have not been marked as mined.</p>"},{"location":"references/services/blockchain_reference/#setblocksubtreesset","title":"SetBlockSubtreesSet","text":"<pre><code>func (b *Blockchain) SetBlockSubtreesSet(ctx context.Context, req *blockchain_api.SetBlockSubtreesSetRequest) (*emptypb.Empty, error)\n</code></pre> <p>Marks a block's subtrees as set.</p>"},{"location":"references/services/blockchain_reference/#getblockssubtreesnotset","title":"GetBlocksSubtreesNotSet","text":"<pre><code>func (b *Blockchain) GetBlocksSubtreesNotSet(ctx context.Context, _ *emptypb.Empty) (*blockchain_api.GetBlocksSubtreesNotSetResponse, error)\n</code></pre> <p>Retrieves blocks whose subtrees have not been set.</p>"},{"location":"references/services/blockchain_reference/#subscription-and-notification-functions","title":"Subscription and Notification Functions","text":""},{"location":"references/services/blockchain_reference/#subscribe","title":"Subscribe","text":"<pre><code>func (b *Blockchain) Subscribe(req *blockchain_api.SubscribeRequest, sub blockchain_api.BlockchainAPI_SubscribeServer) error\n</code></pre> <p>Handles subscription requests to blockchain notifications. Establishes a persistent gRPC streaming connection for real-time blockchain event notifications.</p>"},{"location":"references/services/blockchain_reference/#sendnotification","title":"SendNotification","text":"<pre><code>func (b *Blockchain) SendNotification(ctx context.Context, req *blockchain_api.Notification) (*emptypb.Empty, error)\n</code></pre> <p>Broadcasts a notification to all subscribers.</p>"},{"location":"references/services/blockchain_reference/#state-management-functions","title":"State Management Functions","text":""},{"location":"references/services/blockchain_reference/#getstate","title":"GetState","text":"<pre><code>func (b *Blockchain) GetState(ctx context.Context, req *blockchain_api.GetStateRequest) (*blockchain_api.StateResponse, error)\n</code></pre> <p>Retrieves a value from the blockchain state storage by its key.</p>"},{"location":"references/services/blockchain_reference/#setstate","title":"SetState","text":"<pre><code>func (b *Blockchain) SetState(ctx context.Context, req *blockchain_api.SetStateRequest) (*emptypb.Empty, error)\n</code></pre> <p>Stores a value in the blockchain state storage with the specified key.</p>"},{"location":"references/services/blockchain_reference/#block-validation-functions","title":"Block Validation Functions","text":""},{"location":"references/services/blockchain_reference/#invalidateblock","title":"InvalidateBlock","text":"<pre><code>func (b *Blockchain) InvalidateBlock(ctx context.Context, request *blockchain_api.InvalidateBlockRequest) (*blockchain_api.InvalidateBlockResponse, error)\n</code></pre> <p>Marks a block as invalid in the blockchain.</p>"},{"location":"references/services/blockchain_reference/#revalidateblock","title":"RevalidateBlock","text":"<pre><code>func (b *Blockchain) RevalidateBlock(ctx context.Context, request *blockchain_api.RevalidateBlockRequest) (*emptypb.Empty, error)\n</code></pre> <p>Restores a previously invalidated block.</p>"},{"location":"references/services/blockchain_reference/#additional-block-functions","title":"Additional Block Functions","text":""},{"location":"references/services/blockchain_reference/#getnextblockid","title":"GetNextBlockID","text":"<pre><code>func (b *Blockchain) GetNextBlockID(ctx context.Context, _ *emptypb.Empty) (*blockchain_api.GetNextBlockIDResponse, error)\n</code></pre> <p>Retrieves the next available block ID.</p>"},{"location":"references/services/blockchain_reference/#getchaintips","title":"GetChainTips","text":"<pre><code>func (b *Blockchain) GetChainTips(ctx context.Context, _ *emptypb.Empty) (*blockchain_api.GetChainTipsResponse, error)\n</code></pre> <p>Retrieves information about all known tips in the block tree.</p>"},{"location":"references/services/blockchain_reference/#getlatestblockheaderfromblocklocatorrequest","title":"GetLatestBlockHeaderFromBlockLocatorRequest","text":"<pre><code>func (b *Blockchain) GetLatestBlockHeaderFromBlockLocatorRequest(ctx context.Context, request *blockchain_api.GetLatestBlockHeaderFromBlockLocatorRequest) (*blockchain_api.GetBlockHeaderResponse, error)\n</code></pre> <p>Retrieves the latest block header from a block locator request.</p>"},{"location":"references/services/blockchain_reference/#getblockheadersfromoldestrequest","title":"GetBlockHeadersFromOldestRequest","text":"<pre><code>func (b *Blockchain) GetBlockHeadersFromOldestRequest(ctx context.Context, request *blockchain_api.GetBlockHeadersFromOldestRequest) (*blockchain_api.GetBlockHeadersResponse, error)\n</code></pre> <p>Retrieves block headers from the oldest request.</p>"},{"location":"references/services/blockchain_reference/#getblockheadersfromcommonancestor","title":"GetBlockHeadersFromCommonAncestor","text":"<pre><code>func (b *Blockchain) GetBlockHeadersFromCommonAncestor(ctx context.Context, request *blockchain_api.GetBlockHeadersFromCommonAncestorRequest) (*blockchain_api.GetBlockHeadersResponse, error)\n</code></pre> <p>Retrieves block headers from a common ancestor.</p>"},{"location":"references/services/blockchain_reference/#finite-state-machine-fsm-related-functions","title":"Finite State Machine (FSM) Related Functions","text":""},{"location":"references/services/blockchain_reference/#getfsmcurrentstate","title":"GetFSMCurrentState","text":"<pre><code>func (b *Blockchain) GetFSMCurrentState(ctx context.Context, _ *emptypb.Empty) (*blockchain_api.GetFSMStateResponse, error)\n</code></pre> <p>Retrieves the current state of the finite state machine.</p>"},{"location":"references/services/blockchain_reference/#waitforfsmtotransitiontogivenstate-internal-method","title":"WaitForFSMtoTransitionToGivenState (Internal Method)","text":"<pre><code>func (b *Blockchain) WaitForFSMtoTransitionToGivenState(ctx context.Context, targetState blockchain_api.FSMStateType) error\n</code></pre> <p>Waits for the FSM to transition to a given state. Note: This is an internal helper method and is not exposed as a gRPC endpoint.</p>"},{"location":"references/services/blockchain_reference/#waituntilfsmtransitionfromidlestate","title":"WaitUntilFSMTransitionFromIdleState","text":"<pre><code>func (b *Blockchain) WaitUntilFSMTransitionFromIdleState(ctx context.Context, _ *emptypb.Empty) (*emptypb.Empty, error)\n</code></pre> <p>Waits for the FSM to transition from the IDLE state.</p>"},{"location":"references/services/blockchain_reference/#sendfsmevent","title":"SendFSMEvent","text":"<pre><code>func (b *Blockchain) SendFSMEvent(ctx context.Context, eventReq *blockchain_api.SendFSMEventRequest) (*blockchain_api.GetFSMStateResponse, error)\n</code></pre> <p>Sends an event to the finite state machine.</p>"},{"location":"references/services/blockchain_reference/#run","title":"Run","text":"<pre><code>func (b *Blockchain) Run(ctx context.Context, _ *emptypb.Empty) (*emptypb.Empty, error)\n</code></pre> <p>Transitions the FSM to the RUNNING state.</p>"},{"location":"references/services/blockchain_reference/#catchupblocks","title":"CatchUpBlocks","text":"<pre><code>func (b *Blockchain) CatchUpBlocks(ctx context.Context, _ *emptypb.Empty) (*emptypb.Empty, error)\n</code></pre> <p>Transitions the FSM to the CATCHINGBLOCKS state.</p>"},{"location":"references/services/blockchain_reference/#legacysync","title":"LegacySync","text":"<pre><code>func (b *Blockchain) LegacySync(ctx context.Context, _ *emptypb.Empty) (*emptypb.Empty, error)\n</code></pre> <p>Transitions the FSM to the LEGACYSYNCING state.</p>"},{"location":"references/services/blockchain_reference/#idle","title":"Idle","text":"<pre><code>func (b *Blockchain) Idle(ctx context.Context, _ *emptypb.Empty) (*emptypb.Empty, error)\n</code></pre> <p>Transitions the FSM to the IDLE state.</p>"},{"location":"references/services/blockchain_reference/#legacy-endpoints","title":"Legacy Endpoints","text":""},{"location":"references/services/blockchain_reference/#getblocklocator","title":"GetBlockLocator","text":"<pre><code>func (b *Blockchain) GetBlockLocator(ctx context.Context, req *blockchain_api.GetBlockLocatorRequest) (*blockchain_api.GetBlockLocatorResponse, error)\n</code></pre> <p>Retrieves a block locator for a given block hash and height.</p>"},{"location":"references/services/blockchain_reference/#locateblockheaders","title":"LocateBlockHeaders","text":"<pre><code>func (b *Blockchain) LocateBlockHeaders(ctx context.Context, request *blockchain_api.LocateBlockHeadersRequest) (*blockchain_api.LocateBlockHeadersResponse, error)\n</code></pre> <p>Locates block headers based on a given locator and hash stop.</p>"},{"location":"references/services/blockchain_reference/#getbestheightandtime","title":"GetBestHeightAndTime","text":"<pre><code>func (b *Blockchain) GetBestHeightAndTime(ctx context.Context, _ *emptypb.Empty) (*blockchain_api.GetBestHeightAndTimeResponse, error)\n</code></pre> <p>Retrieves the best height and median time of the blockchain.</p>"},{"location":"references/services/blockchain_reference/#block-id-management-functions","title":"Block ID Management Functions","text":""},{"location":"references/services/blockchain_reference/#getnextblockid_1","title":"GetNextBlockID","text":"<pre><code>func (b *Blockchain) GetNextBlockID(ctx context.Context, _ *emptypb.Empty) (*blockchain_api.GetNextBlockIDResponse, error)\n</code></pre> <p>Retrieves the next available block ID for pre-allocation purposes. This method provides atomic ID generation that ensures unique block IDs across concurrent operations. It is particularly useful during quick validation scenarios where blocks need to be assigned IDs before full processing.</p> <p>The implementation varies by database backend:</p> <ul> <li>PostgreSQL: Uses database sequences for atomic ID generation</li> <li>SQLite: Implements transaction-based increment for thread safety</li> </ul> <p>Returns:</p> <ul> <li><code>next_block_id</code>: The next available unique block ID that can be used for block storage</li> </ul> <p>This method is commonly used during:</p> <ul> <li>Quick validation of checkpointed blocks</li> <li>Parallel block processing where IDs need to be reserved in advance</li> <li>Recovery scenarios where block IDs need to be coordinated across services</li> </ul>"},{"location":"references/services/blockpersister_reference/","title":"Block Persister Service Reference Documentation","text":""},{"location":"references/services/blockpersister_reference/#overview","title":"Overview","text":"<p>The Block Persister Service is responsible for taking blocks from the blockchain service and ensuring they are properly stored in persistent storage along with all related data (transactions, UTXOs, etc.). It plays a critical role in the overall blockchain data persistence strategy by:</p> <ul> <li>Processing and storing complete blocks in the blob store</li> <li>Managing subtree processing for efficient transaction handling</li> <li>Maintaining UTXO set differences for each block</li> <li>Ensuring data consistency and integrity during persistence operations</li> <li>Providing resilient error handling and recovery mechanisms</li> </ul> <p>The service integrates with multiple stores (block store, subtree store, UTXO store) and coordinates between them to ensure consistent and reliable block data persistence. It employs concurrency and batching techniques to optimize performance for high transaction volumes.</p>"},{"location":"references/services/blockpersister_reference/#types","title":"Types","text":""},{"location":"references/services/blockpersister_reference/#server","title":"Server","text":"<pre><code>type Server struct {\n    // ctx is the context for controlling server lifecycle and handling cancellation signals\n    ctx context.Context\n\n    // logger provides structured logging functionality for operational monitoring and debugging\n    logger ulogger.Logger\n\n    // settings contains configuration settings for the server, controlling behavior such as\n    // concurrency levels, batch sizes, and persistence strategies\n    settings *settings.Settings\n\n    // blockStore provides persistent storage for complete blocks\n    // This is typically implemented as a blob store capable of handling large block data\n    blockStore blob.Store\n\n    // subtreeStore provides storage for block subtrees, which are hierarchical structures\n    // containing transaction references that make up parts of a block\n    subtreeStore blob.Store\n\n    // utxoStore provides storage for UTXO (Unspent Transaction Output) data\n    // Used to track the current state of the UTXO set and process changes\n    utxoStore utxo.Store\n\n    // stats tracks operational statistics for monitoring and performance analysis\n    stats *gocore.Stat\n\n    // blockchainClient interfaces with the blockchain service to retrieve block data\n    // and coordinate persistence operations with blockchain state\n    blockchainClient blockchain.ClientI\n\n    // state manages the persister's internal state, tracking which blocks have been\n    // successfully persisted and allowing for recovery after interruptions\n    state *state.State\n}\n</code></pre> <p>The <code>Server</code> type is the main structure for the Block Persister Service. It contains components for managing stores, blockchain interactions, and state management.</p>"},{"location":"references/services/blockpersister_reference/#functions","title":"Functions","text":""},{"location":"references/services/blockpersister_reference/#server-management","title":"Server Management","text":""},{"location":"references/services/blockpersister_reference/#new","title":"New","text":"<pre><code>func New(\n    ctx context.Context,\n    logger ulogger.Logger,\n    tSettings *settings.Settings,\n    blockStore blob.Store,\n    subtreeStore blob.Store,\n    utxoStore utxo.Store,\n    blockchainClient blockchain.ClientI,\n    opts ...func(*Server)\n) *Server\n</code></pre> <p>Creates a new instance of the <code>Server</code> with the provided dependencies.</p> <p>This constructor initializes all components required for block persistence operations, including stores, state management, and client connections. It accepts optional configuration functions to customize the server instance after construction.</p> <p>Parameters:</p> <ul> <li>ctx: Context for controlling the server lifecycle</li> <li>logger: Logger for recording operational events and errors</li> <li>tSettings: Configuration settings that control server behavior</li> <li>blockStore: Storage interface for blocks</li> <li>subtreeStore: Storage interface for block subtrees</li> <li>utxoStore: Storage interface for UTXO data</li> <li>blockchainClient: Client for interacting with the blockchain service</li> <li>opts: Optional configuration functions to apply after construction</li> </ul> <p>Returns a fully constructed and configured Server instance ready for initialization.</p>"},{"location":"references/services/blockpersister_reference/#withsetinitialstate","title":"WithSetInitialState","text":"<pre><code>func WithSetInitialState(height uint32, hash *chainhash.Hash) func(*Server)\n</code></pre> <p>WithSetInitialState is an optional configuration function that sets the initial state of the block persister server. This can be used during initialization to establish a known starting point for block persistence operations.</p> <p>Parameters:</p> <ul> <li>height: The blockchain height to set as the initial state</li> <li>hash: The block hash corresponding to the specified height</li> </ul> <p>Returns a function that, when called with a Server instance, will set the initial state of that server. If the state cannot be set, an error is logged but not returned.</p>"},{"location":"references/services/blockpersister_reference/#health","title":"Health","text":"<pre><code>func (u *Server) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>Performs health checks on the server and its dependencies. This method implements the health.Check interface and is used by monitoring systems to determine the operational status of the service.</p> <p>The health check distinguishes between liveness (is the service running?) and readiness (is the service able to handle requests?) checks:</p> <ul> <li>Liveness checks verify the service process is running and responsive</li> <li>Readiness checks verify all dependencies are available and functioning</li> </ul> <p>Parameters:</p> <ul> <li>ctx: Context for coordinating cancellation or timeouts</li> <li>checkLiveness: When true, only liveness checks are performed; when false, both liveness and readiness checks are performed</li> </ul> <p>Returns:</p> <ul> <li>int: HTTP status code (200 for healthy, 503 for unhealthy)</li> <li>string: Human-readable status message</li> <li>error: Any error encountered during health checking</li> </ul> <p>Dependency checks include:</p> <ul> <li>Blockchain client and FSM status</li> <li>Block store availability</li> <li>Subtree store status</li> <li>UTXO store health</li> </ul>"},{"location":"references/services/blockpersister_reference/#init","title":"Init","text":"<pre><code>func (u *Server) Init(ctx context.Context) (err error)\n</code></pre> <p>Initializes the server, setting up any required resources.</p> <p>This method is called after construction but before the server starts processing blocks. It performs one-time initialization tasks such as setting up Prometheus metrics.</p> <p>Parameters:</p> <ul> <li>ctx: Context for coordinating initialization operations</li> </ul> <p>Returns an error if initialization fails, nil otherwise.</p>"},{"location":"references/services/blockpersister_reference/#start","title":"Start","text":"<pre><code>func (u *Server) Start(ctx context.Context, readyCh chan&lt;- struct{}) error\n</code></pre> <p>Initializes and begins the block persister service operations.</p> <p>This method starts the main processing loop and sets up HTTP services if configured. It waits for the blockchain FSM to transition from IDLE state before beginning block persistence operations to ensure the blockchain is ready.</p> <p>The method implements the following key operations:</p> <ul> <li>Waits for blockchain service readiness</li> <li>Sets up HTTP blob server if required by configuration</li> <li>Starts the main processing loop in a background goroutine</li> <li>Signals service readiness through the provided channel</li> </ul> <p>Parameters:</p> <ul> <li>ctx: Context for controlling the service lifecycle and handling cancellation</li> <li>readyCh: Channel used to signal when the service is ready to accept requests</li> </ul> <p>Returns an error if the service fails to start properly, nil otherwise.</p>"},{"location":"references/services/blockpersister_reference/#stop","title":"Stop","text":"<pre><code>func (u *Server) Stop(_ context.Context) error\n</code></pre> <p>Gracefully shuts down the server.</p> <p>This method is called when the service is being stopped and provides an opportunity to perform any necessary cleanup operations, such as closing connections, flushing buffers, or persisting state.</p> <p>Currently, the Server doesn't need to perform any specific cleanup actions during shutdown as resource cleanup is handled by the context cancellation mechanism in the Start method.</p> <p>Parameters:</p> <ul> <li>ctx: Context for controlling the shutdown operation (currently unused)</li> </ul> <p>Returns an error if shutdown fails, or nil on successful shutdown.</p>"},{"location":"references/services/blockpersister_reference/#internal-methods","title":"Internal Methods","text":""},{"location":"references/services/blockpersister_reference/#persistblock","title":"persistBlock","text":"<pre><code>func (u *Server) persistBlock(ctx context.Context, hash *chainhash.Hash, blockBytes []byte) error\n</code></pre> <p>Stores a block and its associated data to persistent storage.</p> <p>This is a core function of the blockpersister service that handles the complete persistence workflow for a single block. It ensures all components of a block (header, transactions, and UTXO changes) are properly stored in a consistent and recoverable manner.</p> <p>Processing Steps</p> <p>The function implements a multi-stage persistence process:</p> <ol> <li>Convert raw block bytes into a structured block model</li> <li>Create a new UTXO difference set for tracking changes</li> <li>Process the coinbase transaction if no subtrees are present</li> <li>For blocks with subtrees, process each subtree concurrently according to configured limits</li> <li>Close and finalize the UTXO difference set once all transactions are processed</li> <li>Write the complete block to persistent storage</li> </ol> <p>Parameters:</p> <ul> <li><code>ctx</code>: Context for the operation, used for cancellation and tracing</li> <li><code>hash</code>: Hash identifier of the block to persist</li> <li><code>blockBytes</code>: Raw serialized bytes of the complete block</li> </ul> <p>Returns an error if any part of the persistence process fails. The error will be wrapped with appropriate context to identify the specific failure point.</p> <p>Concurrency Management</p> <p>Concurrency is managed through errgroup with configurable parallel processing limits to optimize performance while avoiding resource exhaustion.</p> <p>Atomicity</p> <p>Block persistence is atomic - if any part fails, the entire operation is considered failed and should be retried after resolving the underlying issue.</p>"},{"location":"references/services/blockpersister_reference/#getnextblocktoprocess","title":"getNextBlockToProcess","text":"<pre><code>func (u *Server) getNextBlockToProcess(ctx context.Context) (*model.Block, error)\n</code></pre> <p>Retrieves the next block that needs to be processed based on the current state and configuration.</p> <p>This method determines the next block to persist by comparing the last persisted block height with the current blockchain tip. It ensures blocks are persisted in sequence without gaps and respects the configured persistence age policy to control how far behind persistence can lag.</p> <p>Processing Logic</p> <p>The method follows these steps:</p> <ol> <li>Get the last persisted block height from the state</li> <li>Get the current best block from the blockchain</li> <li>If the difference exceeds BlockPersisterPersistAge, return the next block</li> <li>Otherwise, return nil to indicate no blocks need processing yet</li> </ol> <p>Parameters:</p> <ul> <li><code>ctx</code>: Context for coordinating the block retrieval operation</li> </ul> <p>Returns:</p> <ul> <li><code>*model.Block</code>: The next block to process, or nil if no block needs processing yet</li> <li><code>error</code>: Any error encountered during the operation</li> </ul>"},{"location":"references/services/blockpersister_reference/#readsubtree","title":"readSubtree","text":"<pre><code>func (u *Server) readSubtree(ctx context.Context, subtreeHash chainhash.Hash) (*subtreepkg.Subtree, error)\n</code></pre> <p>Retrieves a subtree from the subtree store and deserializes it.</p> <p>This function is responsible for loading a subtree structure from persistent storage, which contains the hierarchical organization of transactions within a block. It retrieves the subtree file using the provided hash and deserializes it into a usable subtree object.</p> <p>Processing Steps</p> <p>The process includes:</p> <ol> <li>Attempting to read the subtree from the store using the provided hash</li> <li>If the primary read fails, it attempts to read from a secondary location (FileTypeSubtreeToCheck)</li> <li>Deserializing the retrieved subtree data into a subtree object</li> </ol> <p>Parameters:</p> <ul> <li><code>ctx</code>: Context for the operation, enabling cancellation and timeout handling</li> <li><code>subtreeHash</code>: Hash identifier of the subtree to retrieve and deserialize</li> </ul> <p>Returns:</p> <ul> <li><code>*subtreepkg.Subtree</code>: The deserialized subtree object ready for further processing</li> <li><code>error</code>: Any error encountered during retrieval or deserialization</li> </ul>"},{"location":"references/services/blockpersister_reference/#readsubtreedata","title":"readSubtreeData","text":"<pre><code>func (u *Server) readSubtreeData(ctx context.Context, subtreeHash chainhash.Hash) (*subtreepkg.SubtreeData, error)\n</code></pre> <p>Retrieves and deserializes subtree data from the subtree store.</p> <p>This internal method handles the two-stage process of loading subtree information: first retrieving the subtree structure itself, then loading the associated subtree data that contains the actual transaction references and metadata.</p> <p>Processing Steps</p> <p>The function performs these operations:</p> <ol> <li>Retrieves the subtree structure from the subtree store using the provided hash</li> <li>Deserializes the subtree to understand its structure and transaction organization</li> <li>Retrieves the corresponding subtree data file containing transaction references</li> <li>Deserializes the subtree data into a usable format for transaction processing</li> </ol> <p>Parameters:</p> <ul> <li><code>ctx</code>: Context for the operation, enabling cancellation and timeout handling</li> <li><code>subtreeHash</code>: Hash identifier of the subtree to retrieve and deserialize</li> </ul> <p>Returns:</p> <ul> <li><code>*subtreepkg.SubtreeData</code>: The deserialized subtree data ready for transaction processing</li> <li><code>error</code>: Any error encountered during retrieval or deserialization</li> </ul>"},{"location":"references/services/blockpersister_reference/#subtree-processing","title":"Subtree Processing","text":""},{"location":"references/services/blockpersister_reference/#processsubtree","title":"ProcessSubtree","text":"<pre><code>func (u *Server) ProcessSubtree(pCtx context.Context, subtreeHash chainhash.Hash, coinbaseTx *bt.Tx) error\n</code></pre> <p>Processes a subtree of transactions, validating and storing them.</p> <p>A subtree represents a hierarchical structure containing transaction references that make up part of a block. This method retrieves a subtree from the subtree store, processes all the transactions it contains, and creates subtree data for persistent storage.</p> <p>Processing Steps</p> <p>The process follows these key steps:</p> <ol> <li>Check if subtree data already exists - if it does, just set DAH and skip processing</li> <li>Retrieve the subtree from the subtree store using its hash</li> <li>Create subtree data from the subtree structure</li> <li>Add coinbase transaction if the first node is a coinbase placeholder</li> <li>Process transaction metadata using the store</li> <li>Serialize and store the complete subtree data</li> </ol> <p>Performance Optimization</p> <p>The method includes an optimization to skip processing if subtree data already exists, only updating the Delete-At-Height (DAH) setting for persistence.</p> <p>Parameters:</p> <ul> <li><code>pCtx</code>: Parent context for the operation, used for cancellation and tracing</li> <li><code>subtreeHash</code>: Hash identifier of the subtree to process</li> <li><code>coinbaseTx</code>: The coinbase transaction for the block containing this subtree</li> </ul> <p>Returns an error if any part of the subtree processing fails. Errors are wrapped with appropriate context to identify the specific failure point (storage, processing, etc.).</p> <p>Atomicity Note</p> <p>Processing is not atomic across multiple subtrees - each subtree is processed individually, allowing partial block processing to succeed even if some subtrees fail.</p>"},{"location":"references/services/blockpersister_reference/#writetxs","title":"WriteTxs","text":"<pre><code>func WriteTxs(_ context.Context, logger ulogger.Logger, writer *filestorer.FileStorer, txs []*bt.Tx, utxoDiff *utxopersister.UTXOSet) error\n</code></pre> <p>Writes a series of transactions to storage and processes their UTXO changes.</p> <p>This function handles the final persistence of transaction data to storage and optionally processes UTXO set changes. It's a critical component in the block persistence pipeline that ensures transactions are properly serialized and stored.</p> <p>Processing Steps</p> <p>The function performs the following steps:</p> <ol> <li> <p>For each transaction in the provided slice:</p> <ul> <li>Check for nil transactions and log errors if found</li> <li>Write the raw transaction bytes to storage (using normal bytes, not extended)</li> <li>If a UTXO diff is provided, process the transaction's UTXO changes</li> </ul> </li> <li> <p>Report any errors or validation issues encountered</p> </li> </ol> <p>The function includes safety checks to handle nil transactions, logging errors but continuing processing when possible to maximize resilience.</p> <p>Parameters:</p> <ul> <li><code>_</code>: Context parameter (currently unused in implementation)</li> <li><code>logger</code>: Logger for recording operations, errors, and warnings</li> <li><code>writer</code>: FileStorer destination for writing serialized transaction data</li> <li><code>txs</code>: Slice of transaction objects to write</li> <li><code>utxoDiff</code>: UTXO set difference tracker (optional, can be nil if UTXO tracking not needed)</li> </ul> <p>Returns an error if writing fails at any point. Specific error conditions include:</p> <ul> <li>Failure to write individual transaction data</li> <li>Errors during UTXO processing for transactions</li> </ul> <p>Atomicity Consideration</p> <p>The operation is not fully atomic - some transactions may be written successfully even if others fail. The caller should handle partial success scenarios appropriately.</p>"},{"location":"references/services/blockpersister_reference/#configuration","title":"Configuration","text":"<p>The service uses settings from the <code>settings.Settings</code> structure, primarily focused on the Block section. These settings control various aspects of block persistence behavior, from storage locations to processing strategies.</p>"},{"location":"references/services/blockpersister_reference/#block-settings","title":"Block Settings","text":""},{"location":"references/services/blockpersister_reference/#storage-configuration","title":"Storage Configuration","text":"<ul> <li><code>Block.StateFile</code>: File path for state storage. This file maintains persistence state across service restarts.</li> <li><code>Block.BlockStore</code>: Block store URL. Defines the location of the blob store used for block data.</li> </ul>"},{"location":"references/services/blockpersister_reference/#network-configuration","title":"Network Configuration","text":"<ul> <li><code>Block.PersisterHTTPListenAddress</code>: HTTP listener address for the blob server if enabled. Format should be \"host:port\".</li> </ul>"},{"location":"references/services/blockpersister_reference/#processing-configuration","title":"Processing Configuration","text":"<ul> <li><code>Block.BlockPersisterPersistAge</code>: Age threshold (in blocks) for block persistence. Controls how far behind the current tip blocks will be persisted. Higher values allow more blocks to accumulate before persistence occurs.</li> <li><code>Block.BlockPersisterPersistSleep</code>: Sleep duration between processing attempts when no blocks are available to process. Specified in milliseconds.</li> <li><code>Block.BatchMissingTransactions</code>: When true, enables batched retrieval of transaction metadata, which improves performance for high transaction volumes by reducing individual store requests.</li> </ul>"},{"location":"references/services/blockpersister_reference/#interaction-with-other-components","title":"Interaction with Other Components","text":"<p>Component Dependencies</p> <p>The BlockPersister service relies on interactions with several other components:</p> <ul> <li>Blockchain Service: Provides information about the current blockchain state and blocks to be persisted</li> <li>Block Store: Persistent storage for complete blocks</li> <li>Subtree Store: Storage for block subtrees containing transaction references</li> <li>UTXO Store: Storage for the current UTXO set and processing changes</li> </ul>"},{"location":"references/services/blockpersister_reference/#state-management","title":"State Management","text":"<p>The service maintains persistence state through the <code>state.State</code> component:</p> <p>State Management Features</p> <ul> <li>Tracks last persisted block height for sequential processing</li> <li>Manages block hash records for integrity verification</li> <li>Provides atomic state updates for consistency</li> </ul>"},{"location":"references/services/blockpersister_reference/#error-handling","title":"Error Handling","text":"<p>Error Handling Strategy</p> <p>The service implements comprehensive error handling:</p> <ul> <li>Storage errors: Trigger retries after delay</li> <li>Processing errors: Logged with context for debugging</li> <li>Configuration errors: Prevent service startup</li> <li>State management errors: Trigger recovery procedures</li> </ul>"},{"location":"references/services/blockpersister_reference/#metrics","title":"Metrics","text":"<p>The service provides Prometheus metrics for monitoring:</p> <ul> <li>Block persistence timing</li> <li>Subtree validation metrics</li> <li>Transaction processing stats</li> <li>Store health indicators</li> </ul>"},{"location":"references/services/blockpersister_reference/#dependencies","title":"Dependencies","text":"<p>Required components:</p> <ul> <li>Block Store (blob.Store)</li> <li>Subtree Store (blob.Store)</li> <li>UTXO Store (utxo.Store)</li> <li>Blockchain Client (blockchain.ClientI)</li> <li>Logger (ulogger.Logger)</li> <li>Settings (settings.Settings)</li> </ul>"},{"location":"references/services/blockpersister_reference/#processing-flow","title":"Processing Flow","text":""},{"location":"references/services/blockpersister_reference/#block-processing-loop","title":"Block Processing Loop","text":"<p>Block Processing Steps</p> <ol> <li>Check for next block to process based on persistence age</li> <li>Retrieve block data if available</li> <li>Persist block data to storage</li> <li>Update state with successful persistence</li> <li>Sleep if no blocks available or on error</li> </ol>"},{"location":"references/services/blockpersister_reference/#subtree-processing-flow","title":"Subtree Processing Flow","text":"<p>Subtree Processing Steps</p> <ol> <li>Retrieve subtree data from store</li> <li>Process transaction metadata for all transactions</li> <li>Write transactions to storage</li> <li>Update UTXO set with changes</li> <li>Handle errors with appropriate recovery</li> </ol>"},{"location":"references/services/blockpersister_reference/#health-checks","title":"Health Checks","text":"<p>Health Check Types</p> <p>The service implements two types of health checks:</p>"},{"location":"references/services/blockpersister_reference/#liveness-check","title":"Liveness Check","text":"<ul> <li>Basic service health validation</li> <li>No dependency checks</li> <li>Quick response for kubernetes probes</li> </ul>"},{"location":"references/services/blockpersister_reference/#readiness-check","title":"Readiness Check","text":"<ul> <li>Comprehensive dependency validation</li> <li>Store connectivity verification</li> <li>Service operational status</li> </ul>"},{"location":"references/services/blockpersister_reference/#other-resources","title":"Other Resources","text":"<ul> <li>Block Persister</li> <li>Prometheus Metrics</li> </ul>"},{"location":"references/services/blockvalidation_reference/","title":"Block Validation Service Reference Documentation","text":""},{"location":"references/services/blockvalidation_reference/#types","title":"Types","text":""},{"location":"references/services/blockvalidation_reference/#server","title":"Server","text":"<pre><code>type Server struct {\n    // UnimplementedBlockValidationAPIServer provides default implementations of gRPC methods\n    blockvalidation_api.UnimplementedBlockValidationAPIServer\n\n    logger                    ulogger.Logger                          // Structured logging with contextual information\n\n    settings                  *settings.Settings                      // Operational parameters and configuration\n    blockchainClient          blockchain.ClientI                      // Blockchain state and operations client\n\n    subtreeStore              blob.Store                              // Persistent storage for block subtrees\n    txStore                   blob.Store                              // Permanent storage for transactions\n\n    utxoStore                 utxo.Store                              // UTXO set management for transaction validation\n    blockAssemblyClient       blockassembly.ClientI                   // Block assembly service client\n\n    blockFoundCh              chan processBlockFound                  // Channel for newly discovered blocks\n    catchupCh                 chan processBlockCatchup                // Channel for catchup block processing\n\n    blockValidation           *BlockValidation                        // Core validation logic and state\n    validatorClient           validator.Interface                     // Transaction validation services\n\n    kafkaConsumerClient       kafka.KafkaConsumerGroupI              // Kafka message consumption client\n    processSubtreeNotify      *ttlcache.Cache[chainhash.Hash, bool]   // Cache for subtree processing state\n    stats                     *gocore.Stat                            // Operational metrics tracking\n    peerCircuitBreakers       *catchup.PeerCircuitBreakers            // Circuit breakers for peer management\n    peerMetrics               *catchup.CatchupMetrics                 // Peer performance metrics\n    headerChainCache          *catchup.HeaderChainCache               // Block header cache for catchup\n    isCatchingUp              atomic.Bool                             // Atomic flag for catchup operations\n    catchupStatsMu            sync.RWMutex                            // Mutex for catchup statistics\n    lastCatchupTime           time.Time                               // Timestamp of last catchup attempt\n    lastCatchupResult         bool                                    // Result of last catchup operation\n    catchupAttempts           atomic.Int64                            // Total catchup attempts counter\n    catchupSuccesses          atomic.Int64                            // Successful catchup operations counter\n}\n</code></pre> <p>The <code>Server</code> type implements a high-performance block validation service for Bitcoin SV. It coordinates block validation, subtree management, and transaction metadata processing across multiple subsystems while maintaining chain consistency. The server supports both synchronous and asynchronous validation modes, with automatic catchup capabilities when falling behind the chain tip.</p>"},{"location":"references/services/blockvalidation_reference/#processblockfound","title":"processBlockFound","text":"<pre><code>type processBlockFound struct {\n    hash    *chainhash.Hash // Block identifier using double SHA256 hash\n    baseURL string          // Peer URL for additional block data retrieval\n    peerID  string          // P2P peer identifier for metrics tracking\n    errCh   chan error      // Error channel for validation completion\n}\n</code></pre> <p>The <code>processBlockFound</code> type encapsulates information about a newly discovered block that requires validation. It includes both the block identifier and communication channels for handling validation results.</p>"},{"location":"references/services/blockvalidation_reference/#processblockcatchup","title":"processBlockCatchup","text":"<pre><code>type processBlockCatchup struct {\n    block   *model.Block // Full block data for validation\n    baseURL string       // Peer URL for additional data retrieval\n    peerID  string       // P2P peer identifier for metrics tracking\n}\n</code></pre> <p>The <code>processBlockCatchup</code> type contains information needed to process a block during chain catchup operations when the node has fallen behind the current chain tip.</p>"},{"location":"references/services/blockvalidation_reference/#blockvalidation","title":"BlockValidation","text":"<pre><code>type BlockValidation struct {\n    // logger provides structured logging capabilities\n    logger ulogger.Logger\n\n    // settings contains operational parameters and feature flags\n    settings *settings.Settings\n\n    // blockchainClient interfaces with the blockchain for operations\n    blockchainClient blockchain.ClientI\n\n    // subtreeStore provides persistent storage for block subtrees\n    subtreeStore blob.Store\n\n    // subtreeBlockHeightRetention specifies how long subtrees should be retained\n    subtreeBlockHeightRetention uint32\n\n    // txStore handles permanent storage of transactions\n    txStore blob.Store\n\n    // utxoStore manages the UTXO set for transaction validation\n    utxoStore utxo.Store\n\n    // validatorClient handles transaction validation operations\n    validatorClient validator.Interface\n\n    // recentBlocksBloomFilters maintains bloom filters for recent blocks\n    recentBlocksBloomFilters *txmap.SyncedMap[chainhash.Hash, *model.BlockBloomFilter]\n\n    // bloomFilterRetentionSize defines the number of blocks to keep the bloom filter for\n    bloomFilterRetentionSize uint32\n\n    // subtreeValidationClient manages subtree validation processes\n    subtreeValidationClient subtreevalidation.Interface\n\n    // subtreeDeDuplicator prevents duplicate processing of subtrees\n    subtreeDeDuplicator *DeDuplicator\n\n    // lastValidatedBlocks caches recently validated blocks for 2 minutes\n    lastValidatedBlocks *expiringmap.ExpiringMap[chainhash.Hash, *model.Block]\n\n    // blockExists tracks validated block hashes for 2 hours\n    blockExists *expiringmap.ExpiringMap[chainhash.Hash, bool]\n\n    // subtreeExists tracks validated subtree hashes for 10 minutes\n    subtreeExists *expiringmap.ExpiringMap[chainhash.Hash, bool]\n\n    // subtreeCount tracks the number of subtrees being processed\n    subtreeCount atomic.Int32\n\n    // blockHashesCurrentlyValidated tracks blocks in validation process\n    blockHashesCurrentlyValidated *txmap.SwissMap\n\n    // blockBloomFiltersBeingCreated tracks bloom filters being generated\n    blockBloomFiltersBeingCreated *txmap.SwissMap\n\n    // bloomFilterStats collects statistics about bloom filter operations\n    bloomFilterStats *model.BloomStats\n\n    // setMinedChan receives block hashes that need to be marked as mined\n    setMinedChan chan *chainhash.Hash\n\n    // revalidateBlockChan receives blocks that need revalidation\n    revalidateBlockChan chan revalidateBlockData\n\n    // stats tracks operational metrics for monitoring\n    stats *gocore.Stat\n\n    // invalidBlockKafkaProducer publishes invalid block events to Kafka\n    invalidBlockKafkaProducer kafka.KafkaAsyncProducerI\n\n    // backgroundTasks tracks background goroutines to ensure proper shutdown\n    backgroundTasks sync.WaitGroup\n}\n</code></pre> <p>The <code>BlockValidation</code> type handles the core validation logic for blocks in Teranode. It manages block validation, subtree processing, and bloom filter creation.</p>"},{"location":"references/services/blockvalidation_reference/#functions","title":"Functions","text":""},{"location":"references/services/blockvalidation_reference/#server_1","title":"Server","text":""},{"location":"references/services/blockvalidation_reference/#new","title":"New","text":"<pre><code>func New(logger ulogger.Logger, tSettings *settings.Settings, subtreeStore blob.Store, txStore blob.Store, utxoStore utxo.Store, validatorClient validator.Interface, blockchainClient blockchain.ClientI, kafkaConsumerClient kafka.KafkaConsumerGroupI) *Server\n</code></pre> <p>Creates a new instance of the <code>Server</code> with:</p> <ul> <li>Initialization of Prometheus metrics</li> <li>Setup of background processors</li> <li>Configuration of validation components</li> </ul>"},{"location":"references/services/blockvalidation_reference/#health","title":"Health","text":"<pre><code>func (u *Server) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>Performs comprehensive health checks:</p> <ul> <li>Liveness checks when checkLiveness is true</li> <li> <p>Dependency checks including:</p> <ul> <li>Kafka broker status</li> <li>Blockchain client</li> <li>FSM state</li> <li>Subtree store</li> <li>Transaction store</li> <li>UTXO store</li> </ul> </li> </ul>"},{"location":"references/services/blockvalidation_reference/#init","title":"Init","text":"<pre><code>func (u *Server) Init(ctx context.Context) (err error)\n</code></pre> <p>Initializes the service:</p> <ul> <li>Sets up subtree validation client</li> <li>Configures background processors</li> <li>Initializes Kafka consumer</li> <li>Sets up metadata processing queue</li> </ul>"},{"location":"references/services/blockvalidation_reference/#healthgrpc","title":"HealthGRPC","text":"<pre><code>func (u *Server) HealthGRPC(ctx context.Context, _ *blockvalidation_api.EmptyMessage) (*blockvalidation_api.HealthResponse, error)\n</code></pre> <p>Performs a gRPC health check on the service, including:</p> <ul> <li>Full dependency verification</li> <li>Detailed status reporting</li> <li>Timestamp information</li> </ul>"},{"location":"references/services/blockvalidation_reference/#start","title":"Start","text":"<pre><code>func (u *Server) Start(ctx context.Context) error\n</code></pre> <p>Starts all service components:</p> <ul> <li>Kafka consumer</li> <li>HTTP server (if configured)</li> <li>gRPC server</li> <li>Background processors</li> </ul>"},{"location":"references/services/blockvalidation_reference/#stop","title":"Stop","text":"<pre><code>func (u *Server) Stop(_ context.Context) error\n</code></pre> <p>Gracefully stops the service:</p> <ul> <li>Closes TTL cache</li> <li>Shuts down Kafka consumer</li> <li>Cleans up resources</li> </ul>"},{"location":"references/services/blockvalidation_reference/#blockfound","title":"BlockFound","text":"<pre><code>func (u *Server) BlockFound(ctx context.Context, req *blockvalidation_api.BlockFoundRequest) (*blockvalidation_api.EmptyMessage, error)\n</code></pre> <p>Handles notification of new blocks:</p> <ul> <li>Verifies block doesn't already exist</li> <li>Queues block for validation</li> <li>Optionally waits for validation completion</li> </ul>"},{"location":"references/services/blockvalidation_reference/#processblock","title":"ProcessBlock","text":"<pre><code>func (u *Server) ProcessBlock(ctx context.Context, request *blockvalidation_api.ProcessBlockRequest) (*blockvalidation_api.EmptyMessage, error)\n</code></pre> <p>Processes complete blocks:</p> <ul> <li>Validates block structure</li> <li>Handles height calculation</li> <li>Integrates with blockchain state</li> </ul>"},{"location":"references/services/blockvalidation_reference/#subtreefound","title":"SubtreeFound","text":"<pre><code>func (u *Server) SubtreeFound(_ context.Context, req *blockvalidation_api.SubtreeFoundRequest) (*blockvalidation_api.EmptyMessage, error)\n</code></pre> <p>Handles notification of new subtrees.</p>"},{"location":"references/services/blockvalidation_reference/#get","title":"Get","text":"<pre><code>func (u *Server) Get(ctx context.Context, request *blockvalidation_api.GetSubtreeRequest) (*blockvalidation_api.GetSubtreeResponse, error)\n</code></pre> <p>Retrieves subtree data from storage.</p>"},{"location":"references/services/blockvalidation_reference/#exists","title":"Exists","text":"<pre><code>func (u *Server) Exists(ctx context.Context, request *blockvalidation_api.ExistsSubtreeRequest) (*blockvalidation_api.ExistsSubtreeResponse, error)\n</code></pre> <p>Verifies subtree existence in storage.</p>"},{"location":"references/services/blockvalidation_reference/#settxmeta","title":"SetTxMeta","text":"<pre><code>func (u *Server) SetTxMeta(ctx context.Context, request *blockvalidation_api.SetTxMetaRequest) (*blockvalidation_api.SetTxMetaResponse, error)\n</code></pre> <p>Queues transaction metadata updates.</p>"},{"location":"references/services/blockvalidation_reference/#deltxmeta","title":"DelTxMeta","text":"<pre><code>func (u *Server) DelTxMeta(ctx context.Context, request *blockvalidation_api.DelTxMetaRequest) (*blockvalidation_api.DelTxMetaResponse, error)\n</code></pre> <p>Removes transaction metadata.</p>"},{"location":"references/services/blockvalidation_reference/#setminedmulti","title":"SetMinedMulti","text":"<pre><code>func (u *Server) SetMinedMulti(ctx context.Context, request *blockvalidation_api.SetMinedMultiRequest) (*blockvalidation_api.SetMinedMultiResponse, error)\n</code></pre> <p>Marks multiple transactions as mined in a block.</p>"},{"location":"references/services/blockvalidation_reference/#blockvalidation_1","title":"BlockValidation","text":""},{"location":"references/services/blockvalidation_reference/#validateblock","title":"ValidateBlock","text":"<pre><code>func (u *BlockValidation) ValidateBlock(ctx context.Context, block *model.Block, baseURL string, bloomStats *model.BloomStats, disableOptimisticMining ...bool) error\n</code></pre> <p>Performs comprehensive block validation:</p> <ul> <li>Size verification</li> <li>Parent block validation</li> <li>Subtree validation</li> <li>Optional optimistic mining</li> </ul>"},{"location":"references/services/blockvalidation_reference/#getblockexists","title":"GetBlockExists","text":"<pre><code>func (u *BlockValidation) GetBlockExists(ctx context.Context, hash *chainhash.Hash) (bool, error)\n</code></pre> <p>Checks block existence in validation system.</p>"},{"location":"references/services/blockvalidation_reference/#core-features","title":"Core Features","text":""},{"location":"references/services/blockvalidation_reference/#chain-catchup-process","title":"Chain Catchup Process","text":"<p>The Chain Catchup system automatically handles situations when the node falls behind the main blockchain. When activated, it:</p> <p>The system detects missing blocks by comparing the current node's block height with the network height. When it identifies a gap, it initiates the catchup process. The service retrieves blocks in batches of up to 100 blocks at a time to optimize network usage. During catchup, multiple blocks are validated simultaneously using configurable concurrency settings, typically defaulting to 32 concurrent validations. The service manages state transitions through the FSM (Finite State Machine), moving from normal operation to catchup mode and back once synchronization is complete.</p>"},{"location":"references/services/blockvalidation_reference/#optimistic-mining-support","title":"Optimistic Mining Support","text":"<p>Optimistic Mining is a performance optimization technique that allows faster block processing. In this mode:</p> <p>The system accepts new blocks before completing full validation, provisionally adding them to the chain. While the block is being added, validation continues in the background without blocking the main processing pipeline. This behavior can be configured through settings, allowing operators to balance between performance and validation thoroughness based on their requirements.</p>"},{"location":"references/services/blockvalidation_reference/#bloom-filters","title":"Bloom Filters","text":"<p>The Block Validation Service maintains bloom filters for recent blocks to optimize validation. This mechanism provides a probabilistic way to quickly determine if a transaction has been seen in a previous block without needing to perform extensive searches.</p> <p>Creates filters dynamically as new blocks are validated, maintaining them in memory for rapid access. The service uses a configurable retention size parameter (default set in settings) to determine how many recent blocks should maintain bloom filters. The service uses a thread-safe SyncedMap to ensure concurrent access to filters is handled properly.</p>"},{"location":"references/services/blockvalidation_reference/#service-configuration","title":"Service Configuration","text":"<p>The service configuration is managed through several key areas:</p>"},{"location":"references/services/blockvalidation_reference/#validation-settings","title":"Validation Settings","text":"<ul> <li>Maximum block size limits</li> <li>Block validation timeouts</li> <li>Transaction verification parameters</li> <li>Signature checking requirements</li> </ul>"},{"location":"references/services/blockvalidation_reference/#performance-controls","title":"Performance Controls","text":"<ul> <li>Maximum concurrent validations</li> <li>Batch processing sizes</li> <li>Queue buffer sizes</li> <li>Cache retention periods</li> </ul>"},{"location":"references/services/blockvalidation_reference/#network-configuration","title":"Network Configuration","text":"<ul> <li>Kafka broker addresses and topics</li> <li>HTTP server binding and ports</li> <li>Connection timeout values</li> <li>TLS/SSL settings</li> </ul>"},{"location":"references/services/blockvalidation_reference/#chain-catchup-parameters","title":"Chain Catchup Parameters","text":"<ul> <li>Maximum blocks per batch</li> <li>Catchup trigger thresholds</li> <li>Validation concurrency limits</li> <li>State transition timeouts</li> </ul>"},{"location":"references/services/blockvalidation_reference/#error-management","title":"Error Management","text":"<p>The service implements a structured error handling system:</p>"},{"location":"references/services/blockvalidation_reference/#error-categories","title":"Error Categories","text":"<p>Recoverable errors include temporary network issues or resource constraints that can be resolved through retries. Unrecoverable errors indicate fundamental problems like invalid block structures or consensus violations.</p>"},{"location":"references/services/blockvalidation_reference/#error-processing","title":"Error Processing","text":"<p>All errors are wrapped with appropriate context for the gRPC interface, maintaining error type information while adding relevant metadata. The system implements exponential backoff for retryable operations, with configurable retry limits and delays.</p>"},{"location":"references/services/blockvalidation_reference/#performance-monitoring","title":"Performance Monitoring","text":"<p>The service provides comprehensive performance monitoring through Prometheus metrics:</p>"},{"location":"references/services/blockvalidation_reference/#block-processing-metrics","title":"Block Processing Metrics","text":"<ul> <li>Time to validate blocks (histogram)</li> <li>Number of blocks in processing queue</li> <li>Block acceptance/rejection rates</li> <li>Chain catchup progress</li> </ul>"},{"location":"references/services/blockvalidation_reference/#resource-usage-metrics","title":"Resource Usage Metrics","text":"<ul> <li>Transaction queue lengths</li> <li>Memory cache utilization</li> <li>Store operation latencies</li> <li>Background worker statistics</li> </ul>"},{"location":"references/services/legacy_reference/","title":"Legacy Service Reference Documentation","text":""},{"location":"references/services/legacy_reference/#overview","title":"Overview","text":"<p>The Legacy Service provides backward compatibility with legacy Bitcoin peer-to-peer networking protocols. It serves as a bridge between modern Teranode services and older network nodes, enabling seamless communication across different protocol versions. The service integrates with multiple core services including blockchain management, validation, and storage systems to ensure consistent network operation.</p>"},{"location":"references/services/legacy_reference/#core-components","title":"Core Components","text":""},{"location":"references/services/legacy_reference/#server-structure","title":"Server Structure","text":"<p>The Server implements the peer service interface and manages connections with legacy network nodes. Its main structure includes:</p> <pre><code>type Server struct {\n    // UnimplementedPeerServiceServer is embedded to satisfy the gRPC interface\n    peer_api.UnimplementedPeerServiceServer\n\n    // logger provides logging functionality\n    logger ulogger.Logger\n\n    // settings contains the configuration settings for the server\n    settings *settings.Settings\n\n    // stats tracks server statistics\n    stats *gocore.Stat\n\n    // server is the internal server implementation\n    server *server\n\n    // lastHash stores the most recent block hash\n    lastHash *chainhash.Hash\n\n    // height represents the current blockchain height\n    height uint32\n\n    // blockchainClient handles blockchain operations\n    // Used for querying blockchain state and submitting new blocks\n    blockchainClient blockchain.ClientI\n\n    // validationClient handles transaction validation\n    // Used to validate incoming transactions before relay\n    validationClient validator.Interface\n\n    // subtreeStore provides storage for merkle subtrees\n    // Used in block validation and merkle proof verification\n    subtreeStore blob.Store\n\n    // tempStore provides temporary storage\n    // Used for ephemeral data storage during processing\n    tempStore blob.Store\n\n    // utxoStore manages the UTXO set\n    // Used for transaction validation and UTXO queries\n    utxoStore utxo.Store\n\n    // subtreeValidation handles merkle subtree validation\n    // Used to verify merkle proofs and validate block structure\n    subtreeValidation subtreevalidation.Interface\n\n    // blockValidation handles block validation\n    // Used to validate incoming blocks before acceptance\n    blockValidation blockvalidation.Interface\n\n    // blockAssemblyClient handles block assembly operations\n    // Used for mining and block template generation\n    blockAssemblyClient *blockassembly.Client\n}\n</code></pre> <p>Each component serves a specific purpose:</p> <ul> <li>logger: Provides structured logging capabilities</li> <li>settings: Contains configuration settings for the server and its components</li> <li>stats: Tracks server statistics and performance metrics</li> <li>server: Handles the internal peer-to-peer connection management</li> <li>lastHash: Stores the most recent block hash</li> <li>height: Represents the current blockchain height</li> <li>blockchainClient: Interface to the blockchain service for operations like querying state and submitting blocks</li> <li>validationClient: Interface to the transaction validation service</li> <li>subtreeStore: Blob storage interface for merkle subtree data</li> <li>tempStore: Temporary blob storage for ephemeral data</li> <li>utxoStore: Interface to the UTXO (Unspent Transaction Output) database</li> <li>subtreeValidation: Interface to the subtree validation service</li> <li>blockValidation: Interface to the block validation service</li> <li>blockAssemblyClient: Client for the block assembly service (used for mining)</li> </ul>"},{"location":"references/services/legacy_reference/#server-operations","title":"Server Operations","text":""},{"location":"references/services/legacy_reference/#initialization","title":"Initialization","text":"<p>The server initializes through the New function:</p> <pre><code>func New(logger ulogger.Logger,\n    tSettings *settings.Settings,\n    blockchainClient blockchain.ClientI,\n    validationClient validator.Interface,\n    subtreeStore blob.Store,\n    tempStore blob.Store,\n    utxoStore utxo.Store,\n    subtreeValidation subtreevalidation.Interface,\n    blockValidation blockvalidation.Interface,\n    blockAssemblyClient *blockassembly.Client,\n) *Server\n</code></pre> <p>This function creates a new server instance with all required dependencies and initializes Prometheus metrics for monitoring. It is the recommended constructor for the legacy server and sets up all necessary internal state required for proper operation. It does not start any network operations or begin listening for connections until the Start method is called.</p>"},{"location":"references/services/legacy_reference/#health-management","title":"Health Management","text":"<p>The server implements comprehensive health checking through the Health method:</p> <pre><code>func (s *Server) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>This method performs health checks at two levels:</p> <p>For liveness checks (when checkLiveness is true):</p> <ul> <li>Verifies basic server operation</li> <li>Returns quickly without checking dependencies</li> <li>Used to determine if the service should be restarted</li> </ul> <p>For readiness checks (when checkLiveness is false):</p> <ul> <li>Verifies BlockchainClient status and FSM state</li> <li>Checks ValidationClient functionality</li> <li>Confirms SubtreeStore availability</li> <li>Validates UTXOStore operation</li> <li>Verifies SubtreeValidation service</li> <li>Checks BlockValidation service</li> <li>Confirms BlockAssembly client status</li> <li>Checks peer connections to ensure at least one peer is connected</li> <li>Verifies recent peer activity within the last 2 minutes</li> </ul>"},{"location":"references/services/legacy_reference/#server-lifecycle-management","title":"Server Lifecycle Management","text":"<p>The server lifecycle is managed through several key methods:</p> <pre><code>func (s *Server) Init(ctx context.Context) error\n</code></pre> <p>The Init method performs essential setup:</p> <ul> <li>Sets up message size limits for the wire protocol</li> <li>Determines the server's public IP address for listening</li> <li>Creates and configures the internal server implementation</li> <li>Sets up initial peer connections from configuration</li> </ul> <p>Init does not start accepting connections or begin network operations. That happens when Start is called.</p> <pre><code>func (s *Server) Start(ctx context.Context, readyCh chan&lt;- struct{}) error\n</code></pre> <p>The Start method initializes and starts all components of the legacy service:</p> <ul> <li>Waits for the blockchain FSM to transition from IDLE state</li> <li>Starts the internal peer server to handle P2P connections</li> <li>Begins periodic peer statistics logging</li> <li>Sets up API key authentication for protected methods</li> <li>Launches the gRPC service to handle API requests</li> <li>Signals readiness by closing the readyCh channel</li> </ul> <p>Concurrency notes:</p> <ul> <li>Start launches multiple goroutines for different server components</li> <li>Each component runs independently but coordinates via the server state</li> <li>The gRPC service runs in the current goroutine and blocks until completion</li> </ul> <pre><code>func (s *Server) Stop(_ context.Context) error\n</code></pre> <p>The Stop method performs a clean shutdown of all server components:</p> <ul> <li>Closes all peer connections</li> <li>Stops all network listeners</li> <li>Shuts down the internal server state</li> <li>Releases any allocated resources</li> </ul>"},{"location":"references/services/legacy_reference/#peer-management","title":"Peer Management","text":"<pre><code>func (s *Server) GetPeerCount(ctx context.Context, _ *emptypb.Empty) (*peer_api.GetPeerCountResponse, error)\n</code></pre> <p>Returns the total number of connected peers. This method is part of the peer_api.PeerServiceServer gRPC interface.</p> <pre><code>func (s *Server) GetPeers(ctx context.Context, _ *emptypb.Empty) (*peer_api.GetPeersResponse, error)\n</code></pre> <p>This method provides detailed information about all connected peers including:</p> <ul> <li>Peer identification and addressing information</li> <li>Connection statistics (bytes sent/received, timing)</li> <li>Protocol and version information</li> <li>Blockchain synchronization status</li> <li>Ban score and whitelisting status</li> </ul> <p>This method is used by monitoring and control systems to observe the state of the peer network.</p>"},{"location":"references/services/legacy_reference/#ban-management","title":"Ban Management","text":"<pre><code>func (s *Server) IsBanned(ctx context.Context, peer *peer_api.IsBannedRequest) (*peer_api.IsBannedResponse, error)\n</code></pre> <p>Checks if a specific IP address or subnet is currently banned.</p> <pre><code>func (s *Server) ListBanned(ctx context.Context, _ *emptypb.Empty) (*peer_api.ListBannedResponse, error)\n</code></pre> <p>Returns a list of all currently banned IP addresses and subnets.</p> <pre><code>func (s *Server) ClearBanned(ctx context.Context, _ *emptypb.Empty) (*peer_api.ClearBannedResponse, error)\n</code></pre> <p>Removes all entries from the ban list, allowing previously banned addresses to reconnect.</p> <pre><code>func (s *Server) BanPeer(ctx context.Context, peer *peer_api.BanPeerRequest) (*peer_api.BanPeerResponse, error)\n</code></pre> <p>Bans a peer from connecting for a specified duration. The peer will be disconnected if currently connected and prevented from reconnecting until the ban expires.</p> <pre><code>func (s *Server) UnbanPeer(ctx context.Context, peer *peer_api.UnbanPeerRequest) (*peer_api.UnbanPeerResponse, error)\n</code></pre> <p>Removes a ban on a specific peer, allowing it to reconnect immediately. The request is processed asynchronously through a channel to the internal server component.</p>"},{"location":"references/services/legacy_reference/#authentication","title":"Authentication","text":"<p>The Legacy Service implements an authentication system for its gRPC API:</p> <ul> <li>Uses the <code>GRPCAdminAPIKey</code> setting for protected methods</li> <li>Automatically generates a secure 32-byte random API key if none is provided</li> <li>Restricts access to sensitive methods (BanPeer, UnbanPeer) through API key authentication</li> <li>Protected methods require the API key to be provided in the gRPC metadata</li> </ul>"},{"location":"references/services/legacy_reference/#configuration","title":"Configuration","text":"<p>The Legacy Service uses various configuration values from settings, including:</p> <ul> <li><code>settings.Legacy.ListenAddresses</code>: Addresses to listen on for incoming connections</li> <li><code>settings.Legacy.GRPCListenAddress</code>: Address for the gRPC service</li> <li><code>settings.Legacy.ConnectPeers</code>: Peer addresses to connect to on startup</li> <li><code>settings.GRPCAdminAPIKey</code>: API key for securing administrative gRPC methods</li> <li><code>settings.Asset.HTTPAddress</code>: HTTP address for the asset service</li> </ul>"},{"location":"references/services/legacy_reference/#dependencies","title":"Dependencies","text":"<p>The Legacy Service depends on several components:</p> <ul> <li><code>blockchain.ClientI</code>: Interface for blockchain operations</li> <li><code>validator.Interface</code>: Interface for validation operations</li> <li><code>blob.Store</code>: Store for subtree data</li> <li><code>utxo.Store</code>: Store for UTXO data</li> <li><code>subtreevalidation.Interface</code>: Interface for subtree validation</li> <li><code>blockvalidation.Interface</code>: Interface for block validation</li> <li><code>blockassembly.Client</code>: Client for block assembly</li> </ul> <p>These dependencies are injected into the <code>Server</code> struct during initialization</p>"},{"location":"references/services/p2p_reference/","title":"P2P Server Reference Documentation","text":""},{"location":"references/services/p2p_reference/#overview","title":"Overview","text":"<p>The P2P Server facilitates peer-to-peer communication within the Bitcoin SV network, managing the distribution of blocks, transactions, and network-related data. The server integrates with blockchain services, validation systems, and Kafka messaging to ensure efficient data propagation across the network. It implements both WebSocket and HTTP interfaces for peer communication while maintaining secure, scalable connections.</p>"},{"location":"references/services/p2p_reference/#core-components","title":"Core Components","text":""},{"location":"references/services/p2p_reference/#server-structure","title":"Server Structure","text":"<p>The P2P Server is implemented through the Server struct, which coordinates all peer-to-peer communication:</p> <pre><code>type Server struct {\n    p2p_api.UnimplementedPeerServiceServer\n    P2PNode                           p2p.NodeI          // The P2P network node instance from github.com/bsv-blockchain/go-p2p\n    logger                            ulogger.Logger     // Logger instance for the server\n    settings                          *settings.Settings // Configuration settings\n    bitcoinProtocolID                 string             // Bitcoin protocol identifier (format: \"teranode/bitcoin/{version}\")\n    blockchainClient                  blockchain.ClientI // Client for blockchain interactions\n    blockValidationClient             blockvalidation.Interface\n    blockAssemblyClient               blockassembly.ClientI     // Client for block assembly operations\n    AssetHTTPAddressURL               string                    // HTTP address URL for assets\n    e                                 *echo.Echo                // Echo server instance\n    notificationCh                    chan *notificationMsg     // Channel for notifications\n    rejectedTxKafkaConsumerClient     kafka.KafkaConsumerGroupI // Kafka consumer for rejected transactions\n    invalidBlocksKafkaConsumerClient  kafka.KafkaConsumerGroupI // Kafka consumer for invalid blocks\n    invalidSubtreeKafkaConsumerClient kafka.KafkaConsumerGroupI // Kafka consumer for invalid subtrees\n    subtreeKafkaProducerClient        kafka.KafkaAsyncProducerI // Kafka producer for subtrees\n    blocksKafkaProducerClient         kafka.KafkaAsyncProducerI // Kafka producer for blocks\n    banList                           BanListI                  // List of banned peers\n    banChan                           chan BanEvent             // Channel for ban events\n    banManager                        PeerBanManagerI           // Manager for peer banning (interface)\n    gCtx                              context.Context\n    blockTopicName                    string\n    subtreeTopicName                  string\n    miningOnTopicName                 string\n    rejectedTxTopicName               string\n    invalidBlocksTopicName            string       // Kafka topic for invalid blocks\n    invalidSubtreeTopicName           string       // Kafka topic for invalid subtrees\n    handshakeTopicName                string       // pubsub topic for version/verack\n    nodeStatusTopicName               string       // pubsub topic for node status messages\n    topicPrefix                       string       // Chain identifier prefix for topic validation\n    blockPeerMap                      sync.Map     // Map to track which peer sent each block (hash -&gt; peerMapEntry)\n    subtreePeerMap                    sync.Map     // Map to track which peer sent each subtree (hash -&gt; peerMapEntry)\n    startTime                         time.Time    // Server start time for uptime calculation\n    syncManager                       *SyncManager // Manager for peer synchronization and best peer selection\n    peerBlockHashes                   sync.Map     // Map to track peer best block hashes (peerID -&gt; hash string)\n    syncConnectionTimes               sync.Map     // Map to track when we first connected to each sync peer (peerID -&gt; timestamp)\n\n    // Cleanup configuration\n    peerMapCleanupTicker *time.Ticker  // Ticker for periodic cleanup of peer maps\n    peerMapMaxSize       int           // Maximum number of entries in peer maps\n    peerMapTTL           time.Duration // Time-to-live for peer map entries\n}\n</code></pre> <p>The server manages several key components, each serving a specific purpose in the P2P network:</p> <ul> <li>The P2PNode handles direct peer connections and message routing through the p2p.NodeI interface from the public github.com/bsv-blockchain/go-p2p package</li> <li>The bitcoinProtocolID contains the node's user agent string in format \"teranode/bitcoin/{version}\" where version is dynamically determined at build time from Git tags (e.g., \"v1.2.3\") or generated as a pseudo-version (e.g., \"v0.0.0-20250731141601-18714b9\")</li> <li>The various Kafka clients manage message distribution across the network</li> <li>The ban system maintains network security by managing peer access through BanListI and PeerBanManager</li> <li>The notification channel handles real-time event propagation</li> </ul>"},{"location":"references/services/p2p_reference/#p2pnodei-interface","title":"p2p.NodeI Interface","text":"<p>The P2P Server uses the p2p.NodeI interface from the public github.com/bsv-blockchain/go-p2p package. This interface-based design enables better testability and allows external developers to create custom P2P implementations:</p> <pre><code>type NodeI interface {\n    // Core lifecycle methods\n    Start(ctx context.Context, streamHandler func(network.Stream), topicNames ...string) error\n    Stop(ctx context.Context) error\n\n    // Topic-related methods\n    SetTopicHandler(ctx context.Context, topicName string, handler Handler) error\n    GetTopic(topicName string) *pubsub.Topic\n    Publish(ctx context.Context, topicName string, msgBytes []byte) error\n\n    // Peer management methods\n    HostID() peer.ID\n    ConnectedPeers() []PeerInfo\n    CurrentlyConnectedPeers() []PeerInfo\n    DisconnectPeer(ctx context.Context, peerID peer.ID) error\n    SendToPeer(ctx context.Context, pid peer.ID, msg []byte) error\n    SetPeerConnectedCallback(callback func(context.Context, peer.ID))\n    UpdatePeerHeight(peerID peer.ID, height int32)\n\n    // Stats methods\n    LastSend() time.Time\n    LastRecv() time.Time\n    BytesSent() uint64\n    BytesReceived() uint64\n\n    // Additional accessors\n    GetProcessName() string\n    UpdateBytesReceived(bytesCount uint64)\n}\n</code></pre>"},{"location":"references/services/p2p_reference/#server-operations","title":"Server Operations","text":""},{"location":"references/services/p2p_reference/#server-initialization","title":"Server Initialization","text":"<p>The server initializes through the NewServer function:</p> <pre><code>func NewServer( ctx context.Context,\n    logger ulogger.Logger,\n    tSettings *settings.Settings,\n    blockchainClient blockchain.ClientI,\n    rejectedTxKafkaConsumerClient kafka.KafkaConsumerGroupI,\n    subtreeKafkaProducerClient kafka.KafkaAsyncProducerI,\n    blocksKafkaProducerClient kafka.KafkaAsyncProducerI,\n) (*Server, error)\n</code></pre> <p>This function establishes the server with required settings and dependencies, including:</p> <ul> <li>P2P network configuration (IP, port, topics)</li> <li>Topic name generation for various message types</li> <li>Ban list initialization</li> <li>Connection to blockchain services</li> <li>Kafka producer and consumer setup</li> </ul>"},{"location":"references/services/p2p_reference/#health-management","title":"Health Management","text":"<p>The server implements comprehensive health checking through:</p> <pre><code>func (s *Server) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>This method performs two types of health verification:</p> <p>For liveness checks, it verifies basic server operation without dependency checks.</p> <p>For readiness checks, it verifies:</p> <ul> <li>Kafka broker connectivity</li> <li>Blockchain client functionality</li> <li>FSM state verification</li> <li>Block validation client status</li> </ul>"},{"location":"references/services/p2p_reference/#server-lifecycle-management","title":"Server Lifecycle Management","text":"<p>The server manages its lifecycle through several key methods:</p> <p>The Init method prepares the server for operation:</p> <pre><code>func (s *Server) Init(ctx context.Context) (err error)\n</code></pre> <p>It verifies and adjusts HTTP/HTTPS settings based on security requirements and establishes necessary connection URLs.</p> <p>The Start method initiates server operations:</p> <pre><code>func (s *Server) Start(ctx context.Context, readyCh chan&lt;- struct{}) error\n</code></pre> <p>It begins:</p> <ul> <li>Kafka message processing</li> <li>Block validation client setup</li> <li>HTTP/WebSocket server operation</li> <li>P2P network communication</li> <li>Blockchain subscription monitoring</li> <li>Ban event processing</li> <li>Once initialization is complete, it signals readiness by closing the readyCh channel</li> </ul> <p>The Stop method ensures graceful shutdown:</p> <pre><code>func (s *Server) Stop(ctx context.Context) error\n</code></pre> <p>It manages the orderly shutdown of all server components and connections.</p>"},{"location":"references/services/p2p_reference/#ban-management","title":"Ban Management","text":"<p>The P2P Server implements a sophisticated peer banning system through several components:</p> <pre><code>type BanEventHandler interface {\n    OnPeerBanned(peerID string, until time.Time, reason string)\n}\n</code></pre> <p>The <code>BanEventHandler</code> interface allows the system to react to ban events, which is implemented by the P2P Server.</p> <pre><code>type BanListI interface {\n    // IsBanned checks if a peer is banned by its IP address\n    IsBanned(ipStr string) bool\n\n    // Add adds an IP or subnet to the ban list with an expiration time\n    Add(ctx context.Context, ipOrSubnet string, expirationTime time.Time) error\n\n    // Remove removes an IP or subnet from the ban list\n    Remove(ctx context.Context, ipOrSubnet string) error\n\n    // ListBanned returns a list of all currently banned IP addresses and subnets\n    ListBanned() []string\n\n    // Subscribe returns a channel to receive ban events\n    Subscribe() chan BanEvent\n\n    // Unsubscribe removes a subscription to ban events\n    Unsubscribe(ch chan BanEvent)\n\n    // Init initializes the ban list and starts any background processes\n    Init(ctx context.Context) error\n\n    // Clear removes all entries from the ban list and cleans up resources\n    Clear()\n}\n</code></pre> <p>The <code>BanListI</code> interface defines the contract for managing banned peers by IP address or subnet, with methods for adding, removing, listing, and checking ban status. The system uses an SQL-backed implementation that persists ban information.</p> <pre><code>type PeerBanManager struct {\n    ctx           context.Context\n    mu            sync.RWMutex\n    peerBanScores map[string]*BanScore\n    reasonPoints  map[BanReason]int\n    banThreshold  int\n    banDuration   time.Duration\n    decayInterval time.Duration\n    decayAmount   int\n    handler       BanEventHandler\n}\n</code></pre> <p>The <code>PeerBanManager</code> implements the <code>PeerBanManagerI</code> interface and maintains scores for peers, automatically banning them when they exceed a threshold. It provides methods like:</p> <ul> <li><code>AddScore</code>: Increments a peer's score for specific violations</li> <li><code>GetBanScore</code>: Retrieves the current ban score and status</li> <li><code>IsBanned</code>: Checks if a peer is currently banned</li> </ul> <p>The system defines standard ban reasons with associated scoring:</p> <pre><code>type BanReason int\n\nconst (\n    ReasonUnknown BanReason = iota\n    ReasonInvalidSubtree     // 10 points\n    ReasonProtocolViolation  // 20 points\n    ReasonSpam              // 50 points\n    ReasonInvalidBlock      // 10 points\n)\n</code></pre> <p>Peer scores automatically decay over time to allow for recovery from temporary issues.</p>"},{"location":"references/services/p2p_reference/#message-handlers","title":"Message Handlers","text":"<ul> <li><code>handleHandshakeTopic</code>: Handles incoming handshake messages including version and verack exchanges.</li> <li><code>handleBlockTopic</code>: Handles incoming block messages.</li> <li><code>handleSubtreeTopic</code>: Handles incoming subtree messages.</li> <li><code>handleMiningOnTopic</code>: Handles incoming mining-on messages.</li> <li><code>handleNodeStatusTopic</code>: Handles incoming node status update messages.</li> <li><code>handleBanEvent</code>: Handles banning and unbanning events.</li> </ul>"},{"location":"references/services/p2p_reference/#message-structures","title":"Message Structures","text":"<p>The P2P service uses JSON-encoded messages for network communication:</p>"},{"location":"references/services/p2p_reference/#blockmessage","title":"BlockMessage","text":"<p>Announces the availability of a new block to the network:</p> <pre><code>{\n  \"Hash\": \"block_hash_hex\",\n  \"Height\": 123456,\n  \"DataHubURL\": \"http://node-address:port\",\n  \"PeerID\": \"peer_identifier\",\n  \"Header\": \"block_header_hex\"  // NEW: Raw block header in hexadecimal format\n}\n</code></pre> <p>The <code>Header</code> field was added to enable peers to quickly validate block properties without fetching the full block data. This reduces network latency and allows for faster block propagation decisions.</p>"},{"location":"references/services/p2p_reference/#subtreemessage","title":"SubtreeMessage","text":"<p>Announces the availability of a subtree (transaction batch):</p> <pre><code>{\n  \"Hash\": \"subtree_hash_hex\",\n  \"DataHubURL\": \"http://node-address:port\",\n  \"PeerID\": \"peer_identifier\"\n}\n</code></pre>"},{"location":"references/services/p2p_reference/#bestblockmessage","title":"BestBlockMessage","text":"<p>Requests or announces the current best block:</p> <pre><code>{\n  \"Height\": 123456,\n  \"Hash\": \"best_block_hash_hex\"\n}\n</code></pre>"},{"location":"references/services/p2p_reference/#miningonmessage","title":"MiningOnMessage","text":"<p>Notifies that mining has started on a new block:</p> <pre><code>{\n  \"Height\": 123457,\n  \"PreviousHash\": \"previous_block_hash_hex\"\n}\n</code></pre>"},{"location":"references/services/p2p_reference/#handshakemessage","title":"HandshakeMessage","text":"<p>Used for version/verack exchanges during peer connection:</p> <pre><code>{\n  \"type\": \"version\",  // or \"verack\"\n  \"peerID\": \"peer_identifier\",\n  \"bestHeight\": 123456,\n  \"bestHash\": \"best_block_hash_hex\",\n  \"dataHubURL\": \"http://node-address:port\",\n  \"userAgent\": \"teranode/bitcoin/v1.0.0\",\n  \"services\": 1,\n  \"topicPrefix\": \"mainnet\"  // Chain identifier for network isolation\n}\n</code></pre> <p>The <code>topicPrefix</code> field ensures that nodes only connect to peers on the same chain (e.g., mainnet, testnet). This prevents accidental cross-chain connections and maintains network integrity.</p>"},{"location":"references/services/p2p_reference/#handshake-protocol","title":"Handshake Protocol","text":"<p>The P2P handshake protocol establishes connections between peers and exchanges version information:</p> <ol> <li> <p>Version Message: When a peer connects, it sends a version message containing:</p> <ul> <li><code>UserAgent</code>: The node's identifier in format \"teranode/bitcoin/{version}\"</li> <li><code>BestHeight</code>: The peer's current blockchain height</li> <li><code>BestHash</code>: The hash of the peer's best block</li> <li><code>PeerID</code>: The peer's unique identifier</li> <li><code>TopicPrefix</code>: The chain identifier prefix (e.g., \"mainnet\", \"testnet\") for network isolation</li> <li><code>DataHubURL</code>: The URL where the peer's data can be accessed</li> <li><code>Services</code>: Bitmap of services offered by the peer</li> </ul> </li> <li> <p>Verack Message: Upon receiving a version message, the peer responds with a verack (version acknowledgment) containing similar information.</p> </li> <li> <p>Topic Prefix Validation: During handshake, peers validate that they share the same <code>TopicPrefix</code>:</p> <ul> <li>If topic prefixes don't match, the handshake is rejected</li> <li>This ensures nodes on different chains (mainnet vs testnet) don't accidentally connect</li> <li>The topic prefix is configured via <code>ChainCfgParams.TopicPrefix</code> in the settings</li> </ul> </li> <li> <p>Dynamic Version: The version in the UserAgent field is automatically determined at build time:</p> <ul> <li>Tagged releases use the Git tag (e.g., \"teranode/bitcoin/v1.2.3\")</li> <li>Development builds use a pseudo-version (e.g., \"teranode/bitcoin/v0.0.0-20250731141601-18714b9\")</li> </ul> </li> </ol>"},{"location":"references/services/p2p_reference/#key-processes","title":"Key Processes","text":""},{"location":"references/services/p2p_reference/#p2p-communication","title":"P2P Communication","text":"<p>The server uses a P2P node for communication with other peers in the network. It publishes and subscribes to various topics such as best blocks, blocks, subtrees, and mining-on notifications.</p>"},{"location":"references/services/p2p_reference/#blockchain-interaction","title":"Blockchain Interaction","text":"<p>The server interacts with the blockchain client to retrieve and validate block information. It also subscribes to blockchain notifications to propagate relevant information to peers.</p>"},{"location":"references/services/p2p_reference/#kafka-integration","title":"Kafka Integration","text":"<p>The server uses Kafka producers and consumers for efficient distribution of block and subtree data across the network. Kafka connections support TLS/SSL encryption for secure communication in production environments (see Kafka TLS Configuration section for details).</p>"},{"location":"references/services/p2p_reference/#configuration","title":"Configuration","text":"<p>The following settings can be configured for the p2p service:</p>"},{"location":"references/services/p2p_reference/#chain-configuration","title":"Chain Configuration","text":"<ul> <li><code>ChainCfgParams.TopicPrefix</code>: REQUIRED - Chain identifier prefix (e.g., \"mainnet\", \"testnet\", \"stn\")<ul> <li>Used during P2P handshake to ensure network isolation</li> <li>Peers with different topic prefixes will reject connections</li> <li>Prevents accidental cross-chain connections</li> <li>Must match across all nodes in the same network</li> </ul> </li> </ul>"},{"location":"references/services/p2p_reference/#network-configuration","title":"Network Configuration","text":"<ul> <li><code>p2p_listen_addresses</code>: Specifies the IP addresses for the P2P service to bind to.</li> <li><code>p2p_advertise_addresses</code>: Addresses to advertise to other peers in the network. Each address can be specified with or without a port (e.g., <code>192.168.1.1</code> or <code>example.com:9906</code>). When a port is not specified, the system will use the value from <code>p2p_port</code> as the default. Both IP addresses and domain names are supported. Format examples: <code>192.168.1.1</code>, <code>example.com:9906</code>, <code>node.local:8001</code>.</li> <li><code>p2p_port</code>: REQUIRED - Defines the port number on which the P2P service listens.</li> <li><code>p2p_block_topic</code>: REQUIRED - The topic name used for block-related messages in the P2P network.</li> <li><code>p2p_subtree_topic</code>: REQUIRED - Specifies the topic for subtree-related messages within the P2P network.</li> <li><code>p2p_handshake_topic</code>: REQUIRED - Defines the topic for peer handshake messages, used for version and verack exchanges.</li> <li><code>p2p_mining_on_topic</code>: REQUIRED - The topic used for messages related to the start of mining a new block.</li> <li><code>p2p_rejected_tx_topic</code>: REQUIRED - Specifies the topic for broadcasting information about rejected transactions.</li> <li><code>p2p_node_status_topic</code>: Topic for node status update messages.</li> <li><code>p2p_shared_key</code>: A shared key for securing P2P communications, required for private network configurations.</li> <li><code>p2p_dht_protocol_id</code>: Identifier for the DHT protocol used by the P2P network.</li> <li><code>p2p_dht_use_private</code>: A boolean flag indicating whether a private Distributed Hash Table (DHT) should be used, enhancing network privacy.</li> <li><code>p2p_optimise_retries</code>: A boolean setting to optimize retry behavior in P2P communications, potentially improving network efficiency.</li> <li><code>p2p_static_peers</code>: A list of static peer addresses to connect to, ensuring the P2P node can always reach known peers.</li> <li><code>p2p_private_key</code>: The private key for the P2P node, used for secure communications within the network. If not provided, a new Ed25519 key is automatically generated and persistently stored in the blockchain database.</li> <li><code>p2p_http_address</code>: Specifies the HTTP address for external clients to connect to the P2P service.</li> <li><code>p2p_http_listen_address</code>: Specifies the HTTP listen address for the P2P service, enabling HTTP-based interactions.</li> <li><code>p2p_grpc_address</code>: Specifies the gRPC address for external clients to connect to the P2P service.</li> <li><code>p2p_grpc_listen_address</code>: Specifies the gRPC listen address for the P2P service.</li> <li><code>p2p_bootstrap_addresses</code>: List of bootstrap peer addresses for initial network discovery.</li> <li><code>p2p_bootstrap_persistent</code>: A boolean flag (default: false) that controls whether bootstrap addresses are treated as persistent connections that automatically reconnect after disconnection.</li> <li><code>p2p_ban_threshold</code>: Score threshold at which peers are banned from the network.</li> <li><code>p2p_ban_duration</code>: Duration of time a peer remains banned after exceeding the ban threshold.</li> <li><code>securityLevelHTTP</code>: Defines the security level for HTTP communications, where a higher level might enforce HTTPS.</li> <li><code>server_certFile</code> and <code>server_keyFile</code>: These settings specify the paths to the SSL certificate and key files, respectively, required for setting up HTTPS.</li> <li><code>p2p_ban_default_duration</code>: Specifies the default duration for peer bans (defaults to 24 hours if not set).</li> <li><code>p2p_ban_persist_path</code>: Defines the path where ban list information is stored persistently.</li> <li><code>p2p_ban_max_entries</code>: Sets the maximum number of entries allowed in the ban list to prevent memory exhaustion.</li> </ul>"},{"location":"references/services/p2p_reference/#dependencies","title":"Dependencies","text":"<p>The P2P Server depends on several components:</p> <ul> <li><code>blockchain.ClientI</code>: Interface for blockchain operations</li> <li><code>blockvalidation.Client</code>: Client for block validation operations</li> <li><code>p2p.NodeI</code>: P2P node interface from the public <code>github.com/bsv-blockchain/go-p2p</code> package</li> <li>Kafka producers and consumers for message distribution</li> </ul> <p>These dependencies are injected into the <code>Server</code> struct during initialization.</p> <p>The use of the public go-p2p package enables external developers to:</p> <ul> <li>Create custom P2P node implementations that are compatible with Teranode</li> <li>Build applications that can directly integrate with Teranode's P2P network</li> <li>Extend P2P functionality while maintaining compatibility with the standard interface</li> </ul>"},{"location":"references/services/p2p_reference/#error-handling","title":"Error Handling","text":"<p>Errors are logged using the provided logger. Critical errors may result in the server shutting down or specific components failing to start.</p>"},{"location":"references/services/p2p_reference/#concurrency","title":"Concurrency","text":"<p>The server uses goroutines for handling concurrent operations, such as message processing, HTTP server, and blockchain subscription listening. It also uses contexts for cancellation and timeout management.</p>"},{"location":"references/services/p2p_reference/#security","title":"Security","text":"<p>The server supports both HTTP and HTTPS configurations based on the <code>securityLevelHTTP</code> setting. When using HTTPS, it requires certificate and key files to be specified in the configuration.</p>"},{"location":"references/services/propagation_reference/","title":"Propagation Server Reference Documentation","text":""},{"location":"references/services/propagation_reference/#overview","title":"Overview","text":"<p>The Propagation Server is a component of a Bitcoin SV implementation that handles the propagation of transactions across the network. It supports multiple communication protocols, including UDP and gRPC, and integrates with various services such as transaction validation, blockchain, and Kafka for efficient data distribution and processing.</p>"},{"location":"references/services/propagation_reference/#types","title":"Types","text":""},{"location":"references/services/propagation_reference/#propagationserver","title":"PropagationServer","text":"<pre><code>type PropagationServer struct {\n    propagation_api.UnsafePropagationAPIServer\n    logger                       ulogger.Logger                // Structured logging interface\n    settings                     *settings.Settings           // Service configuration settings\n    stats                        *gocore.Stat                 // Performance metrics collection\n    txStore                      blob.Store                   // Transaction storage backend\n    validator                    validator.Interface          // Transaction validation service\n    blockchainClient             blockchain.ClientI           // Blockchain state interface\n    validatorKafkaProducerClient kafka.KafkaAsyncProducerI    // Kafka producer for async validation\n    httpServer                   *echo.Echo                   // HTTP server for REST endpoints\n    validatorHTTPAddr            *url.URL                     // Validator HTTP endpoint URL\n}\n</code></pre> <p>The <code>PropagationServer</code> struct is the main type for the Propagation Server. It contains various components for transaction processing, validation, and distribution.</p>"},{"location":"references/services/propagation_reference/#functions","title":"Functions","text":""},{"location":"references/services/propagation_reference/#new","title":"New","text":"<pre><code>func New(logger ulogger.Logger, tSettings *settings.Settings, txStore blob.Store, validatorClient validator.Interface, blockchainClient blockchain.ClientI, validatorKafkaProducerClient kafka.KafkaAsyncProducerI) *PropagationServer\n</code></pre> <p>Creates a new instance of the Propagation Server with the provided dependencies.</p>"},{"location":"references/services/propagation_reference/#methods","title":"Methods","text":""},{"location":"references/services/propagation_reference/#health","title":"Health","text":"<pre><code>func (ps *PropagationServer) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>Performs health checks on the server and its dependencies.</p>"},{"location":"references/services/propagation_reference/#healthgrpc","title":"HealthGRPC","text":"<pre><code>func (ps *PropagationServer) HealthGRPC(ctx context.Context, _ *propagation_api.EmptyMessage) (*propagation_api.HealthResponse, error)\n</code></pre> <p>Performs a gRPC health check on the Propagation Server.</p>"},{"location":"references/services/propagation_reference/#init","title":"Init","text":"<pre><code>func (ps *PropagationServer) Init(_ context.Context) (err error)\n</code></pre> <p>Initializes the Propagation Server.</p>"},{"location":"references/services/propagation_reference/#start","title":"Start","text":"<pre><code>func (ps *PropagationServer) Start(ctx context.Context, readyCh chan&lt;- struct{}) (err error)\n</code></pre> <p>Starts the Propagation Server, including FSM state restoration (if configured), UDP6 multicast listeners, Kafka producer initialization, HTTP server, and gRPC server setup. Once initialized, it signals readiness by closing the readyCh channel. The function blocks until the gRPC server is running or an error occurs.</p>"},{"location":"references/services/propagation_reference/#stop","title":"Stop","text":"<pre><code>func (ps *PropagationServer) Stop(_ context.Context) error\n</code></pre> <p>Stops the Propagation Server.</p>"},{"location":"references/services/propagation_reference/#processtransaction","title":"ProcessTransaction","text":"<pre><code>func (ps *PropagationServer) ProcessTransaction(ctx context.Context, req *propagation_api.ProcessTransactionRequest) (*propagation_api.EmptyMessage, error)\n</code></pre> <p>Processes a single transaction.</p>"},{"location":"references/services/propagation_reference/#processtransactionbatch","title":"ProcessTransactionBatch","text":"<pre><code>func (ps *PropagationServer) ProcessTransactionBatch(ctx context.Context, req *propagation_api.ProcessTransactionBatchRequest) (*propagation_api.ProcessTransactionBatchResponse, error)\n</code></pre> <p>Processes a batch of transactions.</p>"},{"location":"references/services/propagation_reference/#additional-methods","title":"Additional Methods","text":""},{"location":"references/services/propagation_reference/#startudp6listeners","title":"StartUDP6Listeners","text":"<pre><code>func (ps *PropagationServer) StartUDP6Listeners(ctx context.Context, ipv6Addresses string) error\n</code></pre> <p>Initializes IPv6 multicast listeners for transaction propagation. It creates UDP listeners on specified interfaces and addresses, processing incoming transactions in separate goroutines. The <code>ipv6Addresses</code> parameter is a comma-separated list of IPv6 multicast addresses to listen on.</p>"},{"location":"references/services/propagation_reference/#http-server-methods","title":"HTTP Server Methods","text":"<pre><code>func (ps *PropagationServer) handleSingleTx(ctx context.Context) echo.HandlerFunc\n</code></pre> <p>Handles a single transaction request on the <code>/tx</code> endpoint.</p> <pre><code>func (ps *PropagationServer) handleMultipleTx(ctx context.Context) echo.HandlerFunc\n</code></pre> <p>Handles multiple transactions on the <code>/txs</code> endpoint.</p> <pre><code>func (ps *PropagationServer) startHTTPServer(ctx context.Context, httpAddresses string) error\n</code></pre> <p>Initializes and starts the HTTP server for transaction processing. The <code>httpAddresses</code> parameter is a comma-separated list of address:port combinations to bind to.</p> <pre><code>func (ps *PropagationServer) startAndMonitorHTTPServer(ctx context.Context, httpAddresses string)\n</code></pre> <p>Starts the HTTP server and monitors for shutdown. This method launches the HTTP server in a non-blocking manner and ensures proper cleanup when the context is canceled.</p>"},{"location":"references/services/propagation_reference/#internal-transaction-processing","title":"Internal Transaction Processing","text":"<pre><code>func (ps *PropagationServer) processTransaction(ctx context.Context, req *propagation_api.ProcessTransactionRequest) error\n</code></pre> <p>Handles the core transaction processing logic including validation, storage, and triggering async validation.</p> <pre><code>func (ps *PropagationServer) storeTransaction(ctx context.Context, btTx *bt.Tx) error\n</code></pre> <p>Persists a transaction to the configured storage backend using its chain hash as the key.</p> <pre><code>func (ps *PropagationServer) validateTransactionViaKafka(btTx *bt.Tx) error\n</code></pre> <p>Sends a transaction to the validator through Kafka.</p> <pre><code>func (ps *PropagationServer) validateTransactionViaHTTP(ctx context.Context, btTx *bt.Tx, txSize int, maxKafkaMessageSize int) error\n</code></pre> <p>Sends a transaction to the validator's HTTP endpoint. This is used as a fallback when Kafka message size limits are exceeded.</p> <pre><code>func (ps *PropagationServer) txSanityChecks(btTx *bt.Tx) error\n</code></pre> <p>Performs basic sanity checks on transactions to ensure they have at least one input and one output.</p>"},{"location":"references/services/propagation_reference/#key-processes","title":"Key Processes","text":""},{"location":"references/services/propagation_reference/#transaction-processing","title":"Transaction Processing","text":"<ol> <li>The server receives transactions through various protocols (UDP6 multicast, HTTP, gRPC).</li> <li>Transactions are validated to ensure they are not coinbase transactions and are in the extended format.</li> <li>Valid transactions are stored in the transaction store using their chain hash as the key.</li> <li>Transactions are sent to the validator either via Kafka or HTTP (for large transactions) for further processing.</li> </ol>"},{"location":"references/services/propagation_reference/#udp6-multicast-listening","title":"UDP6 Multicast Listening","text":"<p>The server listens on multiple IPv6 multicast addresses for incoming transactions. The implementation has the following characteristics:</p> <ul> <li>Supports configurable UDP datagram size (default: 512 bytes)</li> <li>Uses the default IPv6 port 9999 for multicast listeners</li> <li>Creates independent listeners for each multicast address specified in <code>settings.Propagation.IPv6Addresses</code></li> <li>Processes incoming datagrams concurrently through separate goroutines</li> </ul>"},{"location":"references/services/propagation_reference/#http-integration","title":"HTTP Integration","text":"<p>The server provides HTTP endpoints for transaction submission configured through <code>settings.Propagation.HTTPListenAddress</code>:</p> <ul> <li><code>/tx</code> endpoint for single transaction submissions</li> <li><code>/txs</code> endpoint for batch transaction submissions</li> <li><code>/health</code> endpoint for service health checks</li> <li>Supports rate limiting for API protection</li> <li>Implements middleware for recovery, CORS, request ID tracking, and logging</li> </ul>"},{"location":"references/services/propagation_reference/#kafka-integration","title":"Kafka Integration","text":"<p>The server uses a Kafka producer to send transactions to a validator service for asynchronous processing. When transactions exceed the Kafka message size limit, it automatically falls back to HTTP-based validation.</p>"},{"location":"references/services/propagation_reference/#configuration","title":"Configuration","text":"<p>The Propagation Server is configured through the settings system instead of directly using <code>gocore.Config()</code>, including:</p> <ul> <li><code>settings.Propagation.IPv6Addresses</code>: Comma-separated list of IPv6 multicast addresses for UDP listeners</li> <li><code>settings.Propagation.IPv6Interface</code>: Network interface for IPv6 multicast (default: \"en0\")</li> <li><code>settings.Propagation.HTTPListenAddress</code>: HTTP addresses for transaction submission endpoints</li> <li><code>settings.Propagation.HTTPRateLimit</code>: HTTP request rate limiting (requests per second)</li> <li><code>settings.Propagation.GRPCListenAddress</code>: gRPC server address for the Propagation API</li> <li><code>settings.Propagation.GRPCMaxConnectionAge</code>: Maximum age for gRPC connections before forced refresh</li> <li><code>settings.Validator.HTTPAddress</code>: HTTP address for the validator service (used for fallback validation)</li> <li><code>settings.Validator.KafkaMaxMessageBytes</code>: Maximum Kafka message size for transaction routing</li> </ul>"},{"location":"references/services/propagation_reference/#dependencies","title":"Dependencies","text":"<p>The Propagation Server depends on several components:</p> <ul> <li><code>blob.Store</code>: For storing transactions</li> <li><code>validator.Interface</code>: For transaction validation</li> <li><code>blockchain.ClientI</code>: For blockchain interactions</li> <li>Kafka producer for sending transactions to the validator</li> </ul> <p>These dependencies are injected into the <code>PropagationServer</code> struct during initialization.</p>"},{"location":"references/services/propagation_reference/#error-handling","title":"Error Handling","text":"<p>Errors are wrapped using a custom error package, providing additional context and maintaining consistency across the application. The server logs errors and, in many cases, returns them to the caller.</p>"},{"location":"references/services/propagation_reference/#concurrency","title":"Concurrency","text":"<p>The server uses goroutines and error groups for handling concurrent operations, such as processing batches of transactions. It also uses contexts for cancellation and timeout management.</p>"},{"location":"references/services/propagation_reference/#security","title":"Security","text":"<p>The server supports various security levels for HTTP/HTTPS configurations.</p>"},{"location":"references/services/propagation_reference/#metrics","title":"Metrics","text":"<p>The server initializes Prometheus metrics for monitoring various aspects of its operation, including:</p> <ul> <li>Processed transactions count and duration</li> <li>Transaction sizes</li> <li>Invalid transactions count</li> </ul>"},{"location":"references/services/propagation_reference/#extensibility","title":"Extensibility","text":"<p>The server is designed to be extensible, supporting multiple communication protocols (UDP, gRPC) for transaction ingestion. New protocols or processing methods can be added by implementing additional handlers and integrating them into the server's start-up process.</p>"},{"location":"references/services/rpc_reference/","title":"RPC Service Reference Documentation","text":""},{"location":"references/services/rpc_reference/#index","title":"Index","text":"<ul> <li>Overview</li> <li>Types<ul> <li>RPCServer</li> </ul> </li> <li>Functions<ul> <li>NewServer</li> </ul> </li> <li>Methods<ul> <li>Start</li> <li>Stop</li> <li>Init</li> <li>Health</li> <li>checkAuth</li> <li>jsonRPCRead</li> </ul> </li> <li>RPC Handlers</li> <li>Configuration</li> <li>Authentication</li> <li>General Format</li> <li>Supported RPC Commands<ul> <li>createrawtransaction - Creates a raw transaction without signing it</li> <li>generate - Mine blocks (for regression testing)</li> <li>generatetoaddress - Mine blocks to a specified address</li> <li>getbestblockhash - Returns the hash of the best (most recent) block in the longest blockchain</li> <li>getblock - Returns information about a block</li> <li>getblockbyheight - Returns information about a block at the specified height</li> <li>getblockhash - Returns the hash of a block at the specified height</li> <li>getblockheader - Returns information about a block header</li> <li>getblockchaininfo - Returns blockchain state information</li> <li>getdifficulty - Returns the proof-of-work difficulty</li> <li>getinfo - Returns general information about the node</li> <li>getmininginfo - Returns mining-related information</li> <li>getpeerinfo - Returns data about each connected network node</li> <li>getrawtransaction - Returns raw transaction data</li> <li>help - Returns help text for RPC commands</li> <li>getminingcandidate - Returns mining candidate information for generating a new block</li> <li>invalidateblock - Permanently marks a block as invalid</li> <li>isbanned - Checks if an IP/subnet is banned</li> <li>listbanned - Lists all banned IPs/subnets</li> <li>clearbanned - Clears all banned IPs</li> <li>reconsiderblock - Removes invalidity status from a block</li> <li>sendrawtransaction - Submits a raw transaction to the network</li> <li>setban - Attempts to add or remove an IP/subnet from the banned list</li> <li>stop - Stops the server</li> <li>submitminingsolution - Submits a mining solution to the network</li> <li>version - Returns the server version information</li> <li>freeze - Freezes specified UTXOs or OUTPUTs</li> <li>unfreeze - Unfreezes specified UTXOs or OUTPUTs</li> <li>reassign - Reassigns specified frozen UTXOs to a new address</li> </ul> </li> <li>Unimplemented RPC Commands</li> <li>Error Handling</li> <li>Rate Limiting</li> <li>Version Compatibility</li> <li>Concurrency</li> <li>Extensibility</li> <li>Limitations</li> <li>Security</li> </ul>"},{"location":"references/services/rpc_reference/#overview","title":"Overview","text":"<p>The RPC Service provides a JSON-RPC interface for interacting with the Bitcoin SV node. It handles various Bitcoin-related commands and manages client connections. The service implements a standard Bitcoin protocol interface while integrating with Teranode's modular architecture to provide high-performance access to blockchain data and network operations.</p>"},{"location":"references/services/rpc_reference/#types","title":"Types","text":""},{"location":"references/services/rpc_reference/#rpcserver","title":"RPCServer","text":"<pre><code>type RPCServer struct {\n    // settings contains the configuration parameters for the RPC server including\n    // authentication credentials, network binding options, and service parameters\n    settings *settings.Settings\n\n    // started indicates whether the server has been started (1) or not (0)\n    // This uses atomic operations for thread-safe access\n    started int32\n\n    // shutdown indicates whether the server is in the process of shutting down (1) or not (0)\n    // This uses atomic operations for thread-safe access\n    shutdown int32\n\n    // authsha contains the SHA256 hash of the HTTP basic auth string for admin-level access\n    // This is used for authenticating clients with full administrative privileges\n    authsha [sha256.Size]byte\n\n    // limitauthsha contains the SHA256 hash of the HTTP basic auth string for limited-level access\n    // This is used for authenticating clients with restricted access (read-only operations)\n    limitauthsha [sha256.Size]byte\n\n    // numClients tracks the number of connected RPC clients for connection limiting\n    // This uses atomic operations for thread-safe access\n    numClients int32\n\n    // statusLines maps HTTP status codes to their corresponding status text lines\n    // Used for proper HTTP response generation\n    statusLines map[int]string\n\n    // statusLock protects concurrent access to the status lines map\n    statusLock sync.RWMutex\n\n    // wg is used to wait for all goroutines to exit during shutdown\n    wg sync.WaitGroup\n\n    // requestProcessShutdown is closed when an authorized RPC client requests a shutdown\n    // This channel is used to notify the main process that a shutdown has been requested\n    requestProcessShutdown chan struct{}\n\n    // quit is used to signal the server to shut down\n    // All long-running goroutines should monitor this channel for termination signals\n    quit chan int\n\n    // logger provides structured logging capabilities for operational and debugging messages\n    logger ulogger.Logger\n\n    // rpcMaxClients is the maximum number of concurrent RPC clients allowed\n    // This setting helps prevent resource exhaustion from too many simultaneous connections\n    rpcMaxClients int\n\n    // rpcQuirks enables backwards-compatible quirks in the RPC server when true\n    // This improves compatibility with clients expecting legacy Bitcoin Core behavior\n    rpcQuirks bool\n\n    // listeners contains the network listeners for the RPC server\n    // Multiple listeners may be active for different IP addresses and ports\n    listeners []net.Listener\n\n    // blockchainClient provides access to blockchain data and operations\n    // Used for retrieving block information, chain state, and blockchain operations\n    blockchainClient blockchain.ClientI\n\n    // blockAssemblyClient provides access to block assembly and mining services\n    // Used for mining-related RPC commands like getminingcandidate and generate\n    blockAssemblyClient blockassembly.ClientI\n\n    // peerClient provides access to legacy peer network services\n    // Used for peer management and information retrieval\n    peerClient peer.ClientI\n\n    // p2pClient provides access to the P2P network services\n    // Used for modern peer management and network operations\n    p2pClient p2p.ClientI\n\n    // assetHTTPURL is the URL where assets (e.g., for HTTP UI) are served from\n    assetHTTPURL *url.URL\n\n    // helpCacher caches help text for RPC commands to improve performance\n    // Prevents regenerating help text for each request\n    helpCacher *helpCacher\n\n    // utxoStore provides access to the UTXO (Unspent Transaction Output) database\n    // Used for transaction validation and UTXO queries\n    utxoStore utxo.Store\n}\n</code></pre> <p>The RPCServer provides a concurrent-safe JSON-RPC server implementation for the Bitcoin protocol. It handles client authentication, request processing, response generation, and maintains connections to other Teranode services to fulfill RPC requests.</p> <p>The server implements a two-tier authentication system that separates administrative capabilities from limited-user operations, providing security through proper authorization. It supports standard Bitcoin Core RPC methods and Bitcoin SV extensions for compatibility with existing tools while enhancing functionality.</p> <p>The RPCServer is designed for concurrent operation, employing synchronization mechanisms to handle multiple simultaneous client connections without race conditions or resource conflicts. It implements proper connection management, graceful shutdown, and health monitoring.</p>"},{"location":"references/services/rpc_reference/#functions","title":"Functions","text":""},{"location":"references/services/rpc_reference/#newserver","title":"NewServer","text":"<pre><code>func NewServer(logger ulogger.Logger, tSettings *settings.Settings, blockchainClient blockchain.ClientI, utxoStore utxo.Store, blockAssemblyClient blockassembly.ClientI, peerClient peer.ClientI, p2pClient p2p.ClientI) (*RPCServer, error)\n</code></pre> <p>Creates a new instance of the RPC Service with the necessary dependencies including logger, settings, blockchain client, UTXO store, and service clients.</p> <p>This factory function creates a fully configured RPCServer instance, setting up:</p> <ul> <li>Authentication credentials from settings</li> <li>Connection limits and parameters</li> <li>Command handlers and help text</li> <li>Client connections to required services</li> </ul> <p>Parameters:</p> <ul> <li><code>logger</code>: Structured logger for operational and debug messages</li> <li><code>tSettings</code>: Configuration settings for the RPC server and related features</li> <li><code>blockchainClient</code>: Interface to the blockchain service for block and chain operations</li> <li><code>utxoStore</code>: Interface to the UTXO database for transaction validation</li> <li><code>blockAssemblyClient</code>: Interface to the block assembly service for mining operations</li> <li><code>peerClient</code>: Interface to the legacy peer network services</li> <li><code>p2pClient</code>: Interface to the P2P network services</li> </ul> <p>The RPC server requires connections to several other Teranode services to function properly, as it primarily serves as an API gateway to underlying node functionality. These dependencies are injected through this constructor to maintain proper service separation and testability.</p>"},{"location":"references/services/rpc_reference/#methods","title":"Methods","text":""},{"location":"references/services/rpc_reference/#start","title":"Start","text":"<pre><code>func (s *RPCServer) Start(ctx context.Context, readyCh chan&lt;- struct{}) error\n</code></pre> <p>Initialization Tasks</p> <p>Starts the RPC server, begins listening for client connections, and signals readiness by closing the readyCh channel once initialization is complete.</p> <p>This method performs several critical initialization tasks:</p> <pre><code>1. **Validates the server** has not already been started (using atomic operations)\n2. **Initializes network listeners** on all configured interfaces and ports\n3. **Launches goroutines** to accept and process incoming connections\n4. **Sets up proper handling** for clean shutdown\n5. **Signals readiness** through the provided channel\n</code></pre> <p>The server supports binding to multiple addresses simultaneously, allowing both IPv4 and IPv6 connections, as well as restricting access to localhost-only if configured for development or testing environments.</p>"},{"location":"references/services/rpc_reference/#stop","title":"Stop","text":"<pre><code>func (s *RPCServer) Stop(ctx context.Context) error\n</code></pre> <p>Gracefully stops the RPC server by:</p> <pre><code>1. **Setting shutdown flag** to prevent new connections\n2. **Closing all active listeners** to stop accepting new requests\n3. **Waiting for active connections** to complete their current operations\n4. **Cleaning up resources** and releasing network ports\n</code></pre> <p>This method implements a thread-safe shutdown mechanism using atomic operations to prevent multiple concurrent shutdown attempts. When called, it closes the quit channel to signal all goroutines to terminate, then waits for them to exit using the wait group before returning.</p>"},{"location":"references/services/rpc_reference/#init","title":"Init","text":"<pre><code>func (s *RPCServer) Init(ctx context.Context) (err error)\n</code></pre> <p>Performs second-stage initialization of the RPC server by establishing connections to dependent services that weren't available during initial construction.</p> <p>Initialization Steps</p> <p>This method completes the RPC server initialization by:</p> <ol> <li>Connecting to the Block Assembly service for mining-related operations</li> <li>Connecting to the P2P service for network peer management</li> <li>Connecting to the Legacy service for compatibility with older protocols</li> <li>Refreshing the help cache with complete command information</li> </ol> <p>The initialization is designed to be idempotent and can be safely called multiple times, though typically it's only called once after NewServer and before Start.</p>"},{"location":"references/services/rpc_reference/#health","title":"Health","text":"<pre><code>func (s *RPCServer) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>Reports the operational status of the RPC service for monitoring and health checking.</p> <p>This method implements the standard Teranode health checking interface used across all services for consistent monitoring, alerting, and orchestration.</p> <p>Health Check Types</p> <p>It provides both readiness and liveness checking capabilities to support different operational scenarios:</p> <ul> <li>Readiness: Indicates whether the service is ready to accept requests (listeners are bound and core dependencies are available)</li> <li>Liveness: Indicates whether the service is functioning correctly (listeners are still working and not in a hung state)</li> </ul> <p>Health Check Components:</p> <ul> <li>Verifying network listeners are active</li> <li>Checking connections to dependent services</li> <li>Validating internal state consistency</li> </ul>"},{"location":"references/services/rpc_reference/#checkauth","title":"checkAuth","text":"<pre><code>func (s *RPCServer) checkAuth(r *http.Request, require bool) (bool, bool, error)\n</code></pre> <p>Implements the two-tier HTTP Basic authentication system for RPC clients. It validates credentials supplied in the HTTP request against configured admin and limited-access username/password combinations.</p> <p>Authentication Flow</p> <p>The method implements a secure authentication flow that:</p> <ol> <li>Extracts the Authorization header from the HTTP request</li> <li>Validates the credentials against both admin and limited-user authentication strings</li> <li>Uses time-constant comparison operations to prevent timing attacks</li> <li>Distinguishes between admin users (who can perform state-changing operations) and limited users (who can only perform read-only operations)</li> </ol> <p>Security Considerations</p> <ul> <li>Uses SHA256 for credential hashing</li> <li>Implements constant-time comparison to prevent timing attacks</li> <li>Properly handles missing or malformed authentication headers</li> <li>Can be configured to require or not require authentication based on settings</li> </ul> <p>Returns:</p> <ul> <li><code>bool</code>: Authentication success (true if successful)</li> <li><code>bool</code>: Authorization level (true for admin access, false for limited access). The value specifies whether the user can change the state of the node.</li> <li><code>error</code>: Authentication error if any occurred, nil on success</li> </ul>"},{"location":"references/services/rpc_reference/#jsonrpcread","title":"jsonRPCRead","text":"<pre><code>func (s *RPCServer) jsonRPCRead(w http.ResponseWriter, r *http.Request, isAdmin bool)\n</code></pre> <p>Handles reading and responding to RPC messages. This method is the core request processing function.</p> <p>Request Processing Steps</p> <ol> <li>Parses incoming JSON-RPC requests from HTTP request bodies</li> <li>Validates request format and structure</li> <li>Routes requests to appropriate command handlers</li> <li>Formats and sends responses back to clients</li> <li>Implements proper error handling and serialization</li> <li>Ensures thread-safety for concurrent request handling</li> </ol> <p>The method supports batch requests, proper HTTP status codes, and includes safeguards against oversized or malformed requests. It also handles authorization checking to ensure admin-only commands cannot be executed by limited-access users.</p>"},{"location":"references/services/rpc_reference/#rpc-handlers","title":"RPC Handlers","text":"<p>The RPC Service implements various handlers for Bitcoin-related commands. All handlers follow a consistent function signature:</p> <pre><code>func handleCommand(ctx context.Context, s *RPCServer, cmd interface{}, closeChan &lt;-chan struct{}) (interface{}, error)\n</code></pre> <p>Each handler receives a context for cancellation and tracing, the RPC server instance, parsed command parameters, and a close notification channel. They return a properly formatted response object or an error.</p> <p>Some key handlers include:</p> <ul> <li><code>handleGetBlock</code>: Retrieves block information at different verbosity levels</li> <li><code>handleGetBlockHash</code>: Gets the hash of a block at a specific height</li> <li><code>handleGetBestBlockHash</code>: Retrieves the hash of the best (most recent) block</li> <li><code>handleCreateRawTransaction</code>: Creates a raw transaction</li> <li><code>handleSendRawTransaction</code>: Broadcasts a raw transaction to the network</li> <li><code>handleGetMiningCandidate</code>: Retrieves a candidate block for mining</li> <li><code>handleSubmitMiningSolution</code>: Submits a solved block to the network</li> <li><code>handleFreeze</code>: Freezes specified UTXOs to prevent spending</li> <li><code>handleUnfreeze</code>: Unfreezes previously frozen UTXOs</li> <li><code>handleReassign</code>: Reassigns specified frozen UTXOs to a new address</li> <li><code>handleSetBan</code>: Adds or removes IP addresses/subnets from the node's ban list</li> <li><code>handleClearBanned</code>: Removes all bans from the node</li> <li><code>handleListBanned</code>: Lists all currently banned IP addresses and subnets</li> </ul>"},{"location":"references/services/rpc_reference/#configuration","title":"Configuration","text":"<p>RPC Configuration Settings</p> <p>The RPC Service uses various configuration values:</p> <p>Authentication Settings:</p> <ul> <li><code>rpc_user</code> and <code>rpc_pass</code>: Credentials for RPC authentication with full admin privileges</li> <li><code>rpc_limit_user</code> and <code>rpc_limit_pass</code>: Credentials for limited RPC access (read-only operations)</li> </ul> <p>Connection Settings:</p> <ul> <li><code>rpc_max_clients</code>: Maximum number of concurrent RPC clients (default: 1000)</li> <li><code>rpc_listener_url</code>: URL for the RPC listener (default: \"http://localhost:8332\")</li> <li><code>rpc_listeners</code>: List of URLs for multiple RPC listeners (overrides rpc_listener_url if set)</li> </ul> <p>Security Settings:</p> <ul> <li><code>rpc_tls_enabled</code>: Enables TLS for secure RPC connections (default: false)</li> <li><code>rpc_tls_cert_file</code>: Path to TLS certificate file</li> <li><code>rpc_tls_key_file</code>: Path to TLS private key file</li> <li><code>rpc_auth_timeouts_seconds</code>: Timeout for authentication in seconds (default: 10)</li> </ul> <p>Timeout Settings:</p> <ul> <li><code>rpc_timeout</code>: Maximum time allowed for an RPC call to complete (default: 30s)<ul> <li>Prevents resource exhaustion from long-running operations</li> <li>Returns error code -30 (ErrRPCTimeout) on timeout</li> </ul> </li> <li><code>rpc_client_call_timeout</code>: Timeout for internal client calls to other services (default: 5s)<ul> <li>Used when RPC handlers call P2P, Legacy peer, or other internal services</li> <li>Prevents hanging when dependent services are unresponsive</li> </ul> </li> </ul> <p>Compatibility Settings:</p> <ul> <li><code>rpc_quirks</code>: Enables compatibility quirks for legacy clients (default: false)</li> </ul> <p>Production Warnings</p> <ul> <li><code>rpc_disable_auth</code>: Disables authentication (NOT recommended for production)</li> <li><code>rpc_cross_origin</code>: Allows cross-origin requests (NOT recommended for production)</li> </ul> <p>Configuration values can be provided through the configuration file, environment variables, or command-line flags, with precedence in that order.</p>"},{"location":"references/services/rpc_reference/#authentication","title":"Authentication","text":"<p>Authentication Levels</p> <p>The server supports two levels of authentication:</p> <ol> <li>Admin-level access with full permissions</li> <li>Limited access with restricted permissions</li> </ol> <p>Authentication is performed using HTTP Basic Auth.</p> <p>Credential Provision Methods:</p> <ul> <li>Username and password in the HTTP header</li> <li>Cookie-based authentication</li> <li>Configuration file settings</li> </ul>"},{"location":"references/services/rpc_reference/#grpc-api-key-authentication","title":"GRPC API Key Authentication","text":"<p>For GRPC services, certain administrative operations require additional API key authentication:</p> <ul> <li>Protected Methods: <code>BanPeer</code> and <code>UnbanPeer</code> operations in both P2P and Legacy GRPC services require API key authentication</li> <li>Configuration: Set via <code>grpc_admin_api_key</code> setting in the configuration file</li> <li>Auto-generation: If no API key is configured, the system generates a random 32-byte key at startup</li> <li>Usage: API key must be included in GRPC requests as metadata with the key <code>x-api-key</code></li> </ul> <p>Security Note</p> <p>The API key provides administrative access to ban/unban operations. Ensure it is properly secured and not exposed in logs or configuration files in production environments.</p>"},{"location":"references/services/rpc_reference/#general-format","title":"General Format","text":"<p>Request Requirements</p> <p>All requests should be POST requests with Content-Type: application/json.</p> <p>Request format:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"id\",\n    \"method\": \"method_name\",\n    \"params\": []\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#supported-rpc-commands","title":"Supported RPC Commands","text":"<p>The following RPC commands are fully implemented and supported in the current version of the node.</p>"},{"location":"references/services/rpc_reference/#createrawtransaction","title":"createrawtransaction","text":"<p>Creates a raw Bitcoin transaction without signing it.</p> <p>Parameters:</p> <ol> <li><code>inputs</code> (array, required):</li> </ol> <pre><code>[\n  {\n    \"txid\": \"hex_string\",       // The transaction id\n    \"vout\": n                   // The output number\n  }\n]\n</code></pre> <ol> <li><code>outputs</code> (object, required):</li> </ol> <pre><code>{\n  \"address\": x.xxx,            // Bitcoin address:amount pairs\n  \"data\": \"hex\"               // Optional data output\n}\n</code></pre> <p>Returns:</p> <ul> <li><code>string</code> - hex-encoded raw transaction</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"createrawtransaction\",\n    \"params\": [\n        [{\"txid\":\"1234abcd...\", \"vout\":0}],\n        {\"1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa\": 0.01}\n    ]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": \"0200000001abcd1234...00000000\",\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#generate","title":"generate","text":"<p>Mines blocks immediately (for testing only).</p> <p>Parameters:</p> <ol> <li><code>nblocks</code> (numeric, required) - Number of blocks to generate</li> <li><code>maxtries</code> (numeric, optional) - Maximum number of iterations to try</li> </ol> <p>Returns:</p> <ul> <li><code>array</code> - hashes of blocks generated</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"generate\",\n    \"params\": [1, 1000000]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": [\n        \"36252b5852a5921bdfca8701f936b39edeb1f8c39fffe73b0d8437921401f9af\"\n    ],\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getbestblockhash","title":"getbestblockhash","text":"<p>Returns the hash of the best (tip) block in the longest blockchain.</p> <p>Parameters: none</p> <p>Returns:</p> <ul> <li><code>string</code> - The block hash</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getbestblockhash\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": \"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\",\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getblock","title":"getblock","text":"<p>Returns information about a block.</p> <p>Parameters:</p> <ol> <li><code>blockhash</code> (string, required) - The block hash</li> <li><code>verbosity</code> (numeric, optional, default=1) - 0 for hex-encoded data, 1 for a json object, 2 for json object with transaction data</li> </ol> <p>Returns:</p> <ul> <li>If verbosity is 0: <code>string</code> - hex-encoded block data</li> <li>If verbosity is 1 or 2: <code>object</code> - JSON object with block information</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getblock\",\n    \"params\": [\n        \"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\",\n        1\n    ]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": {\n        \"hash\": \"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\",\n        \"confirmations\": 1,\n        \"size\": 1000,\n        \"height\": 1000,\n        \"version\": 1,\n        \"versionHex\": \"00000001\",\n        \"merkleroot\": \"4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b\",\n        \"tx\": [\"4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b\"],\n        \"time\": 1231006505,\n        \"mediantime\": 1231006505,\n        \"nonce\": 2083236893,\n        \"bits\": \"1d00ffff\",\n        \"difficulty\": 1,\n        \"chainwork\": \"0000000000000000000000000000000000000000000000000000000100010001\",\n        \"previousblockhash\": \"00000000839a8e6886ab5951d76f411475428afc90947ee320161bbf18eb6048\"\n    },\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getblockbyheight","title":"getblockbyheight","text":"<p>Returns information about a block at the specified height in the blockchain. This is similar to <code>getblock</code> but uses a height parameter instead of a hash.</p> <p>Parameters:</p> <ol> <li><code>height</code> (numeric, required) - The height of the block</li> <li><code>verbosity</code> (numeric, optional, default=1) - 0 for hex-encoded data, 1 for a json object, 2 for json object with transaction data</li> </ol> <p>Returns:</p> <ul> <li>If verbosity is 0: <code>string</code> - hex-encoded block data</li> <li>If verbosity is 1 or 2: <code>object</code> - JSON object with block information</li> </ul> <p>Example Request:</p> <pre><code>{\n   \"jsonrpc\": \"1.0\",\n   \"id\": \"curltest\",\n   \"method\": \"getblockbyheight\",\n   \"params\": [2]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n   \"result\":\n    {\n       \"hash\": \"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\",\n       \"height\": 2,\n       \"version\": 1,\n       \"versionHex\": \"00000001\",\n       \"merkleroot\": \"4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b\",\n       \"time\": 1231006505,\n       \"nonce\": 2083236893,\n       \"bits\": \"1d00ffff\",\n       \"difficulty\": 1,\n       \"previousblockhash\": \"00000000839a8e6886ab5951d76f411475428afc90947ee320161bbf18eb6048\",\n       \"tx\": [\"4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b\"],\n       \"size\": 285,\n       \"confirmations\": 750000\n    },\n   \"error\": null,\n   \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getblockhash","title":"getblockhash","text":"<p>Returns the hash of block at the specified height in the blockchain.</p> <p>Parameters:</p> <ol> <li><code>height</code> (numeric, required) - The height of the block</li> </ol> <p>Returns:</p> <ul> <li><code>string</code> - The block hash</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getblockhash\",\n    \"params\": [2]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": \"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\",\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getblockheader","title":"getblockheader","text":"<p>Returns information about a block header.</p> <p>Parameters:</p> <ol> <li><code>hash</code> (string, required) - The block hash</li> <li><code>verbose</code> (boolean, optional, default=true) - true for a json object, false for the hex-encoded data</li> </ol> <p>Returns:</p> <ul> <li>If verbose=false: <code>string</code> - hex-encoded block header</li> <li>If verbose=true: <code>object</code> - JSON object with header information</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getblockheader\",\n    \"params\": [\n        \"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\",\n        true\n    ]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": {\n        \"hash\": \"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\",\n        \"version\": 1,\n        \"versionHex\": \"00000001\",\n        \"previoushash\": \"00000000839a8e6886ab5951d76f411475428afc90947ee320161bbf18eb6048\",\n        \"merkleroot\": \"4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b\",\n        \"time\": 1231006505,\n        \"nonce\": 2083236893,\n        \"bits\": \"1d00ffff\",\n        \"difficulty\": 1,\n        \"height\": 1000\n    },\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getdifficulty","title":"getdifficulty","text":"<p>Returns the proof-of-work difficulty as a multiple of the minimum difficulty.</p> <p>Parameters: none</p> <p>Returns:</p> <ul> <li><code>number</code> - The current difficulty</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getdifficulty\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": 21448277761059.71,\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getmininginfo","title":"getmininginfo","text":"<p>Returns a json object containing mining-related information.</p> <p>Parameters: none</p> <p>Returns:</p> <pre><code>{\n    \"blocks\": number,           // The current block count\n    \"currentblocksize\": number, // The last block size\n    \"currentblocktx\": number,   // The last block transaction count\n    \"difficulty\": number,       // The current difficulty\n    \"errors\": \"string\",        // Current errors\n    \"networkhashps\": number,   // The estimated network hashes per second\n    \"chain\": \"string\"          // Current network name (main, test, regtest)\n}\n</code></pre> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getmininginfo\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": {\n        \"blocks\": 750000,\n        \"currentblocksize\": 1000000,\n        \"currentblocktx\": 2000,\n        \"difficulty\": 21448277761059.71,\n        \"errors\": \"\",\n        \"networkhashps\": 7.088e+17,\n        \"chain\": \"main\"\n    },\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#sendrawtransaction","title":"sendrawtransaction","text":"<p>Submits a raw transaction to the network.</p> <p>Parameters:</p> <ol> <li><code>hexstring</code> (string, required) - The hex string of the raw transaction</li> <li><code>allowhighfees</code> (boolean, optional, default=false) - Allow high fees</li> </ol> <p>Returns:</p> <ul> <li><code>string</code> - The transaction hash in hex</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"sendrawtransaction\",\n    \"params\": [\"0200000001abcd1234...00000000\"]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": \"a1b2c3d4e5f6...\",\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getminingcandidate","title":"getminingcandidate","text":"<p>Returns information for creating a new block.</p> <p>Parameters:</p> <ol> <li> <p><code>parameters</code> (object, optional):</p> </li> <li> <p><code>coinbaseValue</code> (numeric, optional): Custom coinbase value in satoshis</p> </li> </ol> <p>Returns:</p> <pre><code>{\n    \"id\": \"string\",         // Mining candidate ID\n    \"prevhash\": \"string\",   // Previous block hash\n    \"coinbase\": \"string\",   // Coinbase transaction\n    \"coinbaseValue\": number,  // Coinbase value in satoshis\n    \"version\": number,           // Block version\n    \"nBits\": \"string\",          // Compressed difficulty target\n    \"time\": number,             // Current timestamp\n    \"height\": number,           // Block height\n    \"num_tx\": number,           // Number of transactions\n    \"merkleProof\": [\"string\"],  // Merkle proof\n    \"merkleRoot\": \"string\"      // Merkle root\n}\n</code></pre> <p>Example Request (standard):</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getminingcandidate\",\n    \"params\": []\n}\n</code></pre> <p>Example Request (with custom coinbase value):</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getminingcandidate\",\n    \"params\": [{\"coinbaseValue\": 5000000000}]\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#submitminingsolution","title":"submitminingsolution","text":"<p>Submits a solved block to the network.</p> <p>Parameters:</p> <ol> <li><code>solution</code> (object, required):</li> </ol> <pre><code>{\n    \"id\": \"string\",          // Mining candidate ID\n    \"nonce\": number,         // Block nonce\n    \"coinbase\": \"string\",    // Coinbase transaction\n    \"time\": number,          // Block time\n    \"version\": number        // Block version\n}\n</code></pre> <p>Returns:</p> <ul> <li><code>boolean</code> - True if accepted, false if rejected</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"submitminingsolution\",\n    \"params\": [{\n        \"id\": \"100000\",\n        \"nonce\": 1234567890,\n        \"coinbase\": \"01000000...\",\n        \"time\": 1631234567,\n        \"version\": 1\n    }]\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getblockchaininfo","title":"getblockchaininfo","text":"<p>Returns state information about blockchain processing.</p> <p>Parameters: none</p> <p>Returns:</p> <pre><code>{\n    \"chain\": \"string\",              // Current network name (main, test, regtest)\n    \"blocks\": number,               // Current number of blocks processed\n    \"headers\": number,              // Current number of headers processed\n    \"bestblockhash\": \"string\",      // Hash of the currently best block\n    \"difficulty\": number,           // Current difficulty\n    \"mediantime\": number,          // Median time for the current best block\n    \"verificationprogress\": number, // Estimate of verification progress [0..1]\n    \"chainwork\": \"string\",         // Total amount of work in active chain, in hexadecimal\n    \"pruned\": boolean,             // If the blocks are subject to pruning\n    \"softforks\": [                 // Status of softforks\n        {\n            \"id\": \"string\",        // Name of the softfork\n            \"version\": number,     // Block version that signals this softfork\n            \"enforce\": {           // Progress toward enforcing the softfork rules\n                \"status\": boolean, // True if threshold reached\n                \"found\": number,   // Number of blocks with the new version found\n                \"required\": number,// Number of blocks required\n                \"window\": number   // Maximum size of examined window of recent blocks\n            },\n            \"reject\": {           // Progress toward rejecting pre-softfork blocks\n                \"status\": boolean, // True if threshold reached\n                \"found\": number,   // Number of blocks with the new version found\n                \"required\": number,// Number of blocks required\n                \"window\": number   // Maximum size of examined window of recent blocks\n            }\n        }\n    ]\n}\n</code></pre> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getblockchaininfo\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": {\n        \"chain\": \"main\",\n        \"blocks\": 750000,\n        \"headers\": 750000,\n        \"bestblockhash\": \"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\",\n        \"difficulty\": 21448277761059.71,\n        \"mediantime\": 1657123456,\n        \"verificationprogress\": 0.9999987,\n        \"chainwork\": \"000000000000000000000000000000000000000000a0ff23f0182b0000000000\",\n        \"pruned\": false,\n        \"softforks\": []\n    },\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getinfo","title":"getinfo","text":"<p>Returns general information about the node and blockchain.</p> <p>Parameters: none</p> <p>Returns:</p> <pre><code>{\n    \"version\": number,          // Server version\n    \"protocolversion\": number,  // Protocol version\n    \"blocks\": number,           // Current number of blocks\n    \"timeoffset\": number,       // Time offset in seconds\n    \"connections\": number,      // Number of connections\n    \"proxy\": \"string\",         // Proxy used\n    \"difficulty\": number,      // Current difficulty\n    \"testnet\": boolean,       // If using testnet\n    \"relayfee\": number,       // Minimum relay fee\n    \"errors\": \"string\"        // Current errors\n}\n</code></pre> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getinfo\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": {\n        \"version\": 1000000,\n        \"protocolversion\": 70015,\n        \"blocks\": 750000,\n        \"timeoffset\": 0,\n        \"connections\": 8,\n        \"proxy\": \"\",\n        \"difficulty\": 21448277761059.71,\n        \"testnet\": false,\n        \"relayfee\": 0.00001000,\n        \"errors\": \"\"\n    },\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getpeerinfo","title":"getpeerinfo","text":"<p>Returns data about each connected network node.</p> <p>Parameters: none</p> <p>Returns:</p> <ul> <li>Array of Objects, one per peer:</li> </ul> <pre><code>[\n    {\n        \"id\": number,               // Peer index\n        \"addr\": \"string\",           // IP address and port\n        \"addrlocal\": \"string\",      // Local address\n        \"services\": \"string\",       // Services offered (hex)\n        \"lastsend\": number,         // Time in seconds since last send\n        \"lastrecv\": number,         // Time in seconds since last receive\n        \"bytessent\": number,        // Total bytes sent\n        \"bytesrecv\": number,        // Total bytes received\n        \"conntime\": number,         // Connection time in seconds\n        \"pingtime\": number,         // Ping time in seconds\n        \"version\": number,          // Peer version\n        \"subver\": \"string\",         // Peer subversion string\n        \"inbound\": boolean,         // Inbound (true) or Outbound (false)\n        \"startingheight\": number,   // Starting height when peer connected\n        \"banscore\": number,         // Ban score\n        \"synced_headers\": number,   // Last header we have in common with this peer\n        \"synced_blocks\": number     // Last block we have in common with this peer\n    }\n]\n</code></pre> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getpeerinfo\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": [\n        {\n            \"id\": 1,\n            \"addr\": \"192.168.1.123:8333\",\n            \"addrlocal\": \"192.168.1.100:8333\",\n            \"services\": \"000000000000040d\",\n            \"lastsend\": 1657123456,\n            \"lastrecv\": 1657123455,\n            \"bytessent\": 123456,\n            \"bytesrecv\": 234567,\n            \"conntime\": 1657120000,\n            \"pingtime\": 0.001,\n            \"version\": 70015,\n            \"subver\": \"/Bitcoin SV:1.0.0/\",\n            \"inbound\": false,\n            \"startingheight\": 750000,\n            \"banscore\": 0,\n            \"synced_headers\": 750000,\n            \"synced_blocks\": 750000\n        }\n    ],\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#invalidateblock","title":"invalidateblock","text":"<p>Permanently marks a block as invalid, as if it violated a consensus rule.</p> <p>Parameters:</p> <ol> <li><code>blockhash</code> (string, required) - The hash of the block to mark as invalid</li> </ol> <p>Returns:</p> <ul> <li><code>null</code> on success</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"invalidateblock\",\n    \"params\": [\"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\"]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": null,\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#reconsiderblock","title":"reconsiderblock","text":"<p>Removes invalidity status of a block and its descendants, reconsidering them for activation.</p> <p>Important: This command performs full block validation to ensure the block meets all consensus rules before removing the invalid status. The block must pass complete validation including all transaction validations, merkle root checks, and consensus rules.</p> <p>Parameters:</p> <ol> <li><code>blockhash</code> (string, required) - The hash of the block to reconsider</li> </ol> <p>Returns:</p> <ul> <li><code>null</code> on success (block passed full validation)</li> <li>Error if the block fails validation</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"reconsiderblock\",\n    \"params\": [\"000000000000000004a1b6d6fdfa0d0a0e52a7a2c8a35ee5b5a7518a846387bc\"]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": null,\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#setban","title":"setban","text":"<p>Attempts to add or remove an IP/Subnet from the banned list.</p> <p>Parameters:</p> <ol> <li><code>subnet</code> (string, required) - The IP/Subnet with an optional netmask (default is /32 = single IP)</li> <li><code>command</code> (string, required) - 'add' to add a ban, 'remove' to remove a ban</li> <li><code>bantime</code> (numeric, optional) - Time in seconds how long the ban is in effect, 0 or empty means using the default time of 24h</li> <li><code>absolute</code> (boolean, optional) - If set, the bantime must be an absolute timestamp in seconds since epoch</li> </ol> <p>Returns:</p> <ul> <li><code>null</code> on success</li> </ul> <p>Note: This command internally uses GRPC <code>BanPeer</code> and <code>UnbanPeer</code> methods which require API key authentication. The RPC command handles this authentication automatically.</p> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"setban\",\n    \"params\": [\"192.168.0.6\", \"add\", 86400]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": null,\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#listbanned","title":"listbanned","text":"<p>Returns list of all banned IP addresses/subnets.</p> <p>Parameters: none</p> <p>Returns:</p> <ul> <li>Array of objects with banned addresses and details</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"listbanned\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": [\n        {\n            \"address\": \"192.168.0.6/32\",\n            \"ban_created\": 1621500000,\n            \"ban_reason\": \"manually added\",\n            \"banned_until\": 1621586400\n        }\n    ],\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#clearbanned","title":"clearbanned","text":"<p>Removes all IP address bans.</p> <p>Parameters: none</p> <p>Returns:</p> <ul> <li><code>null</code> on success</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"clearbanned\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": null,\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#stop_1","title":"stop","text":"<p>Safely shuts down the node.</p> <p>Parameters: none</p> <p>Returns:</p> <ul> <li><code>string</code> - A message indicating the node is stopping</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"stop\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": \"Bitcoin server stopping\",\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#version","title":"version","text":"<p>Returns the server version information.</p> <p>Parameters: none</p> <p>Returns:</p> <pre><code>{\n    \"version\": \"string\",      // Server version string\n    \"subversion\": \"string\",   // User agent string\n    \"protocolversion\": number // Protocol version number\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getchaintips","title":"getchaintips","text":"<p>Returns information about all known chain tips in the block tree, including the main chain as well as orphaned branches.</p> <p>Parameters: none</p> <p>Returns:</p> <ul> <li> <p><code>array</code> - Array of chain tip objects, each containing:</p> <ul> <li><code>height</code> (number) - Height of the chain tip</li> <li><code>hash</code> (string) - Block hash of the chain tip</li> <li><code>branchlen</code> (number) - Zero for main chain, otherwise length of branch connecting the tip to the main chain</li> <li><code>status</code> (string) - Status of the chain tip (\"active\" for main chain, \"valid-fork\", \"valid-headers\", \"headers-only\", \"invalid\")</li> </ul> </li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getchaintips\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": [\n        {\n            \"height\": 700001,\n            \"hash\": \"000000000000000004a1b6d6fdfa0d0a...\",\n            \"branchlen\": 0,\n            \"status\": \"active\"\n        },\n        {\n            \"height\": 700000,\n            \"hash\": \"000000000000000003f2c4e5b8d9a1b2...\",\n            \"branchlen\": 1,\n            \"status\": \"valid-fork\"\n        }\n    ],\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"version\",\n    \"params\": []\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": {\n        \"version\": \"1.0.0\",\n        \"subversion\": \"/Bitcoin SV:1.0.0/\",\n        \"protocolversion\": 70015\n    },\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#isbanned","title":"isbanned","text":"<p>Checks if a network address is currently banned.</p> <p>Parameters:</p> <ol> <li><code>address</code> (string, required) - The network address to check, e.g. \"192.168.0.1/24\"</li> </ol> <p>Returns:</p> <ul> <li><code>boolean</code> - True if the address is banned, false otherwise</li> </ul> <p>Note: This command accesses GRPC ban status methods which require API key authentication when accessed directly. The RPC command handles this authentication automatically.</p> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"isbanned\",\n    \"params\": [\"192.168.0.6\"]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": false,\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#freeze","title":"freeze","text":"<p>Freezes a specific UTXO, preventing it from being spent.</p> <p>Parameters:</p> <ol> <li><code>txid</code> (string, required) - Transaction ID of the output to freeze</li> <li><code>vout</code> (numeric, required) - Output index to freeze</li> </ol> <p>Returns:</p> <ul> <li><code>boolean</code> - True if the UTXO was successfully frozen</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"freeze\",\n    \"params\": [\n        \"a08e6907dbbd3d809776dbfc5d82e371b764ed838b5655e72f463568df1aadf0\",\n        1\n    ]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": true,\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#unfreeze","title":"unfreeze","text":"<p>Unfreezes a previously frozen UTXO, allowing it to be spent.</p> <p>Parameters:</p> <ol> <li><code>txid</code> (string, required) - Transaction ID of the frozen output</li> <li><code>vout</code> (numeric, required) - Output index to unfreeze</li> </ol> <p>Returns:</p> <ul> <li><code>boolean</code> - True if the UTXO was successfully unfrozen</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"unfreeze\",\n    \"params\": [\n        \"a08e6907dbbd3d809776dbfc5d82e371b764ed838b5655e72f463568df1aadf0\",\n        1\n    ]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": true,\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#reassign","title":"reassign","text":"<p>Reassigns ownership of a specific UTXO to a new Bitcoin address.</p> <p>Parameters:</p> <ol> <li><code>txid</code> (string, required) - Transaction ID of the output to reassign</li> <li><code>vout</code> (numeric, required) - Output index to reassign</li> <li><code>destination</code> (string, required) - Bitcoin address to reassign the UTXO to</li> </ol> <p>Returns:</p> <ul> <li><code>boolean</code> - True if the UTXO was successfully reassigned</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"reassign\",\n    \"params\": [\n        \"a08e6907dbbd3d809776dbfc5d82e371b764ed838b5655e72f463568df1aadf0\",\n        1,\n        \"1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa\"\n    ]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": true,\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#generatetoaddress","title":"generatetoaddress","text":"<p>Mines blocks immediately to a specified address (for testing only).</p> <p>Parameters:</p> <ol> <li><code>nblocks</code> (numeric, required) - Number of blocks to generate</li> <li><code>address</code> (string, required) - The address to send the newly generated bitcoin to</li> <li><code>maxtries</code> (numeric, optional) - Maximum number of iterations to try</li> </ol> <p>Returns:</p> <ul> <li><code>array</code> - hashes of blocks generated</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"generatetoaddress\",\n    \"params\": [1, \"1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa\", 1000000]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": [\n        \"36252b5852a5921bdfca8701f936b39edeb1f8c39fffe73b0d8437921401f9af\"\n    ],\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#help","title":"help","text":"<p>Returns help text for RPC commands.</p> <p>Parameters:</p> <ol> <li><code>command</code> (string, optional) - The command to get help for. If not provided, returns a list of all commands.</li> </ol> <p>Returns:</p> <ul> <li><code>string</code> - Help text for the specified command or list of all commands</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"help\",\n    \"params\": [\"getblock\"]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": \"getblock \\\"blockhash\\\" ( verbosity )\\n\\nReturns information about a block.\\n\\nArguments:\\n1. blockhash (string, required) The block hash\\n2. verbosity (numeric, optional, default=1) 0 for hex-encoded data, 1 for a json object, 2 for json object with tx data\\n\\nResult:\\n...\",\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#getrawtransaction","title":"getrawtransaction","text":"<p>Returns raw transaction data for a specific transaction.</p> <p>Parameters:</p> <ol> <li><code>txid</code> (string, required) - The transaction id</li> <li><code>verbose</code> (boolean, optional, default=false) - If false, returns a string that is serialized, hex-encoded data for the transaction. If true, returns a JSON object with transaction information.</li> </ol> <p>Returns:</p> <ul> <li>If verbose=false: <code>string</code> - Serialized, hex-encoded data for the transaction</li> <li>If verbose=true: <code>object</code> - A JSON object with transaction information</li> </ul> <p>Example Request:</p> <pre><code>{\n    \"jsonrpc\": \"1.0\",\n    \"id\": \"curltest\",\n    \"method\": \"getrawtransaction\",\n    \"params\": [\n        \"a08e6907dbbd3d809776dbfc5d82e371b764ed838b5655e72f463568df1aadf0\",\n        true\n    ]\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"result\": {\n        \"hex\": \"0200000001abcd1234...00000000\",\n        \"txid\": \"a08e6907dbbd3d809776dbfc5d82e371b764ed838b5655e72f463568df1aadf0\",\n        \"hash\": \"a08e6907dbbd3d809776dbfc5d82e371b764ed838b5655e72f463568df1aadf0\",\n        \"size\": 225,\n        \"version\": 2,\n        \"locktime\": 0,\n        \"vin\": [\n            {\n                \"txid\": \"efgh5678...\",\n                \"vout\": 0,\n                \"scriptSig\": {\n                    \"asm\": \"...\",\n                    \"hex\": \"...\"\n                },\n                \"sequence\": 4294967295\n            }\n        ],\n        \"vout\": [\n            {\n                \"value\": 0.01000000,\n                \"n\": 0,\n                \"scriptPubKey\": {\n                    \"asm\": \"OP_DUP OP_HASH160 hash OP_EQUALVERIFY OP_CHECKSIG\",\n                    \"hex\": \"76a914hash88ac\",\n                    \"reqSigs\": 1,\n                    \"type\": \"pubkeyhash\",\n                    \"addresses\": [\n                        \"1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa\"\n                    ]\n                }\n            }\n        ],\n        \"blockhash\": \"0000000000000000000b9d2ec5a352ecba0592946514a92f14319dc2cf8127f0\",\n        \"confirmations\": 1024,\n        \"time\": 1570747519,\n        \"blocktime\": 1570747519\n    },\n    \"error\": null,\n    \"id\": \"curltest\"\n}\n</code></pre>"},{"location":"references/services/rpc_reference/#unimplemented-rpc-commands","title":"Unimplemented RPC Commands","text":"<p>The following commands are recognized by the RPC server but are not currently implemented (they would return an ErrRPCUnimplemented error):</p> <ul> <li><code>addnode</code> - Adds a node to the peer list</li> <li><code>debuglevel</code> - Changes the debug level on the fly</li> <li><code>decoderawtransaction</code> - Decodes a raw transaction hexadecimal string</li> <li><code>decodescript</code> - Decodes a hex-encoded script</li> <li><code>estimatefee</code> - Estimates the fee per kilobyte</li> <li><code>getaddednodeinfo</code> - Returns information about added nodes</li> <li><code>getbestblock</code> - Returns information about best block</li> <li><code>getblockcount</code> - Returns the current block count</li> <li><code>getblocktemplate</code> - Returns template for block generation</li> <li><code>getcfilter</code> - Returns the committed filter for a block</li> <li><code>getcfilterheader</code> - Returns the filter header for a filter</li> <li><code>getconnectioncount</code> - Returns connection count</li> <li><code>getcurrentnet</code> - Returns the current network ID</li> <li><code>getgenerate</code> - Returns if the server is generating coins</li> <li><code>gethashespersec</code> - Returns hashes per second</li> <li><code>getheaders</code> - Returns header information</li> <li><code>getmempoolinfo</code> - Returns mempool information (Not in scope for Teranode)</li> <li><code>getnettotals</code> - Returns network statistics</li> <li><code>getnetworkhashps</code> - Returns estimated network hashes per second</li> <li><code>gettxout</code> - Returns unspent transaction output</li> <li><code>gettxoutproof</code> - Returns proof that transaction was included in a block</li> <li><code>node</code> - Attempts to add or remove a node</li> <li><code>ping</code> - Pings the server</li> <li><code>searchrawtransactions</code> - Searches for raw transactions</li> <li><code>setgenerate</code> - Sets generation on or off</li> <li><code>submitblock</code> - Submits a block to the network</li> <li><code>uptime</code> - Returns the server uptime</li> <li><code>validateaddress</code> - Validates a Bitcoin address</li> <li><code>verifychain</code> - Verifies the blockchain</li> <li><code>verifymessage</code> - Verifies a signed message</li> <li> <p><code>verifytxoutproof</code> - Verifies a transaction output proof</p> </li> <li> <p><code>addmultisigaddress</code> - Add a multisignature address to the wallet</p> </li> <li><code>backupwallet</code> - Safely copies wallet.dat to the specified file</li> <li><code>createencryptedwallet</code> - Creates a new encrypted wallet</li> <li><code>createmultisig</code> - Creates a multi-signature address</li> <li><code>dumpprivkey</code> - Reveals the private key for an address</li> <li><code>dumpwallet</code> - Dumps wallet keys to a file</li> <li><code>encryptwallet</code> - Encrypts the wallet</li> <li><code>getaccount</code> - Returns the account associated with an address</li> <li><code>getaccountaddress</code> - Returns the address for an account</li> <li><code>getaddressesbyaccount</code> - Returns addresses for an account</li> <li><code>getbalance</code> - Returns the wallet balance</li> <li><code>getnewaddress</code> - Returns a new Bitcoin address for receiving payments</li> <li><code>getrawchangeaddress</code> - Returns a new Bitcoin address for receiving change</li> <li><code>getreceivedbyaccount</code> - Returns amount received by account</li> <li><code>getreceivedbyaddress</code> - Returns amount received by address</li> <li><code>gettransaction</code> - Returns wallet transaction details</li> <li><code>getunconfirmedbalance</code> - Returns unconfirmed balance</li> <li><code>getwalletinfo</code> - Returns wallet state information</li> <li><code>importaddress</code> - Adds an address to the wallet</li> <li><code>importprivkey</code> - Adds a private key to the wallet</li> <li><code>importwallet</code> - Imports keys from a wallet dump file</li> <li><code>keypoolrefill</code> - Refills the key pool</li> <li><code>listaccounts</code> - Lists account balances</li> <li><code>listaddressgroupings</code> - Lists address groupings</li> <li><code>listlockunspent</code> - Lists temporarily unspendable outputs</li> <li><code>listreceivedbyaccount</code> - Lists balances by account</li> <li><code>listreceivedbyaddress</code> - Lists balances by address</li> <li><code>listsinceblock</code> - Lists transactions since a block</li> <li><code>listtransactions</code> - Lists wallet transactions</li> <li><code>listunspent</code> - Lists unspent transaction outputs</li> <li><code>lockunspent</code> - Locks/unlocks unspent outputs</li> <li><code>move</code> - Moves funds between accounts</li> <li><code>sendfrom</code> - Sends from an account</li> <li><code>sendmany</code> - Sends to multiple recipients</li> <li><code>sendtoaddress</code> - Sends to an address</li> <li><code>setaccount</code> - Sets the account for an address</li> <li><code>settxfee</code> - Sets the transaction fee</li> <li><code>signmessage</code> - Signs a message with address key</li> <li><code>signrawtransaction</code> - Signs a raw transaction</li> <li><code>walletlock</code> - Locks the wallet</li> <li><code>walletpassphrase</code> - Unlocks wallet for sending</li> <li><code>walletpassphrasechange</code> - Changes wallet passphrase</li> </ul> <p>Additionally, the following node-related commands are recognized but return ErrRPCUnimplemented:</p> <ul> <li><code>addnode</code> - Add/remove a node from the address manager</li> <li><code>debuglevel</code> - Changes debug logging level</li> <li><code>decoderawtransaction</code> - Decodes a raw transaction</li> <li><code>decodescript</code> - Decodes a script</li> <li><code>estimatefee</code> - Estimates transaction fee</li> <li><code>getaddednodeinfo</code> - Returns information about added nodes</li> <li><code>getbestblock</code> - Returns best block hash and height</li> <li><code>getblockcount</code> - Returns the blockchain height</li> <li><code>getblocktemplate</code> - Returns data for block template creation</li> <li><code>getcfilter</code> - Returns a compact filter for a block</li> <li><code>getcfilterheader</code> - Returns a filter header for a block</li> <li><code>getconnectioncount</code> - Returns connection count</li> <li><code>getcurrentnet</code> - Returns the network (mainnet/testnet)</li> <li><code>getgenerate</code> - Returns if the node is generating blocks</li> <li><code>gethashespersec</code> - Returns mining hashrate</li> <li><code>getheaders</code> - Returns block headers</li> <li><code>getmempoolinfo</code> - Returns mempool information</li> <li><code>getnettotals</code> - Returns network traffic statistics</li> <li><code>getnetworkhashps</code> - Returns estimated network hashrate</li> <li><code>gettxout</code> - Returns transaction output information</li> <li><code>gettxoutproof</code> - Returns proof that transaction was included in a block</li> <li><code>node</code> - Attempts to add or remove a peer node</li> <li><code>ping</code> - Requests the node ping</li> <li><code>searchrawtransactions</code> - Searches for raw transactions</li> <li><code>setgenerate</code> - Sets if the node generates blocks</li> <li><code>submitblock</code> - Submits a block to the network</li> <li><code>uptime</code> - Returns node uptime</li> <li><code>validateaddress</code> - Validates a Bitcoin address</li> <li><code>verifychain</code> - Verifies blockchain database</li> <li><code>verifymessage</code> - Verifies a signed message</li> <li><code>verifytxoutproof</code> - Verifies proof that transaction was included in a block</li> </ul>"},{"location":"references/services/rpc_reference/#error-handling","title":"Error Handling","text":"<p>The RPC service uses a standardized error handling system based on the Bitcoin Core JSON-RPC error codes. All errors returned to clients follow the JSON-RPC 2.0 specification format:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-id\",\n  \"error\": {\n    \"code\": -32000,\n    \"message\": \"Error message\"\n  }\n}\n</code></pre> <p>Standard error codes include:</p> <ul> <li>-1: General error during command handling</li> <li>-3: Wallet error (wallet functionality not implemented)</li> <li>-5: Invalid address or key</li> <li>-8: Out of memory or other resource exhaustion</li> <li>-20: Database inconsistency or corruption</li> <li>-22: Error parsing or validating a block or transaction</li> <li>-25: Transaction or block validation error</li> <li>-26: Transaction rejected by policy rules</li> <li>-27: Transaction already in chain</li> <li>-30: RPC timeout - request exceeded configured timeout limit</li> <li>-32600: Invalid JSON-RPC request</li> <li>-32601: Method not found</li> <li>-32602: Invalid parameters</li> <li>-32603: Internal JSON-RPC error</li> <li>-32700: Parse error (invalid JSON)</li> </ul> <p>Each handler implements specific error checks relevant to its operation and returns appropriately formatted error responses.</p> <p>Errors are wrapped in <code>bsvjson.RPCError</code> structures, providing standardized error codes and messages as per the Bitcoin Core RPC specification.</p> <p>RPC calls return errors in the following format:</p> <pre><code>{\n  \"result\": null,\n  \"error\": {\n    \"code\": -32601,\n    \"message\": \"Method not found\"\n  },\n  \"id\": \"id\"\n}\n</code></pre> <p>Common error codes that may be returned:</p> Code Message Meaning -1 RPC_MISC_ERROR Standard \"catch-all\" error -3 RPC_TYPE_ERROR Unexpected type was passed as parameter -5 RPC_INVALID_ADDRESS Invalid address or key -8 RPC_INVALID_PARAMETER Invalid, missing or duplicate parameter -32600 RPC_INVALID_REQUEST JSON request format error -32601 RPC_METHOD_NOT_FOUND Method not found -32602 RPC_INVALID_PARAMS Invalid method parameters -32603 RPC_INTERNAL_ERROR Internal RPC error -32700 RPC_PARSE_ERROR Error parsing JSON request"},{"location":"references/services/rpc_reference/#rate-limiting","title":"Rate Limiting","text":"<p>The RPC interface implements rate limiting to prevent abuse. Default limits:</p> <ul> <li>Maximum concurrent connections: 16</li> <li>Maximum requests per minute: 60</li> </ul>"},{"location":"references/services/rpc_reference/#version-compatibility","title":"Version Compatibility","text":"<p>These RPC commands are compatible with Bitcoin SV Teranode version 1.0.0 and later.</p>"},{"location":"references/services/rpc_reference/#concurrency","title":"Concurrency","text":"<p>The RPC Service is designed for high-concurrency operation with multiple simultaneous client connections. Key concurrency features include:</p> <ul> <li>Thread-safe request processing with proper synchronization</li> <li>Atomic operations for tracking client connections</li> <li>Connection limits to prevent resource exhaustion</li> <li>Wait groups for coordinated shutdown with in-progress requests</li> <li>Context-based cancellation for long-running operations</li> <li>Non-blocking request handler dispatch</li> <li>Proper mutex usage for shared data structures</li> <li>Per-request goroutines for parallel processing</li> <li>Response caching to reduce contention on frequently accessed data</li> </ul> <p>These mechanisms ensure the RPC service can safely handle many concurrent connections without race conditions or deadlocks, even under high load scenarios.</p> <p>The server uses goroutines to handle multiple client connections concurrently. It also employs various synchronization primitives (mutexes, atomic operations) to ensure thread-safety.</p>"},{"location":"references/services/rpc_reference/#extensibility","title":"Extensibility","text":"<p>The command handling system is designed to be extensible. New RPC commands can be added by implementing new handler functions and registering them in the <code>rpcHandlers</code> map.</p>"},{"location":"references/services/rpc_reference/#limitations","title":"Limitations","text":"<ul> <li>The server does not implement wallet functionality. Wallet-related commands are delegated to a separate wallet service.</li> <li>Several commands are marked as unimplemented and will return an error if called.</li> <li>The UTXO management commands (freeze, unfreeze, reassign) require the node to be properly configured with the necessary capabilities.</li> <li>Memory and connection limits are enforced to prevent resource exhaustion.</li> </ul>"},{"location":"references/services/rpc_reference/#security","title":"Security","text":"<p>The RPC Service implements several security features:</p> <ul> <li>HTTP Basic authentication with SHA256 credential validation</li> <li>Two-tier authentication system separating admin from limited-access operations</li> <li>Connection limiting to prevent denial-of-service attacks</li> <li>Configurable binding to specific network interfaces</li> <li>Proper HTTP request/response handling with appropriate headers</li> <li>Input validation on all parameters</li> <li>Authorization checking for privileged operations</li> <li>Ban management capabilities for malicious IP addresses</li> <li>Context timeouts to prevent resource exhaustion</li> <li>Secure credential handling to prevent information leakage</li> <li>TLS support for encrypted communications (when configured)</li> </ul>"},{"location":"references/services/rpc_reference/#related-documents","title":"Related Documents","text":"<ul> <li>RPC API Docs</li> </ul>"},{"location":"references/services/subtreevalidation_reference/","title":"Subtree Validation Reference Documentation","text":""},{"location":"references/services/subtreevalidation_reference/#overview","title":"Overview","text":"<p>The <code>Server</code> type implements the core functionality for subtree validation in a blockchain system. It handles subtree and transaction metadata processing, interacts with various data stores, and manages Kafka consumers for distributed processing. The service is a critical component in validating transaction subtrees for inclusion in the blockchain.</p>"},{"location":"references/services/subtreevalidation_reference/#types","title":"Types","text":""},{"location":"references/services/subtreevalidation_reference/#server","title":"Server","text":"<p>The <code>Server</code> is the central component of the subtreevalidation package, managing the complete lifecycle of subtree validation including transaction validation, storage, and integration with other Teranode services.</p> <pre><code>type Server struct {\n    // UnimplementedSubtreeValidationAPIServer embeds the auto-generated gRPC server base\n    subtreevalidation_api.UnimplementedSubtreeValidationAPIServer\n\n    // logger handles all logging operations for the service\n    logger ulogger.Logger\n\n    // settings contains the configuration parameters for the service\n    // including connection details, timeouts, and operational modes\n    settings *settings.Settings\n\n    // subtreeStore manages persistent storage of subtrees\n    // This blob store is used to save and retrieve complete subtree structures\n    subtreeStore blob.Store\n\n    // txStore manages transaction storage\n    // This blob store is used to save and retrieve individual transactions\n    txStore blob.Store\n\n    // utxoStore manages the Unspent Transaction Output (UTXO) state\n    // It's used during transaction validation to verify input spending\n    utxoStore utxo.Store\n\n    // validatorClient provides transaction validation services\n    // It's used to validate transactions against consensus rules\n    validatorClient validator.Interface\n\n    // subtreeCount tracks the number of subtrees processed\n    // Uses atomic operations for thread-safe access\n    subtreeCount atomic.Int32\n\n    // stats tracks operational statistics for monitoring and diagnostics\n    stats *gocore.Stat\n\n    // prioritySubtreeCheckActiveMap tracks active priority subtree checks\n    // Maps subtree hash strings to boolean values indicating check status\n    prioritySubtreeCheckActiveMap map[string]bool\n\n    // prioritySubtreeCheckActiveMapLock protects concurrent access to the priority map\n    prioritySubtreeCheckActiveMapLock sync.Mutex\n\n    // blockchainClient interfaces with the blockchain service\n    // Used to retrieve block information and validate chain state\n    blockchainClient blockchain.ClientI\n\n    // subtreeConsumerClient consumes subtree-related Kafka messages\n    // Handles incoming subtree validation requests from other services\n    subtreeConsumerClient kafka.KafkaConsumerGroupI\n\n    // txmetaConsumerClient consumes transaction metadata Kafka messages\n    // Processes transaction metadata updates from other services\n    txmetaConsumerClient kafka.KafkaConsumerGroupI\n\n    // invalidSubtreeKafkaProducer publishes invalid subtree events to Kafka\n    invalidSubtreeKafkaProducer kafka.KafkaAsyncProducerI\n\n    // invalidSubtreeLock is used to synchronize access to the invalid subtree producer\n    invalidSubtreeLock sync.Mutex\n\n    // invalidSubtreeDeDuplicateMap is used to de-duplicate invalid subtree messages\n    invalidSubtreeDeDuplicateMap *expiringmap.ExpiringMap[string, struct{}]\n\n    // orphanage is used to store transactions that are missing parents that can be validated later\n    orphanage *expiringmap.ExpiringMap[chainhash.Hash, *bt.Tx]\n\n    // orphanageLock is used to make sure we only process the orphanage once at a time\n    orphanageLock sync.Mutex\n\n    // pauseSubtreeProcessing is used to pause subtree processing while a block is being processed\n    pauseSubtreeProcessing atomic.Bool\n\n    // bestBlockHeader is used to store the current best block header\n    bestBlockHeader atomic.Pointer[model.BlockHeader]\n\n    // bestBlockHeaderMeta is used to store the current best block header metadata\n    bestBlockHeaderMeta atomic.Pointer[model.BlockHeaderMeta]\n\n    // currentBlockIDsMap is used to store the current block IDs for the current best block height\n    currentBlockIDsMap atomic.Pointer[map[uint32]bool]\n}\n</code></pre>"},{"location":"references/services/subtreevalidation_reference/#validatesubtree","title":"ValidateSubtree","text":"<p>The <code>ValidateSubtree</code> structure encapsulates all the necessary information required to validate a transaction subtree, providing a clean interface for the validation methods.</p> <pre><code>type ValidateSubtree struct {\n    // SubtreeHash is the hash identifier of the subtree to validate\n    SubtreeHash chainhash.Hash\n\n    // BaseURL is the source location for retrieving missing transactions\n    BaseURL string\n\n    // TxHashes contains the hashes of all transactions in the subtree\n    TxHashes []chainhash.Hash\n\n    // AllowFailFast determines whether validation should stop on first error\n    // When true, validation will terminate immediately upon encountering an invalid transaction\n    // When false, validation will attempt to validate all transactions before returning\n    AllowFailFast bool\n}\n</code></pre>"},{"location":"references/services/subtreevalidation_reference/#missingtx","title":"missingTx","text":"<p>This structure pairs a transaction with its index in the original subtree transaction list, allowing the validation process to maintain the correct ordering and relationship of transactions.</p> <pre><code>type missingTx struct {\n    // tx is the actual transaction data that was retrieved\n    tx *bt.Tx\n\n    // idx is the original position of this transaction in the subtree's transaction list\n    idx int\n}\n</code></pre>"},{"location":"references/services/subtreevalidation_reference/#constructor","title":"Constructor","text":""},{"location":"references/services/subtreevalidation_reference/#new","title":"New","text":"<pre><code>func New(\n    ctx context.Context,\n    logger ulogger.Logger,\n    tSettings *settings.Settings,\n    subtreeStore blob.Store,\n    txStore blob.Store,\n    utxoStore utxo.Store,\n    validatorClient validator.Interface,\n    blockchainClient blockchain.ClientI,\n    subtreeConsumerClient kafka.KafkaConsumerGroupI,\n    txmetaConsumerClient kafka.KafkaConsumerGroupI,\n) (*Server, error)\n</code></pre> <p>Creates a new <code>Server</code> instance with the provided dependencies. This factory function constructs and initializes a fully configured subtree validation service, injecting all required dependencies. It follows the dependency injection pattern to ensure testability and proper separation of concerns.</p> <p>Initialization Features:</p> <ul> <li>Quorum management: Initializes singleton quorum manager for distributed locking</li> <li>Transaction metadata caching: Optionally wraps UTXO store with caching layer</li> <li>Kafka producer setup: Configures invalid subtree event publishing if enabled</li> <li>Orphanage initialization: Sets up orphaned transaction storage with configurable timeout</li> <li>Blockchain subscription: Establishes blockchain event listener for block updates</li> <li>Best block tracking: Initializes current blockchain state tracking</li> </ul> <p>Initialization Process</p> <p>The method ensures that the service is configured with proper stores, clients, and settings before it's made available for use. It also initializes internal tracking structures and statistics for monitoring.</p>"},{"location":"references/services/subtreevalidation_reference/#core-methods","title":"Core Methods","text":""},{"location":"references/services/subtreevalidation_reference/#health","title":"Health","text":"<pre><code>func (u *Server) Health(ctx context.Context, checkLiveness bool) (int, string, error)\n</code></pre> <p>Checks the health status of the service and its dependencies. This method implements the standard Teranode health check interface used across all services for consistent monitoring, alerting, and orchestration. It provides both readiness and liveness checking capabilities to support different operational scenarios.</p> <p>Health Check Components</p> <p>The method performs checks appropriate to the service's role, including:</p> <ul> <li>Store access verification: Subtree, transaction, and UTXO data stores</li> <li>Service connections: Validator and blockchain service connectivity</li> <li>Kafka consumer health: Message processing capability</li> <li>Internal state consistency: Service operational status</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#healthgrpc","title":"HealthGRPC","text":"<pre><code>func (u *Server) HealthGRPC(ctx context.Context, _ *subtreevalidation_api.EmptyMessage) (*subtreevalidation_api.HealthResponse, error)\n</code></pre> <p>Implements the gRPC health check endpoint, translating the core health check results to the gRPC protocol format.</p>"},{"location":"references/services/subtreevalidation_reference/#init","title":"Init","text":"<pre><code>func (u *Server) Init(ctx context.Context) (err error)\n</code></pre> <p>Initializes the server metrics and performs any necessary setup. This method completes the initialization process by setting up components that require runtime initialization rather than construction-time setup. It's called after New() but before Start() to ensure all systems are properly initialized.</p> <p>The initialization is designed to be idempotent and can be safely called multiple times, though typically it's only called once after construction and before starting the service.</p>"},{"location":"references/services/subtreevalidation_reference/#start","title":"Start","text":"<pre><code>func (u *Server) Start(ctx context.Context, readyCh chan&lt;- struct{}) error\n</code></pre> <p>Initializes and starts the server components including Kafka consumers and gRPC server. This method launches all the operational components of the subtree validation service.</p> <p>Components Started:</p> <ul> <li>FSM state synchronization: Waits for blockchain FSM to transition from IDLE state</li> <li>Kafka consumers: For subtree and transaction metadata messages with retry configuration</li> <li>gRPC server: For API access and inter-service communication</li> <li>Background workers: Any timers or workers required for operation</li> </ul> <p>Startup Configuration:</p> <ul> <li>Subtree consumer: Configured with retry-and-move-on policy (3 retries, 2 second backoff)</li> <li>Transaction metadata consumer: Configured with no retries (0 retries, 1 second backoff)</li> <li>gRPC registration: Registers SubtreeValidationAPI service endpoints</li> </ul> <p>Startup Sequence</p> <p>The method implements a safe startup sequence to ensure all components are properly initialized before the service is marked as ready. It also handles proper error propagation if any component fails to start.</p> <p>Once all components are successfully started, the method signals readiness through the provided channel and then blocks until the context is canceled or an error occurs. This design allows the caller to coordinate the startup of multiple services.</p>"},{"location":"references/services/subtreevalidation_reference/#stop","title":"Stop","text":"<pre><code>func (u *Server) Stop(_ context.Context) error\n</code></pre> <p>Gracefully shuts down the server components including Kafka consumers. This method ensures a clean and orderly shutdown of all service components, allowing in-progress operations to complete when possible and releasing all resources properly.</p> <p>Shutdown Sequence</p> <p>The method follows a consistent shutdown sequence:</p> <ol> <li>Stop accepting new requests - Prevents new work from starting</li> <li>Pause Kafka consumers - Prevents new messages from being processed</li> <li>Wait for completion - Allows in-progress operations to complete (with timeouts)</li> <li>Release resources - Closes connections and frees allocated resources</li> </ol> <p>The method is designed to be called when the service needs to be terminated, either for normal shutdown or in response to system signals.</p>"},{"location":"references/services/subtreevalidation_reference/#checksubtreefromblock","title":"CheckSubtreeFromBlock","text":"<pre><code>func (u *Server) CheckSubtreeFromBlock(ctx context.Context, request *subtreevalidation_api.CheckSubtreeFromBlockRequest) (*subtreevalidation_api.CheckSubtreeFromBlockResponse, error)\n</code></pre> <p>Validates a subtree and its transactions based on the provided request. This method is the primary gRPC API endpoint for subtree validation, responsible for coordinating the validation process for an entire subtree of interdependent transactions. It ensures that all transactions in the subtree adhere to consensus rules and can be added to the blockchain.</p> <p>Key Features:</p> <ul> <li>Distributed locking: Prevents duplicate validation of the same subtree</li> <li>Retry logic: Lock acquisition with exponential backoff</li> <li>Backward compatibility: Support for both legacy and current validation paths</li> <li>Resource cleanup: Proper cleanup even in error conditions</li> <li>Structured responses: Appropriate gRPC status codes</li> <li>Orphan processing: Handles orphaned transactions after subtree validation</li> </ul> <p>Validation Criteria</p> <p>The validation process ensures that:</p> <ul> <li>Consensus compliance: All transactions are valid according to consensus rules</li> <li>Input validity: All transaction inputs refer to unspent outputs or other transactions in the subtree</li> <li>No double-spending: No conflicts exist within the subtree or with existing chain state</li> <li>Policy compliance: Transactions satisfy all policy rules (fees, standardness, etc.)</li> </ul> <p>Resilience</p> <p>The method will retry lock acquisition for up to 20 seconds with exponential backoff, making it resilient to temporary contention when multiple services attempt to validate the same subtree simultaneously.</p>"},{"location":"references/services/subtreevalidation_reference/#checkblocksubtrees","title":"CheckBlockSubtrees","text":"<pre><code>func (u *Server) CheckBlockSubtrees(ctx context.Context, request *subtreevalidation_api.CheckBlockSubtreesRequest) (*subtreevalidation_api.CheckBlockSubtreesResponse, error)\n</code></pre> <p>Validates all subtrees referenced in a block to ensure they exist in storage and are properly validated. This method is used during block validation to verify that all subtrees in a block are available and valid before the block can be accepted.</p> <p>Key Features:</p> <ul> <li>Pause mechanism: Temporarily pauses subtree processing during validation to avoid conflicts</li> <li>Chain awareness: Only pauses processing for blocks on the current chain or extending it</li> <li>Parallel processing: Validates multiple subtrees concurrently for performance</li> <li>Level-based validation: Processes transactions in dependency order across all subtrees</li> <li>Stream processing: Efficiently processes subtree data directly from HTTP streams</li> <li>Orphan handling: Manages orphaned transactions discovered during validation</li> </ul> <p>Validation Process:</p> <ol> <li>Chain verification: Determines if the block is on the current chain or extending it</li> <li>Subtree existence check: Verifies which subtrees are missing from local storage</li> <li>Data retrieval: Fetches missing subtree data from network sources</li> <li>Transaction extraction: Extracts all transactions from all subtrees</li> <li>Level-based processing: Validates transactions in dependency order</li> <li>Subtree validation: Validates individual subtrees after transaction processing</li> <li>Orphan processing: Handles any orphaned transactions found during validation</li> </ol> <p>Performance Optimization</p> <p>The method uses several optimization techniques including stream processing for direct HTTP data handling, block-wide validation for better dependency resolution, parallel subtree processing, and efficient memory management during large block processing.</p>"},{"location":"references/services/subtreevalidation_reference/#transaction-metadata-management","title":"Transaction Metadata Management","text":""},{"location":"references/services/subtreevalidation_reference/#getuutxostore","title":"GetUutxoStore","text":"<pre><code>func (u *Server) GetUutxoStore() utxo.Store\n</code></pre> <p>Returns the UTXO store instance used by the server. This method provides access to the store that manages unspent transaction outputs.</p>"},{"location":"references/services/subtreevalidation_reference/#setuutxostore","title":"SetUutxoStore","text":"<pre><code>func (u *Server) SetUutxoStore(s utxo.Store)\n</code></pre> <p>Sets the UTXO store instance for the server. This method allows runtime replacement of the UTXO store, which can be useful for testing or for implementing different storage strategies.</p>"},{"location":"references/services/subtreevalidation_reference/#settxmetacache","title":"SetTxMetaCache","text":"<pre><code>func (u *Server) SetTxMetaCache(ctx context.Context, hash *chainhash.Hash, txMeta *meta.Data) error\n</code></pre> <p>Stores transaction metadata in the cache if caching is enabled. This method optimizes validation performance by caching frequently accessed transaction metadata.</p> <p>Implementation Note</p> <p>This method signature is documented but the actual implementation may delegate to the underlying UTXO store's caching mechanism.</p>"},{"location":"references/services/subtreevalidation_reference/#settxmetacachefrombytes","title":"SetTxMetaCacheFromBytes","text":"<pre><code>func (u *Server) SetTxMetaCacheFromBytes(_ context.Context, key, txMetaBytes []byte) error\n</code></pre> <p>Stores raw transaction metadata bytes in the cache. This is a lower-level method that can be more efficient when the raw bytes are already available.</p>"},{"location":"references/services/subtreevalidation_reference/#deltxmetacache","title":"DelTxMetaCache","text":"<pre><code>func (u *Server) DelTxMetaCache(ctx context.Context, hash *chainhash.Hash) error\n</code></pre> <p>Removes transaction metadata from the cache if caching is enabled. This ensures cache consistency when transactions are modified or invalidated.</p>"},{"location":"references/services/subtreevalidation_reference/#deltxmetacachemulti","title":"DelTxMetaCacheMulti","text":"<pre><code>func (u *Server) DelTxMetaCacheMulti(ctx context.Context, hash *chainhash.Hash) error\n</code></pre> <p>Removes multiple transaction metadata entries from the cache. This method is optimized for batch operations when multiple related entries need to be invalidated.</p>"},{"location":"references/services/subtreevalidation_reference/#internal-validation-methods","title":"Internal Validation Methods","text":""},{"location":"references/services/subtreevalidation_reference/#checksubtreefromblock_1","title":"checkSubtreeFromBlock","text":"<pre><code>func (u *Server) checkSubtreeFromBlock(ctx context.Context, request *subtreevalidation_api.CheckSubtreeFromBlockRequest) (ok bool, err error)\n</code></pre> <p>Internal implementation of subtree validation logic. This method contains the core business logic for validating a subtree, separated from the API-level concerns handled by the public CheckSubtreeFromBlock method. The separation allows for cleaner testing and better separation of concerns.</p> <p>Subtree Storage Convention</p> <p>The method expects the subtree to be stored in the subtree store with a special extension (<code>.subtreeToCheck</code> instead of <code>.subtree</code>) to differentiate between validated and unvalidated subtrees. This prevents the validation service from mistakenly treating an unvalidated subtree as already validated.</p> <p>The validation process includes:</p> <ol> <li>Retrieving the subtree data from storage</li> <li>Parsing the subtree structure and transaction list</li> <li>Checking for existing transaction metadata to avoid redundant work</li> <li>Retrieving and validating missing transactions from appropriate sources</li> <li>Verifying transaction dependencies and ordering within the subtree</li> <li>Confirming all transactions meet consensus rules for blockchain inclusion</li> </ol>"},{"location":"references/services/subtreevalidation_reference/#validatesubtreeinternal","title":"ValidateSubtreeInternal","text":"<pre><code>func (u *Server) ValidateSubtreeInternal(ctx context.Context, v ValidateSubtree, blockHeight uint32,\n    blockIds map[uint32]bool, validationOptions ...validator.Option) (err error)\n</code></pre> <p>Performs the actual validation of a subtree. This is the core method of the subtree validation service, responsible for the complete validation process of a transaction subtree. It handles the complex task of verifying that all transactions in a subtree are valid both individually and collectively, ensuring they can be safely added to the blockchain.</p> <p>Validation Process Steps</p> <p>The validation process includes several key steps:</p> <ol> <li>Retrieving the subtree structure and transaction list</li> <li>Identifying which transactions need validation (missing metadata)</li> <li>Retrieving missing transactions from appropriate sources</li> <li>Validating transaction dependencies and ordering</li> <li>Applying consensus rules to each transaction</li> <li>Managing transaction metadata storage and updates</li> <li>Handling any conflicts or validation failures</li> </ol> <p>Performance Optimization Techniques:</p> <ul> <li>Batch processing of transaction validations where possible</li> <li>Caching of transaction metadata to avoid redundant validation</li> <li>Parallel processing of independent transaction validations</li> <li>Early termination for invalid subtrees (when <code>AllowFailFast</code> is true)</li> <li>Efficient retrieval of missing transactions in batches</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#blessmissingtransaction","title":"blessMissingTransaction","text":"<pre><code>func (u *Server) blessMissingTransaction(ctx context.Context, subtreeHash chainhash.Hash, tx *bt.Tx, blockHeight uint32,\n    blockIds map[uint32]bool, validationOptions *validator.Options) (txMeta *meta.Data, err error)\n</code></pre> <p>Validates a transaction and retrieves its metadata, performing the core consensus validation operations required for blockchain inclusion. This method applies full validation to a transaction, ensuring it adheres to all Bitcoin consensus rules and can be properly included in the blockchain.</p> <p>Validation Components</p> <p>The validation includes:</p> <ul> <li>Transaction format: Structure and format validation</li> <li>Input signatures: Cryptographic signature verification</li> <li>UTXO availability: Input UTXO availability and spending authorization</li> <li>Fee calculation: Fee calculation and policy enforcement</li> <li>Script execution: Script execution and validation</li> <li>Double-spend prevention: Conflict detection and prevention</li> </ul> <p>Upon successful validation, the transaction's metadata is calculated and stored, making it available for future reference and for validation of dependent transactions.</p>"},{"location":"references/services/subtreevalidation_reference/#checkcounterconflictingoncurrentchain","title":"checkCounterConflictingOnCurrentChain","text":"<pre><code>func (u *Server) checkCounterConflictingOnCurrentChain(ctx context.Context, txHash chainhash.Hash, blockIds map[uint32]bool) error\n</code></pre> <p>Checks if the counter-conflicting transactions of a given transaction have already been mined on the current chain. If they have, it returns an error indicating that the transaction is invalid.</p>"},{"location":"references/services/subtreevalidation_reference/#transaction-retrieval-methods","title":"Transaction Retrieval Methods","text":""},{"location":"references/services/subtreevalidation_reference/#getsubtreetxhashes","title":"getSubtreeTxHashes","text":"<pre><code>func (u *Server) getSubtreeTxHashes(spanCtx context.Context, stat *gocore.Stat, subtreeHash *chainhash.Hash, baseURL string) ([]chainhash.Hash, error)\n</code></pre> <p>Retrieves transaction hashes for a subtree from a remote source. This method fetches the list of transactions that are part of a given subtree from a network peer or another service.</p> <p>Retrieval Strategy:</p> <ul> <li>Local first: Checks for existing <code>.subtreeToCheck</code> files in local storage</li> <li>Network fallback: Fetches subtree hash list from remote URL if not available locally</li> <li>Buffered processing: Uses buffered I/O for efficient hash list processing</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#processmissingtransactions","title":"processMissingTransactions","text":"<pre><code>func (u *Server) processMissingTransactions(ctx context.Context, subtreeHash chainhash.Hash, subtree *util.Subtree,\n    missingTxHashes []utxo.UnresolvedMetaData, allTxs []chainhash.Hash, baseURL string, txMetaSlice []*meta.Data, blockHeight uint32,\n    blockIds map[uint32]bool, validationOptions ...validator.Option) (err error)\n</code></pre> <p>Handles the retrieval and validation of missing transactions in a subtree, coordinating both the retrieval process and the validation workflow. This method is a critical part of the subtree validation process.</p> <p>Key Responsibilities</p> <ol> <li>Retrieving transactions that are referenced in the subtree but not available locally</li> <li>Organizing transactions into dependency levels for ordered processing</li> <li>Validating each transaction according to consensus rules</li> <li>Managing parallel processing of independent transaction validations</li> <li>Tracking validation results and updating transaction metadata</li> </ol> <p>Resilience Features:</p> <ul> <li>Multiple retrieval methods: Supports both file-based and network-based transaction retrieval</li> <li>Fallback mechanisms: Ensures maximum resilience with automatic failover</li> <li>Level-based processing: Transactions are grouped by dependency level and processed in order</li> <li>Dependency ordering: Parent transactions are validated before their children</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#preparetxsperlevel","title":"prepareTxsPerLevel","text":"<pre><code>func (u *Server) prepareTxsPerLevel(ctx context.Context, transactions []missingTx) (uint32, [][]missingTx, error)\n</code></pre> <p>Organizes transactions by their dependency level for ordered processing. This method implements a topological sorting algorithm to organize transactions based on their dependency relationships. Transactions are grouped into levels, where each level contains transactions that can be processed independently of each other, but depend on transactions from previous levels.</p> <p>Algorithm Details:</p> <ul> <li>Dependency graph construction: Builds adjacency lists for efficient parent-child lookups</li> <li>Level assignment: Assigns each transaction to the appropriate dependency level</li> <li>Memory optimization: Pre-allocates slices based on calculated level sizes</li> <li>Coinbase handling: Properly handles coinbase transactions in dependency analysis</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#getsubtreemissingtxs","title":"getSubtreeMissingTxs","text":"<pre><code>func (u *Server) getSubtreeMissingTxs(ctx context.Context, subtreeHash chainhash.Hash, subtree *util.Subtree,\n    missingTxHashes []utxo.UnresolvedMetaData, allTxs []chainhash.Hash, baseURL string) ([]missingTx, error)\n</code></pre> <p>Retrieves transactions that are referenced in a subtree but not available locally. This method implements an intelligent retrieval strategy for missing transactions with optimizations for different scenarios.</p> <p>Intelligent Retrieval Strategy</p> <p>The method first checks if a complete subtree data file exists locally, which would contain all transactions. If not available, it makes a decision based on the percentage of missing transactions:</p> <ul> <li>High missing percentage: Attempts to fetch the entire subtree data file from the peer to optimize network usage (configurable threshold via <code>PercentageMissingGetFullData</code> setting)</li> <li>Low missing percentage: Retrieves only the specific missing transactions individually</li> <li>Automatic fallback: Falls back to individual transaction retrieval if subtree data fetch fails</li> </ul> <p>Resilience Features:</p> <ul> <li>Fallback mechanisms: Ensures maximum resilience with automatic failover</li> <li>Multiple retrieval methods: Switches between file-based and network-based retrieval as needed</li> <li>Network optimization: Minimizes bandwidth usage through intelligent batching decisions</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#getmissingtransactionsfromfile","title":"getMissingTransactionsFromFile","text":"<pre><code>func (u *Server) getMissingTransactionsFromFile(ctx context.Context, subtreeHash chainhash.Hash, missingTxHashes []utxo.UnresolvedMetaData,\n    allTxs []chainhash.Hash) (missingTxs []missingTx, err error)\n</code></pre> <p>Retrieves missing transactions from a locally stored subtree data file. This method attempts to read transaction data from a locally stored subtree file, which can be more efficient than retrieving individual transactions from the network.</p> <p>File Processing:</p> <ul> <li>Subtree reconstruction: Rebuilds subtree structure from transaction hashes if needed</li> <li>Data file reading: Reads from <code>.subtreeData</code> files in blob storage</li> <li>Transaction mapping: Maps requested transaction hashes to their positions in the subtree</li> <li>Efficient lookup: Uses subtree lookup maps for fast transaction retrieval</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#getmissingtransactionsfrompeer","title":"getMissingTransactionsFromPeer","text":"<pre><code>func (u *Server) getMissingTransactionsFromPeer(ctx context.Context, subtreeHash chainhash.Hash, missingTxHashes []utxo.UnresolvedMetaData,\n    baseURL string) (missingTxs []missingTx, err error)\n</code></pre> <p>Retrieves missing transactions from a peer node. This method handles the network communication required to fetch transactions that are not available locally, organizing the retrieval into batches for efficiency.</p>"},{"location":"references/services/subtreevalidation_reference/#getmissingtransactionsbatch","title":"getMissingTransactionsBatch","text":"<pre><code>func (u *Server) getMissingTransactionsBatch(ctx context.Context, subtreeHash chainhash.Hash, txHashes []utxo.UnresolvedMetaData, baseURL string) ([]*bt.Tx, error)\n</code></pre> <p>Retrieves a batch of transactions from the network. This method optimizes network utilization by fetching multiple transactions in a single request, reducing the overhead of multiple separate requests.</p>"},{"location":"references/services/subtreevalidation_reference/#isprioritysubtreecheckactive","title":"isPrioritySubtreeCheckActive","text":"<pre><code>func (u *Server) isPrioritySubtreeCheckActive(subtreeHash string) bool\n</code></pre> <p>Checks if a priority subtree check is active for the given subtree hash. Priority checks get special handling and resource allocation to ensure critical subtrees are validated promptly.</p>"},{"location":"references/services/subtreevalidation_reference/#processorphans","title":"processOrphans","text":"<pre><code>func (u *Server) processOrphans(ctx context.Context, blockHash chainhash.Hash, blockHeight uint32, blockIds map[uint32]bool)\n</code></pre> <p>Processes orphaned transactions that were discovered during subtree validation. This method attempts to validate transactions that were previously missing parents, organizing them by dependency level and processing them in parallel where possible.</p> <p>Orphan Processing:</p> <ul> <li>Dependency analysis: Organizes orphaned transactions by dependency level</li> <li>Parallel validation: Processes independent transactions concurrently</li> <li>Automatic cleanup: Removes successfully validated transactions from the orphanage</li> <li>Metrics tracking: Tracks the number of orphans processed for monitoring</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#publishinvalidsubtree","title":"publishInvalidSubtree","text":"<pre><code>func (u *Server) publishInvalidSubtree(ctx context.Context, subtreeHash, peerURL, reason string)\n</code></pre> <p>Publishes invalid subtree events to Kafka for system-wide notification. This method helps coordinate the handling of invalid subtrees across the network by notifying other services.</p> <p>Features:</p> <ul> <li>Deduplication: Prevents duplicate notifications for the same invalid subtree</li> <li>State awareness: Only publishes during normal operation (not during sync)</li> <li>Structured messaging: Uses protobuf messages for reliable communication</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#kafka-handlers","title":"Kafka Handlers","text":""},{"location":"references/services/subtreevalidation_reference/#consumermessagehandler","title":"consumerMessageHandler","text":"<pre><code>func (u *Server) consumerMessageHandler(ctx context.Context) func(msg *kafka.KafkaMessage) error\n</code></pre> <p>Returns a function that processes Kafka messages for subtree validation. It handles both recoverable and unrecoverable errors appropriately. The handler includes sophisticated error categorization to determine whether errors should result in message reprocessing or rejection.</p> <p>Handler Features</p> <p>Key features include:</p> <ul> <li>Error categorization: Different handling for recoverable vs. non-recoverable errors</li> <li>State-aware processing: Considers the current blockchain state</li> <li>Context handling: Proper context cancellation handling</li> <li>Idempotent processing: Prevents duplicate validation</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#subtreeshandler","title":"subtreesHandler","text":"<pre><code>func (u *Server) subtreesHandler(msg *kafka.KafkaMessage) error\n</code></pre> <p>Handles incoming subtree messages from Kafka. This method unmarshals the message, extracts the subtree hash and base URL, acquires the appropriate lock, and triggers the validation process. It includes comprehensive error handling and logging for operational visibility.</p>"},{"location":"references/services/subtreevalidation_reference/#txmetahandler","title":"txmetaHandler","text":"<pre><code>func (u *Server) txmetaHandler(msg *kafka.KafkaMessage) error\n</code></pre> <p>Handles incoming transaction metadata messages from Kafka. This method processes updates to transaction metadata that might be required for proper subtree validation, ensuring the metadata store remains consistent with the latest transaction state.</p>"},{"location":"references/services/subtreevalidation_reference/#additional-internal-methods","title":"Additional Internal Methods","text":""},{"location":"references/services/subtreevalidation_reference/#updatebestblock","title":"updateBestBlock","text":"<pre><code>func (u *Server) updateBestBlock(ctx context.Context) error\n</code></pre> <p>Updates the service's cached best block information by querying the blockchain client. This method maintains current blockchain state for validation operations.</p> <p>State Updates:</p> <ul> <li>Best block header: Updates cached best block header</li> <li>Block metadata: Updates cached block metadata including height</li> <li>Block ID mapping: Updates current block IDs map for chain validation</li> <li>Store synchronization: Updates subtree store's current block height</li> </ul>"},{"location":"references/services/subtreevalidation_reference/#blockchainsubscriptionlistener","title":"blockchainSubscriptionListener","text":"<pre><code>func (u *Server) blockchainSubscriptionListener(ctx context.Context)\n</code></pre> <p>Background goroutine that listens for blockchain events and updates the service's cached blockchain state. This method ensures the service maintains current blockchain information for validation operations.</p> <p>Subscription Features:</p> <ul> <li>Automatic reconnection: Handles subscription failures with backoff and retry</li> <li>Block notifications: Processes block addition notifications</li> <li>State synchronization: Updates best block information on block events</li> <li>Graceful shutdown: Properly handles context cancellation</li> </ul>"},{"location":"references/services/utxopersister_reference/","title":"UTXO Persister Service Reference Documentation","text":""},{"location":"references/services/utxopersister_reference/#overview","title":"Overview","text":"<p>The UTXO (Unspent Transaction Output) Persister Service is responsible for managing and persisting UTXO data in a blockchain system. It creates and maintains up-to-date UTXO file sets for each block in the Teranode blockchain. Its primary function is to process the output of the Block Persister service (utxo-additions and utxo-deletions) and generate complete UTXO set files. The resulting UTXO set files can be exported and used to initialize the UTXO store in new Teranode instances, enabling fast synchronization of new nodes.</p>"},{"location":"references/services/utxopersister_reference/#core-components","title":"Core Components","text":""},{"location":"references/services/utxopersister_reference/#server","title":"Server","text":"<p>The <code>Server</code> struct is the main component of the UTXO Persister Service. It coordinates the processing of blocks, extraction of UTXOs, and their persistent storage. The server maintains the state of the UTXO set by handling additions and deletions as new blocks are processed.</p> <pre><code>type Server struct {\n    // logger provides logging functionality\n    logger ulogger.Logger\n\n    // settings contains configuration settings\n    settings *settings.Settings\n\n    // blockchainClient provides access to blockchain operations\n    blockchainClient blockchain.ClientI\n\n    // blockchainStore provides access to blockchain storage\n    blockchainStore blockchain_store.Store\n\n    // blockStore provides access to block storage\n    blockStore blob.Store\n\n    // stats tracks operational statistics\n    stats *gocore.Stat\n\n    // lastHeight stores the last processed block height\n    lastHeight uint32\n\n    // mu provides mutex locking for thread safety\n    mu sync.Mutex\n\n    // running indicates if the server is currently processing\n    running bool\n\n    // triggerCh is used to trigger processing operations\n    triggerCh chan string\n}\n</code></pre>"},{"location":"references/services/utxopersister_reference/#constructors","title":"Constructors","text":"<p>The service provides two initialization paths depending on your architectural needs:</p> <pre><code>// Standard initialization with blockchain client\n// This constructor leverages the blockchain client interface for operations,\n// which is suitable for distributed setups where components may be on different machines.\nfunc New(\n    ctx context.Context,\n    logger ulogger.Logger,\n    tSettings *settings.Settings,\n    blockStore blob.Store,\n    blockchainClient blockchain.ClientI,\n) *Server\n\n// Direct initialization bypassing blockchain client\n// This constructor provides direct access to the blockchain store without using the client interface.\n// This can be more efficient when the server is running in the same process as the blockchain store.\nfunc NewDirect(\n    ctx context.Context,\n    logger ulogger.Logger,\n    tSettings *settings.Settings,\n    blockStore blob.Store,\n    blockchainStore blockchain_store.Store,\n) (*Server, error)\n</code></pre>"},{"location":"references/services/utxopersister_reference/#methods","title":"Methods","text":"<ul> <li> <p><code>Health(ctx context.Context, checkLiveness bool) (int, string, error)</code>: Checks the health status of the server and its dependencies. It performs both liveness and readiness checks based on the checkLiveness parameter. Liveness checks verify that the service is running, while readiness checks also verify that dependencies like blockchain client, FSM, blockchain store, and block store are available.</p> </li> <li> <p><code>Init(ctx context.Context) error</code>: Initializes the server by reading the last processed height from persistent storage. It retrieves the last block height that was successfully processed and sets it as the starting point for future processing.</p> </li> <li> <p><code>Start(ctx context.Context, readyCh chan&lt;- struct{}) error</code>: Starts the server's processing operations. It sets up notification channels, subscribes to blockchain updates, and starts the main processing loop. The loop processes blocks as they are received through the notification channel or on a timer. The readyCh is closed when initialization is complete to signal readiness.</p> </li> <li> <p><code>Stop(ctx context.Context) error</code>: Stops the server's processing operations and performs necessary cleanup for graceful termination.</p> </li> <li> <p><code>trigger(ctx context.Context, source string) error</code>: Initiates the processing of the next block. It ensures only one processing operation runs at a time and handles various trigger sources. The source parameter indicates what triggered the processing (blockchain, timer, startup, etc.).</p> </li> <li> <p><code>processNextBlock(ctx context.Context) (time.Duration, error)</code>: Processes the next block in the chain. It retrieves the next block based on the last processed height, extracts UTXOs, and persists them to storage. It also updates the last processed height. Returns a duration to wait before processing the next block and any error encountered.</p> </li> <li> <p><code>readLastHeight(ctx context.Context) (uint32, error)</code>: Reads the last processed block height from storage. It attempts to retrieve the height from a special file in the block store.</p> </li> <li> <p><code>writeLastHeight(ctx context.Context, height uint32) error</code>: Writes the current block height to storage for recovery purposes. This allows the server to resume processing from the correct point after a restart.</p> </li> <li> <p><code>verifyLastSet(ctx context.Context, hash *chainhash.Hash) error</code>: Verifies the integrity of the last UTXO set. It checks if the UTXO set for the given hash exists and has valid header and footer. This verification ensures that the UTXO set was completely written and is not corrupted.</p> </li> </ul>"},{"location":"references/services/utxopersister_reference/#utxoset","title":"UTXOSet","text":"<p>The <code>UTXOSet</code> struct represents a set of UTXOs for a specific block. It provides functionality to track, store, and retrieve UTXOs. UTXOSet handles both additions (new outputs) and deletions (spent outputs) for maintaining the UTXO state.</p> <pre><code>type UTXOSet struct {\n    // ctx provides context for operations\n    ctx context.Context\n\n    // logger provides logging functionality\n    logger ulogger.Logger\n\n    // settings contains configuration settings\n    settings *settings.Settings\n\n    // blockHash contains the hash of the current block\n    blockHash chainhash.Hash\n\n    // blockHeight represents the height of the current block\n    blockHeight uint32\n\n    // additionsStorer manages storage of UTXO additions\n    additionsStorer *filestorer.FileStorer\n\n    // deletionsStorer manages storage of UTXO deletions\n    deletionsStorer *filestorer.FileStorer\n\n    // store provides blob storage functionality\n    store blob.Store\n\n    // deletionsMap tracks deletions by transaction ID\n    deletionsMap map[[32]byte][]uint32\n\n    // txCount tracks the number of transactions\n    txCount uint64\n\n    // utxoCount tracks the number of UTXOs\n    utxoCount uint64\n\n    // deletionCount tracks the number of deletions\n    deletionCount uint64\n\n    // stats tracks operational statistics\n    stats *gocore.Stat\n\n    // mu provides mutex locking for thread safety\n    mu sync.Mutex\n}\n</code></pre>"},{"location":"references/services/utxopersister_reference/#constructors_1","title":"Constructors","text":"<ul> <li> <p><code>NewUTXOSet(ctx context.Context, logger ulogger.Logger, tSettings *settings.Settings, store blob.Store, blockHash *chainhash.Hash, blockHeight uint32) (*UTXOSet, error)</code>: Creates a new UTXOSet instance for managing UTXOs. It initializes the additions and deletions storers and writes their headers. This constructor prepares the storage for a new block's UTXO additions and deletions.</p> </li> <li> <p><code>GetUTXOSet(ctx context.Context, logger ulogger.Logger, tSettings *settings.Settings, store blob.Store, blockHash *chainhash.Hash) (*UTXOSet, error)</code>: Creates a new UTXOSet instance for an existing block. It's used for reading existing UTXO data rather than creating new data. This method doesn't check if the UTXO set actually exists.</p> </li> <li> <p><code>GetUTXOSetWithExistCheck(ctx context.Context, logger ulogger.Logger, tSettings *settings.Settings, store blob.Store, blockHash *chainhash.Hash) (*UTXOSet, bool, error)</code>: Creates a new UTXOSet instance and checks if it exists. Unlike GetUTXOSet, this method also verifies if the UTXO set for the specified block exists in storage. Returns the UTXOSet, a boolean indicating existence, and any error encountered.</p> </li> </ul>"},{"location":"references/services/utxopersister_reference/#methods_1","title":"Methods","text":"<ul> <li> <p><code>ProcessTx(tx *bt.Tx) error</code>: Processes a transaction, updating the UTXO set accordingly. It handles both spending (deletions) and creation (additions) of UTXOs. This method ensures thread-safety with a mutex lock.</p> </li> <li> <p><code>delete(deletion *UTXODeletion) error</code>: Records a UTXO deletion. It marks a specific UTXO as spent by writing a deletion record.</p> </li> <li> <p><code>Close() error</code>: Finalizes the UTXO set by writing footers and closing storers. It writes EOFMarkers and count information to the addition and deletion files.</p> </li> <li> <p><code>GetUTXOAdditionsReader(ctx context.Context) (io.ReadCloser, error)</code>: Returns a reader for accessing UTXO additions. It creates a reader for the additions file of the current block or a specified block.</p> </li> <li> <p><code>GetUTXODeletionsReader(ctx context.Context) (io.ReadCloser, error)</code>: Returns a reader for accessing UTXO deletions. It creates a reader for the deletions file of the current block or a specified block.</p> </li> <li> <p><code>CreateUTXOSet(ctx context.Context, c *consolidator) error</code>: Generates the UTXO set for the current block, using the previous block's UTXO set and applying additions and deletions from the consolidator. It handles the creation, serialization, and storage of the complete UTXO state after processing a block or range of blocks.</p> </li> <li> <p><code>GetUTXOSetReader(optionalBlockHash ...*chainhash.Hash) (io.ReadCloser, error)</code>: Returns a reader for accessing the UTXO set. It creates a reader for the UTXO set file of the current block or a specified block.</p> </li> </ul>"},{"location":"references/services/utxopersister_reference/#utxo","title":"UTXO","text":"<p>The <code>UTXO</code> struct represents an individual Unspent Transaction Output. It contains the essential components of a Bitcoin transaction output: index, value, and script.</p> <pre><code>type UTXO struct {\n    // Index represents the output index in the transaction\n    Index uint32\n\n    // Value represents the amount in satoshis\n    Value uint64\n\n    // Script contains the locking script\n    Script []byte\n}\n</code></pre>"},{"location":"references/services/utxopersister_reference/#methods_2","title":"Methods","text":"<ul> <li> <p><code>Bytes() []byte</code>: Returns the byte representation of the UTXO. The serialized format includes the index (4 bytes), value (8 bytes), script length (4 bytes), and the script itself. All integers are serialized in little-endian format.</p> </li> <li> <p><code>String() string</code>: Returns a string representation of the UTXO. It includes the output index, value in satoshis, and a hexadecimal representation of the script.</p> </li> </ul>"},{"location":"references/services/utxopersister_reference/#factory-methods","title":"Factory Methods","text":"<ul> <li><code>NewUTXOFromReader(r io.Reader) (*UTXO, error)</code>: Creates a new UTXO from the provided reader. It deserializes a UTXO by reading the index, value, script length, and script bytes.</li> </ul>"},{"location":"references/services/utxopersister_reference/#utxowrapper","title":"UTXOWrapper","text":"<p>The <code>UTXOWrapper</code> struct wraps multiple UTXOs for a single transaction. It encapsulates a transaction ID, block height, coinbase flag, and a collection of UTXOs that belong to a single transaction.</p> <pre><code>type UTXOWrapper struct {\n    // TxID contains the transaction ID\n    TxID chainhash.Hash\n\n    // Height represents the block height\n    Height uint32\n\n    // Coinbase indicates if this is a coinbase transaction\n    Coinbase bool\n\n    // UTXOs contains the unspent transaction outputs\n    UTXOs []*UTXO\n}\n</code></pre>"},{"location":"references/services/utxopersister_reference/#methods_3","title":"Methods","text":"<ul> <li> <p><code>Bytes() []byte</code>: Returns the byte representation of the UTXOWrapper. The serialized format includes the transaction ID, encoded height/coinbase flag, number of UTXOs, and the serialized UTXOs themselves.</p> </li> <li> <p><code>DeletionBytes(index uint32) [36]byte</code>: Returns the bytes representation for deletion of a specific output. It creates a fixed-size array containing the transaction ID and the output index.</p> </li> <li> <p><code>String() string</code>: Returns a string representation of the UTXOWrapper. The string includes the transaction ID, height, coinbase status, number of outputs, and a formatted representation of each UTXO.</p> </li> </ul>"},{"location":"references/services/utxopersister_reference/#factory-methods_1","title":"Factory Methods","text":"<ul> <li> <p><code>NewUTXOWrapperFromReader(ctx context.Context, r io.Reader) (*UTXOWrapper, error)</code>: Creates a new UTXOWrapper from the provided reader. It deserializes the UTXOWrapper data from a byte stream, checking for EOF markers and properly decoding the height, coinbase flag, and UTXOs.</p> </li> <li> <p><code>NewUTXOWrapperFromBytes(b []byte) (*UTXOWrapper, error)</code>: Creates a new UTXOWrapper from the provided bytes. It's a convenience wrapper around NewUTXOWrapperFromReader that uses a bytes.Reader.</p> </li> </ul>"},{"location":"references/services/utxopersister_reference/#eofmarker","title":"EOFMarker","text":"<p>The package defines an <code>EOFMarker</code> which is a byte slice of 32 zero bytes used to identify the end of a data stream when reading UTXOs from a file. When serializing UTXO data to storage, this marker is placed at the end to signify a complete and valid write. When reading, encountering this marker indicates that all UTXOs have been successfully read.</p> <pre><code>var EOFMarker = make([]byte, 32) // 32 zero bytes\n</code></pre>"},{"location":"references/services/utxopersister_reference/#utxodeletion","title":"UTXODeletion","text":"<p>The <code>UTXODeletion</code> struct represents a UTXO to be deleted.</p> <pre><code>type UTXODeletion struct {\n    // TxID contains the transaction ID of the UTXO to delete\n    TxID chainhash.Hash\n    // Index represents the output index to delete\n    Index uint32\n}\n</code></pre>"},{"location":"references/services/utxopersister_reference/#methods_4","title":"Methods","text":"<ul> <li><code>DeletionBytes() []byte</code>: Returns the bytes representation of the UTXODeletion, used when marking a UTXO as spent.</li> <li><code>String() string</code>: Returns a string representation of the UTXODeletion for debugging purposes.</li> </ul>"},{"location":"references/services/utxopersister_reference/#blockindex","title":"BlockIndex","text":"<p>The <code>BlockIndex</code> struct contains metadata about a block for UTXO set validation and linking.</p> <pre><code>type BlockIndex struct {\n    // Hash contains the block hash\n    Hash *chainhash.Hash\n\n    // Height represents the block height\n    Height uint32\n\n    // TxCount represents the number of transactions\n    TxCount uint64\n\n    // BlockHeader contains the block header information\n    BlockHeader *model.BlockHeader\n}\n</code></pre>"},{"location":"references/services/utxopersister_reference/#methods_5","title":"Methods","text":"<ul> <li><code>Serialise(writer io.Writer) error</code>: Writes the BlockIndex data to the provided writer for persistent storage.</li> <li><code>String() string</code>: Returns a string representation of the BlockIndex including hash, height, and transaction count.</li> </ul>"},{"location":"references/services/utxopersister_reference/#factory-methods_2","title":"Factory Methods","text":"<ul> <li><code>NewUTXOHeaderFromReader(reader io.Reader) (*BlockIndex, error)</code>: Creates a new BlockIndex from the provided reader by deserializing block metadata.</li> </ul>"},{"location":"references/services/utxopersister_reference/#consolidator","title":"Consolidator","text":"<p>The <code>consolidator</code> manages the consolidation of UTXO additions and deletions across multiple blocks to create accurate UTXO sets.</p> <pre><code>type consolidator struct {\n    logger              ulogger.Logger\n    settings            *settings.Settings\n    blockchainStore     headerIfc\n    blockchainClient    headerIfc\n    blockStore          blob.Store\n    insertCounter       uint64\n    additions           map[UTXODeletion]*Addition\n    deletions           map[UTXODeletion]struct{}\n    firstBlockHeight    uint32\n    lastBlockHeight     uint32\n    // Additional fields for block tracking\n}\n</code></pre>"},{"location":"references/services/utxopersister_reference/#methods_6","title":"Methods","text":"<ul> <li><code>NewConsolidator(logger, settings, blockchainStore, blockchainClient, blockStore, previousBlockHash) *consolidator</code>: Creates a new consolidator instance.</li> <li><code>ConsolidateBlockRange(ctx context.Context, startBlock, endBlock uint32) error</code>: Consolidates UTXOs across the specified block range, resolving conflicts when outputs are created and spent within the range.</li> <li><code>getSortedUTXOWrappers() []*UTXOWrapper</code>: Returns deterministically sorted UTXO wrappers for output.</li> </ul>"},{"location":"references/services/utxopersister_reference/#file-formats","title":"File Formats","text":"<p>The UTXO Persister service uses specific binary file formats for storing UTXO data:</p>"},{"location":"references/services/utxopersister_reference/#utxo-additions-extension-utxo-additions","title":"UTXO Additions (extension: <code>utxo-additions</code>)","text":"<pre><code>- Transaction ID (32 bytes)\n- Encoded Height and Coinbase (4 bytes)\n  * Height &lt;&lt; 1 | coinbase_flag\n- Output Count (4 bytes)\nFor each output:\n\n    - Index (4 bytes)\n    - Value (8 bytes)\n    - Script Length (4 bytes)\n    - Script (variable)\n</code></pre>"},{"location":"references/services/utxopersister_reference/#utxo-deletions-extension-utxo-deletions","title":"UTXO Deletions (extension: <code>utxo-deletions</code>)","text":"<pre><code>- Transaction ID (32 bytes)\n- Output Index (4 bytes)\n</code></pre>"},{"location":"references/services/utxopersister_reference/#utxo-set-extension-utxo-set","title":"UTXO Set (extension: <code>utxo-set</code>)","text":"<pre><code>- EOF Marker (32 zero bytes)\n- Transaction Count (8 bytes)\n- UTXO/Deletion Count (8 bytes)\n</code></pre> <p>Each file type has a specific header format and contains serialized UTXO data.</p>"},{"location":"references/services/utxopersister_reference/#helper-functions","title":"Helper Functions","text":"<ul> <li><code>BuildHeaderBytes(magic string, blockHash *chainhash.Hash, blockHeight uint32, previousBlockHash ...*chainhash.Hash) ([]byte, error)</code>: Builds the header bytes for UTXO files.</li> <li><code>GetHeaderFromReader(reader io.Reader) (string, *chainhash.Hash, uint32, error)</code>: Reads and parses the header from a reader.</li> <li><code>GetUTXOSetHeaderFromReader(reader io.Reader) (string, *chainhash.Hash, uint32, *chainhash.Hash, error)</code>: Reads and parses the UTXO set header from a reader.</li> <li><code>GetFooter(r io.Reader) (uint64, uint64, error)</code>: Retrieves transaction and UTXO counts from the footer of a UTXO file. Requires a seekable reader and reads the last 16 bytes containing transaction count and UTXO count.</li> <li><code>filterUTXOs(utxos []*UTXO, deletions map[UTXODeletion]struct{}, txID *chainhash.Hash) []*UTXO</code>: Filters out UTXOs that are present in the deletions map. It removes any UTXOs that have been spent (present in the deletions map) from the provided list.</li> <li><code>PadUTXOsWithNil(utxos []*UTXO) []*UTXO</code>: Pads a slice of UTXOs with nil values to match their indices. It creates a new slice with nil values at positions where no UTXO exists, ensuring that UTXOs are at positions matching their output index.</li> <li><code>UnpadSlice[T any](padded []*T) []*T</code>: Removes nil values from a padded slice. It creates a new slice containing only the non-nil elements from the input slice.</li> <li><code>checkMagic(r io.Reader, magic string) error</code>: Verifies the magic number in a file header. It reads and validates that the magic identifier in the header matches the expected value.</li> </ul>"},{"location":"references/services/validator_reference/","title":"TX Validator Service Reference Documentation","text":""},{"location":"references/services/validator_reference/#overview","title":"Overview","text":"<p>The TX Validator Service is responsible for validating transactions in a Bitcoin SV blockchain system. It ensures that transactions conform to the network's consensus rules and policy requirements before they are added to the blockchain. The service provides both gRPC and HTTP endpoints for transaction validation and integrates with block assembly for mining operations.</p> <p>The validator package implements comprehensive transaction validation functionality for Bitcoin SV nodes, including script verification, UTXO management, and policy enforcement. It is a critical component of the Teranode architecture that handles transaction validation according to Bitcoin SV consensus rules, manages UTXO state transitions, and ensures that only valid transactions are accepted into the mempool and blocks.</p>"},{"location":"references/services/validator_reference/#core-components","title":"Core Components","text":""},{"location":"references/services/validator_reference/#server","title":"Server","text":"<p>The <code>Server</code> struct is the main component of the TX Validator Service. It acts as the primary coordinator for transaction validation requests, integrating with multiple components (UTXO store, blockchain, Kafka) to provide a complete validation service.</p> <pre><code>type Server struct {\n    // UnsafeValidatorAPIServer embeds the base gRPC server implementation for the validator API,\n    // providing the foundational gRPC service methods that can be overridden by this Server implementation.\n    // This embedding pattern allows the Server to implement the validator API interface while maintaining\n    // flexibility for custom method implementations and middleware integration.\n    validator_api.UnsafeValidatorAPIServer\n\n    // validator is the core validation engine that implements the Interface contract for transaction validation.\n    // This component handles all validation logic including consensus rule enforcement, script execution,\n    // and UTXO verification. It serves as the primary business logic layer for the validation service.\n    validator Interface\n\n    // logger provides structured logging capabilities for the validator server, enabling comprehensive\n    // monitoring and debugging of validation operations. All server activities, errors, and performance\n    // metrics are logged through this component for operational visibility.\n    logger ulogger.Logger\n\n    // settings contains the complete configuration for the validator service, including validation parameters,\n    // network settings, database connections, and operational thresholds. These settings control the\n    // behavior of all validation operations and service integrations.\n    settings *settings.Settings\n\n    // utxoStore provides direct access to the UTXO database for transaction input validation and\n    // double-spend prevention. This store is used to verify transaction inputs, check UTXO availability,\n    // and maintain the current state of unspent transaction outputs across the blockchain.\n    utxoStore utxo.Store\n\n    // kafkaSignal is a channel used to coordinate graceful shutdown of Kafka-related components.\n    // When a shutdown signal is received, this channel notifies all Kafka consumers and producers\n    // to complete their current operations and terminate cleanly.\n    kafkaSignal chan os.Signal\n\n    // stats collects performance metrics for the validator service, including request counts,\n    // processing times, and error rates. These metrics are used for monitoring and optimization.\n    stats *gocore.Stat\n\n    // ctx is the server's main context used for operation management and cancellation.\n    // This context is used to manage the lifecycle of the server and its components.\n    ctx context.Context\n\n    // blockchainClient connects to the blockchain service for block-related operations,\n    // including block height retrieval, chain state verification, and FSM synchronization.\n    // This client is used to ensure the validator service remains synchronized with the blockchain.\n    blockchainClient blockchain.ClientI\n\n    // consumerClient receives validation requests via Kafka, providing an asynchronous\n    // validation path for high-throughput processing. This client is used to consume\n    // Kafka messages and trigger validation operations.\n    consumerClient kafka.KafkaConsumerGroupI\n\n    // txMetaKafkaProducerClient publishes transaction metadata to Kafka, enabling\n    // downstream processing and analytics. This producer is used to send transaction\n    // metadata to Kafka topics for further processing.\n    txMetaKafkaProducerClient kafka.KafkaAsyncProducerI\n\n    // rejectedTxKafkaProducerClient publishes rejected transaction information to Kafka,\n    // providing visibility into validation failures and error conditions. This producer\n    // is used to send rejected transaction data to Kafka topics for monitoring and analysis.\n    rejectedTxKafkaProducerClient kafka.KafkaAsyncProducerI\n\n    // blockAssemblyClient connects to the block assembly service for mining integration,\n    // enabling the validator service to participate in block template generation and\n    // transaction inclusion in mining operations. This client is used to interact with\n    // the block assembly service and coordinate transaction inclusion.\n    blockAssemblyClient blockassembly.ClientI\n\n    // httpServer handles HTTP API requests for transaction validation, providing a\n    // synchronous validation path for clients. This server is used to process HTTP\n    // requests and return validation results.\n    httpServer *echo.Echo\n}\n</code></pre>"},{"location":"references/services/validator_reference/#constructor","title":"Constructor","text":"<pre><code>func NewServer(\n    logger ulogger.Logger,\n    tSettings *settings.Settings,\n    utxoStore utxo.Store,\n    blockchainClient blockchain.ClientI,\n    consumerClient kafka.KafkaConsumerGroupI,\n    txMetaKafkaProducerClient kafka.KafkaAsyncProducerI,\n    rejectedTxKafkaProducerClient kafka.KafkaAsyncProducerI,\n    blockAssemblyClient blockassembly.ClientI,\n) *Server\n</code></pre> <p>Creates and initializes a new validator server instance with the specified components. This function initializes Prometheus metrics and configures the server with all required dependencies for transaction validation. It does not start any background processes or establish connections - that happens in the <code>Init</code> and <code>Start</code> methods.</p>"},{"location":"references/services/validator_reference/#methods","title":"Methods","text":""},{"location":"references/services/validator_reference/#grpc-endpoints","title":"gRPC Endpoints","text":"<ul> <li><code>Health(ctx context.Context, checkLiveness bool) (int, string, error)</code>: Performs health checks on the validator service and its dependencies. When used as a liveness check (checkLiveness=true), it only verifies basic service operation. When used as a readiness check (checkLiveness=false), it performs comprehensive dependency checks.</li> <li><code>HealthGRPC(ctx context.Context, _ *validator_api.EmptyMessage) (*validator_api.HealthResponse, error)</code>: Implements the gRPC health check endpoint. This method provides the gRPC interface for health checking and records metrics for monitoring purposes.</li> <li><code>ValidateTransaction(ctx context.Context, req *validator_api.ValidateTransactionRequest) (*validator_api.ValidateTransactionResponse, error)</code>: Validates a single transaction. This method is part of the validator_api.ValidatorAPIServer interface and serves as the public API entry point for transaction validation requests.</li> <li><code>ValidateTransactionBatch(ctx context.Context, req *validator_api.ValidateTransactionBatchRequest) (*validator_api.ValidateTransactionBatchResponse, error)</code>: Validates a batch of transactions. This method provides significant performance optimization over individual validation by processing multiple transactions in parallel using Go's errgroup.</li> <li><code>GetBlockHeight(ctx context.Context, _ *validator_api.EmptyMessage) (*validator_api.GetBlockHeightResponse, error)</code>: Returns the current block height. This method provides a critical service for clients needing to know the current chain state.</li> <li><code>GetMedianBlockTime(ctx context.Context, _ *validator_api.EmptyMessage) (*validator_api.GetMedianBlockTimeResponse, error)</code>: Returns the median time of recent blocks. This method provides access to the median timestamp of the last several blocks, which is critical for time-based transaction features like nLockTime.</li> </ul>"},{"location":"references/services/validator_reference/#http-endpoints","title":"HTTP Endpoints","text":"<ul> <li><code>handleSingleTx(ctx context.Context) echo.HandlerFunc</code>: Handles HTTP requests for single transaction validation. This method implements an HTTP handler for validating a single Bitcoin transaction submitted via POST request.</li> <li><code>handleMultipleTx(ctx context.Context) echo.HandlerFunc</code>: Handles HTTP requests for validating multiple transactions. This method implements an HTTP handler for validating a stream of Bitcoin transactions submitted via POST request.</li> <li><code>startHTTPServer(ctx context.Context, httpAddresses string) error</code>: Initializes and starts the HTTP server for transaction processing. This method configures and launches an Echo web server that provides HTTP REST endpoints for transaction validation.</li> <li><code>startAndMonitorHTTPServer(ctx context.Context, httpAddresses string)</code>: Starts the HTTP server and monitors for shutdown. This method launches the HTTP server in a non-blocking manner using goroutines, allowing the main server thread to continue execution.</li> </ul>"},{"location":"references/services/validator_reference/#lifecycle-methods","title":"Lifecycle Methods","text":"<ul> <li><code>Init(ctx context.Context) error</code>: Initializes the validator server and sets up the core validation engine. This method must be called before Start() and after NewServer(). It creates the core validator component and performs necessary setup operations.</li> <li><code>Start(ctx context.Context, readyCh chan&lt;- struct{}) error</code>: Begins the validator server operation and registers handlers for validation requests. This method initiates all background processing, including Kafka consumer setup, HTTP API servers, and synchronization with the blockchain FSM.</li> <li><code>Stop(_ context.Context) error</code>: Gracefully shuts down the validator server and all associated components. This method performs an orderly shutdown of all server resources, including Kafka producers/consumers and any background tasks.</li> </ul>"},{"location":"references/services/validator_reference/#validator","title":"Validator","text":"<p>The <code>Validator</code> struct implements Bitcoin SV transaction validation and manages the lifecycle of transactions from validation through block assembly.</p> <pre><code>type Validator struct {\n    logger                        ulogger.Logger\n    settings                      *settings.Settings\n    txValidator                   TxValidatorI\n    utxoStore                     utxo.Store\n    blockAssembler                blockassembly.Store\n    saveInParallel                bool\n    stats                         *gocore.Stat\n    txmetaKafkaProducerClient     kafka.KafkaAsyncProducerI\n    rejectedTxKafkaProducerClient kafka.KafkaAsyncProducerI\n}\n</code></pre>"},{"location":"references/services/validator_reference/#constructor_1","title":"Constructor","text":"<pre><code>func New(ctx context.Context, logger ulogger.Logger, tSettings *settings.Settings, store utxo.Store,\n    txMetaKafkaProducerClient kafka.KafkaAsyncProducerI,\n    rejectedTxKafkaProducerClient kafka.KafkaAsyncProducerI,\n    blockAssemblyClient blockassembly.ClientI) (Interface, error)\n</code></pre> <p>Creates a new <code>Validator</code> instance with the provided configuration. It initializes the validator with the given logger, UTXO store, and Kafka producers. Returns an error if initialization fails.</p>"},{"location":"references/services/validator_reference/#methods_1","title":"Methods","text":"<ul> <li><code>Health(ctx context.Context, checkLiveness bool) (int, string, error)</code>: Performs health checks on the validator and its dependencies. When checkLiveness is true, only checks service liveness. When false, performs full readiness check including dependencies.</li> <li><code>GetBlockHeight() uint32</code>: Returns the current block height from the UTXO store.</li> <li><code>GetMedianBlockTime() uint32</code>: Returns the median block time from the UTXO store.</li> <li><code>Validate(ctx context.Context, tx *bt.Tx, blockHeight uint32, opts ...Option) (*meta.Data, error)</code>: Performs comprehensive validation of a transaction. It checks transaction finality, validates inputs and outputs, updates the UTXO set, and optionally adds the transaction to block assembly.</li> <li><code>ValidateWithOptions(ctx context.Context, tx *bt.Tx, blockHeight uint32, validationOptions *Options) (*meta.Data, error)</code>: Performs comprehensive validation of a transaction with explicit options. This method is the core transaction validation entry point that implements the full Bitcoin validation ruleset.</li> <li><code>TriggerBatcher()</code>: Triggers the batcher (currently a no-op).</li> <li><code>CreateInUtxoStore(traceSpan tracing.Span, tx *bt.Tx, blockHeight uint32, markAsConflicting bool, markAsLocked bool) (*meta.Data, error)</code>: Stores transaction metadata in the UTXO store. Returns transaction metadata and error if storage fails.</li> </ul>"},{"location":"references/services/validator_reference/#txvalidator","title":"TxValidator","text":"<p>The <code>TxValidator</code> implements transaction validation logic based on Bitcoin SV consensus rules and configurable policy settings.</p> <pre><code>type TxValidator struct {\n    logger      ulogger.Logger\n    settings    *settings.Settings\n    interpreter TxScriptInterpreter\n    options     *TxValidatorOptions\n}\n</code></pre> <p>The validator package defines multiple interfaces for transaction validation:</p>"},{"location":"references/services/validator_reference/#txvalidatori-interface","title":"TxValidatorI Interface","text":"<pre><code>type TxValidatorI interface {\n    // ValidateTransaction performs comprehensive validation of a transaction.\n    // This method enforces all consensus and policy rules against the transaction,\n    // including format, structure, inputs/outputs, signature verification, and fees.\n    ValidateTransaction(tx *bt.Tx, blockHeight uint32, validationOptions *Options) error\n\n    // ValidateTransactionScripts performs script validation for a transaction.\n    // This method specifically handles the script execution and signature verification\n    // portion of validation, which is typically the most computationally intensive part.\n    ValidateTransactionScripts(tx *bt.Tx, blockHeight uint32, utxoHeights []uint32, validationOptions *Options) error\n}\n</code></pre>"},{"location":"references/services/validator_reference/#txscriptinterpreter-interface","title":"TxScriptInterpreter Interface","text":"<pre><code>type TxScriptInterpreter interface {\n    // VerifyScript implements script verification for a transaction\n    VerifyScript(tx *bt.Tx, blockHeight uint32, consensus bool, utxoHeights []uint32) error\n\n    // Interpreter returns the interpreter being used\n    Interpreter() TxInterpreter\n}\n</code></pre> <p>The validator supports multiple script interpreters through a factory pattern:</p> <ul> <li>GoBT: Pure Go implementation from the libsv/go-bt library</li> <li>GoSDK: Bitcoin SV SDK implementation</li> <li>GoBDK: Bitcoin Development Kit implementation</li> </ul>"},{"location":"references/services/validator_reference/#key-functions","title":"Key Functions","text":""},{"location":"references/services/validator_reference/#txvalidator-methods","title":"TxValidator Methods","text":"<ul> <li><code>ValidateTransaction(tx *bt.Tx, blockHeight uint32, validationOptions *Options) error</code>: Performs comprehensive validation checks on a transaction. This includes checking input and output presence, transaction size limits, input values and coinbase restrictions, output values and dust limits, lock time requirements, script operation limits, script validation, and fee requirements.</li> <li><code>ValidateTransactionScripts(tx *bt.Tx, blockHeight uint32, utxoHeights []uint32, validationOptions *Options) error</code>: Validates transaction scripts using the configured script interpreter.</li> <li><code>checkOutputs(tx *bt.Tx, blockHeight uint32) error</code>: Validates transaction outputs, checking for dust values and other output-specific rules.</li> <li><code>checkInputs(tx *bt.Tx, blockHeight uint32) error</code>: Validates transaction inputs, checking for proper formatting and sequence values.</li> <li><code>checkTxSize(txSize int) error</code>: Checks if the transaction size is within the allowed policy limit.</li> <li><code>checkFees(tx *bt.Tx, feeQuote *bt.FeeQuote) error</code>: Verifies if the transaction fee is sufficient according to the fee policy.</li> <li><code>sigOpsCheck(tx *bt.Tx, validationOptions *Options) error</code>: Checks the number of signature operations in the transaction against policy limits.</li> <li><code>pushDataCheck(tx *bt.Tx) error</code>: Ensures that unlocking scripts only push data onto the stack, enforcing Bitcoin's signature script policy.</li> </ul>"},{"location":"references/services/validator_reference/#validator-methods","title":"Validator Methods","text":"<ul> <li><code>validateInternal(ctx context.Context, tx *bt.Tx, blockHeight uint32, validationOptions *Options) (txMetaData *meta.Data, err error)</code>: Performs the core validation logic for a transaction. This method contains the detailed step-by-step transaction validation workflow and manages the entire lifecycle of a transaction from initial validation through UTXO updates and optional block assembly integration.</li> <li><code>validateTransaction(ctx context.Context, tx *bt.Tx, blockHeight uint32, validationOptions *Options) error</code>: Performs transaction-level validation checks. Ensures transaction is properly extended and meets all validation rules.</li> <li><code>validateTransactionScripts(ctx context.Context, tx *bt.Tx, blockHeight uint32, utxoHeights []uint32, validationOptions *Options) error</code>: Performs script validation for a transaction. Returns error if validation fails.</li> <li><code>spendUtxos(traceSpan tracing.Span, tx *bt.Tx, ignoreLocked bool) ([]*utxo.Spend, error)</code>: Attempts to spend the UTXOs referenced by transaction inputs. Returns the spent UTXOs and error if spending fails.</li> <li><code>sendToBlockAssembler(traceSpan tracing.Span, bData *blockassembly.Data, reservedUtxos []*utxo.Spend) error</code>: Sends validated transaction data to the block assembler. Returns error if block assembly integration fails.</li> <li><code>extendTransaction(ctx context.Context, tx *bt.Tx) error</code>: Adds previous output information to transaction inputs. Returns error if required parent transaction data cannot be found.</li> </ul>"},{"location":"references/services/validator_reference/#configuration","title":"Configuration","text":"<p>The TX Validator Service uses various configuration options, including:</p> <ul> <li>Kafka settings: Configuration for Kafka producers and consumers used in transaction processing</li> <li>Policy settings: Transaction validation policy parameters, including fee rates, size limits, and other enforcement rules</li> <li>Network parameters: Mainnet, testnet, or regtest configuration affecting consensus rules</li> <li>Block assembly settings: Configuration for integrating validation with block template generation</li> <li>Script interpreter selection: Choice of script validation engine (GoBT, GoSDK, GoBDK)</li> </ul>"},{"location":"references/services/validator_reference/#error-handling","title":"Error Handling","text":"<p>The service uses custom error types defined in the <code>errors</code> package to provide detailed information about validation failures:</p> <ul> <li>TxInvalidError: Indicates that a transaction failed validation rules</li> <li>ScriptVerifyError: Specific to script verification failures</li> <li>ProcessingError: General processing errors not directly related to transaction validity</li> <li>StorageError: Errors related to UTXO storage operations</li> <li>ConfigurationError: Errors in service configuration</li> </ul>"},{"location":"references/services/validator_reference/#metrics","title":"Metrics","text":"<p>Prometheus metrics are used to monitor various aspects of the validation process, including:</p> <ul> <li>Transaction validation time (histogram)</li> <li>Number of invalid transactions (counter)</li> <li>Transaction size distribution (histogram)</li> <li>Script verification time (histogram)</li> <li>Fee rate distribution (histogram)</li> <li>Number of transactions processed (counter)</li> <li>Number of UTXOs created/spent (counter)</li> </ul>"},{"location":"references/services/validator_reference/#kafka-integration","title":"Kafka Integration","text":"<p>The service integrates with Kafka for:</p> <ul> <li>Consuming transactions to be validated from other services</li> <li>Producing metadata for validated transactions for downstream consumers</li> <li>Producing information about rejected transactions for monitoring and analysis</li> <li>Supporting asynchronous processing for high throughput</li> </ul>"},{"location":"references/settings/kafka_settings/","title":"Kafka Settings","text":"<p>Related Topic: Kafka</p> <p>Kafka configuration in Teranode is primarily specified through URLs. Each Kafka topic has its own URL with parameters that control its behavior. The URL format supports both production Kafka and in-memory testing.</p>"},{"location":"references/settings/kafka_settings/#kafka-url-format","title":"Kafka URL Format","text":""},{"location":"references/settings/kafka_settings/#production-kafka-url-format","title":"Production Kafka URL Format","text":"<pre><code>kafka://host1,host2,.../topic?param1=value1&amp;param2=value2&amp;...\n</code></pre> <p>Components of the URL:</p> <ul> <li>Scheme: Always <code>kafka://</code></li> <li>Hosts: Comma-separated list of Kafka brokers (e.g., <code>localhost:9092,kafka2:9092</code>)</li> <li>Topic: The Kafka topic name (specified as the path component)</li> <li>Parameters: Query parameters that configure specific behavior</li> </ul> <p>Example:</p> <pre><code>kafka://localhost:9092/blocks?partitions=4&amp;consumer_ratio=2&amp;replication=3\n</code></pre>"},{"location":"references/settings/kafka_settings/#in-memory-kafka-url-format-testing","title":"In-Memory Kafka URL Format (Testing)","text":"<pre><code>memory://topic?param1=value1&amp;param2=value2&amp;...\n</code></pre> <p>Components of the URL:</p> <ul> <li>Scheme: Always <code>memory://</code></li> <li>Topic: The in-memory topic name (specified as the path component)</li> <li>Parameters: Same query parameters as production Kafka</li> </ul> <p>Example:</p> <pre><code>memory://test_blocks?partitions=2&amp;consumer_ratio=1\n</code></pre> <p>Usage: The memory scheme is automatically detected by the Kafka utilities and enables in-memory message passing for unit tests and development environments. This eliminates the need for a running Kafka cluster during testing.</p>"},{"location":"references/settings/kafka_settings/#url-parameters","title":"URL Parameters","text":""},{"location":"references/settings/kafka_settings/#consumer-configuration-parameters","title":"Consumer Configuration Parameters","text":"<p>When configuring Kafka consumers via URL, the following query parameters are supported:</p> Parameter Type Default Description <code>partitions</code> int 1 Number of topic partitions to consume from <code>consumer_ratio</code> int 1 Ratio for scaling consumer count (partitions/consumer_ratio) <code>replay</code> int 1 Whether to replay messages from beginning (1=true, 0=false) <code>group_id</code> string - Consumer group identifier for coordination <p>Example Consumer URL:</p> <pre><code>kafka://localhost:9092/transactions?partitions=4&amp;consumer_ratio=2&amp;replay=0&amp;group_id=validator-group\n</code></pre>"},{"location":"references/settings/kafka_settings/#producer-configuration-parameters","title":"Producer Configuration Parameters","text":"<p>When configuring Kafka producers via URL, the following query parameters are supported:</p> Parameter Type Default Description <code>partitions</code> int 1 Number of topic partitions to create <code>replication</code> int 1 Replication factor for topic <code>retention</code> string \"600000\" Message retention period (ms) <code>segment_bytes</code> string \"1073741824\" Segment size in bytes (1GB) <code>flush_bytes</code> int varies Flush threshold in bytes (1MB async, 1KB sync) <code>flush_messages</code> int 50000 Number of messages before flush <code>flush_frequency</code> string \"10s\" Time-based flush frequency <code>flush_timeout</code> Duration 10s Maximum time to wait before flushing pending messages <p>Example Producer URL:</p> <pre><code>kafka://localhost:9092/blocks?partitions=2&amp;replication=3&amp;retention=3600000&amp;flush_frequency=5s\n</code></pre>"},{"location":"references/settings/kafka_settings/#parameter-details","title":"Parameter Details","text":""},{"location":"references/settings/kafka_settings/#partitions","title":"partitions","text":"<ul> <li>Type: Integer</li> <li>Default: 1</li> <li>Description: Number of partitions for the topic</li> <li>Impact: Higher values increase parallelism but also resource usage</li> </ul>"},{"location":"references/settings/kafka_settings/#replication","title":"replication","text":"<ul> <li>Type: Integer</li> <li>Default: 1</li> <li>Description: Replication factor for the topic</li> <li>Impact: Higher values improve fault tolerance but increase storage requirements</li> </ul>"},{"location":"references/settings/kafka_settings/#consumer_ratio","title":"consumer_ratio","text":"<ul> <li>Type: Integer</li> <li>Default: 1</li> <li>Description: Ratio of consumers to partitions for load balancing</li> <li>Formula: <code>consumers = partitions / consumer_ratio</code></li> <li>Impact: Higher values reduce concurrency, lower values increase consumer count</li> <li>Example: <code>kafka://localhost:9092/blocks?consumer_ratio=2</code> creates half as many consumers as partitions</li> </ul>"},{"location":"references/settings/kafka_settings/#retention","title":"retention","text":"<ul> <li>Type: String (milliseconds)</li> <li>Default: \"600000\" (10 minutes)</li> <li>Description: How long messages are retained</li> <li>Impact: Longer retention increases storage requirements</li> </ul>"},{"location":"references/settings/kafka_settings/#segment_bytes","title":"segment_bytes","text":"<ul> <li>Type: String/Integer</li> <li>Default: \"1073741824\" (1GB)</li> <li>Description: Maximum size of a single log segment file</li> <li>Impact: Smaller values create more files but allow more granular cleanup</li> <li>Example: <code>kafka://localhost:9092/blocks?segment_bytes=536870912</code> (512MB)</li> </ul>"},{"location":"references/settings/kafka_settings/#flush_bytes","title":"flush_bytes","text":"<ul> <li>Type: Integer</li> <li>Default: 1024</li> <li>Description: Number of bytes to accumulate before forcing a flush</li> <li>Impact: Larger values improve throughput but increase risk of data loss</li> </ul>"},{"location":"references/settings/kafka_settings/#flush_messages","title":"flush_messages","text":"<ul> <li>Type: Integer</li> <li>Default: 50000</li> <li>Description: Number of messages to accumulate before forcing a flush</li> <li>Impact: Larger values improve throughput but increase risk of data loss</li> </ul>"},{"location":"references/settings/kafka_settings/#flush_frequency","title":"flush_frequency","text":"<ul> <li>Type: Duration (e.g., \"5s\")</li> <li>Default: \"10s\" (10 seconds)</li> <li>Description: Maximum time between flushes</li> <li>Impact: Longer durations improve throughput but increase risk of data loss</li> </ul>"},{"location":"references/settings/kafka_settings/#flush_timeout","title":"flush_timeout","text":"<ul> <li>Type: Duration</li> <li>Default: 10s</li> <li>Description: Maximum time to wait before flushing pending messages</li> <li>Usage: Producer timeout configuration</li> <li>Impact: Ensures messages are sent even with low throughput</li> </ul>"},{"location":"references/settings/kafka_settings/#replay","title":"replay","text":"<ul> <li>Type: Integer (boolean: 0 or 1)</li> <li>Default: 1 (true)</li> <li>Description: Whether to replay messages from the beginning for new consumer groups</li> <li>Impact: Controls initial behavior of new consumers</li> </ul>"},{"location":"references/settings/kafka_settings/#auto-commit-behavior-by-topic","title":"Auto-Commit Behavior by Topic","text":"<p>Teranode implements different auto-commit strategies based on message criticality and service requirements.</p>"},{"location":"references/settings/kafka_settings/#critical-topics-auto-commit-false","title":"Critical Topics (Auto-Commit: false)","text":"<p>These topics require guaranteed message processing and cannot tolerate message loss:</p> <ul> <li><code>kafka_blocksConfig</code>: Block distribution for validation</li> <li>Reason: Missing blocks would break blockchain validation</li> <li>Consumer Behavior: Manual commit after successful processing</li> <li> <p>Failure Handling: Message redelivery on processing failure</p> </li> <li> <p><code>kafka_blocksFinalConfig</code>: Finalized blocks for storage</p> </li> <li>Reason: Missing finalized blocks would corrupt blockchain state</li> <li>Consumer Behavior: Manual commit after successful storage</li> <li>Failure Handling: Message redelivery on storage failure</li> </ul>"},{"location":"references/settings/kafka_settings/#non-critical-topics-auto-commit-true","title":"Non-Critical Topics (Auto-Commit: true)","text":"<p>These topics can tolerate occasional message loss for performance:</p> <ul> <li>TxMeta Cache (Subtree Validation): <code>autoCommit=true</code></li> <li>Rationale: Metadata can be regenerated if lost</li> <li> <p>Performance priority over strict delivery guarantees</p> </li> <li> <p>Rejected Transactions (P2P): <code>autoCommit=true</code></p> </li> <li>Rationale: Rejection notifications are not critical for consistency</li> <li>Network efficiency prioritized</li> </ul>"},{"location":"references/settings/kafka_settings/#service-specific-kafka-settings","title":"Service-Specific Kafka Settings","text":""},{"location":"references/settings/kafka_settings/#kafka-consumer-concurrency","title":"Kafka Consumer Concurrency","text":"<p>Important: Kafka consumer concurrency in Teranode is controlled through the <code>consumer_ratio</code> URL parameter for each topic. The actual number of consumers is calculated as:</p> <pre><code>consumerCount = partitions / consumer_ratio\n</code></pre> <p>Common consumer ratios in use:</p> <ul> <li><code>consumer_ratio=1</code>: One consumer per partition (maximum parallelism)</li> <li><code>consumer_ratio=4</code>: One consumer per 4 partitions (balanced approach)</li> </ul>"},{"location":"references/settings/kafka_settings/#propagation-service-settings","title":"Propagation Service Settings","text":"<ul> <li><code>validator_kafka_maxMessageBytes</code>: Size threshold for routing decisions</li> <li>Purpose: Determines when to use HTTP fallback vs Kafka</li> <li>Default: 1048576 (1MB)</li> <li>Usage: Large transactions routed via HTTP to avoid Kafka message size limits</li> </ul>"},{"location":"references/settings/kafka_settings/#validator-service-settings","title":"Validator Service Settings","text":"<ul> <li><code>validator_kafkaWorkers</code>: Number of concurrent Kafka processing workers</li> <li>Purpose: Controls parallel transaction processing capacity</li> <li>Tuning: Should match CPU cores and expected transaction volume</li> <li>Integration: Works with Block Assembly via direct gRPC (not Kafka)</li> </ul>"},{"location":"references/settings/kafka_settings/#configuration-examples","title":"Configuration Examples","text":""},{"location":"references/settings/kafka_settings/#high-throughput-service-propagation","title":"High-Throughput Service (Propagation)","text":"<pre><code>kafka_validatortxsConfig=kafka://localhost:9092/validator-txs?partitions=8&amp;consumer_ratio=2&amp;flush_frequency=1s\n</code></pre> <p>This configuration creates 4 consumers (8 partitions / 2 ratio) with aggressive flushing for low latency.</p>"},{"location":"references/settings/kafka_settings/#critical-service-block-validation","title":"Critical Service (Block Validation)","text":"<pre><code>kafka_blocksConfig=kafka://localhost:9092/blocks?partitions=4&amp;consumer_ratio=1&amp;retention=3600000\n</code></pre> <p>This configuration creates 4 consumers (maximum parallelism) with 1-hour retention for reliability.</p>"},{"location":"references/settings/kafka_settings/#developmenttesting","title":"Development/Testing","text":"<pre><code>memory://test_blocks?partitions=2&amp;consumer_ratio=1\n</code></pre> <p>This configuration uses in-memory Kafka simulation for testing without infrastructure dependencies.</p>"},{"location":"references/settings/services/alert_settings/","title":"Alert Service Settings","text":"<p>Related Topic: Alert Service</p> <p>The Alert Service can be configured using various settings that control its behavior, storage options, and network connectivity. This section provides a comprehensive reference for all configuration options.</p>"},{"location":"references/settings/services/alert_settings/#core-alert-service-configuration","title":"Core Alert Service Configuration","text":"<ul> <li>Alert Store URL (<code>alert_store</code>): The URL for connecting to the alert system's data store.</li> <li>Type: <code>string</code> (converted to <code>*url.URL</code> internally)</li> <li>Impact: Determines the database backend and connection parameters</li> <li> <p>Example: <code>alert_store = sqlite:///alert</code> or <code>alert_store = postgres://user:pass@host:5432/database?sslmode=disable</code></p> </li> <li> <p>Genesis Keys (<code>alert_genesis_keys</code>): A pipe-separated list of public keys used for genesis alerts.</p> </li> <li>Type: <code>[]string</code></li> <li>Impact: Critical - The service will not start without valid genesis keys</li> <li>Purpose: These keys determine which alerts are valid; only alerts signed by these keys will be processed</li> <li> <p>Example: <code>alert_genesis_keys = \"02a1589f2c8e1a4e7cbf28d4d6b676aa2f30811277883211027950e82a83eb2768 | 03aec1d40f02ac7f6df701ef8f629515812f1bcd949b6aa6c7a8dd778b748b2433\"</code></p> </li> <li> <p>P2P Private Key (<code>alert_p2p_private_key</code>): Private key for P2P communication.</p> </li> <li>Type: string</li> <li>Default: \"\" (empty - triggers auto-generation)</li> <li>Environment Variable: <code>TERANODE_alert_p2p_private_key</code></li> <li>Impact: Establishes node identity in the P2P network</li> <li>Auto-Generation: If not provided, creates private key file at <code>$HOME/.alert-system/private_key.pem</code></li> <li>Directory Creation: Automatically creates <code>~/.alert-system/</code> directory with 0750 permissions</li> <li> <p>Example: <code>alert_p2p_private_key = \"08c7fec91e75046d0ac6a2b4edb2daaae34b1e4c3c25a48b1ebdffe5955e33bc\"</code></p> </li> <li> <p>Protocol ID (<code>alert_protocol_id</code>): Protocol identifier for the P2P alert network.</p> </li> <li>Type: <code>string</code></li> <li>Default: \"/bitcoin/alert-system/1.0.0\"</li> <li>Impact: Determines which P2P protocol group the service will join</li> <li> <p>Example: <code>alert_protocol_id = \"/bsv/alert/1.0.0\"</code></p> </li> <li> <p>Topic Name (<code>alert_topic_name</code>): P2P topic name for alert propagation.</p> </li> <li>Type: string</li> <li>Default: \"bitcoin_alert_system\"</li> <li>Environment Variable: <code>TERANODE_alert_topic_name</code></li> <li>Network Dependency: Automatically prefixed with network name if not on mainnet</li> <li> <p>Network-Specific Behavior:</p> <ul> <li>Mainnet: Uses configured topic name as-is</li> <li>Testnet: Prefixed as \"bitcoin_alert_system_testnet\"</li> <li>Regtest: Prefixed as \"bitcoin_alert_system_regtest\"</li> <li>Example: <code>alert_topic_name = \"bitcoin_alert_system\"</code></li> </ul> </li> <li> <p>P2P Port (<code>alert_p2p_port</code>): Port number for P2P communication.</p> </li> <li>Type: <code>int</code></li> <li>Default: 9908</li> <li>Impact: Required - Service will not start without a valid port</li> <li>Example: <code>alert_p2p_port = 4001</code></li> </ul>"},{"location":"references/settings/services/alert_settings/#data-storage-configuration","title":"Data Storage Configuration","text":"<p>The Alert Service supports multiple database backends through the <code>alert_store</code> URL:</p>"},{"location":"references/settings/services/alert_settings/#sqlite","title":"SQLite","text":"<pre><code>sqlite:///database_name\n</code></pre> <p>The SQLite database will be stored in the <code>DataFolder</code> directory specified in the main Teranode configuration.</p> <ul> <li>Parameters: None</li> <li> <p>Connection Pool Settings (hardcoded):</p> </li> <li> <p>Max Idle Connections: 1</p> </li> <li>Max Open Connections: 1</li> <li>Table Prefix: Uses the database name as prefix</li> </ul>"},{"location":"references/settings/services/alert_settings/#in-memory-sqlite","title":"In-Memory SQLite","text":"<pre><code>sqlitememory:///database_name\n</code></pre> <ul> <li>Parameters: None</li> <li> <p>Connection Pool Settings (hardcoded):</p> </li> <li> <p>Max Idle Connections: 1</p> </li> <li>Max Open Connections: 1</li> <li>Storage: In-memory only</li> <li>Table Prefix: Uses the database name as prefix</li> </ul>"},{"location":"references/settings/services/alert_settings/#postgresql","title":"PostgreSQL","text":"<pre><code>postgres://username:password@host:port/database?param1=value1&amp;param2=value2\n</code></pre> <ul> <li> <p>Parameters:</p> </li> <li> <p><code>sslmode</code>: SSL mode for the connection (default: <code>disable</code>)</p> </li> <li> <p>Connection Pool Settings (hardcoded):</p> </li> <li> <p>Max Idle Connections: 2</p> </li> <li>Max Open Connections: 5</li> <li>Max Connection Idle Time: 20 seconds</li> <li>Max Connection Time: 20 seconds</li> <li>Transaction Timeout: 20 seconds</li> <li>Table Prefix: \"alert_system\"</li> </ul>"},{"location":"references/settings/services/alert_settings/#mysql","title":"MySQL","text":"<pre><code>mysql://username:password@host:port/database?param1=value1&amp;param2=value2\n</code></pre> <ul> <li> <p>Parameters:</p> </li> <li> <p><code>sslmode</code>: SSL mode for the connection (default: <code>disable</code>)</p> </li> <li>Connection Pool Settings (hardcoded): Same as PostgreSQL</li> <li>Table Prefix: \"alert_system\"</li> </ul>"},{"location":"references/settings/services/alert_settings/#internal-settings-and-behaviors","title":"Internal Settings and Behaviors","text":"<p>The following settings are hardcoded in the service and cannot be configured externally:</p> <p>Core Processing Settings:</p> <ul> <li>Alert Processing Interval: 5 minutes</li> <li>Controls how frequently alerts are processed</li> <li>Request Logging: Enabled (true)</li> <li>Controls HTTP request logging</li> <li>Auto Migrate: Enabled (true)</li> <li>Automatically migrates the database schema on startup</li> <li>Database Table Prefix: \"alert_system\" for PostgreSQL/MySQL, database name for SQLite</li> <li>Prefix used for database tables</li> <li>DHT Mode: \"client\"</li> <li>Sets the node as a DHT client for peer discovery</li> <li>Peer Discovery Interval: Uses system default</li> <li>Controls how frequently peers are discovered</li> </ul>"},{"location":"references/settings/services/alert_settings/#service-dependencies","title":"Service Dependencies","text":"<p>The Alert Service depends on several other Teranode services:</p>"},{"location":"references/settings/services/alert_settings/#required-dependencies","title":"Required Dependencies","text":"Dependency Purpose Configuration Impact Failure Impact Blockchain Client Block invalidation, chain state queries Requires blockchain service to be properly configured and running <code>InvalidateBlock</code> functionality unavailable UTXO Store UTXO freezing, unfreezing, reassignment operations Requires UTXO store to be initialized and accessible Core alert functionality (fund management) unavailable Block Assembly Client Mining coordination when alerts affect blocks Requires block assembly service configuration Mining-related alert functions unavailable Peer Client (Legacy) Legacy peer management (ban/unban operations) Requires legacy peer service configuration Legacy peer banning functionality unavailable P2P Client Modern P2P alert distribution and peer management Requires P2P service configuration Alert distribution and modern peer management unavailable"},{"location":"references/settings/services/alert_settings/#configuration-interdependencies","title":"Configuration Interdependencies","text":"<ol> <li>Network Configuration: The Alert service inherits network configuration from global Teranode settings, affecting topic naming and protocol selection</li> <li>Logging Integration: Uses the global Teranode logging configuration for consistent log formatting and output</li> <li>Metrics Integration: Integrates with Teranode's Prometheus metrics system using global metrics configuration</li> </ol>"},{"location":"references/settings/services/alert_settings/#environment-variables","title":"Environment Variables","text":"<p>All settings can also be configured through environment variables using the following pattern:</p> <pre><code>TERANODE_ALERT_&lt;SETTING_NAME&gt;\n</code></pre>"},{"location":"references/settings/services/alert_settings/#standard-environment-variables","title":"Standard Environment Variables","text":"<p>The following environment variables are used to configure the Alert Service:</p> <ul> <li><code>TERANODE_ALERT_GENESIS_KEYS</code>: A pipe-separated list of genesis keys</li> <li><code>TERANODE_ALERT_P2P_PRIVATE_KEY</code>: The P2P private key</li> <li><code>TERANODE_ALERT_PROTOCOL_ID</code>: The P2P protocol identifier</li> <li><code>TERANODE_ALERT_STORE</code>: The database connection URL</li> <li><code>TERANODE_ALERT_TOPIC_NAME</code>: The P2P topic name</li> </ul>"},{"location":"references/settings/services/alert_settings/#special-environment-variables","title":"Special Environment Variables","text":"<p>The following environment variable is used to configure the P2P port:</p> <ul> <li><code>ALERT_P2P_PORT</code>: The P2P port (note: different naming pattern)</li> </ul>"},{"location":"references/settings/services/alert_settings/#security-considerations","title":"Security Considerations","text":""},{"location":"references/settings/services/alert_settings/#genesis-keys-management","title":"Genesis Keys Management","text":"<p>The genesis keys are critical for the security of the alert system. They should be carefully managed:</p> <ul> <li>Store private keys securely and offline</li> <li>Use multiple keys with a threshold signature scheme for increased security</li> <li>Rotate keys periodically according to your security policy</li> </ul>"},{"location":"references/settings/services/alert_settings/#database-security","title":"Database Security","text":"<p>When using PostgreSQL or MySQL:</p> <ul> <li>Use strong passwords</li> <li>Enable SSL for database connections (<code>sslmode=require</code> or stronger)</li> <li>Restrict database access to authorized users only</li> <li>Consider using a dedicated database user with limited permissions</li> </ul>"},{"location":"references/settings/services/alert_settings/#configuration-examples","title":"Configuration Examples","text":""},{"location":"references/settings/services/alert_settings/#complete-configuration-example","title":"Complete Configuration Example","text":"<pre><code># Alert Service Core Configuration\nalert_store = postgres://alert_user:secure_password@db-server:5432/alert_db?sslmode=require\nalert_genesis_keys = \"02a1589f2c8e1a4e7cbf28d4d6b676aa2f30811277883211027950e82a83eb2768 | 03aec1d40f02ac7f6df701ef8f629515812f1bcd949b6aa6c7a8dd778b748b2433\"\nalert_p2p_private_key = \"08c7fec91e75046d0ac6a2b4edb2daaae34b1e4c3c25a48b1ebdffe5955e33bc\"\nalert_protocol_id = \"/bsv/alert/1.0.0\"\nalert_topic_name = \"bitcoin_alert_system\"\nalert_p2p_port = 4001\n</code></pre>"},{"location":"references/settings/services/alert_settings/#development-configuration-example","title":"Development Configuration Example","text":"<pre><code>alert_store = sqlite:///alert\nalert_genesis_keys = \"02a1589f2c8e1a4e7cbf28d4d6b676aa2f30811277883211027950e82a83eb2768\"\nalert_p2p_port = 4001\n</code></pre>"},{"location":"references/settings/services/alert_settings/#error-handling-and-troubleshooting","title":"Error Handling and Troubleshooting","text":""},{"location":"references/settings/services/alert_settings/#common-configuration-errors","title":"Common Configuration Errors","text":"<p>Genesis Keys Errors:</p> <pre><code>Error: ErrNoGenesisKeys\nCause: No genesis keys provided in configuration\n</code></pre> <p>P2P Configuration Errors:</p> <pre><code>Error: ErrNoP2PIP\nCause: P2P IP address validation failed (less than 5 characters)\n\nError: ErrNoP2PPort\nCause: P2P port validation failed (less than 2 characters when converted to string)\n</code></pre> <p>Database Connection Errors:</p> <pre><code>Error: ErrDatastoreUnsupported\nCause: Unsupported database scheme in alert_store URL\nSupported schemes: sqlite://, sqlitememory://, postgres://, mysql://\n</code></pre>"},{"location":"references/settings/services/asset_settings/","title":"Asset Server Settings","text":"<p>Related Topic: Asset Server</p> <p>Configuration settings for the Asset Server service.</p> <p>Note: Settings content is being extracted from the topic document. See section 6-9 of the topic document for current settings information.</p>"},{"location":"references/settings/services/blockassembly_settings/","title":"Block Assembly Settings","text":"<p>Related Topic: Block Assembly</p> <p>Configuration settings for the Block Assembly service.</p> <p>Note: Settings content is being extracted from the topic document. See section 6-9 of the topic document for current settings information.</p>"},{"location":"references/settings/services/blockchain_settings/","title":"Blockchain Settings","text":"<p>Related Topic: Blockchain Service</p> <p>The Blockchain service configuration is organized into several categories to manage different aspects of the service's operation. All settings can be provided via environment variables or configuration files.</p>"},{"location":"references/settings/services/blockchain_settings/#network-configuration","title":"Network Configuration","text":"<ul> <li>GRPC Address (<code>blockchain_grpcAddress</code>): Specifies the address for other services to connect to the Blockchain service's gRPC API. This is how other services will address the blockchain service.</li> <li>Type: string</li> <li> <p>Default Value: <code>localhost:8087</code></p> <ul> <li>Impact: Critical for service discovery and inter-service communication</li> <li>Security Impact: In production environments, should be configured securely based on network architecture</li> </ul> </li> <li> <p>GRPC Listen Address (<code>blockchain_grpcListenAddress</code>): Specifies the network interface and port the Blockchain service's gRPC server binds to for accepting connections.</p> </li> <li>Type: string</li> <li> <p>Default Value: <code>:8087</code></p> <ul> <li>Impact: Controls network interface binding for accepting gRPC connections</li> <li>Security Impact: Binding to <code>0.0.0.0</code> or empty address (<code>:8087</code>) exposes the port on all network interfaces</li> </ul> </li> <li> <p>HTTP Listen Address (<code>blockchain_httpListenAddress</code>): Specifies the network interface and port for the HTTP server that exposes REST endpoints (primarily for block invalidation/revalidation).</p> </li> <li>Type: string</li> <li>Default Value: <code>:8082</code><ul> <li>Impact: Controls network interface binding for HTTP API access</li> <li>Security Impact: Should be configured based on who needs access to these endpoints</li> </ul> </li> </ul>"},{"location":"references/settings/services/blockchain_settings/#data-storage-configuration","title":"Data Storage Configuration","text":"<ul> <li>Store URL (<code>blockchain_store</code>): URL connection string for the blockchain database that stores block data and service state.</li> <li>Type: URL</li> <li>Default Value: <code>sqlite:///blockchain</code></li> <li> <p>Supported Formats:</p> <ul> <li>SQLite: <code>sqlite:///path/to/db</code></li> <li>PostgreSQL: <code>postgres://user:password@host:port/dbname</code></li> <li>Impact: Stores block data and service state</li> <li>Performance Impact: Choice of database affects scalability and performance</li> </ul> </li> <li> <p>DB Timeout (<code>blockchain_store_dbTimeoutMillis</code>): The timeout in milliseconds for database operations.</p> </li> <li>Type: integer</li> <li>Default Value: <code>5000</code> (5 seconds)<ul> <li>Impact: Not currently implemented</li> </ul> </li> </ul>"},{"location":"references/settings/services/blockchain_settings/#state-machine-configuration","title":"State Machine Configuration","text":"<ul> <li>Initialize Node In State (<code>blockchain_initializeNodeInState</code>): Specifies the initial state for the blockchain service's finite state machine (FSM).</li> <li>Type: string</li> <li>Default Value: <code>\"\"</code> (empty, uses default FSM state)</li> <li> <p>Possible Values:</p> <ul> <li><code>\"IDLE\"</code>: Initial inactive state</li> <li><code>\"RUNNING\"</code>: Normal operating state</li> <li><code>\"LEGACY_SYNC\"</code>: Legacy synchronization mode</li> <li><code>\"CATCHUP_BLOCKS\"</code>: Block catch-up mode</li> <li>Impact: Not currently implemented</li> </ul> </li> <li> <p>FSM State Restore (<code>fsm_state_restore</code>): Controls whether the service restores its previous FSM state from storage on startup.</p> </li> <li>Type: boolean</li> <li> <p>Default Value: <code>false</code></p> <ul> <li>Impact: Not currently implemented</li> </ul> </li> <li> <p>FSM State Change Delay (<code>fsm_state_change_delay</code>): FOR TESTING ONLY - introduces an artificial delay when changing FSM states.</p> </li> <li>Type: integer (milliseconds)</li> <li>Default Value: <code>0</code><ul> <li>Impact: Used only in tests for state transition synchronization</li> </ul> </li> <li>Warning: Should not be used in production environments</li> </ul>"},{"location":"references/settings/services/blockchain_settings/#operational-settings","title":"Operational Settings","text":"<ul> <li>Maximum Retries (<code>blockchain_maxRetries</code>): Maximum number of retry attempts for blockchain operations that encounter transient errors.</li> <li>Type: integer</li> <li> <p>Default Value: <code>3</code></p> <ul> <li>Impact: Not currently implemented</li> </ul> </li> <li> <p>Retry Sleep Duration (<code>blockchain_retrySleep</code>): The wait time in milliseconds between retry attempts, implementing a back-off mechanism.</p> </li> <li>Type: integer</li> <li>Default Value: <code>1000</code> (1 second)<ul> <li>Impact: Not currently implemented</li> </ul> </li> <li>Tuning Advice: Adjust based on the nature of expected failures (shorter for quick-recovery scenarios)</li> </ul>"},{"location":"references/settings/services/blockchain_settings/#mining-and-difficulty-settings","title":"Mining and Difficulty Settings","text":"<ul> <li>Difficulty Cache (<code>blockassembly_difficultyCache</code>): Enables difficulty calculation caching for performance optimization.</li> <li>Type: boolean</li> <li>Default Value: <code>true</code><ul> <li>Impact: Improves performance when enabled</li> </ul> </li> </ul>"},{"location":"references/settings/services/blockchain_settings/#configuration-interactions-and-dependencies","title":"Configuration Interactions and Dependencies","text":"<p>Some configuration settings work together or depend on other settings:</p> <ol> <li> <p>gRPC Server Management: <code>blockchain_grpcListenAddress</code> controls server startup and health checks; <code>blockchain_grpcAddress</code> required for client connections</p> </li> <li> <p>HTTP Administrative Interface: <code>blockchain_httpListenAddress</code> must be configured for service startup; empty causes failure</p> </li> <li> <p>Difficulty Calculation: <code>blockassembly_difficultyCache</code> improves performance; Chain parameters control difficulty algorithm behavior</p> </li> <li> <p>FSM State Management: <code>fsm_state_restore</code> and <code>blockchain_initializeNodeInState</code> control service startup behavior and state persistence</p> </li> <li> <p>Database Operations: <code>blockchain_store</code> determines persistence backend; <code>blockchain_store_dbTimeoutMillis</code> controls operation limits</p> </li> </ol>"},{"location":"references/settings/services/blockchain_settings/#critical-configuration-requirements","title":"Critical Configuration Requirements","text":"<ol> <li><code>blockchain_httpListenAddress</code> must be configured (empty causes service startup failure)</li> <li><code>blockchain_grpcAddress</code> must be configured for client functionality</li> <li><code>blockchain_grpcListenAddress</code> can be empty (disables gRPC server)</li> <li><code>blockchain_store</code> must be valid URL format for blockchain persistence</li> <li><code>Chain configuration parameters</code> must be properly configured for difficulty calculations</li> </ol>"},{"location":"references/settings/services/blockchain_settings/#kafka-integration","title":"Kafka Integration","text":"<p>The Blockchain Service integrates with Kafka for block notifications and event streaming.</p>"},{"location":"references/settings/services/blockchain_settings/#message-formats","title":"Message Formats","text":"<p>Block notifications are serialized using Protocol Buffers and contain:</p> <ul> <li>Block header</li> <li>Block height</li> <li>Hash</li> <li>Transaction count</li> <li>Size in bytes</li> <li>Timestamp</li> </ul>"},{"location":"references/settings/services/blockchain_settings/#topics","title":"Topics","text":"<ul> <li>Blocks-Final: Used for finalized block notifications, consumed by the Block Persister service</li> </ul>"},{"location":"references/settings/services/blockchain_settings/#error-handling","title":"Error Handling","text":"<ul> <li>The service implements exponential backoff retry for Kafka publishing failures</li> <li>Failed messages are logged and retried based on the <code>blockchain_maxRetries</code> and <code>blockchain_retrySleep</code> settings</li> <li>Persistent failures, after the retries are exhausted, are reported through the health monitoring endpoints (<code>/health</code> HTTP endpoint and the <code>HealthGRPC</code> gRPC method)</li> </ul>"},{"location":"references/settings/services/blockchain_settings/#error-handling-strategies","title":"Error Handling Strategies","text":"<p>The Blockchain Service employs several strategies to handle errors and maintain resilience:</p>"},{"location":"references/settings/services/blockchain_settings/#network-and-communication-errors","title":"Network and Communication Errors","text":"<ul> <li>Uses timeouts and context cancellation to handle hanging network operations</li> <li>Implements retry mechanisms for transient failures with configured backoff periods</li> </ul>"},{"location":"references/settings/services/blockchain_settings/#validation-errors","title":"Validation Errors","text":"<ul> <li>Blocks with invalid headers, merkle roots, or proofs are rejected with appropriate error codes</li> <li>Invalid blocks can be explicitly marked using the InvalidateBlock method</li> </ul>"},{"location":"references/settings/services/blockchain_settings/#chain-reorganization","title":"Chain Reorganization","text":"<ul> <li>Detects chain splits and reorganizations automatically</li> <li>Uses rollback and catch-up operations to handle chain reorganizations</li> <li>Limits reorganization depth for security (configurable)</li> </ul>"},{"location":"references/settings/services/blockchain_settings/#storage-errors","title":"Storage Errors","text":"<ul> <li>Implements transaction-based operations with the store to maintain consistency</li> <li>Reports persistent storage errors through health endpoints</li> </ul>"},{"location":"references/settings/services/blockpersister_settings/","title":"Block Persister Settings","text":"<p>Related Topic: Block Persister Service</p> <p>The Block Persister service configuration is organized into several categories that control different aspects of the service's behavior. All settings can be provided via environment variables or configuration files.</p>"},{"location":"references/settings/services/blockpersister_settings/#storage-configuration","title":"Storage Configuration","text":""},{"location":"references/settings/services/blockpersister_settings/#state-management","title":"State Management","text":"<ul> <li>State File (<code>blockpersister_stateFile</code>)</li> <li>Type: <code>string</code></li> <li>Default Value: <code>\"file://./data/blockpersister_state.txt\"</code><ul> <li>Purpose: Maintains the persister's processing state (last persisted block height and hash)</li> </ul> </li> <li>Format: Supports both local file paths (<code>file://./path</code>) and remote storage URLs<ul> <li>Impact: Critical for recovery after service restart and maintaining processing continuity</li> </ul> </li> <li>Recovery Implications: If this file is lost, the service will need to reprocess blocks from the beginning</li> </ul>"},{"location":"references/settings/services/blockpersister_settings/#block-storage","title":"Block Storage","text":"<ul> <li>Block Store URL (<code>blockPersisterStore</code>)</li> <li>Type: <code>*url.URL</code></li> <li>Default Value: <code>\"file://./data/blockstore\"</code><ul> <li>Purpose: Defines where block data files are stored</li> </ul> </li> <li> <p>Supported Formats:</p> <ul> <li>S3: <code>s3://bucket-name/prefix</code></li> <li>Local filesystem: <code>file://./path/to/dir</code></li> <li>Impact: Determines the persistence mechanism and reliability characteristics</li> </ul> </li> <li> <p>HTTP Listen Address (<code>blockPersister_httpListenAddress</code>)</p> </li> <li>Type: <code>string</code></li> <li>Default Value: <code>\":8083\"</code><ul> <li>Purpose: Controls the network interface and port for the HTTP server that serves block data</li> <li>Usage: If empty, no HTTP server is started; when configured, enables external access to blob store</li> </ul> </li> <li>Security Consideration: In production environments, should be configured based on network security requirements</li> </ul>"},{"location":"references/settings/services/blockpersister_settings/#processing-configuration","title":"Processing Configuration","text":""},{"location":"references/settings/services/blockpersister_settings/#block-selection-and-timing","title":"Block Selection and Timing","text":"<ul> <li>Persist Age (<code>BlockPersisterPersistAge</code>)</li> <li>Type: <code>uint32</code></li> <li>Default Value: Not specified in settings (varies by configuration)<ul> <li>Purpose: Determines how many blocks behind the tip the persister stays</li> <li>Impact: Critical for avoiding reorgs by ensuring blocks are sufficiently confirmed</li> <li>Example: If set to 100, only blocks that are at least 100 blocks deep are processed</li> </ul> </li> <li> <p>Tuning Advice:</p> <ul> <li>Lower values: More immediate processing but higher risk of reprocessing due to reorgs</li> <li>Higher values: More conservative approach with minimal reorg risk</li> </ul> </li> <li> <p>Persist Sleep (<code>BlockPersisterPersistSleep</code>)</p> </li> <li>Type: <code>time.Duration</code></li> <li>Default Value: Not specified in settings (varies by configuration)<ul> <li>Purpose: Sleep duration between polling attempts when no blocks are available to process</li> <li>Impact: Controls polling frequency and system load during idle periods</li> </ul> </li> <li> <p>Tuning Advice:</p> <ul> <li>Shorter durations: More responsive but higher CPU usage</li> <li>Longer durations: More resource-efficient but less responsive</li> </ul> </li> </ul>"},{"location":"references/settings/services/blockpersister_settings/#performance-tuning","title":"Performance Tuning","text":"<ul> <li>Processing Concurrency (<code>blockpersister_concurrency</code>)</li> <li>Type: <code>int</code></li> <li>Default Value: <code>8</code><ul> <li>Purpose: Controls the number of concurrent goroutines for processing subtrees</li> <li>Impact: Directly affects CPU utilization, memory usage, and throughput</li> </ul> </li> <li> <p>Tuning Advice:</p> <ul> <li>Optimal value typically depends on available CPU cores</li> <li>For systems with 8+ cores, the default value is usually appropriate</li> <li>For high-performance systems, consider increasing to match available cores</li> </ul> </li> <li> <p>Batch Missing Transactions (<code>blockpersister_batchMissingTransactions</code>)</p> </li> <li>Type: <code>bool</code></li> <li>Default Value: <code>true</code><ul> <li>Purpose: Controls whether transactions are fetched in batches from the store</li> <li>Impact: Can significantly improve performance by reducing the number of individual queries</li> </ul> </li> <li> <p>Tuning Advice: Generally should be kept enabled unless encountering specific issues</p> </li> <li> <p>Process TxMeta Using Store Batch Size (<code>blockvalidation_processTxMetaUsingStore_BatchSize</code>)</p> </li> <li>Type: <code>int</code></li> <li>Default Value: <code>1024</code><ul> <li>Purpose: Controls the batch size when processing transaction metadata from the store</li> <li>Impact: Affects performance and memory usage when fetching transaction data</li> </ul> </li> <li>Tuning Advice: Higher values improve throughput at the cost of increased memory usage</li> </ul>"},{"location":"references/settings/services/blockpersister_settings/#utxo-management","title":"UTXO Management","text":"<ul> <li>Skip UTXO Delete (<code>SkipUTXODelete</code>)</li> <li>Type: <code>bool</code></li> <li>Default Value: <code>false</code><ul> <li>Purpose: Controls whether UTXO deletions are skipped during processing</li> <li>Impact: When enabled, improves performance but affects UTXO set completeness</li> </ul> </li> <li> <p>Usage Scenarios:</p> <ul> <li>Enable during initial sync or recovery to improve performance</li> <li>Disable for normal operation to maintain complete UTXO tracking</li> </ul> </li> </ul>"},{"location":"references/settings/services/blockpersister_settings/#configuration-interactions-and-dependencies","title":"Configuration Interactions and Dependencies","text":""},{"location":"references/settings/services/blockpersister_settings/#storage-backend-selection","title":"Storage Backend Selection","text":"<p>The Block Persister supports multiple storage backends through the <code>blockPersisterStore</code> URL:</p> <p>Local Filesystem:</p> <pre><code>file://./path/to/directory\n</code></pre> <ul> <li>Best for: Development, testing, single-node deployments</li> <li>Advantages: Simple setup, fast access, no external dependencies</li> <li>Limitations: Not suitable for distributed deployments</li> </ul> <p>S3-Compatible Storage:</p> <pre><code>s3://bucket-name/prefix\n</code></pre> <ul> <li>Best for: Production deployments, distributed systems, cloud environments</li> <li>Advantages: Highly durable, scalable, supports distributed access</li> <li>Considerations: Requires proper S3 credentials and network connectivity</li> </ul>"},{"location":"references/settings/services/blockpersister_settings/#performance-vs-accuracy-trade-offs","title":"Performance vs. Accuracy Trade-offs","text":"<p>Several settings involve trade-offs between performance and data completeness:</p> <ol> <li><code>blockpersister_concurrency</code>: Higher values improve throughput but increase resource usage</li> <li><code>SkipUTXODelete</code>: When enabled, improves performance during sync but affects UTXO tracking completeness</li> <li><code>blockpersister_batchMissingTransactions</code>: Batching improves efficiency but may increase latency for individual operations</li> </ol>"},{"location":"references/settings/services/blockpersister_settings/#state-management-and-recovery","title":"State Management and Recovery","text":"<p>The <code>blockpersister_stateFile</code> is critical for service continuity:</p> <ul> <li>The state file tracks the last successfully persisted block</li> <li>On restart, the service resumes from this point</li> <li>If the state file is corrupted or lost, manual intervention may be required</li> <li>Consider implementing regular backups of the state file for production systems</li> </ul>"},{"location":"references/settings/services/blockpersister_settings/#timing-and-synchronization","title":"Timing and Synchronization","text":"<p>The interaction between <code>BlockPersisterPersistAge</code> and <code>BlockPersisterPersistSleep</code> controls the service's responsiveness:</p> <ul> <li><code>BlockPersisterPersistAge</code> ensures blocks are sufficiently confirmed before persistence</li> <li><code>BlockPersisterPersistSleep</code> controls how frequently the service checks for new blocks to process</li> <li>Together, these settings balance responsiveness against system load and reorg risk</li> </ul>"},{"location":"references/settings/services/blockvalidation_settings/","title":"Block Validation Settings","text":"<p>Related Topic: Block Validation Service</p> <p>The Block Validation service configuration can be adjusted through environment variables or command-line flags. This section provides a comprehensive overview of all available configuration options organized by functional category.</p>"},{"location":"references/settings/services/blockvalidation_settings/#network-and-communication-settings","title":"Network and Communication Settings","text":"Setting Type Default Description Impact <code>blockvalidation_grpcAddress</code> string \"localhost:8088\" Address that other services use to connect to this service Affects how other services discover and communicate with the Block Validation service <code>blockvalidation_grpcListenAddress</code> string \":8088\" Network interface and port the service listens on for gRPC connections Controls network binding and accessibility of the service"},{"location":"references/settings/services/blockvalidation_settings/#kafka-and-concurrency-settings","title":"Kafka and Concurrency Settings","text":"Setting Type Default Description Impact <code>kafka_blocksConfig</code> string (none) Kafka configuration for block messages Required for consuming blocks from Kafka <code>blockvalidation_kafkaWorkers</code> int 0 (auto) Number of Kafka consumer workers Controls parallelism for Kafka-based block validation"},{"location":"references/settings/services/blockvalidation_settings/#performance-and-optimization","title":"Performance and Optimization","text":"Setting Type Default Description Impact <code>blockvalidation_batch_missing_transactions</code> bool false When enabled, missing transactions are fetched in batches Improves network efficiency at the cost of slightly increased latency <code>blockvalidation_quickValidationEnabled</code> bool true Enable quick validation for checkpointed blocks Dramatically improves sync speed for historical blocks <code>blockvalidation_quickValidationCheckpointHeight</code> int varies Height below which quick validation applies Controls which blocks use optimized validation <code>blockvalidation_concurrency_createAllUTXOs</code> int CPU/2 (min 4) Parallelism for UTXO creation during quick validation Higher values improve performance but increase resource usage <code>blockvalidation_concurrency_spendAllTransactions</code> int CPU/2 (min 4) Parallelism for spending transactions during quick validation Controls parallel processing during validation <code>blockvalidation_processTxMetaUsingCache_BatchSize</code> int 1024 Batch size for processing transaction metadata using cache Affects performance and memory usage during cache operations <code>blockvalidation_processTxMetaUsingCache_Concurrency</code> int 32 Concurrency level for processing transaction metadata using cache Controls parallel cache operations <code>blockvalidation_processTxMetaUsingCache_MissingTxThreshold</code> int 1 Threshold for switching to store-based processing when missing transactions Controls fallback behavior when cache misses occur <code>blockvalidation_processTxMetaUsingStore_BatchSize</code> int CPU/2 (min 4) Batch size for processing transaction metadata using store Affects performance during store operations <code>blockvalidation_processTxMetaUsingStore_Concurrency</code> int 32 Concurrency level for processing transaction metadata using store Controls parallel store operations <code>blockvalidation_processTxMetaUsingStore_MissingTxThreshold</code> int 1 Threshold for store-based processing when missing transactions Controls fallback behavior for store operations <code>blockvalidation_skipCheckParentMined</code> bool false Skips checking if parent block is mined during validation Performance optimization that may reduce validation accuracy <code>blockvalidation_subtreeFoundChConcurrency</code> int 1 Concurrency level for subtree found channel processing Controls parallel subtree processing <code>blockvalidation_subtree_validation_abandon_threshold</code> int 1 Threshold for abandoning subtree validation Controls when to give up on problematic subtrees <code>blockvalidation_validateBlockSubtreesConcurrency</code> int CPU/2 (min 4) Concurrency level for validating block subtrees Higher values improve performance but increase resource usage <code>blockvalidation_validation_max_retries</code> int 3 Maximum number of retries for validation operations Controls resilience to transient failures <code>blockvalidation_validation_retry_sleep</code> duration 5s Sleep duration between validation retries Controls backoff timing for retry operations <code>blockvalidation_isParentMined_retry_max_retry</code> int 20 Maximum retries for checking if parent block is mined Controls persistence when checking parent block status <code>blockvalidation_isParentMined_retry_backoff_multiplier</code> int 30 Backoff multiplier for parent mined check retries Controls exponential backoff timing <code>blockvalidation_subtreeGroupConcurrency</code> int 1 Concurrency level for subtree group processing Controls parallel processing of subtree groups <code>blockvalidation_blockFoundCh_buffer_size</code> int 1000 Buffer size for block found channel Controls memory usage and throughput for block notifications <code>blockvalidation_catchupCh_buffer_size</code> int 10 Buffer size for catchup channel Controls memory usage for catchup operations <code>blockvalidation_useCatchupWhenBehind</code> bool false Enables catchup mechanism when node is behind Improves sync performance but increases complexity <code>blockvalidation_catchupConcurrency</code> int CPU/2 (min 4) Concurrency level for catchup operations Controls parallel processing during catchup <code>blockvalidation_check_subtree_from_block_timeout</code> duration 5m Timeout for checking subtree from block Controls maximum wait time for subtree operations <code>blockvalidation_check_subtree_from_block_retries</code> int 5 Maximum retries for subtree from block checks Controls resilience for subtree operations <code>blockvalidation_check_subtree_from_block_retry_backoff_duration</code> duration 30s Backoff duration for subtree check retries Controls timing between retry attempts <code>blockvalidation_secret_mining_threshold</code> uint32 10 Threshold for detecting secret mining attacks Security parameter for chain reorganization detection <code>blockvalidation_previous_block_header_count</code> uint64 100 Number of previous block headers to maintain Controls memory usage and validation depth <code>blockvalidation_maxPreviousBlockHeadersToCheck</code> uint64 100 Maximum previous block headers to check during validation Limits validation scope for performance <code>blockvalidation_fail_fast_validation</code> bool true Enables fail-fast validation mode Improves performance by stopping validation early on errors <code>blockvalidation_finalizeBlockValidationConcurrency</code> int 8 Concurrency level for finalizing block validation Controls parallel finalization operations <code>blockvalidation_getMissingTransactions</code> int 32 Concurrency level for retrieving missing transactions Controls parallel transaction retrieval"},{"location":"references/settings/services/blockvalidation_settings/#transaction-processing","title":"Transaction Processing","text":"Setting Type Default Description Impact <code>blockvalidation_localSetTxMinedConcurrency</code> int 8 Concurrency level for marking transactions as mined Higher values improve performance but increase memory usage <code>blockvalidation_missingTransactionsBatchSize</code> int 5000 Batch size for retrieving missing transactions Larger batches improve throughput but increase memory usage"},{"location":"references/settings/services/blockvalidation_settings/#bloom-filter-management","title":"Bloom Filter Management","text":"Setting Type Default Description Impact <code>blockvalidation_bloom_filter_retention_size</code> uint32 GlobalBlockHeightRetention + 2 Number of recent blocks to maintain bloom filters for Affects memory usage and duplicate transaction detection efficiency. Automatically set based on global retention settings"},{"location":"references/settings/services/blockvalidation_settings/#advanced-settings","title":"Advanced Settings","text":"Setting Type Default Description Impact <code>blockvalidation_optimistic_mining</code> bool true When enabled, blocks are conditionally accepted before full validation Dramatically improves throughput at the cost of temporary chain inconsistency if validation fails <code>blockvalidation_invalidBlockTracking</code> bool true Track invalid blocks during validation Prevents reprocessing of known invalid blocks <code>blockvalidation_validation_warmup_count</code> int 128 Number of validation operations during warmup Helps prime caches and establish performance baselines <code>excessiveblocksize</code> int 4GB Maximum allowed block size Limits resource consumption for extremely large blocks"},{"location":"references/settings/services/blockvalidation_settings/#storage-and-state-management","title":"Storage and State Management","text":"Setting Type Default Description Impact <code>blockValidationMaxRetries</code> int 3 Maximum retry attempts for block validation operations Controls resilience and retry behavior for failed validation operations <code>blockValidationRetrySleep</code> duration 1s Sleep duration between retry attempts Controls retry timing and system load during failures <code>utxostore</code> URL (none) URL for the UTXO store Required for UTXO validation and updates <code>fsm_state_restore</code> bool false Enables FSM state restoration Affects recovery behavior after service restart <code>blockvalidation_subtreeBlockHeightRetention</code> uint32 (global setting) How long to keep subtrees (in terms of block height) Affects storage utilization and historical data availability"},{"location":"references/settings/services/blockvalidation_settings/#validator-integration-settings","title":"Validator Integration Settings","text":"Setting Type Default Description Impact <code>blockValidationDelay</code> int 0 Delay for block validation operations in validator Controls timing of validation operations within the validator component <code>blockValidationMaxRetries</code> int 3 Maximum retries for validator block validation operations Controls validator-specific retry behavior for block validation <code>blockValidationRetrySleep</code> string \"1s\" Sleep duration between validator retry attempts Controls validator retry timing and backoff behavior"},{"location":"references/settings/services/blockvalidation_settings/#policy-and-chain-configuration-dependencies","title":"Policy and Chain Configuration Dependencies","text":"<p>The Block Validation service depends on several chain configuration parameters that must be properly configured for difficulty calculations and block validation.</p>"},{"location":"references/settings/services/legacy_settings/","title":"Legacy Settings","text":"<p>Related Topic: Legacy Service</p> <p>The Legacy service bridges traditional Bitcoin nodes with the Teranode architecture, making its configuration particularly important for network integration. This section provides a comprehensive overview of all configuration options, organized by functional category.</p>"},{"location":"references/settings/services/legacy_settings/#network-and-communication-settings","title":"Network and Communication Settings","text":"Setting Type Default Description Impact <code>legacy_listen_addresses</code> []string [] (falls back to external IP:8333) Network addresses for the service to listen on for peer connections Controls which interfaces and ports accept connections from other nodes <code>legacy_connect_peers</code> []string [] Peer addresses to connect to on startup Forces connections to specific peers rather than using automatic peer discovery <code>legacy_grpcAddress</code> string \"\" Address for other services to connect to the Legacy service Enables service-to-service communication <code>legacy_grpcListenAddress</code> string \"\" Interface and port to listen on for gRPC connections Controls network binding for the gRPC server <code>network</code> string \"mainnet\" Specifies which blockchain network to connect to Determines network rules, consensus parameters, and compatibility"},{"location":"references/settings/services/legacy_settings/#memory-and-storage-management","title":"Memory and Storage Management","text":"Setting Type Default Description Impact <code>legacy_orphanEvictionDuration</code> duration 10m How long orphan transactions are kept before eviction Affects memory usage and ability to process delayed transactions <code>legacy_writeMsgBlocksToDisk</code> bool false Enable disk-based block queueing during synchronization Significantly reduces memory usage by writing incoming blocks to temporary disk storage with 4MB buffered I/O and automatic 10-minute cleanup. Essential for resource-constrained environments and high-volume sync operations <code>legacy_storeBatcherSize</code> int 1024 Batch size for store operations Affects efficiency of storage operations and memory usage <code>legacy_spendBatcherSize</code> int 1024 Batch size for spend operations Affects efficiency of spend operations and memory usage <code>legacy_outpointBatcherSize</code> int 1024 Batch size for outpoint operations Affects efficiency of outpoint processing <code>legacy_workingDir</code> string \"../../data\" Directory where service stores data files Controls where peer information and other data is stored <code>temp_store</code> string \"file://./data/tempstore\" Temporary storage URL for Legacy Service operations Controls temporary file storage location for block processing and peer data"},{"location":"references/settings/services/legacy_settings/#concurrency-and-performance","title":"Concurrency and Performance","text":"Setting Type Default Description Impact <code>legacy_storeBatcherConcurrency</code> int 32 Number of concurrent store operations Controls parallelism for storage operations <code>legacy_spendBatcherConcurrency</code> int 32 Number of concurrent spend operations Controls parallelism for spend operations <code>legacy_outpointBatcherConcurrency</code> int 32 Number of concurrent outpoint operations Controls parallelism for outpoint operations"},{"location":"references/settings/services/legacy_settings/#peer-management-and-timeouts","title":"Peer Management and Timeouts","text":"Setting Type Default Description Impact <code>legacy_savePeers</code> bool false Save peer information to disk for reuse on restart Enables persistent peer connections across service restarts <code>legacy_allowSyncCandidateFromLocalPeers</code> bool false Allow local peers as sync candidates Affects peer selection for blockchain synchronization <code>legacy_printInvMessages</code> bool false Print inventory messages to logs Increases log verbosity for debugging <code>legacy_peerIdleTimeout</code> duration 125s Timeout for idle peer connections Controls when peers are disconnected due to inactivity. Set to 125s to accommodate 2-minute ping/pong intervals <code>legacy_peerProcessingTimeout</code> duration 3m Timeout for peer message processing Maximum time allowed for processing messages from peers. Block processing is typically the largest operation"},{"location":"references/settings/services/legacy_settings/#feature-flags","title":"Feature Flags","text":"Setting Type Default Description Impact <code>legacy_allowBlockPriority</code> bool false Prioritize transactions based on block priority Affects transaction selection for block creation"},{"location":"references/settings/services/legacy_settings/#configuration-interactions-and-dependencies","title":"Configuration Interactions and Dependencies","text":""},{"location":"references/settings/services/legacy_settings/#peer-discovery-and-connection-management","title":"Peer Discovery and Connection Management","text":"<p>The Legacy service's peer connection behavior is controlled by several interrelated settings:</p> <ul> <li><code>legacy_listen_addresses</code> determines where the service listens for incoming connections</li> <li><code>legacy_connect_peers</code> forces outbound connections to specific peers</li> <li><code>legacy_savePeers</code> enables storing peer information across restarts</li> <li>If no <code>legacy_listen_addresses</code> are specified, the service detects the external IP and uses port 8333</li> </ul> <p>These settings should be configured together based on your network architecture and security requirements.</p>"},{"location":"references/settings/services/legacy_settings/#memory-management-considerations","title":"Memory Management Considerations","text":"<p>Several settings affect the memory usage patterns of the Legacy service:</p> <ul> <li><code>legacy_writeMsgBlocksToDisk</code> significantly reduces memory usage during blockchain synchronization by writing blocks to disk rather than keeping them in memory</li> <li><code>legacy_orphanEvictionDuration</code> controls how aggressively orphan transactions are removed from memory</li> <li>The various batcher size settings control memory usage during batch operations</li> </ul> <p>For resource-constrained environments, enable <code>legacy_writeMsgBlocksToDisk</code> and use smaller batch sizes.</p>"},{"location":"references/settings/services/legacy_settings/#disk-based-block-queueing-legacy_writemsgblockstodisk","title":"Disk-Based Block Queueing (<code>legacy_writeMsgBlocksToDisk</code>)","text":"<p>The <code>legacy_writeMsgBlocksToDisk</code> setting enables a sophisticated disk-based queueing mechanism that fundamentally changes how the Legacy service handles incoming blocks during synchronization.</p> <p>How It Works:</p> <ul> <li>It creates a streaming pipeline that writes blocks directly to disk, employing 4MB buffered readers for optimal disk performance</li> <li>Blocks are stored with a 10-minute TTL to prevent disk accumulation</li> </ul> <p>Technical Implementation:</p> <pre><code>Incoming Block \u2192 io.Pipe() \u2192 4MB Buffer \u2192 Temporary Disk Storage \u2192 Validation Queue\n                     \u2193\n              Background Goroutine\n</code></pre> <p>Benefits:</p> <ul> <li>Memory Efficiency: Prevents memory exhaustion during large-scale synchronization</li> <li>Scalability: Handles high-volume block arrivals without resource constraints</li> <li>Reliability: Reduces risk of out-of-memory errors during initial sync</li> <li>Performance: Maintains processing throughput while using minimal memory</li> </ul> <p>When to Enable:</p> <ul> <li>\u2705 Initial blockchain synchronization - Essential for syncing from genesis</li> <li>\u2705 Resource-constrained environments - Limited RAM availability</li> <li>\u2705 High-volume block processing - During catch-up operations</li> <li>\u274c Normal operation - Not typically needed when fully synchronized</li> </ul>"},{"location":"references/settings/services/p2p_settings/","title":"P2P Settings","text":"<p>Related Topic: P2P Service</p> <p>The P2P service serves as the communication backbone of the Teranode network, enabling nodes to discover, connect, and exchange data with each other. This section provides a comprehensive overview of all configuration options, organized by functional category.</p>"},{"location":"references/settings/services/p2p_settings/#network-and-discovery-settings","title":"Network and Discovery Settings","text":"Setting Type Default Description Impact <code>p2p_listen_addresses</code> []string [] Network addresses for the P2P service to listen on Controls which interfaces and ports the service binds to for accepting connections <code>p2p_advertise_addresses</code> []string [] Addresses to advertise to other peers Affects how other peers discover and connect to this node. Supports both IP addresses and domain names with optional port specification (e.g., <code>192.168.1.1</code>, <code>example.com:9906</code>). When port is omitted, the <code>p2p_port</code> value is used. <code>p2p_port</code> int 9906 Default port for P2P communication Used as the fallback port when addresses don't specify a port <code>p2p_share_private_addresses</code> bool true Advertise private/local IP addresses to peers Controls whether private addresses are shared for local/test environments vs production privacy <code>p2p_bootstrapAddresses</code> []string [] Initial peer addresses for bootstrapping the DHT Helps new nodes join the network by providing entry points <code>p2p_bootstrap_persistent</code> bool false Add bootstrap addresses to static peers When enabled, bootstrap addresses are automatically added to the static peers list for persistent connections <code>p2p_static_peers</code> []string [] Peer addresses to connect to on startup Ensures connections to specific peers regardless of discovery <code>p2p_dht_protocol_id</code> string \"\" Protocol identifier for DHT communications Affects how nodes discover each other in the network <code>p2p_dht_use_private</code> bool false Use private DHT mode Restricts DHT communication to trusted peers when enabled <code>p2p_optimise_retries</code> bool false Optimize retry behavior for peer connections Improves efficiency of connection attempts in certain network conditions <code>listen_mode</code> string \"full\" Node operation mode Controls node behavior: \"full\" for normal operation, \"listen_only\" for passive mode"},{"location":"references/settings/services/p2p_settings/#service-endpoints","title":"Service Endpoints","text":"Setting Type Default Description Impact <code>p2p_grpcAddress</code> string \"\" Address for other services to connect to this service Enables service-to-service communication <code>p2p_grpcListenAddress</code> string \":9906\" Interface and port to listen on for gRPC connections Controls network binding for the gRPC server <code>p2p_httpAddress</code> string \"localhost:9906\" Address other services use to connect to HTTP API Affects how other services discover the P2P HTTP API <code>p2p_httpListenAddress</code> string \"\" Interface and port to listen on for HTTP connections Controls network binding for the HTTP server <code>securityLevelHTTP</code> int 0 HTTP security level (0=HTTP, 1=HTTPS) Determines whether HTTP or HTTPS is used for web connections <code>server_certFile</code> string \"\" Path to SSL certificate file Required for HTTPS when security level is 1 <code>server_keyFile</code> string \"\" Path to SSL key file Required for HTTPS when security level is 1"},{"location":"references/settings/services/p2p_settings/#peer-to-peer-topics","title":"Peer-to-Peer Topics","text":"Setting Type Default Description Impact <code>p2p_bestblock_topic</code> string \"\" Topic name for best block announcements Controls subscription and publication to the best block channel <code>p2p_block_topic</code> string \"\" Topic name for block announcements Controls subscription and publication to the block channel <code>p2p_subtree_topic</code> string \"\" Topic name for subtree announcements Controls subscription and publication to the subtree channel <code>p2p_mining_on_topic</code> string \"\" Topic name for mining status announcements Controls subscription and publication to the mining status channel <code>p2p_rejected_tx_topic</code> string \"\" Topic name for rejected transaction announcements Controls subscription to rejected transaction notifications <code>p2p_handshake_topic</code> string \"\" REQUIRED - Topic name for peer handshake messages Used for version and verack exchanges between peers. During handshake, peers exchange their user agent (format: <code>teranode/bitcoin/{version}</code>), best block height, topic prefix for chain validation, and other connection metadata. Service will fail to start if not configured. <code>p2p_handshake_topic_size</code> int 1 Minimum topic size for handshake publishing Controls reliability of handshake message delivery <code>p2p_handshake_topic_timeout</code> duration 5s Timeout for handshake topic operations Maximum wait time for handshake topic readiness <code>p2p_node_status_topic</code> string \"\" Topic name for node status messages Controls subscription and publication to the node status channel"},{"location":"references/settings/services/p2p_settings/#authentication-and-security","title":"Authentication and Security","text":"Setting Type Default Description Impact <code>p2p_peer_id</code> string \"\" Unique identifier for the P2P node Used to identify this node in the P2P network <code>p2p_private_key</code> string \"\" Private key for P2P authentication Provides cryptographic identity for the node; if not provided, will be auto-generated and stored <code>p2p_shared_key</code> string \"\" Shared key for private network communication When provided, ensures only nodes with the same shared key can communicate <code>grpcAdminAPIKey</code> string \"\" API key for gRPC admin operations Required for administrative gRPC calls to the P2P service"},{"location":"references/settings/services/p2p_settings/#nat-traversal-essential-for-production","title":"NAT Traversal (Essential for Production)","text":"Setting Type Default Description Impact <code>p2p_enable_nat_service</code> bool true Enable AutoNAT service for address discovery Essential for nodes behind NAT - Helps detect public addresses and connectivity status <code>p2p_enable_hole_punching</code> bool true Enable NAT hole punching (DCUtR) Critical for peer-to-peer connectivity - Allows direct connections through NATs <code>p2p_enable_relay</code> bool true Enable relay service (Circuit Relay v2) Fallback for difficult NAT scenarios - Allows nodes to relay traffic when direct connection fails <code>p2p_enable_nat_port_map</code> bool true Enable NAT port mapping (UPnP/NAT-PMP) Automatic port forwarding - Configures router port forwarding when supported"},{"location":"references/settings/services/p2p_settings/#connection-limits-prevents-resource-exhaustion","title":"Connection Limits (Prevents Resource Exhaustion)","text":"Setting Type Default Description Impact <code>p2p_enable_conn_manager</code> bool true Enable connection manager with limits Prevents resource exhaustion - Controls connection limits and pruning <code>p2p_conn_low_water</code> int 200 Minimum connections to maintain Lower bound for connection manager <code>p2p_conn_high_water</code> int 400 Maximum connections before pruning Upper bound before connection manager starts pruning"},{"location":"references/settings/services/p2p_settings/#ban-management","title":"Ban Management","text":"Setting Type Default Description Impact <code>p2p_ban_threshold</code> int 100 Score threshold at which peers are banned Controls how aggressively misbehaving peers are banned <code>p2p_ban_duration</code> duration 24h Duration for which peers remain banned Controls how long banned peers are excluded from the network"},{"location":"references/settings/services/p2p_settings/#operation-mode-settings","title":"Operation Mode Settings","text":"Setting Type Default Description Impact <code>listen_mode</code> string full Operation mode for the P2P service (<code>full</code> or <code>listen_only</code>) Controls whether the node participates fully in the network or only receives data without propagating. <code>listen_only</code> mode is useful for monitoring nodes that need to stay synchronized without contributing to network traffic. See Using Listen Mode for detailed usage."},{"location":"references/settings/services/p2p_settings/#external-service-dependencies","title":"External Service Dependencies","text":"Setting Type Default Description Impact <code>chainCfgParams_topicPrefix</code> string \"mainnet\" Chain identifier prefix for all topic names REQUIRED - Provides network isolation by prefixing all pubsub topics (e.g., \"mainnet-blocks\", \"testnet-blocks\") <code>asset_httpPublicAddress</code> string \"\" Public HTTP address of Asset service Used for constructing data hub URLs in peer messages <code>asset_httpAddress</code> string \"\" Internal HTTP address of Asset service Fallback for data hub URL construction <code>version</code> string \"\" Service version Included in node status messages for network compatibility <code>commit</code> string \"\" Git commit hash Included in node status messages for version tracking <code>coinbase_arbitraryText</code> string \"\" Miner name from coinbase configuration Used as miner identifier in node status messages"},{"location":"references/settings/services/p2p_settings/#grpc-client-configuration","title":"GRPC Client Configuration","text":"Setting Type Default Description Impact <code>grpcMaxRetries</code> int 3 Maximum retry attempts for gRPC operations Controls resilience of service-to-service communication <code>grpcRetryBackoff</code> duration 1s Backoff duration between gRPC retries Controls retry timing for failed gRPC calls"},{"location":"references/settings/services/p2p_settings/#kafka-integration","title":"Kafka Integration","text":"Setting Type Default Description Impact <code>kafka_invalidBlocksConfig</code> URL nil Kafka URL for invalid blocks consumer When set, enables consumption of invalid block notifications <code>kafka_hosts</code> []string [] Kafka broker hosts Required for Kafka connectivity when invalid blocks consumer is enabled <code>kafka_port</code> int 9092 Kafka broker port Default port for Kafka broker connections <code>kafka_invalidBlocks</code> string \"\" Kafka topic name for invalid blocks Topic to consume invalid block notifications from <code>kafka_partitions</code> int 1 Number of Kafka partitions Controls parallelism for Kafka message processing"},{"location":"references/settings/services/p2p_settings/#subtree-validation","title":"Subtree Validation","text":"Setting Type Default Description Impact <code>subtreeValidation_blacklistedBaseURLs</code> map[string]struct{} {} Map of blacklisted base URLs Prevents processing of subtrees from blacklisted sources"},{"location":"references/settings/services/p2p_settings/#blockchain-store-integration","title":"Blockchain Store Integration","text":"Setting Type Default Description Impact <code>blockchain_storeURL</code> URL \"\" Blockchain store URL Required for ban list persistence and private key storage"},{"location":"references/settings/services/p2p_settings/#configuration-validation-rules","title":"Configuration Validation Rules","text":"<p>The P2P service enforces several validation rules during startup:</p>"},{"location":"references/settings/services/p2p_settings/#required-configuration","title":"Required Configuration","text":"<ul> <li><code>p2p_listen_addresses</code> - Must be non-empty array or service will fail with \"p2p_listen_addresses not set in config\"</li> <li><code>p2p_port</code> - Must be &gt; 0 or service will fail with \"p2p_port not set in config\"</li> <li><code>p2p_shared_key</code> - Must be set or service will fail with \"error getting p2p_shared_key\"</li> <li><code>p2p_block_topic</code> - Must be set or service will fail with \"p2p_block_topic not set in config\"</li> <li><code>p2p_subtree_topic</code> - Must be set or service will fail with \"p2p_subtree_topic not set in config\"</li> <li><code>p2p_handshake_topic</code> - Must be set or service will fail with \"p2p_handshake_topic not set in config\"</li> <li><code>p2p_mining_on_topic</code> - Must be set or service will fail with \"p2p_mining_on_topic not set in config\"</li> <li><code>p2p_rejected_tx_topic</code> - Must be set or service will fail with \"p2p_rejected_tx_topic not set in config\"</li> <li><code>ChainCfgParams.TopicPrefix</code> - Must be set or service will fail with \"missing config ChainCfgParams.TopicPrefix\"</li> <li>This chain identifier ensures network isolation between different chains (mainnet, testnet, etc.)</li> <li>Peers validate topic prefix during handshake and reject connections from different chains</li> </ul>"},{"location":"references/settings/services/p2p_settings/#key-management","title":"Key Management","text":"<ul> <li>If <code>p2p_private_key</code> is not provided, a new Ed25519 key is automatically generated and stored in <code>p2p.key</code> file</li> <li>The key file is created in the same directory as the peer cache (<code>p2p_peer_cache_dir</code> or binary directory)</li> <li>Listen Mode Validation: <code>listen_mode</code> must be either \"full\" or \"listen_only\" or service will fail with validation error</li> </ul>"},{"location":"references/settings/services/p2p_settings/#configuration-interactions-and-dependencies","title":"Configuration Interactions and Dependencies","text":""},{"location":"references/settings/services/p2p_settings/#network-binding-and-discovery","title":"Network Binding and Discovery","text":"<p>The P2P service's network presence is controlled by several interrelated settings:</p> <ul> <li><code>p2p_listen_addresses</code> determines which interfaces/ports the service listens on</li> <li><code>p2p_advertise_addresses</code> controls what addresses are advertised to peers</li> <li>Each address can be specified with or without a port (e.g., <code>192.168.1.1</code> or <code>example.com:9906</code>)</li> <li>For addresses without a port, the system automatically uses the value from <code>p2p_port</code></li> <li>Both IP addresses and domain names are supported with proper multiaddress formatting</li> <li><code>p2p_port</code> provides a default when addresses don't specify ports</li> <li>If no <code>p2p_listen_addresses</code> are specified, the service may not be reachable</li> <li>The gRPC and HTTP listen addresses control how other services can interact with the P2P service</li> </ul> <p>These settings should be configured together based on your network architecture and security requirements.</p>"},{"location":"references/settings/services/p2p_settings/#peer-discovery-and-connection","title":"Peer Discovery and Connection","text":"<p>Peer discovery in the P2P service uses a multi-layered approach:</p> <ul> <li><code>p2p_bootstrapAddresses</code> provides initial entry points to the network</li> <li><code>p2p_bootstrap_persistent</code> controls whether bootstrap addresses are automatically added to static peers</li> <li><code>p2p_static_peers</code> ensures connections to specific peers regardless of DHT discovery</li> <li><code>p2p_dht_protocol_id</code> and <code>p2p_dht_use_private</code> affect the DHT-based peer discovery mechanism</li> <li><code>p2p_optimise_retries</code> impacts connection retry behavior when peers are unreachable</li> <li><code>listen_mode</code> determines whether the node operates in \"full\" or \"listen_only\" mode</li> </ul> <p>In private network deployments, you should configure static peers and bootstrap addresses carefully to ensure nodes can find each other.</p>"},{"location":"references/settings/services/p2p_settings/#bootstrap-vs-static-peers","title":"Bootstrap vs Static Peers","text":"<p>By default, bootstrap addresses are used only for initial network discovery and DHT bootstrapping. If you need bootstrap servers to maintain persistent connections that automatically reconnect after disconnection, enable <code>p2p_bootstrap_persistent=true</code>. This will treat bootstrap addresses as static peers for connection purposes while still using them for DHT bootstrapping.</p>"},{"location":"references/settings/services/p2p_settings/#security-and-authentication","title":"Security and Authentication","text":"<p>The P2P service uses several security mechanisms:</p> <ul> <li><code>p2p_private_key</code> establishes the node's identity, which is reflected in its <code>p2p_peer_id</code></li> <li><code>p2p_shared_key</code> enables private network functionality, restricting communication to nodes with the same shared key</li> <li>If no private key is provided, it will be auto-generated and stored in the blockchain store via <code>blockchain_storeURL</code></li> <li>The ban system uses <code>p2p_ban_threshold</code> and <code>p2p_ban_duration</code> for score-based peer management</li> <li><code>grpcAdminAPIKey</code> protects administrative gRPC operations</li> <li>HTTPS support via <code>securityLevelHTTP</code>, <code>server_certFile</code>, and <code>server_keyFile</code></li> </ul> <p>These settings enable secure and authenticated communication between nodes in the network.</p>"},{"location":"references/settings/services/propagation_settings/","title":"Propagation Settings","text":"<p>Related Topic: Propagation Service</p> <p>The Propagation service serves as the transaction intake and distribution system for the Teranode network, critical for handling transaction flow between clients and the internal services. This section provides a comprehensive overview of all configuration options, organized by functional category.</p>"},{"location":"references/settings/services/propagation_settings/#network-and-communication-settings","title":"Network and Communication Settings","text":"Setting Type Default Description Impact <code>propagation_grpcListenAddress</code> string \"\" Address for gRPC server to listen on Controls the endpoint where the gRPC API is exposed <code>propagation_grpcAddresses</code> []string [] List of gRPC addresses for other services to connect to Affects how other services discover and communicate with this service <code>propagation_httpListenAddress</code> string \"\" Address for HTTP server to listen on Controls if and where the HTTP transaction API is exposed <code>propagation_httpAddresses</code> []string [] List of HTTP addresses for other services to connect to Affects how other services discover this service's HTTP API <code>ipv6_addresses</code> string \"\" Comma-separated list of IPv6 multicast addresses Controls which IPv6 multicast addresses are used for transaction reception <code>ipv6_interface</code> string \"\" Network interface name for IPv6 multicast Determines which network interface is used for multicast communication"},{"location":"references/settings/services/propagation_settings/#performance-and-throttling-settings","title":"Performance and Throttling Settings","text":"Setting Type Default Description Impact <code>propagation_grpcMaxConnectionAge</code> duration 90s Maximum duration for gRPC connections before forced renewal Controls connection lifecycle and helps with load balancing <code>propagation_httpRateLimit</code> int 1024 Rate limit for HTTP API requests (per second) Controls how many requests per second the HTTP API can handle <code>propagation_sendBatchSize</code> int 100 Maximum number of transactions to send in a batch Affects efficiency and throughput of transaction processing <code>propagation_sendBatchTimeout</code> int 5 Timeout in milliseconds for batch sending operations Controls how long the service waits to collect a full batch before processing"},{"location":"references/settings/services/propagation_settings/#transport-and-behavior-settings","title":"Transport and Behavior Settings","text":"Setting Type Default Description Impact <code>propagation_alwaysUseHTTP</code> bool false Forces using HTTP instead of gRPC for transaction validation Affects performance and reliability of transaction validation"},{"location":"references/settings/services/propagation_settings/#dependency-injected-settings-from-other-services","title":"Dependency-Injected Settings (from other services)","text":"Setting Type Default Description Impact <code>validator_httpAddress</code> url null URL for validator HTTP API Used as fallback for large transactions exceeding Kafka limits <code>validator_kafkaMaxMessageBytes</code> int varies Maximum size for Kafka messages in bytes Determines when HTTP fallback is used for large transactions <code>useLocalValidator</code> bool false Daemon-level setting for validator deployment mode Controls whether validator runs in-process or as separate service"},{"location":"references/settings/services/propagation_settings/#configuration-interactions-and-dependencies","title":"Configuration Interactions and Dependencies","text":""},{"location":"references/settings/services/propagation_settings/#transaction-ingestion-paths","title":"Transaction Ingestion Paths","text":"<p>The Propagation service supports multiple methods for receiving transactions, controlled by several related settings:</p> <ul> <li>HTTP API (<code>propagation_httpListenAddress</code>): REST-based transaction submission, rate-limited by <code>propagation_httpRateLimit</code></li> <li>gRPC API (<code>propagation_grpcListenAddress</code>): High-performance RPC interface for transaction submission</li> <li>IPv6 Multicast (<code>ipv6_addresses</code> on the specified <code>ipv6_interface</code>): Efficient multicast reception of transactions</li> </ul> <p>At least one ingestion path must be configured for the service to be functional. For production deployments, all three methods should be configured for maximum compatibility and performance.</p>"},{"location":"references/settings/services/propagation_settings/#transaction-validation-architecture","title":"Transaction Validation Architecture","text":"<p>The Propagation service interacts with the Validator service using one of two architectural patterns:</p> <ul> <li> <p>Local Validator Mode (<code>useLocalValidator=true</code>):</p> </li> <li> <p>Validator runs in-process with the Propagation service</p> </li> <li>Eliminates network overhead for validation operations</li> <li> <p>Recommended for production deployments to minimize latency</p> </li> <li> <p>Remote Validator Mode (<code>useLocalValidator=false</code>):</p> </li> <li> <p>Propagation service communicates with a separate Validator service</p> </li> <li> <p>Transaction Size-Based Routing: Transactions are automatically routed based on size:</p> <ul> <li>Normal transactions (\u2264 <code>validator_kafka_maxMessageBytes</code>): Sent via Kafka for async validation</li> <li>Large transactions (&gt; <code>validator_kafka_maxMessageBytes</code>): Automatically sent via HTTP to <code>validator_httpAddress</code></li> <li>Transport Override: <code>propagation_alwaysUseHTTP=true</code> forces all transactions to use HTTP regardless of size</li> <li>Automatic Fallback: HTTP fallback ensures reliability for transactions of any size</li> </ul> </li> </ul>"},{"location":"references/settings/services/propagation_settings/#client-side-batch-processing-optimization","title":"Client-Side Batch Processing Optimization","text":"<p>The propagation client uses batching to optimize throughput, controlled by:</p> <ul> <li><code>propagation_sendBatchSize</code>: Determines maximum batch size for client transaction processing (default: 100)</li> <li><code>propagation_sendBatchTimeout</code>: Controls how long to wait in milliseconds for a batch to fill before processing (default: 5ms)</li> </ul> <p>These settings should be tuned together based on expected transaction volume and size characteristics:</p> <ul> <li>Higher batch sizes improve throughput but increase latency</li> <li>Shorter timeouts decrease latency but may result in smaller, less efficient batches</li> </ul>"},{"location":"references/settings/services/rpc_settings/","title":"RPC Settings","text":"<p>Related Topic: RPC Service</p> <p>The RPC service relies on a set of configuration settings that control its behavior, security, and performance. This section provides a comprehensive overview of these settings, organized by functional category, along with their impacts, dependencies, and recommended configurations for different deployment scenarios.</p>"},{"location":"references/settings/services/rpc_settings/#configuration-categories","title":"Configuration Categories","text":"<p>RPC service settings can be organized into the following functional categories:</p> <ol> <li>Authentication &amp; Security: Settings that control user access and authentication mechanisms</li> <li>Network Configuration: Settings that determine how the service binds to network interfaces</li> <li>Performance &amp; Scaling: Settings that manage resource usage and connection handling</li> <li>Compatibility: Settings that control backward compatibility with legacy clients</li> </ol>"},{"location":"references/settings/services/rpc_settings/#authentication-security-settings","title":"Authentication &amp; Security Settings","text":"<p>These settings control access to the RPC service, implementing a two-tier authentication system with full and limited access.</p> Setting Type Default Description Impact <code>rpc_user</code> string <code>\"\"</code> (empty string) Username for RPC authentication with full access Controls who can access the RPC service with full permissions; required for secure deployments <code>rpc_pass</code> string <code>\"\"</code> (empty string) Password for RPC authentication with full access Controls authentication for full access; required for secure deployments <code>rpc_limit_user</code> string <code>\"\"</code> (empty string) Username for RPC authentication with limited access Controls who can access the RPC service with limited permissions; optional but recommended <code>rpc_limit_pass</code> string <code>\"\"</code> (empty string) Password for RPC authentication with limited access Controls authentication for limited access; optional but recommended"},{"location":"references/settings/services/rpc_settings/#authentication-interactions-and-dependencies","title":"Authentication Interactions and Dependencies","text":"<p>The RPC service implements a two-tier authentication system:</p> <ul> <li>Full Access: Authenticated using <code>rpc_user</code> and <code>rpc_pass</code>, grants access to all RPC commands including sensitive operations like <code>stop</code> and administrative functions</li> <li>Limited Access: Authenticated using <code>rpc_limit_user</code> and <code>rpc_limit_pass</code>, grants access to a subset of commands, restricting sensitive operations</li> </ul> <p>For security, credentials should be set explicitly as they default to empty strings.</p>"},{"location":"references/settings/services/rpc_settings/#network-configuration-settings","title":"Network Configuration Settings","text":"<p>These settings control how the RPC service binds to network interfaces and accepts connections.</p> Setting Type Default Description Impact <code>rpc_listener_url</code> *url.URL <code>nil</code> URL where the RPC service listens for connections, in format \"http://hostname:port\" Controls which network interface and port the service binds to; critical for accessibility and security"},{"location":"references/settings/services/rpc_settings/#network-configuration-interactions-and-dependencies","title":"Network Configuration Interactions and Dependencies","text":"<p>The <code>rpc_listener_url</code> setting determines both the binding interface and port. For example:</p> <ul> <li><code>http://localhost:8332</code> binds to the local interface only, allowing connections only from the same machine</li> <li><code>http://0.0.0.0:8332</code> binds to all interfaces, allowing remote connections if the machine is network-accessible</li> </ul> <p>This setting has significant security implications. In production environments, the RPC service should typically only bind to local interfaces unless remote RPC access is explicitly required.</p>"},{"location":"references/settings/services/rpc_settings/#performance-scaling-settings","title":"Performance &amp; Scaling Settings","text":"<p>These settings control resource usage, caching behavior, and the service's ability to handle multiple simultaneous clients.</p> Setting Type Default Description Impact <code>rpc_max_clients</code> int <code>1</code> Maximum number of concurrent RPC client connections allowed Limits resource usage; prevents overload from too many simultaneous connections <code>rpc_cache_enabled</code> bool <code>true</code> Enable response caching for frequently accessed data Improves performance by caching responses for commands like getblock, getblockheader, and getrawtransaction <code>rpc_timeout</code> time.Duration <code>30s</code> Timeout for RPC request processing Controls maximum time allowed for processing individual RPC requests; prevents hung requests <code>rpc_client_call_timeout</code> time.Duration <code>5s</code> Timeout for calls to dependent services (P2P, Peer services) Controls timeout when RPC service calls other Teranode services for data"},{"location":"references/settings/services/rpc_settings/#performance-scaling-interactions-and-dependencies","title":"Performance Scaling Interactions and Dependencies","text":"<p>The performance settings work together to optimize RPC service behavior:</p> <ul> <li>Connection Management: <code>rpc_max_clients</code> directly impacts server resource usage. When the limit is reached, new connections are rejected with a 503 Service Unavailable response</li> <li>Response Caching: <code>rpc_cache_enabled</code> significantly improves performance for read-heavy workloads by caching responses for blockchain data queries</li> <li>Timeout Control: <code>rpc_timeout</code> and <code>rpc_client_call_timeout</code> prevent resource exhaustion from hung requests and service calls</li> </ul> <p>These settings should be adjusted based on:</p> <ul> <li>Available server resources (CPU, memory, network capacity)</li> <li>Expected client load patterns and caching benefits</li> <li>Network latency to dependent services</li> <li>Complexity of RPC requests (some operations are more resource-intensive)</li> </ul> <p>The default values are conservative to prevent resource exhaustion in default configurations.</p>"},{"location":"references/settings/services/rpc_settings/#timeout-configuration-guidelines","title":"Timeout Configuration Guidelines","text":"<p>The timeout settings interact with service dependencies and operation complexity:</p> <ul> <li> <p><code>rpc_timeout</code> should be set higher than the longest expected operation:</p> </li> <li> <p>Mining operations: 30-60s recommended</p> </li> <li>Large block retrievals: 20-30s recommended</li> <li> <p>Simple queries: 10-15s sufficient</p> </li> <li> <p><code>rpc_client_call_timeout</code> should be short enough to fail fast:</p> </li> <li> <p>Network operations: 5-10s recommended</p> </li> <li>Local service calls: 3-5s recommended</li> <li>Critical paths: Keep as low as practical</li> </ul> <p>Monitor timeout errors (code -30) to identify operations requiring adjustment.</p>"},{"location":"references/settings/services/rpc_settings/#compatibility-settings","title":"Compatibility Settings","text":"<p>These settings control the service's behavior when dealing with legacy Bitcoin clients.</p> Setting Type Default Description Impact <code>rpc_quirks</code> bool <code>true</code> Enables backward compatibility quirks for legacy Bitcoin RPC clients Affects how responses are formatted; enables legacy behavior for older clients"},{"location":"references/settings/services/rpc_settings/#compatibility-interactions-and-dependencies","title":"Compatibility Interactions and Dependencies","text":"<p>The <code>rpc_quirks</code> setting primarily affects response formatting and error handling behavior:</p> <ul> <li>When enabled, responses may include additional fields or formatting to maintain compatibility with older clients</li> <li>When disabled, responses strictly follow current Bitcoin RPC standards</li> </ul> <p>This setting should be left enabled unless all clients are confirmed to support modern Bitcoin RPC conventions.</p>"},{"location":"references/settings/services/rpc_settings/#dependency-injected-settings","title":"Dependency-Injected Settings","text":"<p>These settings are not directly part of the RPC configuration but are required dependencies that the RPC service uses from other services.</p> Setting Type Default Description Impact <code>asset_httpAddress</code> string <code>\"\"</code> (required) HTTP address of the Asset service for transaction hex retrieval Required: RPC service fails to start if not configured; used for getrawtransaction hex responses"},{"location":"references/settings/services/rpc_settings/#dependency-configuration-interactions","title":"Dependency Configuration Interactions","text":"<p>The RPC service acts as an API gateway and requires several service dependencies:</p> <ul> <li>Asset Service Integration: <code>asset_httpAddress</code> is mandatory for transaction data retrieval functionality. The RPC service constructs URLs like <code>{asset_httpAddress}/api/v1/tx/{txid}/hex</code> for transaction hex responses</li> <li>Service Context: The <code>Context</code> setting is used for HTTP listener setup and management</li> <li>Client Dependencies: The service requires active connections to Blockchain, UTXO Store, Block Assembly, Peer, and P2P services</li> </ul>"},{"location":"references/settings/services/rpc_settings/#configuration-relationships-and-dependencies","title":"Configuration Relationships and Dependencies","text":"<p>The RPC service configuration settings have several important relationships:</p>"},{"location":"references/settings/services/rpc_settings/#authentication-flow","title":"Authentication Flow","text":"<ul> <li><code>rpc_user</code> + <code>rpc_pass</code> \u2192 Full admin access (all RPC commands)</li> <li><code>rpc_limit_user</code> + <code>rpc_limit_pass</code> \u2192 Limited access (read-only commands)</li> <li>Both credential pairs are SHA256 hashed for secure validation</li> </ul>"},{"location":"references/settings/services/rpc_settings/#performance-optimization","title":"Performance Optimization","text":"<ul> <li><code>rpc_cache_enabled</code> + <code>rpc_timeout</code> \u2192 Balanced performance vs responsiveness</li> <li><code>rpc_max_clients</code> + <code>rpc_client_call_timeout</code> \u2192 Resource management</li> </ul>"},{"location":"references/settings/services/rpc_settings/#service-integration","title":"Service Integration","text":"<ul> <li><code>asset_httpAddress</code> \u2192 Required for transaction hex retrieval</li> <li><code>rpc_listener_url</code> \u2192 Required for HTTP server binding</li> </ul>"},{"location":"references/settings/services/rpc_settings/#compatibility-and-behavior","title":"Compatibility and Behavior","text":"<ul> <li><code>rpc_quirks</code> \u2192 Affects response formatting for legacy client compatibility</li> </ul>"},{"location":"references/settings/services/subtreevalidation_settings/","title":"Subtree Validation Settings","text":"<p>Related Topic: Subtree Validation Service</p> <p>The Subtree Validation service relies on a set of configuration settings that control its behavior, performance, and resource usage. This section provides a comprehensive overview of these settings, organized by functional category, along with their impacts, dependencies, and recommended configurations for different deployment scenarios.</p>"},{"location":"references/settings/services/subtreevalidation_settings/#configuration-categories","title":"Configuration Categories","text":"<p>Subtree Validation service settings can be organized into the following functional categories:</p> <ol> <li>Quorum &amp; Coordination: Settings that control how subtree validation is coordinated across the system</li> <li>Transaction Processing: Settings that manage transaction retrieval and validation</li> <li>Performance &amp; Scaling: Settings that control concurrency, batch sizes, and resource usage</li> <li>Caching &amp; Memory: Settings that manage the transaction metadata cache</li> <li>Network &amp; Communication: Settings for network binding and service communication</li> </ol>"},{"location":"references/settings/services/subtreevalidation_settings/#quorum-coordination-settings","title":"Quorum &amp; Coordination Settings","text":"<p>These settings control how subtree validation is coordinated to prevent duplicate work and ensure consistency.</p> Setting Type Default Description Impact <code>subtree_quorum_path</code> string <code>\"\"</code> (empty) REQUIRED - Directory path where quorum data is stored for subtree validation Critical - service will not initialize without this path configured <code>subtree_quorum_absolute_timeout</code> time.Duration <code>30s</code> Maximum time to wait for quorum operations to complete Controls deadlock prevention and failure recovery during validation <code>subtreevalidation_subtree_validation_abandon_threshold</code> int <code>1</code> Number of sequential validation failures before abandoning validation attempts Controls resilience and retry behavior for validation errors <code>subtreevalidation_failfast_validation</code> bool <code>true</code> Controls whether validation stops at the first error or continues Affects validation performance and error reporting behavior <code>subtreevalidation_orphanageTimeout</code> time.Duration <code>15m</code> Timeout for orphaned transactions in the orphanage cache Controls memory usage and cleanup of orphaned transaction data <code>subtreevalidation_subtreeBlockHeightRetention</code> uint32 <code>globalBlockHeightRetention</code> Block height retention for subtree data Controls how long subtree data is retained based on block height <code>subtreevalidation_blockHeightRetentionAdjustment</code> int32 <code>0</code> Adjustment to global block height retention (can be positive or negative) Fine-tunes retention period for subtree-specific requirements"},{"location":"references/settings/services/subtreevalidation_settings/#quorum-interactions-and-dependencies","title":"Quorum Interactions and Dependencies","text":"<p>The quorum mechanism ensures that subtree validation is coordinated across the system, preventing duplicate validation work:</p> <ul> <li>The quorum path specifies where lock files are stored to coordinate validation work</li> <li>The absolute timeout prevents deadlocks if a lock holder crashes or network issues occur</li> <li>Quorum locks are acquired per subtree hash to ensure only one validator processes each subtree</li> </ul> <p>When a subtree fails validation multiple times (reaching the abandon threshold), it is marked as permanently invalid to prevent wasting resources on unrecoverable subtrees.</p>"},{"location":"references/settings/services/subtreevalidation_settings/#transaction-processing-settings","title":"Transaction Processing Settings","text":"<p>These settings control how transactions are retrieved, validated, and processed during subtree validation.</p> Setting Type Default Description Impact <code>subtreevalidation_failfast_validation</code> bool <code>true</code> Controls whether validation stops at the first error or continues Affects validation performance and error reporting behavior <code>subtreevalidation_validation_max_retries</code> int <code>30</code> Maximum number of retry attempts for validation operations Controls resilience and error handling during validation <code>subtreevalidation_validation_retry_sleep</code> string <code>\"5s\"</code> Time to wait between validation retry attempts Controls backoff behavior during validation errors <code>subtreevalidation_batch_missing_transactions</code> bool <code>true</code> Controls whether missing transaction retrieval is batched Affects network efficiency and transaction retrieval performance <code>subtreevalidation_missingTransactionsBatchSize</code> int <code>16384</code> Maximum number of transactions to retrieve in a single batch Controls network efficiency and memory usage during retrieval <code>subtreevalidation_percentageMissingGetFullData</code> float64 <code>20.0</code> Percentage threshold for switching to full data retrieval Controls when the service retrieves full transaction data based on missing rate <code>subtreevalidation_blacklisted_baseurls</code> map[string]struct{} <code>{}</code> (empty) Set of blacklisted base URLs for subtree validation Prevents validation attempts from known problematic sources"},{"location":"references/settings/services/subtreevalidation_settings/#transaction-processing-interactions-and-dependencies","title":"Transaction Processing Interactions and Dependencies","text":"<p>The transaction processing settings work together to balance performance, reliability, and resource usage:</p> <ul> <li>When a subtree contains transactions not available locally, the service retrieves them from remote sources</li> <li>Batch processing (<code>batch_missing_transactions</code>) improves network efficiency by grouping requests</li> <li>The retry mechanism (<code>validation_max_retries</code> and <code>validation_retry_sleep</code>) handles transient failures</li> <li>The percentage threshold (<code>percentageMissingGetFullData</code>) optimizes retrieval strategies based on the proportion of missing transactions</li> </ul>"},{"location":"references/settings/services/subtreevalidation_settings/#performance-scaling-settings","title":"Performance &amp; Scaling Settings","text":"<p>These settings control concurrency, parallelism, and batch sizes to optimize performance and resource usage.</p> Setting Type Default Description Impact <code>subtreevalidation_getMissingTransactions</code> int max(4, numCPU/2) Number of concurrent workers for fetching missing transactions Controls parallelism for transaction retrieval operations <code>subtreevalidation_subtreeDAHConcurrency</code> int <code>8</code> Number of concurrent workers for processing Direct Acyclic Hash operations Controls parallelism for DAH computations during validation <code>subtreevalidation_processTxMetaUsingStoreConcurrency</code> int <code>32</code> Number of concurrent workers for store-based metadata processing Controls parallelism and I/O load during validation <code>subtreevalidation_processTxMetaUsingCacheConcurrency</code> int <code>32</code> Number of concurrent workers for cached metadata processing Controls parallelism and CPU utilization during validation <code>subtreevalidation_spendBatcherSize</code> int <code>1024</code> Batch size for processing spend operations Controls I/O patterns and throughput for UTXO spend operations <code>subtreevalidation_processTxMetaUsingStoreBatchSize</code> int <code>1024</code> Batch size for processing transaction metadata from store Affects I/O patterns and throughput for store-based metadata processing <code>subtreevalidation_processTxMetaUsingCacheBatchSize</code> int <code>1024</code> Batch size for processing transaction metadata using cache Affects memory usage and throughput for cached metadata processing <code>subtreevalidation_check_block_subtrees_concurrency</code> int <code>32</code> Number of concurrent workers for processing block subtrees Controls parallelism for block subtree validation operations"},{"location":"references/settings/services/subtreevalidation_settings/#performance-scaling-interactions-and-dependencies","title":"Performance &amp; Scaling Interactions and Dependencies","text":"<p>These settings work together to optimize resource usage and throughput:</p> <ul> <li>Concurrency settings control how many operations can run in parallel, balancing CPU utilization against context switching overhead</li> <li>Batch size settings control memory usage and I/O efficiency by grouping related operations</li> <li>Different operations (retrieval, processing, validation) have separate concurrency controls to optimize each phase</li> <li>Default values are selected to balance performance against resource consumption, but may need tuning based on specific hardware and workloads</li> </ul>"},{"location":"references/settings/services/subtreevalidation_settings/#caching-memory-settings","title":"Caching &amp; Memory Settings","text":"<p>These settings control the transaction metadata cache, which significantly impacts performance and memory usage.</p> Setting Type Default Description Impact <code>subtreevalidation_txMetaCacheEnabled</code> bool <code>true</code> Controls whether transaction metadata caching is enabled Significantly affects performance and memory usage patterns <code>txMetaCacheMaxMB</code> int <code>256</code> Maximum memory (in MB) to use for transaction metadata cache Controls memory footprint and cache hit rates <code>subtreevalidation_processTxMetaUsingCacheMissingTxThreshold</code> int <code>1</code> Threshold for switching to full transaction data retrieval when using cache Controls when the service fetches full transaction data vs. metadata <code>subtreevalidation_processTxMetaUsingStoreMissingTxThreshold</code> int <code>1</code> Threshold for switching to full transaction data retrieval when using store Controls when the service fetches full transaction data vs. metadata <code>subtreevalidation_txChanBufferSize</code> int <code>0</code> Buffer size for transaction processing channels Controls buffering behavior for transaction processing queues"},{"location":"references/settings/services/subtreevalidation_settings/#caching-memory-interactions-and-dependencies","title":"Caching &amp; Memory Interactions and Dependencies","text":"<p>The caching system is critical for high-performance validation:</p> <ul> <li>When caching is enabled (<code>txMetaCacheEnabled=true</code>), a memory-based cache layer is created over the UTXO store</li> <li>The cache size (<code>txMetaCacheMaxMB</code>) directly impacts memory usage and performance - larger caches improve hit rates but consume more memory</li> <li>The missing transaction thresholds control when the service switches from metadata validation to full transaction validation, which is more resource-intensive but necessary in some cases</li> <li>Separate thresholds for cache and store operations allow optimizing each path independently</li> </ul>"},{"location":"references/settings/services/subtreevalidation_settings/#storage-data-settings","title":"Storage &amp; Data Settings","text":"<p>These settings control data storage and retention for subtree validation.</p> Setting Type Default Description Impact <code>subtreestore</code> string <code>\"\"</code> (empty) REQUIRED - URL for subtree blob storage backend Determines where subtrees are stored and retrieved <code>utxostore</code> string <code>\"\"</code> (empty) REQUIRED - UTXO store URL for validation operations Critical for transaction validation and UTXO state access"},{"location":"references/settings/services/subtreevalidation_settings/#kafka-integration-settings","title":"Kafka Integration Settings","text":"<p>These settings control Kafka message queue integration for system-wide communication.</p> Setting Type Default Description Impact <code>kafka_invalidSubtrees</code> string <code>\"invalid-subtrees\"</code> Kafka topic name for publishing invalid subtree notifications Enables system-wide error notification and monitoring <code>kafka_invalidSubtreesConfig</code> string <code>\"\"</code> (empty) Kafka URL configuration for invalid subtrees topic Overrides default Kafka configuration for invalid subtrees <code>kafka_invalidBlocksConfig</code> string <code>\"\"</code> (empty) Kafka URL configuration for invalid blocks notifications Enables integration with block validation error handling"},{"location":"references/settings/services/subtreevalidation_settings/#network-communication-settings","title":"Network &amp; Communication Settings","text":"<p>These settings control how the service communicates with other components in the system.</p> Setting Type Default Description Impact <code>subtreevalidation_grpcAddress</code> string <code>\"localhost:8089\"</code> REQUIRED - Address for connecting to the subtree validation service Affects how other services connect to this service <code>subtreevalidation_grpcListenAddress</code> string <code>:8089</code> Address where the service listens for gRPC connections Controls network binding for service communication <code>subtreevalidation_subtreeValidationTimeout</code> int <code>1000</code> Timeout (ms) for subtree validation operations Controls error recovery and prevents hanging validation processes <code>validator.useLocalValidator</code> bool <code>false</code> Controls whether to use a local or remote validator Affects system architecture and validation performance"},{"location":"references/settings/services/utxopersister_settings/","title":"UTXO Persister Settings","text":"<p>Related Topic: UTXO Persister Service</p> <p>The UTXO Persister service relies on a set of configuration settings that control its behavior, performance, and resource usage. This section provides a comprehensive overview of these settings, organized by functional category, along with their impacts, dependencies, and recommended configurations for different deployment scenarios.</p>"},{"location":"references/settings/services/utxopersister_settings/#configuration-categories","title":"Configuration Categories","text":"<p>UTXO Persister service settings can be organized into the following functional categories:</p> <ol> <li>Performance Tuning: Settings that control I/O performance and memory usage</li> <li>Storage Management: Settings that manage file storage and retention policies</li> <li>Deployment Architecture: Settings that determine how the service interacts with other components</li> <li>Operational Controls: Settings that affect general service behavior</li> </ol>"},{"location":"references/settings/services/utxopersister_settings/#performance-tuning-settings","title":"Performance Tuning Settings","text":"<p>These settings control the I/O performance and memory usage patterns of the UTXO Persister service.</p> Setting Type Default Description Impact <code>utxoPersister_buffer_size</code> string <code>\"4KB\"</code> Controls the buffer size for reading from and writing to UTXO files Affects I/O performance and memory usage when processing UTXO data"},{"location":"references/settings/services/utxopersister_settings/#performance-tuning-interactions-and-dependencies","title":"Performance Tuning Interactions and Dependencies","text":"<p>The buffer size setting directly affects how efficiently the service reads and writes UTXO data:</p> <ul> <li>Larger buffer sizes (e.g., 64KB to 1MB) can significantly improve I/O throughput by reducing the number of system calls needed for file operations</li> <li>Smaller buffer sizes reduce memory usage but may increase CPU overhead due to more frequent I/O operations</li> <li>The optimal buffer size depends on the hardware characteristics, particularly disk I/O capabilities, available memory, and the size of typical UTXO files</li> </ul> <p>For high-throughput environments with fast storage systems (like SSDs), larger buffer sizes provide better performance. For memory-constrained environments, smaller buffers may be necessary despite the performance impact.</p>"},{"location":"references/settings/services/utxopersister_settings/#storage-management-settings","title":"Storage Management Settings","text":"<p>These settings control how UTXO data is stored and retained.</p> Setting Type Default Description Impact <code>blockpersister_skipUTXODelete</code> bool <code>false</code> When true, previous block's UTXO sets aren't deleted after processing Controls storage usage and retention policy for historical UTXO sets <code>blockstore</code> *url.URL <code>\"file://./data/blockstore\"</code> Specifies the URL for the block storage backend Determines where block data, including UTXO sets, are stored <code>txstore</code> *url.URL <code>\"\"</code> Specifies the URL for the transaction storage backend Determines where transaction data is stored for block processing <code>txmeta_store</code> *url.URL <code>\"\"</code> Specifies the URL for the UTXO metadata storage backend Determines where UTXO metadata is stored for efficient lookups"},{"location":"references/settings/services/utxopersister_settings/#storage-management-interactions-and-dependencies","title":"Storage Management Interactions and Dependencies","text":"<p>These settings determine the storage footprint and data persistence behavior of the service:</p> <ul> <li>When <code>blockpersister_skipUTXODelete</code> is <code>false</code> (default), the service maintains minimal storage by only keeping the UTXO set for the most recent processed block</li> <li>When set to <code>true</code>, the service preserves all historical UTXO sets, which increases storage requirements but enables historical analysis and validation</li> <li>The <code>blockstore</code> setting defines where all block-related data is stored, affecting both read and write performance based on the underlying storage system</li> <li>The <code>txstore</code> and <code>txmeta_store</code> settings determine where transaction data and UTXO metadata are stored, respectively, which impacts performance and data availability</li> </ul> <p>Storage requirements grow significantly when keeping historical UTXO sets, as each set contains the complete state of all unspent outputs at a given block height.</p>"},{"location":"references/settings/services/utxopersister_settings/#deployment-architecture-settings","title":"Deployment Architecture Settings","text":"<p>These settings control how the UTXO Persister interacts with other components in the system.</p> Setting Type Default Description Impact <code>direct</code> bool <code>true</code> Controls whether the service connects directly to the blockchain store or uses the client interface Affects performance and deployment architecture"},{"location":"references/settings/services/utxopersister_settings/#deployment-architecture-interactions-and-dependencies","title":"Deployment Architecture Interactions and Dependencies","text":"<p>The deployment architecture settings determine how the service integrates with the broader system:</p> <ul> <li>When <code>direct</code> is <code>true</code>, the service bypasses the blockchain client interface and connects directly to the blockchain store, which improves performance but requires the service to be deployed in the same process</li> <li>When <code>direct</code> is <code>false</code>, the service uses the blockchain client interface, allowing for distributed deployment at the cost of additional network overhead</li> </ul> <p>This setting has significant implications for system design and deployment flexibility. Direct access provides better performance but limits deployment options, while client-based access enables more flexible deployment topologies but may impact performance.</p>"},{"location":"references/settings/services/utxopersister_settings/#operational-controls-settings","title":"Operational Controls Settings","text":"<p>These settings control general operational aspects of the service.</p> Setting Type Default Description Impact <code>network</code> string <code>\"mainnet\"</code> Specifies the blockchain network (mainnet, testnet, regtest) Determines genesis hash and chain parameters used for validation"},{"location":"references/settings/services/utxopersister_settings/#operational-controls-interactions-and-dependencies","title":"Operational Controls Interactions and Dependencies","text":"<ul> <li>The <code>network</code> setting determines the genesis hash and chain parameters via chaincfg.GetChainParams(), which is critical for genesis block detection and UTXO set validation</li> <li>Genesis hash validation is used throughout the service to determine when to skip certain operations like UTXO deletion</li> </ul> <p>This setting is fundamental to the service's operation and must match the target blockchain network.</p>"},{"location":"references/settings/services/utxopersister_settings/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li> <p>Performance Monitoring: Regularly monitor I/O performance metrics when adjusting buffer sizes. Balance memory usage against throughput based on your specific hardware capabilities.</p> </li> <li> <p>Storage Planning: When using <code>blockpersister_skipUTXODelete=true</code>, implement a storage monitoring and management strategy. UTXO sets grow significantly over time and may require substantial storage capacity.</p> </li> <li> <p>Deployment Architecture: Choose direct access (<code>direct=true</code>) whenever possible for best performance, unless your system architecture specifically requires distributed deployment.</p> </li> <li> <p>Network Configuration: Ensure the <code>network</code> setting matches your target blockchain environment. Incorrect network configuration can lead to validation failures and data corruption.</p> </li> <li> <p>Storage Location: Use persistent, reliable storage locations for the <code>blockstore</code> setting in production environments, ideally on dedicated, high-performance storage systems.</p> </li> <li> <p>Backup Strategy: Implement regular backups of your UTXO data, especially the most recent UTXO set, to enable rapid recovery in case of data corruption or storage failures.</p> </li> <li> <p>Service Coordination: Ensure that blockchain services and UTXO Persister services are properly coordinated in terms of startup sequence and operational dependencies, particularly when using direct access mode.</p> </li> </ol>"},{"location":"references/settings/services/validator_settings/","title":"Validator Settings","text":"<p>Related Topic: Validator Service</p> <p>The TX Validator service relies on a set of configuration settings that control its behavior, performance, and integration with other services. This section provides a comprehensive overview of these settings, organized by functional category, along with their impacts, dependencies, and recommended configurations for different deployment scenarios.</p>"},{"location":"references/settings/services/validator_settings/#configuration-categories","title":"Configuration Categories","text":"<p>TX Validator service settings can be organized into the following functional categories:</p> <ol> <li>Deployment Architecture: Settings that determine how the validator is deployed and integrated</li> <li>Network &amp; Communication: Settings that control network binding and API endpoints</li> <li>Performance &amp; Throughput: Settings that affect transaction processing performance</li> <li>Kafka Integration: Settings for message-based communication</li> <li>Validation Rules: Settings that control transaction validation policies</li> <li>Monitoring &amp; Debugging: Settings for observability and troubleshooting</li> </ol>"},{"location":"references/settings/services/validator_settings/#deployment-architecture-settings","title":"Deployment Architecture Settings","text":"<p>These settings fundamentally change how the validator operates within the system architecture.</p> Setting Type Default Description Impact <code>useLocalValidator</code> bool <code>false</code> Controls whether validation is performed locally or via remote service Significantly affects system architecture, latency, and deployment model <code>blockassembly_disabled</code> bool <code>false</code> Controls whether validator integrates with block assembly service Critical for mining operations - when disabled, validated transactions are not sent to block assembly"},{"location":"references/settings/services/validator_settings/#network-communication-settings","title":"Network &amp; Communication Settings","text":"<p>These settings control how the validator communicates over the network.</p> Setting Type Default Description Impact <code>validator_grpcAddress</code> string <code>\"localhost:8081\"</code> Address for connecting to the validator gRPC service Determines how other services connect to the validator <code>validator_grpcListenAddress</code> string <code>\":8081\"</code> Address on which the validator gRPC server listens Controls network binding for incoming validation requests <code>validator_httpListenAddress</code> string <code>\"\"</code> Address on which the HTTP API server listens Enables HTTP-based validation requests when set <code>validator_httpAddress</code> *url.URL <code>nil</code> URL for connecting to the validator HTTP API Determines how other services connect to the HTTP interface <code>validator_httpRateLimit</code> int <code>1024</code> Maximum number of HTTP requests per second Prevents resource exhaustion from excessive requests"},{"location":"references/settings/services/validator_settings/#performance-throughput-settings","title":"Performance &amp; Throughput Settings","text":"<p>These settings control how efficiently transactions are processed.</p> Setting Type Default Description Impact <code>validator_sendBatchSize</code> int <code>100</code> Number of transactions to accumulate before batch processing Balances throughput against latency for transaction processing <code>validator_sendBatchTimeout</code> int <code>2</code> Maximum time (milliseconds) to wait before processing a partial batch Ensures timely processing of transactions even when volume is low <code>validator_sendBatchWorkers</code> int <code>10</code> Number of worker goroutines for batch send operations Controls parallelism for outbound transaction processing <code>validator_blockvalidation_delay</code> int <code>0</code> Artificial delay (milliseconds) introduced before block validation Can be used for testing or throttling validation operations <code>validator_blockvalidation_maxRetries</code> int <code>5</code> Maximum number of retries for failed block validations Affects resilience against transient failures <code>validator_blockvalidation_retrySleep</code> string <code>\"2s\"</code> Time to wait between block validation retry attempts Controls backoff strategy for failed validations"},{"location":"references/settings/services/validator_settings/#kafka-integration-settings","title":"Kafka Integration Settings","text":"<p>These settings control message-based communication via Kafka.</p> Setting Type Default Description Impact <code>validator_kafkaWorkers</code> int <code>0</code> Number of worker goroutines for Kafka message processing Affects concurrency and throughput for Kafka-based transaction handling <code>kafka_txmetaConfig</code> URL <code>\"\"</code> URL for the Kafka configuration for transaction metadata Configures how transaction metadata is published to Kafka for subtree validation <code>kafka_rejectedTxConfig</code> URL <code>\"\"</code> URL for the Kafka configuration for rejected transactions Configures how rejected transaction information is published to P2P notification <code>validator_kafka_maxMessageBytes</code> int <code>1048576</code> Maximum size of Kafka messages in bytes Limits the size of transactions that can be processed via Kafka <p>Critical Configuration Requirements:</p> <ul> <li><code>kafka_txmetaConfig</code> must be configured for the validator service to function - without it, transaction metadata cannot be published to the Subtree Validation service</li> <li><code>validator_kafka_maxMessageBytes</code> must be coordinated with Kafka broker's <code>message.max.bytes</code> setting to prevent message rejection</li> </ul>"},{"location":"references/settings/services/validator_settings/#validation-rules-settings","title":"Validation Rules Settings","text":"<p>These settings control the rules and policies applied during transaction validation.</p> Setting Type Default Description Impact <code>maxtxsizepolicy</code> int Varies Maximum allowed transaction size in bytes Restricts oversized transactions from entering the mempool <code>minminingtxfee</code> float64 Varies Minimum fee required for transaction acceptance Sets economic barrier for transaction inclusion"},{"location":"references/settings/services/validator_settings/#monitoring-debugging-settings","title":"Monitoring &amp; Debugging Settings","text":"<p>These settings control observability and diagnostics.</p> Setting Type Default Description Impact <code>validator_verbose_debug</code> bool <code>false</code> Enables detailed debug logging for validator operations Provides additional diagnostic information at the cost of log verbosity <code>fsm_state_restore</code> bool <code>false</code> Controls whether the service restores from a previously saved state Affects recovery behavior after restarts or failures"},{"location":"references/settings/services/validator_settings/#configuration-settings-affecting-validator-behavior","title":"Configuration Settings Affecting Validator Behavior","text":"<p>The Validator service behavior is controlled by several key configuration parameters:</p> <ul> <li><code>KafkaMaxMessageBytes</code> (default: 1MB): Controls size-based routing - large transactions that exceed this threshold are routed via HTTP instead of Kafka to avoid message size limitations.</li> <li><code>UseLocalValidator</code> (default: false): Determines whether to use a local validator instance or connect to a remote validator service via gRPC.</li> <li><code>KafkaWorkers</code> (default: 0): Controls the number of concurrent Kafka message processing workers. When set to 0, Kafka consumer processing is disabled.</li> <li><code>HTTPRateLimit</code> (default: 1024): Sets the rate limit for HTTP API requests to prevent service overload.</li> <li><code>VerboseDebug</code> (default: false): Enables detailed validation logging for troubleshooting.</li> </ul>"},{"location":"references/settings/services/validator_settings/#rejected-transaction-handling","title":"Rejected Transaction Handling","text":"<p>When the Transaction Validator Service identifies an invalid transaction, it employs a Kafka-based notification system to inform other components of the system:</p> <ol> <li>Transaction Validation: The Validator receives a transaction for validation via a gRPC call to the <code>ValidateTransaction</code> method</li> <li>Identification of Invalid Transactions: If the transaction fails any of the validation checks, it is deemed invalid (rejected)</li> <li>Notification of Rejected Transactions: When a transaction is rejected, the Validator publishes information about the rejected transaction to a dedicated Kafka topic (<code>rejectedTx</code>)</li> <li> <p>Kafka Message Content: The Kafka message for a rejected transaction typically includes:</p> <ul> <li>The transaction ID (hash)</li> <li>The reason for rejection (error message)</li> <li>Consumption of Rejected Transaction Notifications: Other services in the system, such as the P2P Service, can subscribe to this Kafka topic</li> </ul> </li> </ol>"},{"location":"references/settings/services/validator_settings/#two-phase-transaction-commit-process","title":"Two-Phase Transaction Commit Process","text":"<p>The Validator implements a two-phase commit process for transaction creation and addition to block assembly:</p>"},{"location":"references/settings/services/validator_settings/#phase-1-transaction-creation-with-locked-flag","title":"Phase 1 - Transaction Creation with Locked Flag","text":"<p>When a transaction is created, it is initially stored in the UTXO store with an \"locked\" flag set to <code>true</code>. This flag prevents the transaction outputs from being spent while it's in the process of being validated and added to block assembly, protecting against potential double-spend attempts.</p>"},{"location":"references/settings/services/validator_settings/#phase-2-unsetting-the-locked-flag","title":"Phase 2 - Unsetting the Locked Flag","text":"<p>The locked flag is unset in two key scenarios:</p> <p>a. After Successful Addition to Block Assembly:</p> <ul> <li>When a transaction is successfully validated and added to the block assembly, the Validator service immediately unsets the \"locked\" flag (sets it to <code>false</code>)</li> <li>This makes the transaction outputs available for spending in subsequent transactions, even before the transaction is mined in a block</li> </ul> <p>b. When Mined in a Block (Fallback Mechanism):</p> <ul> <li>As a fallback mechanism, if the flag hasn't been unset already, it will be unset when the transaction is mined in a block</li> <li>When the transaction is mined in a block and that block is processed by the Block Validation service, the \"locked\" flag is unset (set to <code>false</code>) during the <code>SetMinedMulti</code> operation</li> </ul>"},{"location":"references/settings/services/validator_settings/#ignoring-locked-flag-for-block-transactions","title":"Ignoring Locked Flag for Block Transactions","text":"<p>When processing transactions that are part of a block (as opposed to new transactions to include in an upcoming block), the validator can be configured to ignore the locked flag. This is necessary because transactions in a block have already been validated by miners and must be accepted regardless of their locked status. The validator uses the <code>WithIgnoreLocked</code> option to control this behavior during transaction validation.</p> <p>This two-phase commit approach ensures that transactions are only made spendable after they've been successfully added to block assembly, reducing the risk of race conditions and double-spend attempts during the transaction processing lifecycle.</p> <p>For a comprehensive explanation of the two-phase commit process across the entire system, see the Two-Phase Transaction Commit Process documentation.</p>"},{"location":"references/settings/stores/blob_settings/","title":"Blob Store Settings","text":"<p>Related Topic: Blob Store</p> <p>The Blob Server can be configured through URL-based settings and option parameters. This section provides a comprehensive reference for all configuration options.</p>"},{"location":"references/settings/stores/blob_settings/#url-format","title":"URL Format","text":"<p>Blob store configurations use URLs that follow this general structure:</p> <pre><code>&lt;scheme&gt;://&lt;path&gt;[?&lt;parameter1&gt;=&lt;value1&gt;&amp;&lt;parameter2&gt;=&lt;value2&gt;...]\n</code></pre> <p>Components:</p> <ul> <li>Scheme: Determines the storage backend type (file, memory, s3, http, null)</li> <li>Path: Specifies the storage location or identifier</li> <li>Parameters: Optional query parameters that modify behavior</li> </ul>"},{"location":"references/settings/stores/blob_settings/#store-url-settings","title":"Store URL Settings","text":""},{"location":"references/settings/stores/blob_settings/#common-url-parameters","title":"Common URL Parameters","text":"Parameter Type Default Description Impact <code>hashPrefix</code> Integer 2 Number of characters from start of hash for directory organization Improves file organization and lookup performance <code>hashSuffix</code> Integer - Number of characters from end of hash for directory organization Alternative to hashPrefix; uses end of hash <code>batch</code> Boolean false Enables batch wrapper for improved performance Aggregates operations into larger batches <code>sizeInBytes</code> Integer 4194304 Maximum batch size in bytes when batching enabled Controls memory usage and batch efficiency <code>writeKeys</code> Boolean false Store key index alongside batch data Enables key-based retrieval from batches <code>localDAHStore</code> String \"\" Enable Delete-At-Height functionality Requires value like \"memory://\" to enable DAH <code>localDAHStorePath</code> String \"/tmp/localDAH\" Path for DAH metadata storage Directory for blockchain height tracking <code>logger</code> Boolean false Enable debug logging wrapper Adds detailed operation logging"},{"location":"references/settings/stores/blob_settings/#file-backend-parameters","title":"File Backend Parameters","text":"Parameter Type Default Description Path component String None (required) Base directory for file storage <code>checksum</code> Boolean false Enable SHA256 checksumming of stored blobs <code>header</code> String <code>(empty)</code> Custom header prepended to stored blobs (hex or plain text) <code>eofmarker</code> String <code>(empty)</code> Custom footer appended to stored blobs (hex or plain text)"},{"location":"references/settings/stores/blob_settings/#s3-backend-parameters","title":"S3 Backend Parameters","text":"Parameter Type Default Description Path component String None (required) S3 bucket name <code>region</code> String None (required) AWS region for S3 bucket <code>endpoint</code> String AWS S3 endpoint Custom endpoint for S3-compatible storage <code>forcePathStyle</code> Boolean false Force path-style addressing <code>subDirectory</code> String <code>(none)</code> S3 object key prefix for organization <code>MaxIdleConns</code> Integer 100 Maximum number of idle HTTP connections <code>MaxIdleConnsPerHost</code> Integer 100 Maximum idle connections per host <code>IdleConnTimeoutSeconds</code> Integer 100 Idle connection timeout in seconds <code>TimeoutSeconds</code> Integer 30 Request timeout for S3 operations <code>KeepAliveSeconds</code> Integer 300 Connection keep-alive duration"},{"location":"references/settings/stores/blob_settings/#http-backend-parameters","title":"HTTP Backend Parameters","text":"Parameter Type Default Description Host+Path String None (required) Remote HTTP blob server endpoint Timeout Duration 30s HTTP client timeout (hardcoded, not configurable via URL)"},{"location":"references/settings/stores/blob_settings/#store-options-optionsoptions","title":"Store Options (options.Options)","text":"<p>Additional configuration is provided through Store Options when creating a store:</p> Option Type Default Description Impact <code>BlockHeightRetention</code> uint32 0 Default block height retention for all files Controls automatic deletion after specified blockchain height <code>DAH</code> uint32 0 Default Delete-At-Height value for individual files Sets blockchain height for automatic file deletion <code>Filename</code> string <code>(hash-based)</code> Custom filename override Overrides default hash-based file naming <code>SubDirectory</code> string <code>(none)</code> Subdirectory within main path Organizes data in storage hierarchy <code>HashPrefix</code> int 2 Number of hash characters for directory organization Improves file organization and lookup performance <code>AllowOverwrite</code> bool false Allow overwriting existing files Controls file replacement behavior <code>SkipHeader</code> bool false Skip file headers for CLI readability Enables CLI-friendly file formats <code>PersistSubDir</code> string <code>(none)</code> Subdirectory for persistent storage Directory for persistent data organization <code>LongtermStoreURL</code> *url.URL nil URL for longterm storage backend Enables three-tier storage (memory, local, longterm) <code>BlockHeightCh</code> chan uint32 nil Block height tracking channel for DAH functionality Required for DAH-enabled stores"},{"location":"references/settings/stores/blob_settings/#file-operation-options-functional-options","title":"File Operation Options (Functional Options)","text":"<p>These options can be specified per operation using functional option pattern:</p> Option Type Description Impact <code>WithBlockHeightRetention(uint32)</code> uint32 Sets block height retention for this file Controls per-file automatic deletion <code>WithDAH(uint32)</code> uint32 Sets Delete-At-Height value for this file Controls per-file data retention based on blockchain height <code>WithFilename(string)</code> string Sets specific filename Overrides default hash-based naming <code>WithSubDirectory(string)</code> string Sets subdirectory for this file Organizes individual files in storage hierarchy <code>WithHashPrefix(int)</code> int Sets hash prefix for this file Controls directory organization for this file <code>WithAllowOverwrite(bool)</code> bool Controls overwriting of existing files Prevents accidental data loss when false <code>WithSkipHeader(bool)</code> bool Skip file headers for this operation Enables CLI-friendly file formats <code>WithPersistSubDir(string)</code> string Sets persistent subdirectory Directory for persistent storage of this file <code>WithLongtermStoreURL(*url.URL)</code> *url.URL Sets longterm storage URL for this file Enables per-file longterm storage configuration <code>WithBlockHeightCh(chan uint32)</code> chan uint32 Sets block height channel for DAH Required for DAH functionality"},{"location":"references/settings/stores/utxo_settings/","title":"UTXO Store Settings","text":"<p>Related Topic: UTXO Store</p> <p>The UTXO Store can be configured using various connection URLs and configuration parameters. This section provides a comprehensive reference of all available configuration options.</p>"},{"location":"references/settings/stores/utxo_settings/#connection-url-format","title":"Connection URL Format","text":"<p>The <code>utxostore</code> setting determines which datastore implementation to use. The connection URL format varies depending on the selected backend:</p>"},{"location":"references/settings/stores/utxo_settings/#aerospike","title":"Aerospike","text":"<pre><code>aerospike://host:port/namespace?param1=value1&amp;param2=value2\n</code></pre> <p>Example:</p> <pre><code>utxostore=aerospike://aerospikeserver.teranode.dev:3000/teranode-store?set=txmeta&amp;externalStore=blob://blobserver:8080/utxo\n</code></pre> <p>URL Parameters:</p> <ul> <li><code>set</code>: Aerospike set name (default: \"txmeta\")</li> <li><code>externalStore</code>: URL for storing large transactions (required)</li> <li><code>ConnectionQueueSize</code>: Connection queue size for Aerospike client</li> <li><code>LimitConnectionsToQueueSize</code>: Whether to limit connections to queue size</li> </ul>"},{"location":"references/settings/stores/utxo_settings/#sql-postgresqlsqlite","title":"SQL (PostgreSQL/SQLite)","text":"<p>PostgreSQL:</p> <pre><code>postgres://username:password@host:port/dbname?param1=value1&amp;param2=value2\n</code></pre> <p>Example:</p> <pre><code>utxostore=postgres://miner1:miner1@postgresserver.teranode.dev:5432/teranode-store?expiration=24h\n</code></pre> <p>SQLite:</p> <pre><code>sqlite:///path/to/file.sqlite?param1=value1&amp;param2=value2\n</code></pre> <p>Example:</p> <pre><code>utxostore=sqlite:///data/utxo.sqlite?expiration=24h\n</code></pre> <p>In-memory SQLite:</p> <pre><code>sqlitememory:///name?param1=value1&amp;param2=value2\n</code></pre> <p>Example:</p> <pre><code>utxostore=sqlitememory:///utxo?expiration=24h\n</code></pre> <p>URL Parameters:</p> <ul> <li><code>expiration</code>: Duration after which spent UTXOs are cleaned up (e.g., \"24h\", \"7d\")</li> <li><code>logging</code>: Enable SQL query logging (true/false)</li> </ul>"},{"location":"references/settings/stores/utxo_settings/#memory","title":"Memory","text":"<pre><code>memory://host:port/mode\n</code></pre> <p>Example:</p> <pre><code>utxostore=memory://localhost:${UTXO_STORE_GRPC_PORT}/splitbyhash\n</code></pre> <p>Modes:</p> <ul> <li><code>splitbyhash</code>: Distributes UTXOs based on hash</li> <li><code>all</code>: Stores all UTXOs in memory</li> </ul>"},{"location":"references/settings/stores/utxo_settings/#nullstore","title":"Nullstore","text":"<pre><code>null:///\n</code></pre> <p>Example:</p> <pre><code>utxostore=null:///\n</code></pre>"},{"location":"references/settings/stores/utxo_settings/#configuration-parameters","title":"Configuration Parameters","text":"<p>The UTXO Store can be configured through the <code>UtxoStoreSettings</code> struct which contains various parameters to control the behavior of the store.</p>"},{"location":"references/settings/stores/utxo_settings/#general-settings","title":"General Settings","text":"Parameter Type Description Default <code>UtxoStore</code> *url.URL Connection URL for the UTXO store \"\" (must be configured) <code>BlockHeightRetention</code> uint32 Number of blocks to retain data for globalBlockHeightRetention <code>BlockHeightRetentionAdjustment</code> int32 Adjustment to global block height retention (can be positive or negative) 0 <code>UnminedTxRetention</code> uint32 Retention period for unmined transactions in blocks 1008 blocks (~7 days) <code>ParentPreservationBlocks</code> uint32 Parent transaction preservation period in blocks 1440 blocks (~10 days) <code>UtxoBatchSize</code> int Batch size for UTXOs (critical - do not change after initial setup) 128 <code>DBTimeout</code> time.Duration Timeout for database operations 5s <code>UseExternalTxCache</code> bool Whether to use external transaction cache true <code>ExternalizeAllTransactions</code> bool Whether to externalize all transactions false <code>VerboseDebug</code> bool Enable verbose debugging false <code>UpdateTxMinedStatus</code> bool Whether to update transaction mined status true <code>DisableDAHCleaner</code> bool Disable Delete-At-Height cleaner process false"},{"location":"references/settings/stores/utxo_settings/#sql-specific-settings","title":"SQL-specific Settings","text":"Parameter Type Description Default <code>PostgresMaxIdleConns</code> int Maximum number of idle connections to the PostgreSQL database 10 <code>PostgresMaxOpenConns</code> int Maximum number of open connections to the PostgreSQL database 80"},{"location":"references/settings/stores/utxo_settings/#batch-processing-settings","title":"Batch Processing Settings","text":"<p>The UTXO Store uses batch processing to improve performance. The following settings control the behavior of various batchers:</p> Parameter Type Description Default <code>StoreBatcherSize</code> int Batch size for store operations 100 <code>StoreBatcherDurationMillis</code> int Maximum duration in milliseconds for store batching 100 <code>SpendBatcherSize</code> int Batch size for spend operations 100 <code>SpendBatcherDurationMillis</code> int Maximum duration in milliseconds for spend batching 100 <code>OutpointBatcherSize</code> int Batch size for outpoint operations 100 <code>OutpointBatcherDurationMillis</code> int Maximum duration in milliseconds for outpoint batching 10 <code>IncrementBatcherSize</code> int Batch size for increment operations 256 <code>IncrementBatcherDurationMillis</code> int Maximum duration in milliseconds for increment batching 10 <code>SetDAHBatcherSize</code> int Batch size for Delete-At-Height operations 256 <code>SetDAHBatcherDurationMillis</code> int Maximum duration in milliseconds for DAH batching 10 <code>LockedBatcherSize</code> int Batch size for locked operations 256 <code>LockedBatcherDurationMillis</code> int Maximum duration in milliseconds for locked batching 10 <code>GetBatcherSize</code> int Batch size for get operations 1 <code>GetBatcherDurationMillis</code> int Maximum duration in milliseconds for get batching 10 <code>MaxMinedRoutines</code> int Maximum number of concurrent goroutines for processing mined transactions 128 <code>MaxMinedBatchSize</code> int Maximum number of mined transactions processed in a batch 1024"},{"location":"references/settings/stores/utxo_settings/#important-notes","title":"Important Notes","text":"<p>Critical Setting Warning: The <code>UtxoBatchSize</code> setting must not be changed after the UTXO store has been running with Aerospike backend. It determines record organization and changing it would break store integrity.</p> <p>Block Height Retention: The effective retention is calculated as <code>GlobalBlockHeightRetention + BlockHeightRetentionAdjustment</code>. The global value varies by deployment, and the adjustment can be positive or negative to fine-tune retention per store.</p> <p>URL Query Parameters: The UTXO store URL supports a <code>logging=true</code> query parameter to enable detailed operation logging for debugging purposes.</p>"},{"location":"references/stores/blob_reference/","title":"Blob Store and Service Reference Documentation","text":""},{"location":"references/stores/blob_reference/#overview","title":"Overview","text":"<p>The Blob Store provides an interface for storing and retrieving binary large objects (blobs). It implements a key-value store with additional features like TTL management and range requests.</p> <p>The Blob Store Service provides a HTTP interface for the Blob Store.</p>"},{"location":"references/stores/blob_reference/#core-components","title":"Core Components","text":""},{"location":"references/stores/blob_reference/#httpblobserver","title":"HTTPBlobServer","text":"<p>The <code>HTTPBlobServer</code> struct is the main component of the Blob Store Service.</p> <pre><code>type HTTPBlobServer struct {\n    // store is the underlying blob storage implementation\n    store Store\n    // logger provides structured logging for server operations\n    logger ulogger.Logger\n}\n</code></pre>"},{"location":"references/stores/blob_reference/#constructor","title":"Constructor","text":"<pre><code>func NewHTTPBlobServer(logger ulogger.Logger, storeURL *url.URL, opts ...options.StoreOption) (*HTTPBlobServer, error)\n</code></pre> <p>Creates a new <code>HTTPBlobServer</code> instance with the provided logger and store URL.</p>"},{"location":"references/stores/blob_reference/#methods","title":"Methods","text":"<ul> <li><code>Start(ctx context.Context, addr string) error</code>: Starts the HTTP server on the specified address.</li> <li><code>ServeHTTP(w http.ResponseWriter, r *http.Request)</code>: Handles incoming HTTP requests.</li> <li><code>setCurrentBlockHeight(height uint32) error</code>: Updates the current block height in the underlying store if it supports this operation. Used for DAH (Delete-At-Height) functionality.</li> </ul>"},{"location":"references/stores/blob_reference/#store-interface","title":"Store Interface","text":"<p>The <code>Store</code> interface defines the contract for blob storage operations.</p> <pre><code>type Store interface {\n    // Health checks the health status of the blob store.\n    // Parameters:\n    //   - ctx: The context for the operation\n    //   - checkLiveness: Whether to perform a liveness check\n    // Returns:\n    //   - int: HTTP status code indicating health status\n    //   - string: Description of the health status\n    //   - error: Any error that occurred during the health check\n    Health(ctx context.Context, checkLiveness bool) (int, string, error)\n\n    // Exists checks if a blob exists in the store.\n    // Parameters:\n    //   - ctx: The context for the operation\n    //   - key: The key identifying the blob\n    //   - fileType: The type of the file\n    //   - opts: Optional file options\n    // Returns:\n    //   - bool: True if the blob exists, false otherwise\n    //   - error: Any error that occurred during the check\n    Exists(ctx context.Context, key []byte, fileType fileformat.FileType, opts ...options.FileOption) (bool, error)\n\n    // Get retrieves a blob from the store.\n    // Parameters:\n    //   - ctx: The context for the operation\n    //   - key: The key identifying the blob\n    //   - fileType: The type of the file\n    //   - opts: Optional file options\n    // Returns:\n    //   - []byte: The blob data\n    //   - error: Any error that occurred during retrieval\n    Get(ctx context.Context, key []byte, fileType fileformat.FileType, opts ...options.FileOption) ([]byte, error)\n\n    // GetIoReader returns an io.ReadCloser for streaming blob data.\n    // Parameters:\n    //   - ctx: The context for the operation\n    //   - key: The key identifying the blob\n    //   - fileType: The type of the file\n    //   - opts: Optional file options\n    // Returns:\n    //   - io.ReadCloser: Reader for streaming the blob data\n    //   - error: Any error that occurred during setup\n    GetIoReader(ctx context.Context, key []byte, fileType fileformat.FileType, opts ...options.FileOption) (io.ReadCloser, error)\n\n    // Set stores a blob in the store.\n    // Parameters:\n    //   - ctx: The context for the operation\n    //   - key: The key identifying the blob\n    //   - fileType: The type of the file\n    //   - value: The blob data to store\n    //   - opts: Optional file options\n    // Returns:\n    //   - error: Any error that occurred during storage\n    Set(ctx context.Context, key []byte, fileType fileformat.FileType, value []byte, opts ...options.FileOption) error\n\n    // SetFromReader stores a blob from an io.ReadCloser.\n    // Parameters:\n    //   - ctx: The context for the operation\n    //   - key: The key identifying the blob\n    //   - fileType: The type of the file\n    //   - reader: Reader providing the blob data\n    //   - opts: Optional file options\n    // Returns:\n    //   - error: Any error that occurred during storage\n    SetFromReader(ctx context.Context, key []byte, fileType fileformat.FileType, reader io.ReadCloser, opts ...options.FileOption) error\n\n    // SetDAH sets the delete at height for a blob.\n    // Parameters:\n    //   - ctx: The context for the operation\n    //   - key: The key identifying the blob\n    //   - fileType: The type of the file\n    //   - dah: The delete at height\n    //   - opts: Optional file options\n    // Returns:\n    //   - error: Any error that occurred during DAH setting\n    SetDAH(ctx context.Context, key []byte, fileType fileformat.FileType, dah uint32, opts ...options.FileOption) error\n\n    // GetDAH retrieves the remaining time-to-live for a blob.\n    // Parameters:\n    //   - ctx: The context for the operation\n    //   - key: The key identifying the blob\n    //   - fileType: The type of the file\n    //   - opts: Optional file options\n    // Returns:\n    //   - uint32: The delete at height value\n    //   - error: Any error that occurred during retrieval\n    GetDAH(ctx context.Context, key []byte, fileType fileformat.FileType, opts ...options.FileOption) (uint32, error)\n\n    // Del deletes a blob from the store.\n    // Parameters:\n    //   - ctx: The context for the operation\n    //   - key: The key identifying the blob to delete\n    //   - fileType: The type of the file\n    //   - opts: Optional file options\n    // Returns:\n    //   - error: Any error that occurred during deletion\n    Del(ctx context.Context, key []byte, fileType fileformat.FileType, opts ...options.FileOption) error\n\n    // Close closes the blob store and releases any resources.\n    // Parameters:\n    //   - ctx: The context for the operation\n    // Returns:\n    //   - error: Any error that occurred during closure\n    Close(ctx context.Context) error\n\n    // SetCurrentBlockHeight sets the current block height for the store.\n    // Parameters:\n    //   - height: The current block height\n    SetCurrentBlockHeight(height uint32)\n}\n</code></pre>"},{"location":"references/stores/blob_reference/#http-endpoints","title":"HTTP Endpoints","text":"<p>The service exposes the following HTTP endpoints:</p> <ul> <li><code>GET /health</code>: Check the health status of the service.</li> <li><code>HEAD /blob/{key}.{fileType}</code>: Check if a blob exists.</li> <li><code>GET /blob/{key}.{fileType}</code>: Retrieve a blob (supports Range headers for partial content).</li> <li><code>POST /blob/{key}.{fileType}</code>: Store a new blob.</li> <li><code>PATCH /blob/{key}.{fileType}</code>: Set the delete-at-height (DAH) value for a blob via <code>dah</code> query parameter.</li> <li><code>DELETE /blob/{key}.{fileType}</code>: Delete a blob.</li> </ul> <p>Note: <code>{key}</code> is a base64-encoded blob identifier and <code>{fileType}</code> is the file extension corresponding to the blob type.</p>"},{"location":"references/stores/blob_reference/#key-features","title":"Key Features","text":"<ol> <li>Health Checks: The service provides a health check endpoint.</li> <li>Range Requests: Supports partial content requests using the <code>Range</code> header.</li> <li>DAH Management: Allows setting and retrieving Delete-At-Height values for blob lifecycle management.</li> <li>Streaming: Supports streaming for both storing and retrieving blobs.</li> <li>Metadata Support: Allows retrieving header and footer metadata from blobs.</li> </ol>"},{"location":"references/stores/blob_reference/#error-handling","title":"Error Handling","text":"<p>The service uses HTTP status codes to indicate the result of operations:</p> <ul> <li>200 OK: Successful operation</li> <li>201 Created: Blob successfully stored</li> <li>204 No Content: Blob successfully deleted</li> <li>206 Partial Content: Range request successfully processed</li> <li>400 Bad Request: Invalid input</li> <li>404 Not Found: Blob not found</li> <li>405 Method Not Allowed: Unsupported HTTP method</li> <li>409 Conflict: Blob already exists</li> <li>500 Internal Server Error: Server-side error</li> </ul>"},{"location":"references/stores/blob_reference/#key-functions","title":"Key Functions","text":"<ul> <li><code>handleHealth</code>: Handles health check requests.</li> <li><code>handleExists</code>: Checks if a blob exists.</li> <li><code>handleGet</code>: Retrieves a blob, including support for range requests.</li> <li><code>handleRangeRequest</code>: Processes partial content requests using the Range header.</li> <li><code>handleSet</code>: Stores a new blob.</li> <li><code>handleSetDAH</code>: Sets the delete-at-height value for a blob.</li> <li><code>handleDelete</code>: Deletes a blob.</li> </ul>"},{"location":"references/stores/blob_reference/#utility-functions","title":"Utility Functions","text":"<ul> <li><code>parseRange</code>: Parses the <code>Range</code> header for partial content requests.</li> <li><code>getKeyFromPath</code>: Extracts and decodes the base64-encoded blob key and file type from the request path.</li> </ul>"},{"location":"references/stores/blob_reference/#configuration","title":"Configuration","text":"<p>The service can be configured with various options through the <code>options.StoreOption</code> parameter in the constructor. The server includes built-in timeouts:</p> <ul> <li>ReadTimeout: 15 seconds</li> <li>WriteTimeout: 15 seconds</li> <li>IdleTimeout: 60 seconds</li> <li>Shutdown timeout: 5 seconds</li> </ul> <p>Query parameters are automatically converted to <code>options.FileOption</code> using <code>options.QueryToFileOptions()</code> for per-request configuration.</p>"},{"location":"references/stores/utxo_reference/","title":"UTXO Store Reference Documentation","text":""},{"location":"references/stores/utxo_reference/#overview","title":"Overview","text":"<p>The UTXO (Unspent Transaction Output) Store provides an interface for managing and querying UTXO data in a blockchain system.</p>"},{"location":"references/stores/utxo_reference/#core-types","title":"Core Types","text":""},{"location":"references/stores/utxo_reference/#spend","title":"Spend","text":"<p>Represents a UTXO being spent.</p> <pre><code>type Spend struct {\n    // TxID is the transaction ID that created this UTXO\n    TxID *chainhash.Hash `json:\"txId\"`\n\n    // Vout is the output index in the creating transaction\n    Vout uint32 `json:\"vout\"`\n\n    // UTXOHash is the unique identifier of this UTXO\n    UTXOHash *chainhash.Hash `json:\"utxoHash\"`\n\n    // SpendingData contains information about the transaction that spends this UTXO\n    // This will be nil if the UTXO is unspent\n    SpendingData *spend.SpendingData `json:\"spendingData,omitempty\"`\n\n    // ConflictingTxID is the transaction ID that conflicts with this UTXO\n    ConflictingTxID *chainhash.Hash `json:\"conflictingTxId,omitempty\"`\n\n    // BlockIDs is the list of blocks the transaction has been mined into\n    BlockIDs []uint32 `json:\"blockIDs,omitempty\"`\n\n    // error is the error that occurred during the spend operation\n    Err error `json:\"err,omitempty\"`\n}\n</code></pre> <p>The <code>Spend</code> struct also provides a <code>Clone()</code> method that creates a deep copy of the spend object.</p>"},{"location":"references/stores/utxo_reference/#spendresponse","title":"SpendResponse","text":"<p>Represents the response from a GetSpend operation.</p> <pre><code>type SpendResponse struct {\n    // Status indicates the current state of the UTXO\n    Status int `json:\"status\"`\n\n    // SpendingData contains information about the transaction that spent this UTXO, if any\n    SpendingData *spend.SpendingData `json:\"spendingData,omitempty\"`\n\n    // LockTime is the block height or timestamp until which this UTXO is locked\n    LockTime uint32 `json:\"lockTime,omitempty\"`\n}\n</code></pre> <p><code>SpendResponse</code> provides serialization methods:</p> <ul> <li><code>Bytes()</code>: Serializes the response to a byte slice</li> <li><code>FromBytes(b []byte)</code>: Deserializes from a byte slice</li> </ul>"},{"location":"references/stores/utxo_reference/#minedblockinfo","title":"MinedBlockInfo","text":"<p>Contains information about a block where a transaction appears.</p> <pre><code>type MinedBlockInfo struct {\n    // BlockID is the unique identifier of the block\n    BlockID     uint32\n\n    // BlockHeight is the height of the block in the blockchain\n    BlockHeight uint32\n\n    // SubtreeIdx is the index of the subtree where the transaction appears\n    SubtreeIdx  int\n\n    // UnsetMined if true, the mined info will be removed from the tx\n    UnsetMined  bool\n}\n</code></pre>"},{"location":"references/stores/utxo_reference/#unresolvedmetadata","title":"UnresolvedMetaData","text":"<p>Holds metadata for unresolved transactions.</p> <pre><code>type UnresolvedMetaData struct {\n    // Hash is the transaction hash\n    Hash chainhash.Hash\n    // Idx is the index in the original list of hashes passed to BatchDecorate\n    Idx int\n    // Data holds the fetched metadata, nil until fetched\n    Data *meta.Data\n    // Fields specifies which metadata fields should be fetched\n    Fields []fields.FieldName\n    // Err holds any error encountered while fetching the metadata\n    Err error\n}\n</code></pre>"},{"location":"references/stores/utxo_reference/#unminedtransaction","title":"UnminedTransaction","text":"<p>Represents an unmined transaction in the UTXO store.</p> <pre><code>type UnminedTransaction struct {\n    // Hash is the transaction hash\n    Hash       *chainhash.Hash\n    // Fee is the transaction fee in satoshis\n    Fee        uint64\n    // Size is the serialized size of the transaction in bytes\n    Size       uint64\n    // TxInpoints contains the transaction inpoints\n    TxInpoints subtree.TxInpoints\n    // CreatedAt is the timestamp when the unmined transaction was first added\n    CreatedAt  int\n    // Locked indicates whether the transaction outputs are marked as locked\n    Locked     bool\n}\n</code></pre>"},{"location":"references/stores/utxo_reference/#unminedtxiterator","title":"UnminedTxIterator","text":"<p>Provides an interface to iterate over unmined transactions efficiently.</p> <pre><code>type UnminedTxIterator interface {\n    // Next advances the iterator and returns the next unmined transaction, or nil if iteration is done\n    Next(ctx context.Context) (*UnminedTransaction, error)\n    // Err returns the first error encountered during iteration\n    Err() error\n    // Close releases any resources held by the iterator\n    Close() error\n}\n</code></pre>"},{"location":"references/stores/utxo_reference/#ignoreflags","title":"IgnoreFlags","text":"<p>Options for ignoring certain flags during UTXO operations.</p> <pre><code>type IgnoreFlags struct {\n    IgnoreConflicting bool\n    IgnoreLocked bool\n}\n</code></pre>"},{"location":"references/stores/utxo_reference/#createoptions","title":"CreateOptions","text":"<p>Options for creating a new UTXO entry.</p> <pre><code>type CreateOptions struct {\n    MinedBlockInfos []MinedBlockInfo\n    TxID            *chainhash.Hash\n    IsCoinbase      *bool\n    Frozen          bool\n    Conflicting     bool\n    Locked          bool\n}\n</code></pre>"},{"location":"references/stores/utxo_reference/#store-interface","title":"Store Interface","text":"<p>The <code>Store</code> interface defines the contract for UTXO storage operations. Implementations must be thread-safe as they will be accessed concurrently.</p> <pre><code>type Store interface {\n    // Health checks the health status of the UTXO store.\n    // If checkLiveness is true, it performs additional liveness checks.\n    // Returns status code, status message and any error encountered.\n    Health(ctx context.Context, checkLiveness bool) (int, string, error)\n\n    // Create stores a new transaction's outputs as UTXOs and returns associated metadata.\n    // The blockHeight parameter is used to determine coinbase maturity.\n    // Additional options can be specified using CreateOption functions.\n    Create(ctx context.Context, tx *bt.Tx, blockHeight uint32, opts ...CreateOption) (*meta.Data, error)\n\n    // Get retrieves UTXO metadata for a given transaction hash.\n    // The fields parameter can be used to specify which metadata fields to retrieve.\n    // If fields is empty, all fields will be retrieved.\n    Get(ctx context.Context, hash *chainhash.Hash, fields ...fields.FieldName) (*meta.Data, error)\n\n    // Delete removes a UTXO and its associated metadata from the store.\n    Delete(ctx context.Context, hash *chainhash.Hash) error\n\n    // GetSpend retrieves information about a UTXO's spend status.\n    GetSpend(ctx context.Context, spend *Spend) (*SpendResponse, error)\n\n    // GetMeta retrieves transaction metadata.\n    GetMeta(ctx context.Context, hash *chainhash.Hash) (*meta.Data, error)\n\n    // Spend marks all the UTXOs of the transaction as spent.\n    Spend(ctx context.Context, tx *bt.Tx, ignoreFlags ...IgnoreFlags) ([]*Spend, error)\n\n    // Unspend reverses a previous spend operation, marking UTXOs as unspent.\n    // This is used during blockchain reorganizations.\n    Unspend(ctx context.Context, spends []*Spend, flagAsLocked ...bool) error\n\n    // SetMinedMulti updates the block ID for multiple transactions that have been mined.\n    // Returns a map of transaction hashes to block IDs where they were already mined,\n    // enabling detection of duplicate transaction mining across different blocks.\n    SetMinedMulti(ctx context.Context, hashes []*chainhash.Hash, minedBlockInfo MinedBlockInfo) (map[chainhash.Hash][]uint32, error)\n\n    // BatchDecorate efficiently fetches metadata for multiple transactions.\n    // The fields parameter specifies which metadata fields to retrieve.\n    BatchDecorate(ctx context.Context, unresolvedMetaDataSlice []*UnresolvedMetaData, fields ...fields.FieldName) error\n\n    // PreviousOutputsDecorate fetches information about transaction inputs' previous outputs.\n    PreviousOutputsDecorate(ctx context.Context, tx *bt.Tx) error\n\n    // FreezeUTXOs marks UTXOs as frozen, preventing them from being spent.\n    // This is used by the alert system to prevent spending of UTXOs.\n    FreezeUTXOs(ctx context.Context, spends []*Spend, tSettings *settings.Settings) error\n\n    // UnFreezeUTXOs removes the frozen status from UTXOs, allowing them to be spent again.\n    UnFreezeUTXOs(ctx context.Context, spends []*Spend, tSettings *settings.Settings) error\n\n    // ReAssignUTXO reassigns a UTXO to a new transaction output.\n    // The UTXO will become spendable after ReAssignedUtxoSpendableAfterBlocks blocks.\n    ReAssignUTXO(ctx context.Context, utxo *Spend, newUtxo *Spend, tSettings *settings.Settings) error\n\n    // GetCounterConflicting returns the counter conflicting transactions for a given transaction hash.\n    GetCounterConflicting(ctx context.Context, txHash chainhash.Hash) ([]chainhash.Hash, error)\n\n    // GetConflictingChildren returns the children of the given conflicting transaction.\n    GetConflictingChildren(ctx context.Context, txHash chainhash.Hash) ([]chainhash.Hash, error)\n\n    // SetConflicting marks transactions as conflicting or not conflicting and returns the affected spends.\n    SetConflicting(ctx context.Context, txHashes []chainhash.Hash, value bool) ([]*Spend, []chainhash.Hash, error)\n\n    // SetLocked marks transactions as locked and not spendable.\n    SetLocked(ctx context.Context, txHashes []chainhash.Hash, value bool) error\n\n    // SetBlockHeight updates the current block height in the store.\n    SetBlockHeight(height uint32) error\n\n    // GetBlockHeight returns the current block height from the store.\n    GetBlockHeight() uint32\n\n    // SetMedianBlockTime updates the median block time in the store.\n    SetMedianBlockTime(height uint32) error\n\n    // GetMedianBlockTime returns the current median block time from the store.\n    GetMedianBlockTime() uint32\n\n    // GetUnminedTxIterator returns an iterator for all unmined transactions in the store.\n    // This is used by the Block Assembly service to recover transactions on startup.\n    GetUnminedTxIterator() (UnminedTxIterator, error)\n\n    // QueryOldUnminedTransactions returns transaction hashes for unmined transactions older than the cutoff height.\n    // This method is used by the store-agnostic cleanup implementation to identify transactions for removal.\n    QueryOldUnminedTransactions(ctx context.Context, cutoffBlockHeight uint32) ([]chainhash.Hash, error)\n\n    // PreserveTransactions marks transactions to be preserved from deletion until a specific block height.\n    // This clears any existing DeleteAtHeight and sets PreserveUntil to the specified height.\n    // Used to protect parent transactions when cleaning up unmined transactions.\n    PreserveTransactions(ctx context.Context, txIDs []chainhash.Hash, preserveUntilHeight uint32) error\n\n    // ProcessExpiredPreservations handles transactions whose preservation period has expired.\n    ProcessExpiredPreservations(ctx context.Context, currentHeight uint32) error\n\n    // Note: Close method is not part of the Store interface in the current implementation\n}\n</code></pre>"},{"location":"references/stores/utxo_reference/#key-functions","title":"Key Functions","text":"<ul> <li><code>Health</code>: Checks the health status of the UTXO store, optionally verifying liveness.</li> <li><code>Create</code>: Creates new UTXO entries from a transaction's outputs with configurable options.</li> <li><code>Get</code>: Retrieves UTXO metadata for specific fields with field-level filtering.</li> <li><code>Delete</code>: Removes a UTXO entry and its associated metadata.</li> <li><code>GetSpend</code>: Retrieves information about a UTXO's spend status, including spending transaction data.</li> <li><code>Spend</code>: Marks UTXOs as spent by a transaction, with optional flags for handling conflicts.</li> <li><code>Unspend</code>: Reverses spend operations during blockchain reorganization.</li> <li><code>BatchDecorate</code>: Efficiently fetches metadata for multiple transactions in a single operation.</li> <li><code>FreezeUTXOs</code>/<code>UnFreezeUTXOs</code>: Manages frozen status of UTXOs for the alert system.</li> <li><code>SetConflicting</code>/<code>SetLocked</code>: Controls transaction conflict and spendability status.</li> <li><code>GetMeta</code>: Retrieves transaction metadata for a single transaction.</li> <li><code>SetMinedMulti</code>: Updates block information for multiple mined transactions and returns a map of transaction hashes to block IDs.</li> <li><code>PreviousOutputsDecorate</code>: Fetches information about transaction inputs' previous outputs from a transaction.</li> <li><code>ReAssignUTXO</code>: Reassigns a UTXO to a new transaction output with safety measures.</li> <li><code>GetCounterConflicting</code>/<code>GetConflictingChildren</code>: Manages conflict relationships between transactions.</li> <li><code>SetBlockHeight</code>/<code>GetBlockHeight</code>/<code>SetMedianBlockTime</code>/<code>GetMedianBlockTime</code>: Manages blockchain state.</li> <li><code>GetUnminedTxIterator</code>: Returns an iterator for efficiently accessing all unmined transactions.</li> <li><code>QueryOldUnminedTransactions</code>: Identifies unmined transactions older than a specified block height for cleanup.</li> <li><code>PreserveTransactions</code>: Protects transactions from deletion by setting a preservation period.</li> <li><code>ProcessExpiredPreservations</code>: Handles cleanup of expired preservation markers.</li> </ul>"},{"location":"references/stores/utxo_reference/#create-options","title":"Create Options","text":"<ul> <li><code>WithMinedBlockInfo</code>: Sets the block information (ID, height, and subtree index) for a new UTXO entry. This replaces the deprecated <code>WithBlockIDs</code> option and provides more detailed tracking of where UTXOs appear in the blockchain.</li> <li><code>WithTXID</code>: Sets the transaction ID for a new UTXO entry.</li> <li><code>WithSetCoinbase</code>: Sets the coinbase flag for a new UTXO entry.</li> <li><code>WithFrozen</code>: Sets the frozen status for a new UTXO entry.</li> <li><code>WithConflicting</code>: Sets the conflicting status for a new UTXO entry.</li> <li><code>WithLocked</code>: Sets the transaction as locked on creation.</li> </ul>"},{"location":"references/stores/utxo_reference/#constants","title":"Constants","text":"<ul> <li><code>MetaFields</code>: Default fields for metadata retrieval.</li> <li><code>MetaFieldsWithTx</code>: Metadata fields including the transaction.</li> </ul>"},{"location":"references/stores/utxo_reference/#mock-implementation","title":"Mock Implementation","text":"<p>The <code>MockUtxostore</code> struct provides a mock implementation of the <code>Store</code> interface for testing purposes.</p>"},{"location":"staging/TeranodePOC/AWS-POC-Setup/","title":"AWS POC Setup","text":""},{"location":"staging/TeranodePOC/AWS-POC-Setup/#teranode-poc-environment","title":"Teranode - POC Environment","text":""},{"location":"staging/TeranodePOC/AWS-POC-Setup/#1-teranode-aws-setup","title":"1. Teranode AWS Setup","text":"<p>On February 2024, the BSV team started a multi-month test exercise (the proof-of-concepts or POC). The objective of the exercise is to prove that Teranode BSV can scale to process millions of transactions per second.</p> <p>For the purpose of this test, Amazon AWS was chosen as the cloud server provider. Three (3) nodes were created in 3 separate AWS regions. Each node is composed of a number of server instances. The purpose of this document is to describe the setup of each node.</p>"},{"location":"staging/TeranodePOC/AWS-POC-Setup/#2-instance-details","title":"2. Instance Details","text":"<p>The table below provides a summary of the server groups, the number of instances per group, and their technical specification.</p> Instance Group Instances Instance Type vCPUs Memory (GiB) Network (Gbps) Processor Asset-sm 1 c6in.24xlarge 96 192 150 Intel Xeon 8375C (Ice Lake) Block Assembly 1 m6in.32xlarge 128 512 200 Intel Xeon 8375C (Ice Lake) Block Validation 1 r7i.48xlarge 192 1536 50 Intel Xeon Scalable (Sapphire Rapids) Main Pool 3 c6gn.12xlarge 48 96 75 AWS Graviton2 Processor Propagation Servers 16 c6in.16xlarge 64 128 100 Intel Xeon 8375C (Ice Lake) Proxies 6 c7gn.8xlarge 32 64 100 AWS Graviton3 Processor TX Blasters 13 c6gn.8xlarge 32 64 50 AWS Graviton2 Processor Aerospike 10 i3en.24xlarge 96 768 100 Intel Xeon Platinum 8175 <p>AWS EKS (Kubernetes) is used to run services in all instances. Aerospike has been run in and out of Kubernetes as part of the test exercise.</p> <p>Please note that exact instance count under the Main Pool, Propagation Servers, Proxies, TX Blaster and Aerospike groups can vary during the test.</p> <p>Additionally, a Teranode node can have a number of other servers and containers related to secondary services, such as monitoring (prometheus, datadog). Such secondary instances are out of the scope of this document.</p>"},{"location":"staging/TeranodePOC/AWS-POC-Setup/#3-instance-group-definitions","title":"3. Instance Group Definitions","text":"<p>Here's a structured table summarizing the instance groups, their descriptions, and purposes based on the provided details:</p> Instance Group Description Purpose Proxies Ingress Proxy Servers Managing region-to-region ingress traffic with Traefik k8s services. Asset-sm Asset (Small Instance) Hosting the Asset Server Teranode microservice. Block Assembly Block Assembly Running the Block Assembly Teranode microservice. Block Validation Block Validation Running the Block Validation Teranode microservice. Main Pool Overlay Services Running the p2p, coinbase, blockchain, postgres, faucet, and other microservices. Propagation Servers Propagation Servers A pool of Propagation Teranode microservices. TX Blasters TX Blasters A pool of TX Blaster Teranode microservices. Aerospike Aerospike A cluster of Aerospike NoSQL DB services."},{"location":"staging/TeranodePOC/AWS-POC-Setup/#4-storage","title":"4. Storage","text":"<ul> <li>AWS S3 - Configured to work with Lustre filesystem, shared across services, and used for Subtrees and TX data storage.</li> <li>Aerospike - Used for UTXO and TX Meta data storage.</li> <li>Postgresql - Within the Main Pool instance group, an instance of Postgres is run. The Blockchain service stores the blockchain data in postgres.</li> </ul>"},{"location":"staging/TeranodePOC/AWS-POC-Setup/#5-performance-metrics","title":"5. Performance Metrics","text":"<p>In the context of our AWS-based infrastructure, the primary consideration for specifying instance types has been to meet the high network bandwidth requirements essential for the Teranode services. This approach was adopted following the identification of network bandwidth as a critical bottleneck in previous discovery phases.</p> <p>However, the services are not fully utilizing the allocated resources in terms of CPU utilization, memory utilization, and disk I/O. There is excess capacity in these areas which may present future opportunities for cost optimization or reallocation of resources to better match actual usage patterns.</p>"},{"location":"staging/github-workflows/GitHub-Workflows/","title":"GitHub Workflows","text":""},{"location":"staging/github-workflows/GitHub-Workflows/#github-workflow-documentation-test-build-deploy","title":"GitHub Workflow Documentation: test-build-deploy","text":""},{"location":"staging/github-workflows/GitHub-Workflows/#overview","title":"Overview","text":"<p>The <code>test-build-deploy</code> GitHub workflow is defined in the <code>gke.yaml</code> file, and it is triggered on two specific conditions: 1. Push events to the tags that match 'v' or 'scaling-v'. 2. Push events to the branches: 'main' and 'staging'.</p>"},{"location":"staging/github-workflows/GitHub-Workflows/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>REPO</code>: Specifies the repository name, set to 'teranode'.</li> <li><code>ECR_REGION</code>: Specifies the AWS ECR region, set to 'eu-north-1'.</li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#jobs","title":"Jobs","text":""},{"location":"staging/github-workflows/GitHub-Workflows/#1-get_tag","title":"1. <code>get_tag</code>","text":"<ul> <li>Purpose: Determines the deployment tag based on the GitHub event (tag or SHA).</li> <li>Runner: Ubuntu latest version.</li> <li> <p>Steps:</p> </li> <li> <p>Check if the event is a tag and export it; otherwise, use the SHA.</p> </li> <li> <p>Outputs:</p> </li> <li> <p><code>deployment_tag</code>: The determined tag or SHA.</p> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#2-test_and_lint","title":"2. <code>test_and_lint</code>","text":"<ul> <li>Purpose: Run tests and perform code linting.</li> <li>Uses: Reference to <code>.github/workflows/gke_tests.yaml</code>.</li> <li>Secrets: Inherits all secrets from the context.</li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#3-build_amd64","title":"3. <code>build_amd64</code>","text":"<ul> <li>Purpose: Build the Docker image for AMD64 architecture.</li> <li>Dependencies: Depends on the <code>get_tag</code> job to fetch the tag.</li> <li>Uses: Reference to <code>.github/workflows/gke_build.yaml</code>.</li> <li> <p>Parameters:</p> </li> <li> <p>Architecture, repository, tag, region.</p> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#4-build_arm64","title":"4. <code>build_arm64</code>","text":"<ul> <li>Purpose: Build the Docker image for ARM64 architecture.</li> <li>Dependencies: Depends on the <code>get_tag</code> job.</li> <li>Uses: Same as <code>build_amd64</code> but with ARM64 architecture.</li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#5-create_manifest","title":"5. <code>create_manifest</code>","text":"<ul> <li>Purpose: Create a Docker manifest for multi-architecture support.</li> <li>Dependencies: Depends on <code>test_and_lint</code>, <code>build_amd64</code>, <code>build_arm64</code>, and <code>get_tag</code>.</li> <li>Uses: Reference to <code>.github/workflows/gke_manifest.yaml</code>.</li> <li> <p>Parameters:</p> </li> <li> <p>Region, repository, tag.</p> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#6-deploy","title":"6. <code>deploy</code>","text":"<ul> <li>Purpose: Deploy the application using the created Docker image.</li> <li>Dependencies: Depends on <code>create_manifest</code> and <code>get_tag</code>.</li> <li>Uses: Reference to <code>.github/workflows/gke_deploy.yaml</code>.</li> <li> <p>Parameters:</p> </li> <li> <p>Repository, tag.</p> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#the-test_and_lint-job","title":"The <code>test_and_lint</code> Job","text":"<p>The <code>gke_tests.yaml</code> GitHub workflow is designed to be invoked through a workflow call. This workflow is focused on running linting and testing for Go code, as well as performing a SonarQube scan for code quality analysis.</p>"},{"location":"staging/github-workflows/GitHub-Workflows/#jobs_1","title":"Jobs","text":""},{"location":"staging/github-workflows/GitHub-Workflows/#1-go-lint-and-test","title":"1. <code>go-lint-and-test</code>","text":"<ul> <li>Purpose: Performs linting and testing of Go code.</li> <li>Runner: Custom runner labeled <code>teranode-runner</code>.</li> <li> <p>Strategy:</p> <ul> <li><code>fail-fast</code>: True (If any job fails, the entire workflow will terminate).</li> <li> <p>Steps:</p> </li> <li> <p>Checkout: Checks out the source code at the full depth to ensure better relevancy of reporting and to support tools that require complete git history.</p> </li> <li>Set up Go: Sets up Go environment with specified version <code>1.21.0</code> using the <code>actions/setup-go@v5</code> action.</li> <li>Run Go tests: Executes long-running Go tests using <code>make longtests</code>. This includes integration and other extensive testing procedures.</li> </ul> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#2-sonarqube","title":"2. <code>sonarqube</code>","text":"<ul> <li>Purpose: Runs a SonarQube scan to analyze the code quality and detect bugs, vulnerabilities, and code smells.</li> <li>Runner: Uses the same custom runner <code>teranode-runner</code>.</li> <li> <p>Steps:</p> <ul> <li>Checkout: Performs a repository checkout similar to the lint and test job.</li> <li>Twingate Action: Establishes a secure connection using Twingate, necessary for accessing internal resources during the CI process.</li> <li> <p>SonarQube Scan:</p> <ul> <li>Utilizes the <code>sonarsource/sonarqube-scan-action@v2.0.1</code>.</li> <li>Configured with the SonarQube server URL and a token for authentication, facilitating the scanning of the checked-out code.</li> </ul> </li> </ul> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#the-build_amd64-and-build_arm64-job","title":"The <code>build_amd64</code> and <code>build_arm64</code> Job","text":"<p>The <code>gke_build.yaml</code> workflow is designed to build Docker images for specified architectures using AWS services, specifically targeting Amazon Elastic Container Registry (ECR). It is triggered via a <code>workflow_call</code> event, allowing it to be invoked by other workflows that provide the necessary parameters.</p>"},{"location":"staging/github-workflows/GitHub-Workflows/#trigger","title":"Trigger","text":"<ul> <li>Event: Triggered by a <code>workflow_call</code>.</li> <li> <p>Inputs:</p> <ul> <li><code>repo</code>: Repository name.</li> <li><code>tag</code>: Deployment tag.</li> <li><code>region</code>: AWS region where operations will be performed.</li> <li><code>arch</code>: Architecture for which the Docker image will be built, such as 'amd64' or 'arm64'.</li> </ul> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#job-build-docker","title":"Job: <code>build-docker</code>","text":"<ul> <li>Purpose: Builds and pushes a Docker image to AWS ECR based on the input parameters.</li> <li>Runner: Dynamically determined based on the architecture input; supports both 'ARM64' and 'X64' on self-hosted Linux runners.</li> <li> <p>Strategy:</p> <ul> <li><code>fail-fast</code>: True. If any step fails, the entire job will terminate.</li> <li>Steps:</li> <li>Checkout: Clones the repository using <code>actions/checkout@v4</code>.</li> <li>Setup Docker Buildx: Configures Docker Buildx for building multi-architecture Docker images using <code>docker/setup-buildx-action@v3</code>.</li> <li>Configure AWS CLI: Sets up AWS CLI credentials to interact with AWS services using <code>aws-actions/configure-aws-credentials@v4</code>.</li> <li>Login to Amazon ECR: Authenticates to Amazon ECR using <code>aws-actions/amazon-ecr-login@v2</code>.</li> <li>Get Cluster Base ID: Retrieves the ID for the base image from a predefined file.</li> <li>Get Cluster Run ID: Retrieves the ID for the run image from a predefined file.</li> <li> <p>Build and Push Docker Image:</p> <ul> <li>Executes the Docker build and push using <code>docker/build-push-action@v5</code>.</li> <li>Utilizes build arguments to include specific settings like SHA, debug flags, and references to the base and run images.</li> </ul> </li> </ul> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#the-deploy-job","title":"The <code>deploy</code> Job","text":"<p>The <code>gke_deploy.yaml</code> GitHub workflow is designed for deploying Docker images to Amazon Elastic Kubernetes Service (EKS) across different geographic regions based on the deployment tag. This workflow is invoked via a <code>workflow_call</code>, making it reusable across multiple projects or pipelines.</p>"},{"location":"staging/github-workflows/GitHub-Workflows/#trigger_1","title":"Trigger","text":"<ul> <li>Event: Triggered by a <code>workflow_call</code>.</li> <li> <p>Inputs:</p> <ul> <li><code>repo</code>: The repository name for which the deployment is being made.</li> <li><code>tag</code>: The specific deployment tag associated with the Docker image.</li> </ul> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#environment-variables_1","title":"Environment Variables","text":"<ul> <li><code>ECR_URL</code>: Specifies the Amazon ECR URL where the Docker images are hosted.</li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#jobs_2","title":"Jobs","text":""},{"location":"staging/github-workflows/GitHub-Workflows/#1-send_msg","title":"1. <code>send_msg</code>","text":"<ul> <li>Purpose: Logs the deployment details.</li> <li>Runner: Ubuntu latest version.</li> <li> <p>Steps:</p> <ul> <li>Logs the full repository and tag being deployed.</li> <li>Logs the GitHub event reference.</li> </ul> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#2-regional-deployment-jobs","title":"2. Regional Deployment Jobs","text":"<p>These jobs deploy the Docker image to specific AWS EKS clusters based on the region and the type of deployment tag. The deployment is conditional on the tag reference starting with specific prefixes.</p> <ul> <li> <p>Common Settings:</p> <ul> <li>Runner: Uses a reusable workflow located at <code>.github/workflows/deploy-to-region.yaml</code>.</li> <li>Secrets: Inherits all secrets from the parent workflow.</li> <li>Conditions: Each job has a conditional start based on the prefix of the GitHub event ref (e.g., 'refs/tags/v' for version releases, 'refs/tags/scaling-v' for scaling deployments).</li> <li> <p>Parameters:</p> <ul> <li><code>region</code>: Specifies the AWS region for deployment, such as <code>eu-west-1</code>, <code>us-east-1</code>, <code>ap-south-1</code>, etc.</li> <li><code>teranode_env</code>: Specifies the environment configuration, either <code>allinone</code> or <code>scaling</code>.</li> </ul> </li> </ul> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#the-create_manifest-job","title":"The <code>create_manifest</code> Job","text":"<p>The <code>gke_manifest.yaml</code> GitHub workflow is designed to create Docker manifests for multi-architecture Docker images. This process ensures that Docker images are available across different computing architectures, such as AMD64 and ARM64. The workflow is triggered via a <code>workflow_call</code>, making it reusable and modular, suitable for integration into various CI/CD pipelines.</p>"},{"location":"staging/github-workflows/GitHub-Workflows/#trigger_2","title":"Trigger","text":"<ul> <li>Event: Triggered by a <code>workflow_call</code>.</li> <li> <p>Inputs:</p> <ul> <li><code>repo</code>: The repository name where the Docker images are stored.</li> <li><code>tag</code>: The specific tag associated with the Docker images.</li> <li><code>region</code>: The AWS region where the Amazon ECR resides.</li> </ul> </li> </ul>"},{"location":"staging/github-workflows/GitHub-Workflows/#job-build_manifest","title":"Job: <code>build_manifest</code>","text":"<ul> <li>Purpose: To create a Docker manifest that represents an image available in multiple architectures.</li> <li>Runner: Ubuntu latest version.</li> <li> <p>Strategy:</p> <ul> <li><code>fail-fast</code>: True. The workflow will terminate if any step fails.</li> <li> <p>Outputs:</p> </li> <li> <p><code>registry</code>: The Amazon ECR registry URL returned from the login step.</p> </li> <li>Steps:</li> <li>Twingate VPN Setup: Establishes a secure connection for actions that might require access to protected resources using Twingate's GitHub action.</li> <li>AWS CLI Configuration: Configures AWS credentials to interact with AWS services.</li> <li>Amazon ECR Login: Authenticates with Amazon ECR to allow subsequent operations such as pushing or pulling Docker images.</li> <li>Deployment Message Logging: Logs a deployment message indicating the creation of the Docker manifest for the given tag.</li> <li> <p>Create Docker Manifest:</p> <ul> <li>Utilizes <code>int128/docker-manifest-create-action@v1</code> to create a Docker manifest.</li> <li>Configures the action to create a manifest that includes images with suffixes <code>-amd64</code> and <code>-arm64</code>, thus supporting these two architectures.</li> </ul> </li> </ul> </li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/","title":"Cloud Services Operation Runbook","text":""},{"location":"staging/operations/ClusterManagementAndOperations/#introduction","title":"Introduction","text":"<p>This document describes the processes involved in configuring, deploying, and managing the Teranode BSV services using Docker, Kubernetes (k8s), and AWS. It covers the initial setup, common commands, and troubleshooting steps necessary for efficient microservice operation.</p> <p>Note: This document has been created and tested using macOS. Some of the commands will differ for other operating systems.</p>"},{"location":"staging/operations/ClusterManagementAndOperations/#docker-and-kubernetes-setup","title":"Docker and Kubernetes Setup","text":"<ul> <li>Dockerfile Location: Within the GitHub project repository, the Dockerfile is located at <code>./Dockerfile</code>. This file specifies all requirements for the core set of Teranode microservices, including necessary software packages and environment variables.</li> <li>Compilation Process: The Dockerfile builds a single executable binary named <code>teranode.run</code> to be included in the image.</li> <li>Deployment to Kubernetes: The image, including the <code>teranode.run</code> binary, is deployed across our Kubernetes instances. This ensures the binary is consistently deployed and managed across all instances.</li> <li>Execution by Kubernetes: Kubernetes executes a specific command for each microservice application, as defined in their respective Kubernetes manifest files (e.g., Deployment or Pod specifications). This flexibility allows a single binary to be used in multiple contexts by leveraging symbolic links (symlinks) to point to <code>teranode.run</code>.</li> <li> <p>Filesystem Configuration Example:</p> <pre><code>root@blockassembly1-7874b7bf7c-668bk:/app# ls -ltr\ntotal 141072\n...\n-rwxr-xr-x 1 root root 125468128 Jan 29 23:37 teranode.run\nlrwxrwxrwx 1 root root         8 Jan 29 23:38 utxostoreblaster.run -&gt; teranode.run\n...\n</code></pre> </li> </ul> <p>In the above filesystem snapshot from a Kubernetes pod, notice how all applications refer to the same <code>teranode.run</code> binary via symlinks. This setup enhances maintainability and efficiency by centralizing the application logic in a single binary, while symlinks provide the flexibility to execute different aspects of the binary as needed for each microservice.</p> <p>Technical Detailing: Kubernetes interprets these commands and symlinks through the <code>command</code> and <code>args</code> fields in the container spec within a Pod's manifest.</p>"},{"location":"staging/operations/ClusterManagementAndOperations/#service-yaml-files","title":"Service YAML Files","text":"<p>For each microservice, we maintain several YAML files that define its behavior within a Kubernetes cluster. These files include:</p> <ul> <li> <p>Application YAML Configuration Files: These files manage the deployment setup of containerized applications. Key fields within these files include:</p> </li> <li> <p><code>name</code>: Serves as a base prefix for application naming within Kubernetes.</p> </li> <li><code>REPO:IMAGE:TAG</code>: Specifies the Docker image to be used. This reference is updated with the actual image name during the CI build process.</li> <li><code>command</code>: Indicates the command to be executed within the Docker container.</li> <li><code>role</code>: Defines which Kubernetes nodes can run the application, based on assigned roles. Node roles can be checked with the command <code>$ kubectl get node -L role</code>.</li> <li><code>replicas</code>: Determines the number of application instances to start within the service. A setting of 0 requires manual service startup.</li> <li> <p><code>volumeMounts</code> (optional): Specifies volume configurations, such as for <code>lustre</code> volumes, enabling detailed storage setup. For example, in <code>blockassembly.yaml</code>, a volume mount might be defined as follows:</p> <pre><code>volumes:\n\n  - name: lustre-storage\n    persistentVolumeClaim:\n      claimName: subtree-pvc\n</code></pre> <p>Here, <code>lustre-pvc</code> references a specific PersistentVolumeClaim defined in <code>lustre-pvc.yaml</code>, indicating a Lustre filesystem storage configuration.</p> </li> <li> <p>Volume YAML Configuration Files: These files define persistent volumes for storage purposes. A <code>lustre-pvc.yaml</code> file, for instance, would specify the configuration for a Lustre filesystem storage volume.</p> </li> <li> <p>Service YAML Configuration Files: Detail the Kubernetes service configuration for running specific containerized applications. For example, <code>tx-blaster-service.yaml</code> outlines the service setup for the TXBlaster application, including port mappings between the internal and external interfaces.</p> </li> <li> <p>Traefik Ingress YAML Configuration Files: As part of Traefik's Kubernetes CRDs, these files facilitate advanced routing and ingress management, leveraging Traefik's capabilities as an edge router. An example file, <code>asset-grpc-ingress.yaml</code>, would define ingress rules for gRPC assets.</p> </li> <li> <p>Kustomize Customization YAML Files: Utilizing Kustomize, a built-in tool in Kubernetes since version 1.14, these files allow for the customization of Kubernetes configurations based on templates. The CI process employs Kustomize to adjust settings such as <code>REPO:IMAGE:TAG</code> and to apply specific ingress rules or modify application names and hosts to suit different environments or nodes within those environments.</p> </li> <li> <p>Deployment Scripts: Central to the deployment process is the <code>deploy-to-region.yaml</code> script, which uses Kustomize to generate the final set of Kubernetes configuration files tailored for specific deployment environments.</p> </li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#settings","title":"Settings","text":"<p>Microservice configurations can be influenced by settings. There are 2 ways to provide settings:</p> <ul> <li>Via settings in the <code>settings.conf</code> and <code>settings_local.conf</code> files, embedded within the Docker image.</li> <li>By Kubernetes pod-specific settings. These settings can override configurations defined in the <code>.conf</code> files, allowing for dynamic adjustment based on the deployment environment or specific operational requirements.</li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#kubernetes-resolver-for-grpc","title":"Kubernetes Resolver for gRPC","text":"<ul> <li>Kubernetes Resolver for gRPC</li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#how-to","title":"How to","text":""},{"location":"staging/operations/ClusterManagementAndOperations/#aws-cli-configuration","title":"AWS CLI Configuration","text":"<ul> <li>Install the AWS Command Line Interface (CLI) via Homebrew: <code>brew install awscli</code>. Check the AWS CLI Installation Guide for other installation methods.</li> <li>Run <code>aws configure</code> to set up the AWS Command Line Interface (CLI).</li> <li>Enter the <code>AWS Access Key ID</code>, <code>AWS Secret Access Key</code>, <code>Default region name</code> (e.g., <code>eu-north-1</code>), and <code>Default output format</code> as required.</li> <li>Please check with your DevOPS team for the specific credentials available to you.</li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#kubernetes-k8s-cluster-access","title":"Kubernetes (k8s) Cluster Access","text":"<ol> <li> <p>Kubectl Installation:</p> <ul> <li>Install kubectl via Homebrew: <code>brew install kubectl</code>.</li> </ul> </li> <li> <p>Configure kubeconfig for EKS:</p> <ul> <li>Use the <code>aws eks update-kubeconfig</code> command to configure kubectl to interact with your Amazon EKS clusters.</li> <li>Example: <code>aws eks update-kubeconfig --name aws-teranode-playground --region &lt;region&gt;</code><ul> <li>For example, the staging environment supported regions include <code>ap-south-1</code>, <code>eu-west-1</code>, and <code>us-east-1</code>.</li> </ul> </li> <li>Verify the configuration with <code>kubectl config view</code>. Alternatively, you can verify the raw data in the <code>~/.kube/config</code> file.</li> </ul> </li> <li> <p>Zsh Configuration:</p> <ul> <li>Download the <code>k8s_shortcuts.sh</code> shortcuts file from the shared repository and place it in the home directory.</li> </ul> <pre><code>cp docs/staging/operations/.k8s_shortcuts.sh ~/.k8s_shortcuts.sh\n</code></pre> <ul> <li>Add Kubernetes shortcuts to <code>.zprofile</code> for ease of use.</li> </ul> <pre><code>{\necho \"\"\necho \"# Load k8s shortcuts\"\necho \"autoload -Uz compinit\"\necho \"compinit\"\necho \"source ~/.k8s_shortcuts.sh\"\n} &gt;&gt; ~/.zprofile\n</code></pre> <ul> <li>Please read more about the shortcuts in k8Shortcuts.md.</li> </ul> </li> </ol>"},{"location":"staging/operations/ClusterManagementAndOperations/#deployment-and-scaling","title":"Deployment and Scaling","text":""},{"location":"staging/operations/ClusterManagementAndOperations/#service-start-order","title":"Service Start Order","text":"<p>The Teranode services must be started in a specific order, as follows:</p> <ol> <li>Blockchain Server.</li> <li>Asset Server.</li> <li>Block Validation.</li> <li>Block Assembly.</li> <li>Propagation Server.</li> <li>P2P Service.</li> <li>The rest of services.</li> </ol> <p>Failing to start services in the right order will lead to errors and incorrect behaviour.</p> <p>Service restarts or downtime can cause the node to be left in an inconsistent state. For example, the Block Assembly will miss any transaction received during a downtime, being unable to promptly recover.</p>"},{"location":"staging/operations/ClusterManagementAndOperations/#lustre","title":"Lustre","text":"<p>Lustre is a type of parallel distributed file system, primarily used for large-scale cluster computing. The system is designed to support high-performance, large-scale data storage and workloads, widely used in environments that require fast and efficient access to large volumes of data across many nodes.</p> <p>Teranode microservices make use of the Lustre file system in order to share information related to subtrees, eliminating the need for redundant propagation of subtrees over grpc or message queues.</p> <p></p> <p>As seen in the diagram above, the Block Validation, Block Assembly and Asset Services share a lustre file system storage. The data is ultimately persisted in AWS S3.</p>"},{"location":"staging/operations/ClusterManagementAndOperations/#key-kubernetes-commands-documentation","title":"Key Kubernetes Commands Documentation","text":""},{"location":"staging/operations/ClusterManagementAndOperations/#environment-switching-and-namespace-management","title":"Environment Switching and Namespace Management","text":"<ol> <li> <p>Environment Shortcuts:</p> <ul> <li>Define aliases for switching between environments (e.g., <code>m1</code>, <code>m2</code>, <code>m3</code>) in <code>.zprofile</code>.<ul> <li>Please check with your DevOPS team for the specific environments available to you.</li> </ul> </li> </ul> </li> <li> <p>Namespace Configuration:</p> <ul> <li>Use <code>kcn</code> command to switch Kubernetes namespaces easily.</li> <li>Example:</li> </ul> <pre><code>kcn m1\n</code></pre> </li> </ol>"},{"location":"staging/operations/ClusterManagementAndOperations/#viewing-pods-in-a-specific-cluster","title":"Viewing Pods in a Specific Cluster","text":"<ul> <li><code>kgp</code>: Displays all current services running in a cluster.</li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#listing-all-services","title":"Listing All Services","text":"<ul> <li><code>kgpa</code>: Lists all services across all namespaces.</li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#monitoring-pod-resource-usage","title":"Monitoring Pod Resource Usage","text":"<ul> <li> <p><code>kubectl top pods</code>: Shows the resource usage for pods.</p> </li> <li> <p><code>watch -n 3 kubectl top pods</code>: Runs <code>kubectl top pods</code> every 3 seconds, providing a real-time view of pod resource usage.</p> </li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#starting-services","title":"Starting Services","text":"<ul> <li><code>ksd {service} --replicas=n</code>: Starts a service.</li> </ul> <p>Examples:</p> <ul> <li> <p><code>ksd coinbase1 --replicas=1</code></p> </li> <li> <p><code>ksd propagation1 --replicas=13</code></p> </li> <li> <p><code>ksd tx-blaster1 --replicas=11</code></p> </li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#checking-the-number-of-service-instances","title":"Checking the Number of Service Instances","text":"<ul> <li><code>k get node -L role | grep prop | wc -l</code>: Counts the number of nodes labeled with the role <code>prop</code>, useful for assessing the scale of the propagation service within the cluster.</li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#viewing-logs","title":"Viewing Logs","text":"<ul> <li><code>kl {pod}</code>:</li> </ul> <p>Example:</p> <ul> <li><code>kl tx-blaster1-234234</code>: Views the logs for a specific <code>tx-blaster1</code> pod.</li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#resetting-a-service","title":"Resetting a Service","text":"<ul> <li> <p>To reset a Service:</p> </li> <li> <p>If you're experiencing issues with a service not functioning as expected, you might attempt to delete all pods associated with a namespace to force them to restart. An example can be seen here:</p> <pre><code>kubectl delete pod -n [namespace] --all\n</code></pre> </li> <li> <p>This command deletes all pods in the specified namespace, which should cause them to be recreated based on their deployment or stateful set configurations.</p> </li> <li> <p>To reset a specific service in all namespaces:</p> </li> <li> <p>Example to reset the p2p service can be seen here:</p> <pre><code>kubectl delete pod p2p1\n</code></pre> </li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#viewing-traefik-namespaces","title":"Viewing Traefik Namespaces","text":"<ul> <li><code>kpg -n traefik</code>: To view pods within the Traefik namespace, ensuring traffic management components are running as expected.</li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#accessing-the-shell-for-any-service","title":"Accessing the Shell for any Service","text":"<ul> <li>You can open an interactive terminal (<code>bash</code>) in a specified pod with the <code>keti {pod} -- bash</code> command. For example:</li> </ul> <pre><code># Get the list of pods for the AWS node you are connected to\n$  kubectl get pod\n\n# Access one specific microservices, out of the list of pods obtained in the previous step\n$   keti blockassembly1-7874b7bf7c-668bk --bash\n\n$   ls -ltr\n</code></pre> <p>The results will look similar to the below:</p> <pre><code>root@blockassembly1-7874b7bf7c-668bk:/app# ls -ltr\ntotal 141072\n\n-rwxr-xr-x 1 root root  18768613 Dec  7 23:59 dlv\n-rw-rw-r-- 1 root root         0 Jan 25 00:12 settings.conf\ndrwxr-xr-x 2 root root       116 Jan 25 00:12 certs\n-rw-rw-r-- 1 root root     49314 Jan 29 14:30 settings_local.conf\n-rwxr-xr-x 1 root root 125468128 Jan 29 23:37 teranode.run\nlrwxrwxrwx 1 root root         8 Jan 29 23:38 utxostoreblaster.run -&gt; teranode.run\nlrwxrwxrwx 1 root root         8 Jan 29 23:38 propagationblaster.run -&gt; teranode.run\nlrwxrwxrwx 1 root root         8 Jan 29 23:38 blockassemblyblaster.run -&gt; teranode.run\nlrwxrwxrwx 1 root root         8 Jan 29 23:38 blaster.run -&gt; teranode.run\nlrwxrwxrwx 1 root root         8 Jan 29 23:38 s3blaster.run -&gt; teranode.run\n\n\nlrwxrwxrwx 1 root root         8 Jan 29 23:38 aerospiketest.run -&gt; teranode.run\n</code></pre>"},{"location":"staging/operations/ClusterManagementAndOperations/#managing-persistent-storage","title":"Managing Persistent Storage","text":"<ul> <li> <p><code>k get pv</code>: Lists all Persistent Volumes (PVs) in the cluster, essential for managing storage resources.</p> </li> <li> <p><code>k describe pvc {storage}</code>: Provides detailed information about a Persistent Volume Claim (PVC).</p> </li> </ul> <p>Example:</p> <pre><code># Provides detailed information about the Persistent Volume Claim (PVC) named `subtree-lustre-pv`\nk describe pvc subtree-lustre-pv\n</code></pre>"},{"location":"staging/operations/ClusterManagementAndOperations/#forwarding-ports","title":"Forwarding Ports","text":"<ul> <li> <p>To forward a pod port to localhost:</p> </li> <li> <p>Set the Kubernetes configuration file:</p> <pre><code>export KUBECONFIG=~/.kube/config\n</code></pre> </li> <li> <p>Forward the port:</p> <pre><code>kubectl port-forward asset1-63453452352-23452 8090:8090\n</code></pre> <p>This command forwards the port <code>8090</code> from the specified <code>asset1</code> pod to the local port <code>8090</code>.</p> </li> </ul>"},{"location":"staging/operations/ClusterManagementAndOperations/#overriding-pod-settings","title":"Overriding pod settings","text":"<p>While the different application settings are provided in the <code>settings.conf</code> and <code>settings_local.conf</code>, users can modify them directly in Kubernetes.</p> <p>To do so, you have to edit the {application}.run file for a given pod. The example below shows how to access the settings in a Block Assembly application pod.</p> <pre><code>ked block-assembly-213131 -- vi block-assembly.run\n# Modify the specific config setting, and save\n</code></pre> <p>After a change, the service will automatically restart.</p>"},{"location":"staging/operations/applicationYamlDocumentation/","title":"applicationYamlDocumentation","text":"<p>This Kubernetes Deployment configuration outlines a detailed setup for managing the deployment of a containerized application within a Kubernetes cluster.</p>"},{"location":"staging/operations/applicationYamlDocumentation/#overview","title":"Overview","text":"<ul> <li><code>apiVersion</code>: Specifies the version of the Kubernetes API you're using to create this object.</li> <li><code>kind</code>: The type of Kubernetes object being created; in this case, a <code>Deployment</code>.</li> </ul>"},{"location":"staging/operations/applicationYamlDocumentation/#metadata","title":"Metadata","text":"<ul> <li><code>metadata</code>: Holds metadata about the deployment, including its name and labels for identification and grouping.</li> </ul>"},{"location":"staging/operations/applicationYamlDocumentation/#spec-specification","title":"Spec (Specification)","text":"<ul> <li><code>replicas</code>: Defines the desired number of pod instances. Setting this to 0 indicates no instances should be running initially, which can be scaled up later as needed.</li> <li><code>strategy</code>: Specifies the strategy used to replace old pods with new ones. <code>RollingUpdate</code> is a common strategy that updates pods in a rolling fashion to ensure service availability during the update.</li> <li><code>selector</code>: This field selects the pods that belong to this deployment using labels.</li> <li><code>template</code>: Defines the pods to be created, including their metadata and spec.</li> </ul>"},{"location":"staging/operations/applicationYamlDocumentation/#template-spec","title":"Template Spec","text":"<ul> <li><code>serviceAccountName</code>: Specifies the service account for permissions.</li> <li><code>affinity</code>: Controls pod scheduling preferences to ensure or avoid co-locating pods under certain conditions.</li> <li><code>nodeSelector</code>: Ensures pods are scheduled on nodes with specific labels, used for targeting specific types of nodes for the workload.</li> <li><code>tolerations</code>: Allow (but do not require) the pods to schedule onto nodes with matching taints, enabling finer control over pod placement.</li> <li><code>containers</code>: Defines the container(s) to be run in the pod, including the image to use, commands, arguments, and ports.</li> </ul>"},{"location":"staging/operations/applicationYamlDocumentation/#containers-section","title":"Containers Section","text":"<ul> <li><code>image</code>: Specifies the container image along with its repository and tag.</li> <li><code>command</code> and <code>args</code>: Determine what command and arguments to run inside the container. Overrides the default if specified.</li> <li><code>resources</code>: Defines the resource requirements (memory and CPU) for the container, specifying both requests (minimum needed) and limits (maximum allowed).</li> <li><code>readinessProbe</code> and <code>livenessProbe</code>: Health check configurations to determine when a container is ready to start accepting traffic (<code>readinessProbe</code>) and if it is still running as expected (<code>livenessProbe</code>).</li> </ul>"},{"location":"staging/operations/applicationYamlDocumentation/#probes","title":"Probes","text":"<ul> <li><code>readinessProbe</code>: Ensures the pod does not receive traffic until the probe succeeds.</li> <li><code>livenessProbe</code>: Ensures that if the probe fails, the container will be restarted.</li> </ul>"},{"location":"staging/operations/applicationYamlDocumentation/#role-and-tolerations","title":"Role and Tolerations","text":"<ul> <li>Roles: Used for targeting deployments to specific nodes with certain roles. Nodes can be labeled with roles, and using <code>nodeSelector</code>, deployments can target these nodes.</li> <li>Tolerations: Work in conjunction with taints on nodes to allow (but not require) pods to schedule on nodes with specific taints.</li> </ul>"},{"location":"staging/operations/applicationYamlDocumentation/#volume-mounts","title":"Volume Mounts","text":"<ul> <li>Not explicitly detailed in the provided configuration, but volume mounts allow a pod to store data persistently across pod restarts and share data between containers in the same pod.</li> </ul>"},{"location":"staging/operations/applicationYamlDocumentation/#notes-on-usage","title":"Notes on Usage","text":"<ul> <li><code>replicas</code>: Can be adjusted to scale the application up or down based on demand or maintenance needs.</li> <li><code>nodeSelector</code> and <code>tolerations</code>: Provide mechanisms for influencing where pods should or shouldn't be scheduled based on node labels and taints, which is crucial for optimizing resource usage and access to specific node features.</li> <li><code>resources</code>: Important for ensuring that the application has sufficient resources to run efficiently while also imposing limits to prevent it from consuming excessive cluster resources.</li> <li><code>readinessProbe</code> and <code>livenessProbe</code>: Critical for managing the lifecycle of pods, ensuring they are only serving traffic when ready and are restarted if they become unhealthy.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/","title":"Kubernetes Command Runbook Reference","text":""},{"location":"staging/operations/k8Shortcuts/#introduction","title":"Introduction","text":"<p>This runbook serves as a comprehensive reference for managing Kubernetes resources using a set of aliases, functions, and configurations intended to streamline daily operations. It assumes the user has basic knowledge of Kubernetes (<code>kubectl</code>) and shell scripting.</p>"},{"location":"staging/operations/k8Shortcuts/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>kubectl</code> installed and accessible from the command line.</li> <li>Zsh shell with access to <code>autoload</code> and <code>compinit</code> for command completion.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#aliases","title":"Aliases","text":""},{"location":"staging/operations/k8Shortcuts/#general-shortcuts","title":"General Shortcuts","text":"<ul> <li><code>k</code>: Alias for <code>kubectl</code>.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#applying-and-managing-kubernetes-resources","title":"Applying and Managing Kubernetes Resources","text":"<ul> <li><code>kaf</code>: Stands for \"kubectl apply -f\". This command applies a configuration file or directory of files to your cluster, creating or updating resources.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#executing-commands-across-all-namespaces","title":"Executing Commands Across All Namespaces","text":"<ul> <li><code>kca</code>: A function that wraps <code>kubectl</code> commands to run them against all namespaces. Useful for operations that need to be performed cluster-wide, without specifying a namespace each time.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#interactive-terminal-access","title":"Interactive Terminal Access","text":"<ul> <li><code>keti</code>: Stands for \"kubectl exec -t -i\". This command is used to execute an interactive terminal session on a container within a pod. It's invaluable for debugging or managing applications directly within their running environment.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#configuration-and-context-management","title":"Configuration and Context Management","text":"<ul> <li><code>kcuc</code>: \"kubectl config use-context\". Switches the current kubectl context, effectively changing the cluster you're interacting with.</li> <li><code>kcsc</code>: \"kubectl config set-context\". Modifies kubeconfig files, which can alter how kubectl connects to clusters.</li> <li><code>kcdc</code>: \"kubectl config delete-context\". Removes a specified context from the kubeconfig file.</li> <li><code>kccc</code>: \"kubectl config current-context\". Displays the current context without changing it.</li> <li><code>kcgc</code>: \"kubectl config get-contexts\". Lists all contexts saved in the kubeconfig file.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#resource-deletion","title":"Resource Deletion","text":"<ul> <li><code>kdel</code>, <code>kdelf</code>: Shortcuts for deleting Kubernetes resources. The former deletes resources by name, and the latter deletes resources defined in a file.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#pod-management","title":"Pod Management","text":"<ul> <li><code>kgp</code>: \"kubectl get pods\". Lists pods in the current namespace.</li> <li><code>kgpa</code>: Lists pods across all namespaces.</li> <li><code>kgpw</code>, <code>kgpwide</code>: Variants of <code>kgp</code> that watch for changes in real-time or display extended information, respectively.</li> <li><code>kep</code>, <code>kdp</code>, <code>kdelp</code>: Edit, describe, or delete pods. These commands are essential for pod lifecycle management.</li> <li><code>kgpl</code>, <code>kgpn</code>: Get pods by label or namespace, providing a filtered view of pods based on specific criteria.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#service-and-ingress-management","title":"Service and Ingress Management","text":"<ul> <li><code>kgs</code>, <code>kgsa</code>: List services in the current or all namespaces.</li> <li><code>kes</code>, <code>kds</code>, <code>kdels</code>: Edit, describe, or delete services.</li> <li><code>kgi</code>, <code>kgia</code>: List ingress resources, crucial for managing access to services from outside the cluster.</li> <li><code>kei</code>, <code>kdi</code>, <code>kdeli</code>: Edit, describe, or delete ingress resources.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#namespace-and-configuration-management","title":"Namespace and Configuration Management","text":"<ul> <li><code>kgns</code>: \"kubectl get namespaces\". Lists all namespaces in the cluster.</li> <li><code>kens</code>, <code>kdns</code>, <code>kdelns</code>: Edit, describe, or delete namespaces.</li> <li><code>kgcm</code>, <code>kgcma</code>: List ConfigMaps, which are key-value pairs used for configuration.</li> <li><code>kecm</code>, <code>kdcm</code>, <code>kdelcm</code>: Edit, describe, or delete ConfigMaps.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#secret-management","title":"Secret Management","text":"<ul> <li><code>kgsec</code>, <code>kgseca</code>: List secrets, which store sensitive data like passwords or tokens.</li> <li><code>kdsec</code>, <code>kdelsec</code>: Describe or delete secrets.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#deployment-statefulset-and-daemonset-management","title":"Deployment, StatefulSet, and DaemonSet Management","text":"<ul> <li><code>kgd</code>, <code>kgda</code>: List deployments, a key resource for managing applications.</li> <li><code>ked</code>, <code>kdd</code>, <code>kdeld</code>: Edit, describe, or delete deployments.</li> <li>Similar patterns apply for StatefulSets (<code>kgss</code>, <code>kess</code>, <code>kdss</code>, <code>kdelss</code>) and DaemonSets (<code>kgds</code>, <code>keds</code>, <code>kdds</code>, <code>kdelds</code>), which manage stateful applications and node-level services, respectively.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#rollout-and-debugging","title":"Rollout and Debugging","text":"<ul> <li><code>kpf</code>: \"kubectl port-forward\". Forwards one or more local ports to a pod, facilitating debugging and local access.</li> <li><code>kl</code>, <code>klf</code>: View logs of a pod, with the <code>-f</code> flag tailing the log output.</li> <li><code>kcp</code>: \"kubectl cp\". Copies files and directories to and from containers in pods.</li> <li><code>kgno</code>, <code>keno</code>, <code>kdno</code>, <code>kdelno</code>: Manage and get information about cluster nodes.</li> </ul>"},{"location":"staging/operations/k8Shortcuts/#advanced-functionality","title":"Advanced Functionality","text":"<ul> <li><code>kres</code>: A custom function to refresh an environment variable across resources, showcasing</li> </ul> <p>the flexibility of using shell functions to extend <code>kubectl</code>'s capabilities.</p> <p>These aliases and functions are designed to streamline Kubernetes workflows, making daily tasks more efficient and reducing the cognitive load of remembering complex command syntax.</p>"},{"location":"topics/functionalRequirementTests/","title":"QA Guide &amp; Instructions for Functional Requirement Tests","text":""},{"location":"topics/functionalRequirementTests/#scope","title":"Scope","text":"<p>This document explains how tests are structured within the TERANODE codebase and how they are organized within the test folder in relation to the Functional Requirements for Teranode document.</p> <p>The Functional Requirements for Teranode document describes the responsibilities and features of Teranode. Accordingly, the tests are organized to verify that these responsibilities and features are correctly implemented in the code.</p>"},{"location":"topics/functionalRequirementTests/#structure","title":"Structure","text":"<p>The functional requirements are organized into the following categories:</p> <ul> <li>Node responsibilities (TNA)</li> <li>Receive and validate transactions (TNB)</li> <li>Assemble blocks (TNC)</li> <li>Propagate blocks to the network (TND)</li> <li>Receive and validate blocks (TNE)</li> <li>Keep track of the longest honest chain (TNF)</li> <li>Store the blockchain (TNG)</li> <li>Store the UTXO set (TNH)</li> <li>Exchange information with mining pool software (TNI)</li> <li>Adherence to Standard Policies (Consensus Rules) (TNJ)</li> <li>Ability to define Local Policies (TNK)</li> <li>Alert System Functionality (TNL)</li> <li>Joining the Network (TNM)</li> <li>Technical Errors (TEC)</li> </ul>"},{"location":"topics/functionalRequirementTests/#test-organization","title":"Test Organization","text":"<p>Tests are organized in two main locations:</p> <ol> <li> <p>Dedicated test folders (<code>/test/[category]/</code>):</p> </li> <li> <p><code>/test/tna/</code> - Node responsibilities tests</p> </li> <li><code>/test/tnb/</code> - Transaction validation tests</li> <li><code>/test/tnd/</code> - Block propagation tests</li> <li><code>/test/tnf/</code> - Longest chain tracking tests</li> <li><code>/test/tnj/</code> - Consensus rules tests</li> <li> <p><code>/test/tec/</code> - Error handling tests</p> </li> <li> <p>Integration tests (<code>/test/e2e/daemon/</code>):</p> </li> <li> <p><code>tnc*_test.go</code> - Block assembly tests</p> </li> <li><code>tne*_test.go</code> - Block validation tests</li> <li>Additional integration tests for various requirements</li> </ol> <p>Test files follow a naming convention: <code>tn[x][number]_test.go</code> where x is the category letter and number corresponds to the specific requirement.</p>"},{"location":"topics/functionalRequirementTests/#test-framework","title":"Test Framework","text":"<p>The test framework is built on the testify suite package and is defined primarily in <code>test/utils/arrange.go</code> and <code>test/utils/testenv.go</code>. These files contain the main structs and functions used to set up, run, and control the execution of the test suites.</p>"},{"location":"topics/functionalRequirementTests/#test-suites-and-setup","title":"Test Suites and Setup","text":"<p>The test framework provides two main components:</p> <ul> <li>TeranodeTestSuite (<code>test/utils/arrange.go</code>): Base test suite that embeds testify's suite.Suite and provides common test setup/teardown</li> <li>TeranodeTestEnv (<code>test/utils/testenv.go</code>): Manages the test environment including Docker Compose stacks, node clients, and service connections</li> </ul> <p>Each test suite can customize its configuration through the TConfig system, which defines settings contexts and Docker Compose files to use when running tests.</p>"},{"location":"topics/functionalRequirementTests/#test-execution","title":"Test Execution","text":"<p>Prerequisites:</p> <ul> <li>Docker must be running</li> <li>Docker compose must be installed</li> </ul> <p>To execute a given test suite (e.g., TNA, TNB, TNJ), use the following standard command template from the terminal:</p> <pre><code>cd /teranode/test/&lt;test-suite-folder&gt;\ngo test -v -tags test_tn[x]\n</code></pre> <p>Where [x] should be replaced with the letter corresponding to the desired suite. For example:</p> <pre><code># For TNA tests\ncd /teranode/test/tna\ngo test -v -tags test_tna\n\n# For TNJ tests\ncd /teranode/test/tnj\ngo test -v -tags test_tnj\n</code></pre> <p>Note: Some tests don't require build tags and can be run directly:</p> <pre><code># For TNC tests in e2e/daemon\ncd /teranode/test/e2e/daemon\ngo test -v -run \"^TestCoinbaseTXAmount$\"\n</code></pre> <p>To execute a single test, use:</p> <pre><code># For tests using testify suites\ngo test -v -run \"^&lt;TestSuiteName&gt;$/^&lt;TestName&gt;$\" -tags &lt;build_tag&gt;\n\n# For standalone tests\ngo test -v -run \"^&lt;TestName&gt;$\"\n</code></pre> <p>For example:</p> <pre><code># Suite-based test (TNA)\ngo test -v -run \"^TestTNA1TestSuite$/TestBroadcastNewTxAllNodes$\" -tags test_tna ./test/tna/tna1_test.go\n\n# Standalone test (TNC in e2e/daemon)\ngo test -v -run \"^TestCoinbaseTXAmount$\" ./test/e2e/daemon/tnc1_3_test.go\n</code></pre>"},{"location":"topics/functionalRequirementTests/#tna","title":"TNA","text":"<p>As outlined in the Functional Requirements for Teranode reference document, the tests in the <code>/test/tna</code> folder verify the essential conditions that a node must follow in order to be part of the BSV network.</p> <p>The naming convention for files in this folder is as follows:</p> <p><code>tna_&lt;number&gt;.go</code> corresponds to the TNA- test in the Functional Requirements for Teranode document. <p>For example:</p> <ul> <li><code>tna1_test.go</code> covers TNA-1: <code>Teranode must broadcast new transactions to all nodes (although new transaction broadcasts do not necessarily need to reach all nodes)</code>.</li> <li> <p>Implementation: The test starts three distinct Teranode instances in Docker containers. It sends 35 transactions to the first node and verifies transaction propagation by checking that at least one of these transactions appears in subtree notifications received from the network, confirming the broadcast functionality.</p> </li> <li> <p><code>tna2_test.go</code> covers TNA-2: <code>Teranode collects new transactions into a block</code>.</p> </li> <li> <p>Implementation: The test starts three Teranode instances in Docker containers and runs three sub-tests to verify transaction collection: (1) single transaction propagation - sending one transaction and verifying it appears in all nodes' block assembly, (2) multiple transaction propagation - sending 5 sequential transactions and confirming all appear in all nodes' block assembly, and (3) concurrent transaction propagation - sending 10 simultaneous transactions and verifying their presence in block assembly on all nodes.</p> </li> <li> <p><code>tna4_test.go</code> covers TNA-4: <code>Teranode must broadcast the block to all nodes when it finds a proof-of-work</code>.</p> </li> <li> <p>Implementation: The test configures three Teranode nodes and sets up notification listeners on two receiving nodes. It sends transactions to the first node, mines a block containing these transactions, and then verifies that all receiving nodes get block notifications with the correct block hash. It also confirms that each node successfully stores the block in its blockchain database, proving complete block propagation across the network.</p> </li> <li> <p>TNA-5: <code>Teranode must only accept the block if all transactions in it are valid and not already spent</code></p> </li> <li> <p>Implementation: This requirement is covered extensively by the <code>test/sequentialtest/double_spend/double_spend_test.go</code> file, which contains comprehensive tests for double-spend detection and handling across multiple storage backends (SQLite, Postgres, and Aerospike). These tests verify:</p> <ol> <li>Single Double-Spend Detection: Tests that Teranode can detect and properly handle simple double-spend attempts.</li> <li>Multiple Conflicting Transactions: Verifies detection of multiple transactions conflicting with each other across different blocks.</li> <li>Transaction Chain Conflicts: Tests handling of entire chains of transactions that conflict with each other.</li> <li>Double-Spend Forks: Tests complex scenarios with competing chain forks containing conflicting transactions.</li> <li>Chain Reorganization: Verifies proper handling of transaction status during chain reorganizations.</li> <li>Triple-Forked Chains: Tests advanced scenarios with three competing chain forks.</li> <li>Nested Transaction Conflicts: Ensures correct handling of nested transaction dependencies in double-spend scenarios.</li> <li>Frozen Transaction Handling: Tests interaction of double-spends with frozen transactions.</li> </ol> <p>The tests confirm that Teranode correctly identifies double-spend attempts, properly marks conflicting transactions, and maintains UTXO integrity throughout chain reorganizations. Additional transaction validity checks are provided by the Block Development Kit (BDK) tests.   - These tests collectively ensure that Teranode only accepts blocks containing valid, non-double-spent transactions, fulfilling the TNA-5 requirement.</p> </li> <li> <p><code>tna6_test.go</code> covers TNA-6: <code>Teranode must express its acceptance of a block by working on creating the next block in the chain, using the hash of the accepted block as the previous hash</code>.</p> </li> <li>Implementation: The test instantiates three Teranode nodes, mines a block on the first node, and then verifies that the block is accepted by checking that it becomes the best block. It then retrieves a mining candidate from each node and confirms that they all use the accepted block's hash as their previous hash, demonstrating that all nodes are building on top of the accepted block.</li> </ul>"},{"location":"topics/functionalRequirementTests/#running-tna-tests","title":"Running TNA Tests","text":""},{"location":"topics/functionalRequirementTests/#running-all-tna-tests","title":"Running All TNA Tests","text":"<pre><code>cd /teranode/test/tna\ngo test -v -tags test_tna\n</code></pre>"},{"location":"topics/functionalRequirementTests/#running-specific-tna-tests","title":"Running Specific TNA Tests","text":"<pre><code>cd /teranode/test/tna\ngo test -v -run \"^TestTNA1TestSuite$/TestTNA1$\"\n</code></pre> <p>Examples for each TNA test:</p> <ul> <li>For TNA-1 (transaction broadcasting):</li> </ul> <pre><code>cd /teranode/test/tna\ngo test -v -run \"^TestTNA1TestSuite$/TestBroadcastNewTxAllNodes$\" -tags test_tna\n</code></pre> <ul> <li>For TNA-2 (transaction collection):</li> </ul> <pre><code>cd /teranode/test/tna\ngo test -v -run \"^TestTNA2TestSuite$/TestTxsReceivedAllNodes$\" -tags test_tna\n</code></pre> <ul> <li>For TNA-4 (block broadcasting):</li> </ul> <pre><code>cd /teranode/test/tna\ngo test -v -run \"^TestTNA4TestSuite$/TestBlockBroadcast$\" -tags test_tna\n</code></pre> <ul> <li>For TNA-6 (block acceptance):</li> </ul> <pre><code>cd /teranode/test/tna\ngo test -v -run \"^TestTNA6TestSuite$/TestAcceptanceNextBlock$\" -tags test_tna\n</code></pre>"},{"location":"topics/functionalRequirementTests/#tnb","title":"TNB","text":"<p>As outlined in the Functional Requirements for Teranode reference document, the tests in the <code>/test/tnb</code> folder verify that transactions are correctly validated and processed within Teranode. These tests ensure that transaction validation rules are correctly enforced and that the UTXO set is properly maintained.</p> <p>The naming convention for files in this folder is as follows:</p> <p><code>tnb&lt;number&gt;_test.go</code> corresponds to the TNB- test in the Functional Requirements for Teranode document."},{"location":"topics/functionalRequirementTests/#tnb1-test-description","title":"TNB1 Test Description","text":""},{"location":"topics/functionalRequirementTests/#tnb-1-transaction-processing","title":"TNB-1: Transaction Processing","text":"<p><code>tnb1_test.go</code> covers TNB-1: Teranode must receive transactions sent by any node and add them to its mempool if they are valid.</p> <ul> <li>Implementation: The test, <code>TestSendTxsInBatch</code>, creates and sends a batch of transactions concurrently to a Teranode node. It then subscribes to blockchain notifications to receive subtree notifications, retrieves transaction hashes from the subtree, and verifies that all sent transactions are included in the subtree. The test ensures that transactions are correctly received, validated, and included in mining candidates, demonstrating that Teranode properly processes transactions in batches and maintains them for block assembly.</li> </ul>"},{"location":"topics/functionalRequirementTests/#tnb2-test-description","title":"TNB2 Test Description","text":""},{"location":"topics/functionalRequirementTests/#tnb-2-transaction-validation","title":"TNB-2: Transaction Validation","text":"<p><code>tnb2_test.go</code> covers TNB-2: Teranode must validate transactions to ensure they follow consensus rules.</p> <ul> <li>Implementation: The test includes multiple test cases:</li> <li>TestUTXOValidation: Verifies that Teranode correctly validates transaction inputs against the UTXO set by creating a transaction using coinbase funds, sending it to a node, mining a block to confirm the transaction, and then attempting to double-spend the same UTXO. The test ensures that the double-spend is rejected, confirming that Teranode properly tracks and validates UTXO spending.</li> <li>TestScriptValidation: Validates that Teranode correctly verifies transaction scripts by creating both valid and invalid signature scenarios. It creates a transaction with an invalid signature (using a different private key than the one that controls the UTXO) and verifies that Teranode rejects it, ensuring proper script execution and validation.</li> </ul>"},{"location":"topics/functionalRequirementTests/#additional-network-policy-and-integration-tests","title":"Additional Network Policy and Integration Tests","text":"<p>In addition to the functional test suite, there are two important test files in the <code>services/validator</code> directory that provide comprehensive testing of transaction validation:</p> <ol> <li> <p>TxValidator_test.go: Tests transaction validation network policies:</p> <ul> <li>MaxTxSizePolicy: Tests that transactions exceeding the maximum size are rejected.</li> <li>MaxOpsPerScriptPolicy: Verifies enforcement of operation count limits in scripts.</li> <li>MaxScriptSizePolicy: Ensures scripts exceeding the maximum size are rejected.</li> <li>MaxTxSigopsCountsPolicy: Tests enforcement of signature operation count limits.</li> <li>MinFeePolicy: Validates that transactions with insufficient fees are rejected based on their size and the presence of OP_RETURN data.</li> </ul> </li> <li> <p>Validator_test.go: Tests integration of the validator with other Teranode components:</p> <ul> <li>TestValidate_CoinbaseTransaction: Verifies correct handling of coinbase transactions.</li> <li>TestValidate_BlockAssemblyAndTxMetaChannels: Tests the integration between transaction validation and block assembly.</li> <li>TestValidate_RejectedTransactionChannel: Ensures rejected transactions are properly handled and reported.</li> <li>TestValidate_BlockAssemblyError: Verifies proper error handling during the block assembly process.</li> <li>Transaction-specific tests: Contains tests for several specific real-world transactions to ensure they validate correctly, including edge cases like non-zero OP_RETURN outputs and complex script execution.</li> </ul> </li> </ol> <p>Together, these tests ensure that Teranode maintains proper network health by enforcing transaction validation rules and correctly integrating with the block assembly process, UTXO management, and Kafka messaging. They provide critical protection against potential denial-of-service vectors and ensure that transactions are properly processed throughout the entire validation pipeline.</p>"},{"location":"topics/functionalRequirementTests/#tnb6-test-description","title":"TNB6 Test Description","text":""},{"location":"topics/functionalRequirementTests/#tnb-6-utxo-set-management","title":"TNB-6: UTXO Set Management","text":"<p><code>tnb6_test.go</code> covers TNB-6: All outputs of validated transactions must be added to Teranode's unspent transaction output (UTXO) set.</p> <ul> <li>Implementation: The <code>TestUnspentTransactionOutputs</code> test case verifies proper UTXO set management by creating a transaction with multiple outputs, sending it to a node, and then verifying that all outputs are correctly added to the UTXO set. The test checks UTXO metadata (amounts, scripts) and verifies the spending status of each output, ensuring that the UTXO store correctly maintains all transaction outputs for future spending.</li> </ul>"},{"location":"topics/functionalRequirementTests/#tnb7-test-description","title":"TNB7 Test Description","text":""},{"location":"topics/functionalRequirementTests/#tnb-7-transaction-input-spending","title":"TNB-7: Transaction Input Spending","text":"<p><code>tnb7_test.go</code> covers TNB-7: All inputs of validated transactions must be marked as spent in Teranode's UTXO set.</p> <ul> <li>Implementation: The <code>TestValidatedTxShouldSpendInputs</code> test case creates a transaction that spends a coinbase output, sends it to a node, and then verifies that the input is correctly marked as spent in the UTXO store. It checks the spending status and confirms that the spending transaction ID is correctly recorded, ensuring that Teranode properly updates the UTXO set when transactions are processed.</li> </ul>"},{"location":"topics/functionalRequirementTests/#running-tnb-tests","title":"Running TNB Tests","text":""},{"location":"topics/functionalRequirementTests/#running-all-tnb-tests","title":"Running All TNB Tests","text":"<pre><code>cd /teranode/test/tnb\ngo test -v -tags test_tnb\n</code></pre>"},{"location":"topics/functionalRequirementTests/#running-specific-tnb-tests","title":"Running Specific TNB Tests","text":"<pre><code>cd /teranode/test/tnb\ngo test -v -run \"^TestTNB1TestSuite$/TestSendTxsInBatch$\" -tags test_tnb\n</code></pre> <p>Examples for each TNB test:</p> <ul> <li>For TNB-1 (transaction processing):</li> </ul> <pre><code>cd /teranode/test/tnb\ngo test -v -run \"^TestTNB1TestSuite$/TestSendTxsInBatch$\" -tags test_tnb\n</code></pre> <ul> <li>For TNB-2 (transaction validation):</li> </ul> <pre><code>cd /teranode/test/tnb\ngo test -v -run \"^TestTNB2TestSuite$/TestUTXOValidation$\" -tags test_tnb\n</code></pre> <ul> <li>For TNB-6 (UTXO set management):</li> </ul> <pre><code>cd /teranode/test/tnb\ngo test -v -run \"^TestTNB6TestSuite$/TestUnspentTransactionOutputs$\" -tags test_tnb\n</code></pre> <ul> <li>For TNB-7 (transaction input spending):</li> </ul> <pre><code>cd /teranode/test/tnb\ngo test -v -run \"^TestTNB7TestSuite$/TestValidatedTxShouldSpendInputs$\" -tags test_tnb\n</code></pre>"},{"location":"topics/functionalRequirementTests/#tnc","title":"TNC","text":"<p>As outlined in the Functional Requirements for Teranode reference document, the TNC tests verify that transactions are correctly assembled into Merkle tree structures and consequently organized into valid blocks to propagate to mining pool software.</p> <p>Note: TNC tests are located in <code>/test/e2e/daemon/</code> rather than a dedicated <code>/test/tnc/</code> folder, as they require full daemon integration.</p> <p>The naming convention for these test files is:</p> <p><code>tnc&lt;number&gt;_&lt;subnumber&gt;_test.go</code> corresponds to the TNC-. test in the Functional Requirements for Teranode document."},{"location":"topics/functionalRequirementTests/#tnc1-test-description","title":"TNC1 Test Description","text":""},{"location":"topics/functionalRequirementTests/#tnc-11-merkle-root-calculation","title":"TNC-1.1: Merkle Root Calculation","text":"<p><code>tnc1_1_test.go</code> covers TNC-1.1: Teranode must calculate the Merkle root of all the transactions included in the candidate block.</p> <ul> <li>Implementation: The test verifies the Merkle root calculation by first obtaining a mining candidate with no additional transactions. It then mines a block and retrieves the best block. The test validates that the block properly validates its subtrees and that the Merkle root is correctly calculated. This ensures that Teranode is capable of properly calculating the Merkle root for blocks, even in the simplest case of a coinbase-only block.</li> </ul>"},{"location":"topics/functionalRequirementTests/#tnc-12-previous-block-hash-reference","title":"TNC-1.2: Previous Block Hash Reference","text":"<p><code>tnc1_2_test.go</code> covers TNC-1.2: Teranode must refer to the hash of the previous block upon which the candidate block is being built.</p> <ul> <li>Implementation: The test consists of two test cases:</li> <li>TestCheckPrevBlockHash: This test sends transactions, mines a block, and then verifies that the mining candidate's previous hash reference matches the current best block hash. This ensures that new blocks are properly built on top of the current chain.</li> <li>TestPrevBlockHashAfterReorg: This test creates a chain reorganization scenario by mining blocks on different nodes to create a longer chain. It then verifies that the mining candidate from the first node correctly updates to reference the tip of the longer chain. This confirms that Teranode properly handles chain reorganizations when building candidate blocks.</li> </ul>"},{"location":"topics/functionalRequirementTests/#tnc-13-coinbase-transaction","title":"TNC-1.3: Coinbase Transaction","text":"<p><code>tnc1_3_test.go</code> covers TNC-1.3: Teranode must add a Coinbase transaction in the block, the amount of this transaction corresponding to the sum of the block reward and the sum of all transaction fees.</p> <ul> <li>Implementation: The test contains multiple test cases:</li> <li>TestCandidateContainsAllTxs: This test subscribes to blockchain notifications, sends multiple transactions, and verifies that the mining candidate contains all these transactions by checking subtree notifications and comparing Merkle proofs across different nodes.</li> <li>TestCheckHashPrevBlockCandidate: A test that verifies the previous block hash is correctly referenced in the candidate block.</li> <li>TestCoinbaseTXAmount (TNC-1.3-TC-01): This test case specifically verifies the balance between the mining candidate coinbase value and the total output satoshis of the coinbase transaction, ensuring that the block reward is correctly calculated.</li> <li>TestCoinbaseTXAmount2 (TNC-1.3-TC-02): This test case focuses on fee calculation, ensuring that the fees from all transactions are correctly included in the coinbase amount.</li> </ul>"},{"location":"topics/functionalRequirementTests/#tnc2-test-description","title":"TNC2 Test Description","text":""},{"location":"topics/functionalRequirementTests/#tnc-21-unique-candidate-identifiers","title":"TNC-2.1: Unique Candidate Identifiers","text":"<p><code>tnc2_1_test.go</code> covers TNC-2.1: Each candidate block must have a unique identifier.</p> <ul> <li>Implementation: The test contains two test cases:</li> <li>TestUniqueCandidateIdentifiers: This test obtains multiple mining candidates and verifies they have unique identifiers, both within the same height and across different block heights after mining. This ensures that each candidate has a proper identifier for tracking.</li> <li>TestConcurrentCandidateIdentifiers: This test performs concurrent requests for mining candidates and verifies that all returned candidates have unique identifiers even under load. It creates 10 concurrent requests and checks that all returned IDs are unique, ensuring the candidate generation process maintains uniqueness under concurrent conditions.</li> </ul>"},{"location":"topics/functionalRequirementTests/#additional-block-assembly-system-tests","title":"Additional Block Assembly System Tests","text":"<p>In addition to the functional test suite, <code>services/blockassembly/blockassembly_system_test.go</code> provides comprehensive system-level testing that covers aspects of both TNC and TNA requirements. These tests examine the block assembly process in an integrated system environment:</p> <ul> <li> <p>Test_CoinbaseSubsidyHeight: Verifies correct coinbase subsidy calculation at different block heights, ensuring proper reward halving and fee handling.</p> </li> <li> <p>TestDifficultyAdjustment: Tests the difficulty adjustment mechanism to ensure blocks maintain the proper difficulty target.</p> </li> <li> <p>TestShouldFollowLongerChain: Verifies chain selection logic, ensuring the node correctly follows the chain with the most proof-of-work.</p> </li> <li> <p>TestShouldFollowChainWithMoreChainwork: Explicitly tests TNA-3 (proof-of-work) by verifying that the node properly calculates and follows the chain with the most accumulated work.</p> </li> <li> <p>TestShouldAddSubtreesToLongerChain: Tests both TNA-3 (proof-of-work) and TNA-6 (block acceptance), verifying that subtrees are correctly added to the chain and that blocks reference the proper previous hash.</p> </li> <li> <p>TestShouldHandleReorg and TestShouldHandleReorgWithLongerChain: Verify blockchain reorganization handling, ensuring the node can properly reorganize its chain when a better chain appears.</p> </li> <li> <p>TestShouldFailCoinbaseArbitraryTextTooLong: Tests validation of coinbase size policy, ensuring blocks with oversized coinbase transactions are rejected.</p> </li> <li> <p>TestReset: Verifies that the block assembler can properly reset its state and continue operation after a system event.</p> </li> </ul> <p>These system tests provide a more integrated view of how block assembly functions within the complete Teranode system, complementing the more focused functional tests in the TNC directory.</p>"},{"location":"topics/functionalRequirementTests/#additional-unit-tests-for-block-assembly","title":"Additional Unit Tests for Block Assembly","text":"<p>In addition to the TNC functional tests and system tests, <code>services/blockassembly/BlockAssembler_test.go</code> provides unit-level testing for the core block assembly component, covering key TNC requirements:</p> <ul> <li> <p>TestBlockAssembly_GetMiningCandidate: Tests the creation of mining candidates with proper previous block hash references (TNC-1.2), correct Merkle proof/root calculation (TNC-1.1), and accurate coinbase value calculation including transaction fees (TNC-1.3).</p> </li> <li> <p>TestBlockAssembly_ShouldNotAllowMoreThanOneCoinbaseTx: Verifies that the block assembler properly enforces the rule that only one coinbase transaction is allowed per block.</p> </li> <li> <p>TestBlockAssembly_GetMiningCandidate_MaxBlockSize: Tests block size constraints and transaction selection when assembling candidate blocks.</p> </li> </ul> <p>These unit tests provide additional verification of the core block assembly functionality at a lower level than the functional tests, ensuring that the individual components work correctly before they're integrated into the complete system.</p>"},{"location":"topics/functionalRequirementTests/#tnd","title":"TND","text":"<p>As outlined in the Functional Requirements for Teranode reference document, the tests in the <code>/test/tnd</code> folder verify the block propagation functionality between nodes in the network.</p> <p>The naming convention for files in this folder is as follows:</p> <p><code>tnd&lt;number&gt;_&lt;subnumber&gt;_test.go</code> corresponds to the TND-. test in the Functional Requirements for Teranode document."},{"location":"topics/functionalRequirementTests/#tnd1-test-description","title":"TND1 Test Description","text":""},{"location":"topics/functionalRequirementTests/#tnd-11-block-propagation","title":"TND-1.1: Block Propagation","text":"<p><code>tnd1_1_test.go</code> covers TND-1.1: Teranode must propagate blocks to all connected nodes after they are mined.</p> <ul> <li>Implementation: The test establishes a three-node Teranode network and verifies block propagation functionality. It first mines a block on one node and then checks that the block hash is properly propagated to all connected nodes through the network. The test confirms this by retrieving the best block header from each node and verifying they all have the same block hash, proving that block propagation functions correctly across the network.</li> </ul>"},{"location":"topics/functionalRequirementTests/#running-tnd-tests","title":"Running TND Tests","text":""},{"location":"topics/functionalRequirementTests/#running-all-tnd-tests","title":"Running All TND Tests","text":"<pre><code>cd /teranode/test/tnd\ngo test -v -tags test_tnd\n</code></pre>"},{"location":"topics/functionalRequirementTests/#running-specific-tnd-tests","title":"Running Specific TND Tests","text":"<pre><code>cd /teranode/test/tnd\ngo test -v -run \"^TestTND1TestSuite$/TestTND1_1$\" -tags test_tnd\n</code></pre>"},{"location":"topics/functionalRequirementTests/#tne","title":"TNE","text":"<p>As outlined in the Functional Requirements for Teranode reference document, the TNE tests verify that nodes correctly validate blocks received from the network and optimize verification processes.</p> <p>Note: TNE tests are located in <code>/test/e2e/daemon/</code> rather than a dedicated <code>/test/tne/</code> folder, as they require full daemon integration.</p> <p>The naming convention for these test files is:</p> <p><code>tne&lt;number&gt;_&lt;subnumber&gt;_test.go</code> corresponds to the TNE-. test in the Functional Requirements for Teranode document."},{"location":"topics/functionalRequirementTests/#tne1-test-description","title":"TNE1 Test Description","text":""},{"location":"topics/functionalRequirementTests/#tne-11-optimized-transaction-verification","title":"TNE-1.1: Optimized Transaction Verification","text":"<p><code>tne1_1_test.go</code> covers TNE-1.1: Teranode must not re-verify transactions that it has already verified.</p> <ul> <li>Implementation: The test establishes a Teranode network and verifies that transactions are not re-verified once they've been processed. It sends a set of transactions to a node, confirms they are processed, and then mines blocks containing these transactions. The test then verifies that when these transactions appear in blocks, they aren't re-verified by examining internal metrics and logs. This ensures efficiency in block processing, particularly when handling blocks that contain transactions already present in the node's mempool.</li> </ul>"},{"location":"topics/functionalRequirementTests/#running-tne-tests","title":"Running TNE Tests","text":""},{"location":"topics/functionalRequirementTests/#running-tne-tests_1","title":"Running TNE Tests","text":"<pre><code>cd /teranode/test/e2e/daemon\n# Run specific TNE test\ngo test -v -run \"^TestTNE1_1$\" tne1_1_test.go\n</code></pre> <p>Note: TNE tests are integration tests in the e2e/daemon directory and typically don't require build tags.</p>"},{"location":"topics/functionalRequirementTests/#tnf","title":"TNF","text":"<p>As outlined in the Functional Requirements for Teranode reference document, the tests in the <code>/test/tnf</code> folder verify that Teranode correctly tracks and maintains the longest honest chain, including handling chain reorganizations and block invalidation.</p> <p>The naming convention for files in this folder is as follows:</p> <p><code>tnf&lt;number&gt;_test.go</code> corresponds to the TNF- test in the Functional Requirements for Teranode document."},{"location":"topics/functionalRequirementTests/#tnf6-test-description","title":"TNF6 Test Description","text":""},{"location":"topics/functionalRequirementTests/#tnf-6-block-invalidation","title":"TNF-6: Block Invalidation","text":"<p><code>tnf6_test.go</code> covers TNF-6: Teranode must allow for manual invalidation of blocks to handle potentially malicious or invalid chains.</p> <ul> <li>Implementation: The test establishes a three-node Teranode network to verify block invalidation functionality. It first ensures all nodes are synchronized on the same chain, then restarts one node with modified settings to trigger a specific blockchain state. The test then identifies blocks mined by a specific miner and uses the <code>InvalidateBlock</code> API to manually invalidate those blocks across all nodes. After invalidation, it verifies that the best block on all nodes has changed and no longer references the invalidated miner, confirming that Teranode properly handles manual block invalidation. This capability is crucial for network resilience against potentially malicious chains.</li> </ul>"},{"location":"topics/functionalRequirementTests/#additional-chain-reorganization-tests","title":"Additional Chain Reorganization Tests","text":"<p>In addition to the dedicated TNF tests, the <code>test/e2e/daemon/reorg_test.go</code> file provides more comprehensive e2e tests for blockchain reorganization handling, which is a key aspect of keeping track of the longest honest chain:</p> <ul> <li> <p>TestMoveUp: Tests block propagation between nodes and verifies that a newly generated block is properly propagated through the network.</p> </li> <li> <p>TestMoveDownMoveUp: Tests chain reorganization when a node with a shorter chain (200 blocks) connects to a node with a longer chain (300 blocks). This verifies that the node correctly abandons its shorter chain and reorganizes to follow the longer chain, fulfilling a core TNF requirement.</p> </li> <li> <p>TestTDRestart: Tests persistence of the blockchain after a node restart, ensuring that chain state is properly maintained across restarts.</p> </li> </ul> <p>These smoke tests provide additional verification of the chain reorganization functionality in more realistic network scenarios with actual running nodes, complementing the more focused TNF unit tests.</p>"},{"location":"topics/functionalRequirementTests/#running-tnf-tests","title":"Running TNF Tests","text":""},{"location":"topics/functionalRequirementTests/#running-all-tnf-tests","title":"Running All TNF Tests","text":"<pre><code>cd /teranode/test/tnf\ngo test -v -tags test_tnf\n</code></pre>"},{"location":"topics/functionalRequirementTests/#running-specific-tnf-tests","title":"Running Specific TNF Tests","text":"<pre><code>cd /teranode/test/tnf\ngo test -v -run \"^TestTNFTestSuite$/TestInvalidateBlock$\" -tags test_tnf\n</code></pre>"},{"location":"topics/functionalRequirementTests/#tnj","title":"TNJ","text":"<p>As outlined in the Functional Requirements for Teranode reference document, the tests in the <code>/test/tnj</code> folder verify that Teranode correctly implements and enforces the standard consensus rules of the Bitcoin SV protocol.</p> <p>The naming convention for files in this folder is descriptive, using names like <code>locktime_test.go</code> that indicate the specific consensus rule being tested. Some TNJ tests may also be found in other directories (e.g., <code>test/e2e/daemon/</code>) when they require full daemon integration.</p>"},{"location":"topics/functionalRequirementTests/#consensus-rules-test-description","title":"Consensus Rules Test Description","text":""},{"location":"topics/functionalRequirementTests/#tnj-4-coinbase-transaction-maturity","title":"TNJ-4: Coinbase Transaction Maturity","text":"<p><code>block_subsidy_test.go</code> (located in <code>test/e2e/daemon/</code>) covers TNJ-4: Coinbase transactions must not be spent until they have matured (reached 100 blocks of confirmation).</p> <ul> <li>Implementation: The test establishes a multi-node Teranode network to verify coinbase transaction maturity rules. It creates a block with a coinbase transaction, then attempts to spend the outputs from that coinbase transaction immediately. The test verifies that this transaction is not included in subsequent blocks, demonstrating that Teranode correctly enforces the coinbase maturity rule. After generating 100 blocks to ensure maturity, it then confirms that the transaction spending from the mature coinbase can now be included in a block. This test ensures that Teranode enforces one of the fundamental consensus rules regarding coinbase outputs.</li> </ul>"},{"location":"topics/functionalRequirementTests/#transaction-locktime-rules","title":"Transaction Locktime Rules","text":"<p><code>locktime_test.go</code> covers the consensus rules regarding transaction locktime enforcement:</p> <ul> <li>Implementation: The test runs multiple locktime scenarios to verify Teranode's enforcement of transaction locktimes. It tests four specific scenarios:</li> <li>Future Height Non-Final: Transactions with a locktime set to a future block height and non-final sequence number, verifying they are not included in blocks.</li> <li>Future Height Final: Transactions with a locktime set to a future block height but with a final sequence number (0xFFFFFFFF), confirming they are included in blocks despite the locktime.</li> <li>Future Timestamp Non-Final: Transactions with a locktime set to a future timestamp and non-final sequence number, verifying they are not included in blocks.</li> <li>Future Timestamp Final: Transactions with a locktime set to a future timestamp but with a final sequence number, confirming they are included in blocks despite the locktime.</li> </ul> <p>These tests verify that Teranode correctly implements Bitcoin's locktime functionality, which allows transactions to be time-locked until a specific block height or time is reached, unless overridden by setting all input sequence numbers to 0xFFFFFFFF.</p>"},{"location":"topics/functionalRequirementTests/#running-tnj-tests","title":"Running TNJ Tests","text":""},{"location":"topics/functionalRequirementTests/#running-all-tnj-tests","title":"Running All TNJ Tests","text":"<pre><code>cd /teranode/test/tnj\ngo test -v -tags test_tnj\n</code></pre>"},{"location":"topics/functionalRequirementTests/#running-specific-tnj-tests","title":"Running Specific TNJ Tests","text":"<pre><code>cd /teranode/test/e2e/daemon\ngo test -v -run \"^TestBlockSubsidy$\"\n</code></pre> <pre><code>cd /teranode/test/tnj\ngo test -v -run \"^TestTNJLockTimeTestSuite$/TestLocktimeScenarios$\" -tags test_tnj\n</code></pre>"},{"location":"topics/functionalRequirementTests/#tec","title":"TEC","text":"<p>The tests in the <code>/test/tec</code> folder check  Teranode's ability to recover from multiple types of errors, from incorrect settings to communication errors on the message channels between microservices.</p> <p>The naming convention for files in this folder is as follows:</p> <p><code>tec_blk_&lt;number&gt;_test.go</code> corresponds to the TEC- test in the Functional Requirements for Teranode document. <ul> <li>TEC-BLK-1: Blockchain Service Reliability and Recoverability - Blockchain Service - Blockchain Store Failure</li> <li>TEC-BLK-2: Blockchain Service Reliability and Recoverability - Blockchain Service - UTXO Store Failure</li> <li>TEC-BLK-3: Blockchain Service Reliability and Recoverability - Blockchain Service - Block Assembly Failure</li> <li>TEC-BLK-4: Blockchain Service Reliability and Recoverability - Blockchain Service - Block Validation Failure</li> <li>TEC-BLK-5: Blockchain Service Reliability and Recoverability - Blockchain Service - P2P Service Failure</li> <li>TEC-BLK-6: Blockchain Service Reliability and Recoverability - Blockchain Service - Asset Server Failure</li> <li>TEC-BLK-7: Blockchain Service Reliability and Recoverability - Blockchain Service - Kafka Failure</li> </ul>"},{"location":"topics/technologyStack/","title":"\ud83d\udd27 Technology Stack","text":""},{"location":"topics/technologyStack/#go-1252-or-above","title":"Go (1.25.2 or above)","text":"<p>Teranode is written in Go, leveraging its efficiency and simplicity to build scalable, concurrent microservices. Go was chosen for its high performance, especially when dealing with concurrent processes, which is crucial for handling the scale of blockchain validation and transaction processing within Teranode.</p>"},{"location":"topics/technologyStack/#specific-usage-in-teranode","title":"Specific Usage in Teranode","text":"<ol> <li> <p>Concurrency with Goroutines: In Teranode, Go's goroutines are extensively used to manage concurrent processes like transaction validation and block propagation. This allows the system to handle multiple tasks simultaneously without compromising speed.</p> </li> <li> <p>Efficient Resource Utilization: Go's memory management and garbage collection are vital for managing large amounts of data efficiently.</p> </li> <li> <p>Cross-Platform Compilation: Since Teranode is designed to operate across various environments, Go's cross-platform capabilities ensure that binaries can be compiled for different systems seamlessly.</p> </li> </ol> <p>For more details on Go, visit the official site.</p>"},{"location":"topics/technologyStack/#grpc","title":"gRPC","text":"<p>gRPC is employed in Teranode to enable efficient and fast communication between its various microservices. As Teranode is composed of multiple independent services (e.g., block validation, transaction propagation), gRPC ensures low-latency, high-throughput communication, which is essential for a system processing blockchain transactions in real-time.</p>"},{"location":"topics/technologyStack/#specific-usage-in-teranode_1","title":"Specific Usage in Teranode","text":"<ol> <li> <p>Service Definitions in Protobuf: Teranode defines its RPC services using Protobuf files, which are compiled into Go code to generate gRPC stubs for the client and server.</p> </li> <li> <p>Bidirectional Streaming: Certain operations in Teranode, such as transaction streaming or real-time blockchain updates, use gRPC's bidirectional streaming capabilities to maintain a continuous flow of data between nodes or services, enhancing performance.</p> </li> </ol> <p>Read more on gRPC here.</p>"},{"location":"topics/technologyStack/#protobuf","title":"Protobuf","text":"<p>Protocol Buffers (Protobuf) is the serialization format used in Teranode to define the messages exchanged between services. This ensures that data (such as blocks, transactions, and validation results) is serialized in a compact binary format, minimizing the payload size for efficient communication.</p>"},{"location":"topics/technologyStack/#specific-usage-in-teranode_2","title":"Specific Usage in Teranode","text":"<ol> <li> <p>Data Serialization: In Teranode, Protobuf is used to define the data structures for blockchain transactions, blocks, and validation requests. These definitions are stored in <code>.proto</code> files and compiled into Go, allowing the system to serialize and deserialize these structures efficiently.</p> </li> <li> <p>Backward Compatibility: Teranode leverages Protobuf\u2019s backward compatibility to evolve the blockchain system without breaking older services.</p> </li> </ol> <p>You can find more on Protobuf here.</p>"},{"location":"topics/technologyStack/#p2p-networking-libp2p","title":"P2P Networking (libp2p)","text":"<p>libp2p is the modular peer-to-peer networking stack used in Teranode for node discovery and communication. It provides a robust foundation for building distributed systems with features like peer discovery, secure communication, and publish-subscribe messaging.</p>"},{"location":"topics/technologyStack/#specific-usage-in-teranode_3","title":"Specific Usage in Teranode","text":"<ol> <li> <p>Peer Discovery with Kademlia DHT: Teranode uses Kademlia Distributed Hash Table for discovering and maintaining connections with other nodes in the network, ensuring a well-connected mesh topology.</p> </li> <li> <p>GossipSub for Message Broadcasting: The GossipSub protocol is employed for efficient message propagation across the network, allowing nodes to broadcast blocks and transactions to multiple peers simultaneously.</p> </li> <li> <p>Persistent Peer Identity: Each node maintains a persistent private key for its peer ID and encryption, ensuring consistent identity across restarts and secure communications.</p> </li> <li> <p>Future IPv6 Multicast Support: While currently using libp2p, Teranode's architecture is designed to potentially benefit from IPv6 multicast in the future for even more efficient transaction dissemination, though this is currently limited by AWS platform constraints.</p> </li> </ol>"},{"location":"topics/technologyStack/#stores","title":"Stores","text":"<p>Teranode employs various storage technologies, each selected for its specific strengths in terms of speed, scalability, and reliability. The choice of storage depends on the service's needs, such as real-time transaction processing, persistent blockchain storage, or temporary data caching.</p>"},{"location":"topics/technologyStack/#specific-usage-in-teranode_4","title":"Specific Usage in Teranode","text":"<ol> <li> <p>Aerospike:</p> <ul> <li>Teranode uses Aerospike as a distributed, high-performance NoSQL database for UTXO (Unspent Transaction Output) storage</li> <li>Provides fast, consistent reads and writes for transaction validation</li> <li>Handles UTXO state management including spent/unspent/frozen states</li> <li>Supports cleanup operations for transaction lifecycle management</li> </ul> </li> <li> <p>PostgreSQL:</p> <ul> <li>Used for blockchain metadata storage requiring complex queries</li> <li>Stores block headers, chain state, and FSM (Finite State Machine) state</li> <li>Provides SQL querying capabilities for blockchain analysis</li> <li>Maintains block statistics and chain organization data</li> </ul> </li> <li> <p>Blob Storage:</p> <ul> <li> <p>Multiple implementations for flexible deployment:</p> <ul> <li>S3: AWS S3 for cloud-based blob storage</li> <li>File System: Local file system for development/testing</li> <li>HTTP: HTTP-based blob storage service</li> <li>Memory: In-memory storage for caching and testing<ul> <li>Stores blocks, transactions, and Merkle subtrees</li> <li>Supports concurrent access with batching capabilities</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Shared Storage:</p> <ul> <li>Enables services to share large datasets across multiple nodes</li> <li>Used for block and transaction persistence in distributed deployments</li> <li>Lustre file systems (provided via S3-backed shared volumes) offer high-throughput storage solution, and it is a recommended shared storage option</li> </ul> </li> </ol>"},{"location":"topics/technologyStack/#kafka","title":"Kafka","text":"<p>Apache Kafka is employed in Teranode for event-driven architectures, enabling real-time data pipelines. Kafka handles the ingestion and distribution of blockchain-related events, ensuring that all nodes are updated with the latest state of the network.</p>"},{"location":"topics/technologyStack/#specific-usage-in-teranode_5","title":"Specific Usage in Teranode","text":"<ol> <li> <p>Event Streaming: Kafka is used to stream real-time events like transaction creation and block mining. These events are then consumed by different microservices to update the blockchain state or trigger validations.</p> </li> <li> <p>High Availability: Kafka\u2019s fault tolerance and replication ensure that event streams are durable and highly available, which is essential for the integrity of a distributed system like Teranode.</p> </li> </ol>"},{"location":"topics/technologyStack/#containerization-orchestration","title":"Containerization &amp; Orchestration","text":"<p>Teranode uses containerization as a crucial part of its deployment and scaling strategy, with Docker for packaging services and Docker Compose for local / test orchestration, while supporting Kubernetes for production deployments.</p>"},{"location":"topics/technologyStack/#docker","title":"Docker","text":"<ol> <li> <p>Service Isolation: Each microservice (validator, block assembly, blockchain, etc.) is packaged into its own Docker container, ensuring isolated runtime environments and easier management.</p> </li> <li> <p>Multi-stage Builds: Uses optimized multi-stage Dockerfile with separate build and runtime images to minimize container size and improve security.</p> </li> <li> <p>Portability: Docker ensures services run consistently across development, testing, and production environments.</p> </li> </ol>"},{"location":"topics/technologyStack/#docker-compose","title":"Docker Compose","text":"<ol> <li> <p>Local Development &amp; Testing: Primary orchestration tool for local development and testing environments</p> </li> <li> <p>Multi-node Testing: Supports complex multi-node setups for integration testing (e.g., 3-node test configurations)</p> </li> <li> <p>Service Dependencies: Manages service startup order and inter-service dependencies</p> </li> </ol>"},{"location":"topics/technologyStack/#kubernetes-production","title":"Kubernetes (Production)","text":"<ol> <li> <p>Production Orchestration: Kubernetes can be used for production deployments, providing automatic scaling, load balancing, and self-healing capabilities.</p> </li> <li> <p>Service Discovery: Ensures seamless communication between microservices using built-in service discovery mechanisms.</p> </li> <li> <p>Scalability: Enables horizontal scaling of services based on workload demands.</p> </li> </ol>"},{"location":"topics/technologyStack/#real-time-communication","title":"Real-time Communication","text":""},{"location":"topics/technologyStack/#websocket-centrifuge","title":"WebSocket &amp; Centrifuge","text":"<p>Teranode uses WebSocket connections and the Centrifuge library for real-time bidirectional communication between clients and services.</p>"},{"location":"topics/technologyStack/#specific-usage-in-teranode_6","title":"Specific Usage in Teranode","text":"<ol> <li> <p>Real-time Updates: WebSocket connections enable real-time blockchain updates, transaction notifications, and state changes to be pushed to connected clients.</p> </li> <li> <p>Centrifuge Framework: Provides a scalable real-time messaging server with features like:</p> <ul> <li>Channel subscriptions for different event types</li> <li>Presence information for connected clients</li> <li>Automatic reconnection handling</li> <li>Message history and recovery</li> </ul> </li> <li> <p>Asset Service Integration: The Asset Server uses WebSocket/Centrifuge for streaming blockchain data to clients in real-time.</p> </li> </ol>"},{"location":"topics/technologyStack/#observability-monitoring","title":"Observability &amp; Monitoring","text":"<p>Teranode implements comprehensive observability to monitor system health, performance, and debug issues in the distributed environment.</p>"},{"location":"topics/technologyStack/#prometheus","title":"Prometheus","text":"<ol> <li>Metrics Collection: Collects and stores time-series metrics from all services</li> <li>Custom Metrics: Tracks blockchain-specific metrics like transaction throughput, block validation times, UTXO set size</li> <li>Alerting: Configured alerts for system health and performance thresholds</li> </ol>"},{"location":"topics/technologyStack/#opentelemetry-jaeger","title":"OpenTelemetry &amp; Jaeger","text":"<ol> <li>Distributed Tracing: Traces requests across multiple services to identify bottlenecks</li> <li>Span Collection: Collects detailed timing information for each operation</li> <li>Jaeger Backend: Provides UI for visualizing and analyzing traces</li> <li>Context Propagation: Maintains trace context across gRPC and HTTP calls</li> </ol>"},{"location":"topics/technologyStack/#logging","title":"Logging","text":"<ol> <li>Structured Logging: Uses structured JSON logging for easy parsing and analysis</li> <li>Log Aggregation: Centralized log collection for debugging distributed issues</li> <li>Log Levels: Configurable log levels per service for detailed debugging</li> </ol>"},{"location":"topics/technologyStack/#testing-infrastructure","title":"Testing Infrastructure","text":""},{"location":"topics/technologyStack/#testcontainers","title":"TestContainers","text":"<p>Teranode uses TestContainers for integration and end-to-end testing, providing isolated, reproducible test environments.</p> <ol> <li>Container Management: Automatically starts and stops Docker containers for tests</li> <li>Service Dependencies: Manages complex test setups with multiple services (Aerospike, PostgreSQL, Kafka)</li> <li>Network Isolation: Creates isolated networks for each test suite</li> <li>Cleanup: Ensures proper cleanup of resources after test completion</li> </ol>"},{"location":"topics/technologyStack/#testing-framework","title":"Testing Framework","text":"<ol> <li>Multi-node Testing: Supports testing with multiple Teranode instances</li> <li>State Management: Provides consistent initial blockchain state for tests</li> <li>Mock Services: Includes mock implementations for testing service interactions</li> <li>Performance Testing: Infrastructure for load testing and benchmarking</li> </ol>"},{"location":"topics/teranodeIntro/","title":"Teranode Introduction","text":"<p>The Bitcoin (BTC) scalability issue refers to the challenge faced by the original Bitcoin network in processing a high volume of transactions efficiently. In the early stages of Bitcoin's development, a block size limit of 1 megabyte per block was introduced as a temporary measure. This limit effectively restricts the network's capacity to approximately 3.3 to 7 transactions per second. As Bitcoin gained popularity, this limitation led to slower transaction times and increased fees.</p> <p>With Bitcoin SV (BSV), the block size limit was significantly increased to 4 gigabytes, improving the network's performance. However, the current BSV node software (SV Node) still has scalability and performance limits, capping the network\u2019s capacity at several thousand transactions per second.</p> <p>Teranode is BSV\u2019s solution to these limitations by employing a horizontal scaling approach. Instead of relying solely on increasing the capacity of a single node (vertical scaling), Teranode distributes the workload across multiple machines. This architecture, combined with an unbounded block size, allows the network\u2019s capacity to grow in line with demand, as more cluster nodes can be added.</p> <p>With Teranode, BSV achieves true scalability, consistently supporting over 1 million transactions per second. Teranode is designed as a collection of microservices, each responsible for different functions of the BSV network, ensuring high performance while staying true to the principles outlined in the original Bitcoin whitepaper.</p>"},{"location":"topics/transactionLifecycle/","title":"Transaction Lifecycle in Teranode","text":"<p>Understanding how transactions flow through Teranode is fundamental to comprehending the system's architecture and its approach to achieving massive scalability. Unlike traditional Bitcoin nodes that process transactions sequentially, Teranode employs a microservices architecture that parallelizes transaction processing across multiple specialized services.</p>"},{"location":"topics/transactionLifecycle/#overview","title":"Overview","text":"<p>In Teranode, a transaction goes through several distinct phases from initial submission to final inclusion in a block. Each phase is handled by specialized services that communicate asynchronously through Apache Kafka and gRPC, enabling horizontal scaling and fault tolerance.</p> <p></p>"},{"location":"topics/transactionLifecycle/#transaction-lifecycle-phases","title":"Transaction Lifecycle Phases","text":""},{"location":"topics/transactionLifecycle/#1-transaction-ingress-propagation-service","title":"1. Transaction Ingress (Propagation Service)","text":"<p>The transaction lifecycle begins when a transaction enters the Teranode network through the Propagation Service. This service acts as the primary gateway for transaction ingress and supports multiple protocols:</p> <ul> <li>gRPC API: High-performance programmatic access for applications</li> <li>HTTP REST API: Web-based integrations and REST client access</li> <li>JSON-RPC: Legacy Bitcoin client compatibility through the RPC service</li> </ul> <p>When a transaction arrives at the Propagation Service:</p> <ol> <li>Initial Format Validation: The service performs basic format checks to ensure the transaction is well-formed</li> <li>Transaction Storage: The raw transaction data is stored in the configured Blob Store (S3, file system, or HTTP endpoint)</li> <li>Batch Processing: For efficiency, clients can submit batches of up to 1,024 transactions in a single request (via gRPC <code>ProcessTransactionBatch</code> or HTTP POST <code>/txs</code>)</li> <li>Validation Handoff: After storage, transactions are passed to the Validator for comprehensive validation</li> </ol>"},{"location":"topics/transactionLifecycle/#2-transaction-validation-validator","title":"2. Transaction Validation (Validator)","text":"<p>The Validator can run in two modes:</p> <ul> <li>As a standalone microservice: Subscribes to Kafka topics and processes transactions asynchronously</li> <li>As an embedded library: Instantiated locally within other services (like Propagation Service) for direct validation</li> </ul> <p></p> <p>The Propagation Service implements flexible validation routing based on configuration:</p> <p>Microservice Mode:</p> <ul> <li>Standard transactions: A <code>KafkaTxValidationTopicMessage</code> is published to Kafka for asynchronous processing by the Validator service</li> <li>Large transactions: When a transaction exceeds Kafka message limits (configurable, typically 1MB), it's sent directly to the Validator's HTTP endpoint as a fallback (no Kafka event)</li> </ul> <p>Embedded Mode:</p> <ul> <li>All transactions: Validated synchronously using the embedded Validator instance via direct method calls (<code>validator.Validate()</code>)</li> <li>No size-based routing needed since there are no message size constraints with direct method calls</li> </ul> <p>The Validator performs comprehensive validation of each transaction:</p> <ol> <li>Script Validation: Executes input scripts against referenced outputs using configurable script interpreters (Go-BT, Go-SDK, or Go-BDK)</li> <li> <p>Consensus Rule Enforcement: Validates against Bitcoin consensus rules including:</p> <ul> <li>Transaction size limits</li> <li>Input and output structure verification</li> <li>Non-dust output values</li> <li>Script operation count limits</li> <li>Signature verification</li> <li> <p>Policy Rule Enforcement: Applies configurable node policies such as:</p> </li> <li> <p>Minimum fee requirements</p> </li> <li>Script complexity limits</li> <li>Custom validation rules</li> <li>UTXO Verification: Checks that all inputs reference valid, unspent outputs</li> <li>UTXO Management: Manages UTXO (Unspent Transaction Output) creation and spending</li> </ul> </li> </ol> <p>After validation:</p> <ul> <li>Valid transactions: Published to Kafka with metadata for downstream processing</li> <li>Invalid transactions: Rejected with specific error reasons</li> </ul> <p>The Validator supports parallel validation of multiple transactions, significantly improving throughput compared to sequential processing.</p>"},{"location":"topics/transactionLifecycle/#3-block-assembly","title":"3. Block Assembly","text":"<p>The Block Assembly Service collects validated transactions and organizes them into candidate blocks:</p> <ol> <li> <p>Transaction Selection: Chooses transactions based on:</p> <ul> <li>Arrival order (first-come-first-served via FIFO queue processing)</li> <li>Transaction dependencies (parent transactions must be included before children)</li> <li>Block size limits</li> </ul> </li> <li> <p>Subtree Processing: Transactions are grouped into subtrees:</p> <ul> <li>Each subtree contains related transactions</li> <li>Subtrees can be validated independently</li> <li>This enables parallel block validation</li> </ul> </li> </ol> <p></p> <ol> <li> <p>Subtree Broadcasting: Newly created subtrees are broadcast via the P2P Service to other Teranodes in the network for distributed validation</p> </li> <li> <p>Mining Candidate Creation: Prepares block templates for miners including:</p> <ul> <li>Selected subtrees</li> <li>Coinbase transaction</li> <li>Block header template</li> </ul> </li> </ol>"},{"location":"topics/transactionLifecycle/#4-block-creation-and-mining","title":"4. Block Creation and Mining","text":"<p>When a miner finds a valid proof-of-work solution:</p> <ol> <li>Solution Submission: The mining solution is submitted to the RPC Service</li> <li>Solution Forwarding: The RPC Service forwards the solution to the Block Assembly Service</li> <li> <p>Block Finalization: The Block Assembly Service creates the final block with:</p> <ul> <li>Completed block header with valid proof-of-work</li> <li>All subtrees with final coinbase transaction</li> <li>Proper Merkle root calculation</li> </ul> </li> <li> <p>Block Submission: The finalized block is sent to the Blockchain Service</p> </li> </ol>"},{"location":"topics/transactionLifecycle/#5-subtree-and-block-validation","title":"5. Subtree and Block Validation","text":"<p>Two separate validation processes run in parallel:</p> <p>Subtree Validation Service (validates subtrees as they are assembled):</p> <ol> <li>P2P Reception: Receives subtree notifications from other Teranode instances via the P2P Service</li> <li>Kafka Processing: Processes <code>KafkaSubtreeTopicMessage</code> published by the P2P Service</li> <li>Subtree Content Validation: Validates the internal structure and transactions within subtrees</li> <li>Transaction Discovery: Routes unknown transactions to the Validator service for validation if needed</li> <li>Distributed Verification: Enables network-wide validation of subtree components</li> </ol> <p>Block Validation Service (validates complete blocks when available):</p> <ol> <li>Header Validation: Verifies proof-of-work difficulty target and validates header fields</li> <li>Merkle Root Validation: Calculates and verifies that the block's merkle root matches the subtree hashes</li> <li>Subtree Content Validation: Calls the Subtree Validation Service to validate subtree contents via <code>CheckBlockSubtrees</code></li> <li>Block Structure Verification: Ensures the block is properly formed and all components are valid</li> </ol>"},{"location":"topics/transactionLifecycle/#6-blockchain-integration","title":"6. Blockchain Integration","text":"<p>Finally, the Blockchain Service (acting as the FSM - Finite State Machine):</p> <ol> <li>Chain Extension: Adds the validated block to the blockchain</li> <li>State Updates: Updates the current chain tip and related metadata</li> <li>Network Broadcasting: Notifies the P2P Service to broadcast the new block to other Teranode instances</li> <li>Reorganization Handling: Manages chain reorganizations if a longer valid chain is found</li> <li>Event Broadcasting: Notifies all services of the new block through Kafka events</li> </ol>"},{"location":"topics/transactionLifecycle/#utxo-management","title":"UTXO Management","text":"<p>Central to the Teranode model, the UTXO (Unspent Transaction Output) datastore maintains comprehensive transaction state and metadata:</p> <p></p> <p>Core UTXO Operations:</p> <ol> <li>UTXO Creation: New outputs from the transaction are added to the UTXO set</li> <li>UTXO Spending: Referenced inputs are marked as spent and removed from the UTXO set</li> <li>Double-Spend Detection: The system ensures no UTXO is spent twice, with conflicting transactions marked for resolution</li> </ol> <p>Detailed Tracking Information:</p> <ul> <li>Block Information: Block IDs and heights where the transaction was mined (supports fork handling)</li> <li>Subtree Organization: Subtree indexes within blocks for efficient parallel processing</li> <li>Transaction Metadata: Fee, size, input outpoints, and raw transaction data</li> <li>State Flags: Locked status (two-phase commit), conflicting status, frozen status, coinbase designation</li> <li>Spending Details: Which transactions spent specific UTXOs and when</li> <li>Maturity Tracking: Block heights for coinbase maturity and locktime validation</li> </ul> <p>The UTXO system uses pluggable storage backends:</p> <ul> <li>Aerospike: High-performance distributed database for production use</li> <li>PostgreSQL: SQL-based storage for development and smaller deployments</li> </ul>"},{"location":"topics/transactionLifecycle/#p2p","title":"P2P","text":"<p>The P2P Service implements a peer-to-peer network for distributed validation and block propagation. Among others, it handles:</p> <ol> <li>Subtree Notifications: Receives subtree notifications from other Teranode instances via the P2P Service</li> <li>Block Notifications: Receives block notifications from other Teranode instances via the P2P Service</li> </ol> <p></p>"},{"location":"topics/transactionLifecycle/#key-differences-from-traditional-nodes","title":"Key Differences from Traditional Nodes","text":"<p>Teranode's transaction lifecycle differs significantly from traditional Bitcoin nodes:</p> <ol> <li>Architectural Approach: Traditional nodes use monolithic processing with limited parallelization; Teranode uses distributed microservices with extensive parallel processing</li> <li>Microservices vs Monolithic: Specialized services handle different aspects of transaction processing</li> <li>Subtree Organization: Transactions are grouped into independently validatable subtrees</li> <li>Horizontal Scaling: Services can be scaled independently based on load</li> <li>Asynchronous Processing: Event-driven architecture enables better resource utilization</li> </ol>"},{"location":"topics/transactionLifecycle/#conclusion","title":"Conclusion","text":"<p>The transaction lifecycle in Teranode represents a fundamental reimagining of how Bitcoin transactions are processed. By decomposing the traditional monolithic approach into specialized, scalable microservices, Teranode achieves the throughput necessary for global-scale Bitcoin adoption while maintaining the security and reliability expected from the Bitcoin network.</p> <p>This architecture enables Teranode to consistently process over 1 million transactions per second, making it suitable for enterprise and global payment use cases that require both high throughput and low latency.</p>"},{"location":"topics/transactionLifecycle/#other-resources","title":"Other Resources","text":"<p>For deeper understanding of Teranode's transaction processing, explore these related documentation topics:</p>"},{"location":"topics/transactionLifecycle/#architecture-and-design","title":"Architecture and Design","text":"<ul> <li>Overall System Design - Complete system design documentation</li> <li>Microservices Overview - Detailed microservices architecture</li> </ul>"},{"location":"topics/transactionLifecycle/#service-documentation","title":"Service Documentation","text":"<ul> <li>Propagation Service - Transaction ingress and routing</li> <li>Validator Service - Transaction validation logic</li> <li>Block Assembly Service - Block creation and mining</li> <li>Block Validation Service - Block validation processes</li> <li>Subtree Validation Service - Subtree validation details</li> <li>Blockchain Service - Blockchain state management</li> <li>P2P Service - Peer-to-peer networking and distributed validation</li> <li>RPC Service - JSON-RPC interface for miners and clients</li> </ul>"},{"location":"topics/transactionLifecycle/#data-models","title":"Data Models","text":"<ul> <li>Transaction Data Model - Transaction structure and format</li> <li>UTXO Data Model - Unspent Transaction Output management</li> <li>Block Data Model - Block structure and organization</li> <li>Subtree Data Model - Subtree organization and merkle trees</li> </ul>"},{"location":"topics/transactionLifecycle/#communication-and-storage","title":"Communication and Storage","text":"<ul> <li>Kafka Integration - Inter-service messaging patterns</li> <li>Blob Store - Transaction and block storage</li> <li>UTXO Store - UTXO persistence layer</li> </ul>"},{"location":"topics/transactionLifecycle/#features-and-advanced-topics","title":"Features and Advanced Topics","text":"<ul> <li>Two-Phase Commit - UTXO consistency mechanisms</li> <li>Understanding Double Spends - Double-spend detection and handling</li> <li>State Management - Blockchain state handling</li> </ul>"},{"location":"topics/understandingTheTestingFramework/","title":"Understanding the TERANODE Testing Framework","text":""},{"location":"topics/understandingTheTestingFramework/#architectural-philosophy","title":"Architectural Philosophy","text":""},{"location":"topics/understandingTheTestingFramework/#why-a-custom-test-framework","title":"Why a Custom Test Framework?","text":"<p>Bitcoin SV node testing presents unique challenges that standard testing frameworks can't fully address:</p> <ol> <li> <p>Complex Distributed Systems</p> <ul> <li>Multiple nodes need to interact</li> <li>Network conditions must be simulated</li> <li>State must be synchronized across components</li> <li>Services need to be orchestrated independently</li> </ul> </li> <li> <p>State Management</p> <ul> <li>Blockchain state must be maintained consistently</li> <li>UTXO set needs to be tracked</li> <li>Multiple data stores must stay synchronized</li> <li>Test state needs to be cleanly reset between runs</li> </ul> </li> <li> <p>Real-world Scenarios</p> <ul> <li>Network partitions</li> <li>Node failures</li> <li>Service degradation</li> <li>Race conditions</li> <li>Data consistency challenges</li> </ul> </li> </ol>"},{"location":"topics/understandingTheTestingFramework/#core-design-principles","title":"Core Design Principles","text":"<ol> <li> <p>Isolation</p> <ul> <li>Each test runs in its own containerized environment</li> <li>State is cleanly reset between tests</li> <li>Network conditions can be controlled</li> <li>Services can be manipulated independently</li> </ul> </li> <li> <p>Observability</p> <ul> <li>Comprehensive logging across all components</li> <li>Health checks for all services</li> <li>Clear error reporting</li> <li>State inspection capabilities</li> </ul> </li> <li> <p>Reproducibility</p> <ul> <li>Deterministic test environments</li> <li>Consistent initial states</li> <li>Controlled timing and sequencing</li> <li>Docker-based setup ensures consistency</li> </ul> </li> </ol>"},{"location":"topics/understandingTheTestingFramework/#testing-strategy","title":"Testing Strategy","text":""},{"location":"topics/understandingTheTestingFramework/#multi-node-testing-architecture","title":"Multi-Node Testing Architecture","text":"<p>The framework uses three nodes by default because this is the minimum number needed to test:</p> <ol> <li> <p>Network Consensus</p> <ul> <li>Majority agreement (2 out of 3)</li> <li>Fork resolution</li> <li>Chain selection</li> </ul> </li> <li> <p>Network Propagation</p> <ul> <li>Transaction broadcasting</li> <li>Block propagation</li> <li>Peer discovery</li> <li>Network partitioning scenarios</li> </ul> </li> <li> <p>Failure Scenarios</p> <ul> <li>Node isolation</li> <li>Service degradation</li> <li>Recovery processes</li> <li>State synchronization</li> </ul> </li> </ol>"},{"location":"topics/understandingTheTestingFramework/#service-organization","title":"Service Organization","text":"<pre><code>Node Instance\n\u251c\u2500\u2500 Blockchain Service\n\u251c\u2500\u2500 Block Assembly Service\n\u251c\u2500\u2500 Validator Service\n\u251c\u2500\u2500 Propagation Service\n\u251c\u2500\u2500 RPC Service\n\u251c\u2500\u2500 P2P Service\n\u251c\u2500\u2500 Block Store (Blob Store)\n\u251c\u2500\u2500 UTXO Store (Aerospike)\n\u2514\u2500\u2500 Subtree Store (Blob Store)\n</code></pre> <p>This architecture allows:</p> <ul> <li>Independent service control</li> <li>Isolated failure testing</li> <li>Clear responsibility boundaries</li> <li>Flexible configuration</li> </ul>"},{"location":"topics/understandingTheTestingFramework/#test-categories-explained","title":"Test Categories Explained","text":"<p>The test categories align with Teranode's functional requirements, ensuring comprehensive coverage of all node responsibilities.</p>"},{"location":"topics/understandingTheTestingFramework/#main-node-activity-tests","title":"Main Node Activity Tests","text":""},{"location":"topics/understandingTheTestingFramework/#tna-node-responsibilities","title":"TNA (Node Responsibilities)","text":"<p>Tests core responsibilities from Bitcoin White Paper Section 5:</p> <ul> <li>Broadcasting new transactions to all nodes</li> <li>Collecting transactions into blocks</li> <li>Finding proof-of-work for blocks</li> <li>Network message propagation</li> <li>Basic node operations</li> </ul> <p>Why it matters: These tests ensure nodes fulfill their fundamental network responsibilities.</p>"},{"location":"topics/understandingTheTestingFramework/#tnb-receive-and-validate-transactions","title":"TNB (Receive and Validate Transactions)","text":"<p>Tests transaction reception and validation:</p> <ul> <li>Extended Format transaction handling</li> <li>Consensus rule validation</li> <li>UTXO verification</li> <li>Script validation</li> <li>Fee calculation</li> <li>Transaction batching and processing</li> </ul> <p>Why it matters: Transaction validation is critical for maintaining network consensus and preventing double-spends.</p>"},{"location":"topics/understandingTheTestingFramework/#tnc-assemble-blocks","title":"TNC (Assemble Blocks)","text":"<p>Tests block assembly functionality:</p> <ul> <li>Building candidate blocks from validated transactions</li> <li>Merkle tree calculation and validation</li> <li>Coinbase transaction creation</li> <li>Managing multiple candidate blocks</li> <li>Subtree verification</li> </ul> <p>Why it matters: Proper block assembly ensures valid block creation and mining operations.</p>"},{"location":"topics/understandingTheTestingFramework/#tnd-propagate-blocks-to-network","title":"TND (Propagate Blocks to Network)","text":"<p>Tests block propagation:</p> <ul> <li>Broadcasting solved blocks to other nodes</li> <li>Block announcement protocols</li> <li>Network-wide block distribution</li> </ul> <p>Why it matters: Ensures blocks are efficiently distributed across the network.</p>"},{"location":"topics/understandingTheTestingFramework/#tne-receive-and-validate-blocks","title":"TNE (Receive and Validate Blocks)","text":"<p>Tests block reception and validation:</p> <ul> <li>Downloading blocks from other nodes</li> <li>Block validation against consensus rules</li> <li>Transaction verification within blocks</li> <li>Chain acceptance decisions</li> </ul> <p>Why it matters: Ensures nodes can properly validate and accept blocks from the network.</p>"},{"location":"topics/understandingTheTestingFramework/#supporting-infrastructure-tests","title":"Supporting Infrastructure Tests","text":""},{"location":"topics/understandingTheTestingFramework/#tnf-keep-track-of-longest-honest-chain","title":"TNF (Keep Track of Longest Honest Chain)","text":"<p>Tests chain management:</p> <ul> <li>Maintaining awareness of the longest valid chain</li> <li>Chain synchronization</li> <li>Fork tracking and resolution</li> <li>Chain reorganization handling</li> </ul> <p>Why it matters: Ensures nodes follow the longest honest chain as per Bitcoin protocol.</p>"},{"location":"topics/understandingTheTestingFramework/#tnj-adherence-to-standard-policies-consensus-rules","title":"TNJ (Adherence to Standard Policies - Consensus Rules)","text":"<p>Tests consensus rule compliance:</p> <ul> <li>Genesis rule adherence</li> <li>Block size limits</li> <li>Difficulty adjustment</li> <li>Transaction validation rules</li> <li>Locktime validation</li> <li>Script interpretation</li> <li>Edge cases in consensus rules</li> </ul> <p>Why it matters: Validates strict adherence to Bitcoin protocol consensus rules.</p>"},{"location":"topics/understandingTheTestingFramework/#error-handling-tests","title":"Error Handling Tests","text":""},{"location":"topics/understandingTheTestingFramework/#tec-error-cases","title":"TEC (Error Cases)","text":"<p>Verifies system resilience:</p> <ul> <li>Service failures</li> <li>Network partitions</li> <li>Resource exhaustion</li> <li>Recovery procedures</li> <li>Invalid block handling</li> <li>Service restart scenarios</li> <li>Malformed data handling</li> </ul> <p>Why it matters: A production system must handle failures gracefully and recover automatically.</p>"},{"location":"topics/understandingTheTestingFramework/#e2e-end-to-end-tests","title":"E2E (End-to-End Tests)","text":"<p>Full system integration tests:</p> <ul> <li>Multi-node scenarios</li> <li>Complete transaction lifecycle</li> <li>Block propagation across network</li> <li>RPC interface testing</li> <li>Legacy node compatibility</li> </ul> <p>Why it matters: Validates the entire system working together in realistic scenarios.</p>"},{"location":"topics/understandingTheTestingFramework/#consensus-tests","title":"Consensus Tests","text":"<p>Bitcoin protocol consensus validation:</p> <ul> <li>Script interpreter tests</li> <li>Transaction validation rules</li> <li>Signature verification</li> <li>Opcode execution</li> <li>BIP implementations</li> </ul> <p>Why it matters: Ensures compliance with Bitcoin consensus rules.</p>"},{"location":"topics/understandingTheTestingFramework/#understanding-test-flow","title":"Understanding Test Flow","text":""},{"location":"topics/understandingTheTestingFramework/#test-lifecycle","title":"Test Lifecycle","text":"<ol> <li>Setup</li> </ol> <pre><code>Initialize Framework\n\u251c\u2500\u2500 Create Docker environment\n\u251c\u2500\u2500 Start services\n\u251c\u2500\u2500 Initialize clients\n\u2514\u2500\u2500 Verify health\n</code></pre> <ol> <li>Execution</li> </ol> <pre><code>Run Test\n\u251c\u2500\u2500 Prepare test state\n\u251c\u2500\u2500 Execute test actions\n\u251c\u2500\u2500 Verify results\n\u2514\u2500\u2500 Cleanup\n</code></pre> <ol> <li>Teardown</li> </ol> <pre><code>Cleanup\n\u251c\u2500\u2500 Stop services\n\u251c\u2500\u2500 Remove containers\n\u251c\u2500\u2500 Clean data\n\u2514\u2500\u2500 Release resources\n</code></pre>"},{"location":"topics/understandingTheTestingFramework/#running-tests","title":"Running Tests","text":""},{"location":"topics/understandingTheTestingFramework/#makefile-targets","title":"Makefile Targets","text":"<p>The project provides several make targets for different test scenarios:</p> <pre><code># Run unit tests (excludes test/ directory)\nmake test\n\n# Run long-running tests\nmake longtest\n\n# Run sequential tests (one by one, in order)\nmake sequentialtest\n\n# Run smoke tests (basic functionality)\nmake smoketest\n\n# Run all tests\nmake testall\n</code></pre>"},{"location":"topics/understandingTheTestingFramework/#running-specific-test-categories","title":"Running Specific Test Categories","text":"<p>Tests can be run with specific build tags:</p> <pre><code># Run TNA tests\ngo test -v -tags \"test_tna\" ./test/tna\n\n# Run TNB tests\ngo test -v ./test/tnb\n\n# Run TEC tests\ngo test -v -tags \"test_tec\" ./test/tec\n\n# Run a specific test\ngo test -v -run \"^TestTNA1TestSuite/TestBroadcastNewTxAllNodes$\" -tags test_tna ./test/tna/tna1_test.go\n\n# Run E2E daemon tests\ngo test -v -race -timeout=5m ./test/e2e/daemon/...\n\n# Run consensus tests\ngo test -v ./test/consensus/...\n</code></pre>"},{"location":"topics/understandingTheTestingFramework/#test-configuration","title":"Test Configuration","text":""},{"location":"topics/understandingTheTestingFramework/#tconfig-system","title":"TConfig System","text":"<p>Tests use a configuration system (<code>tconfig</code>) to manage test environments:</p> <ul> <li>Settings Contexts: Different configuration profiles (e.g., <code>docker.ci</code>, <code>test</code>, <code>dev.system.test</code>)</li> <li>Docker Compose Files: Multiple compose files can be combined for different test scenarios</li> <li>Node Configuration: Each test can specify which nodes and services to use</li> </ul> <p>Example configuration in test:</p> <pre><code>TConfig: tconfig.LoadTConfig(\n    map[string]any{\n        tconfig.KeyTeranodeContexts: []string{\n            \"docker.teranode1.test.tna1Test\",\n            \"docker.teranode2.test.tna1Test\",\n            \"docker.teranode3.test.tna1Test\",\n        },\n    },\n)\n</code></pre>"},{"location":"topics/understandingTheTestingFramework/#test-data-management","title":"Test Data Management","text":""},{"location":"topics/understandingTheTestingFramework/#seed-data","title":"Seed Data","text":"<p>The framework includes pre-configured blockchain state for testing:</p> <ul> <li>Location: <code>test/utils/data/seed/</code></li> <li>Contains UTXO sets and headers for known blockchain states</li> <li>Used to initialize tests with consistent starting conditions</li> </ul>"},{"location":"topics/understandingTheTestingFramework/#test-isolation","title":"Test Isolation","text":"<p>Each test run creates its own data directory:</p> <ul> <li>Pattern: <code>test/data/test/{TEST_ID}</code></li> <li>Automatically cleaned up after test completion</li> <li>Ensures tests don't interfere with each other</li> </ul>"},{"location":"topics/understandingTheTestingFramework/#test-utilities-and-helpers","title":"Test Utilities and Helpers","text":""},{"location":"topics/understandingTheTestingFramework/#teranodetestsuite","title":"TeranodeTestSuite","text":"<p>Base test suite providing common functionality:</p> <ul> <li>Multi-node setup and teardown</li> <li>Service client initialization</li> <li>Logging configuration</li> <li>Docker compose management</li> </ul>"},{"location":"topics/understandingTheTestingFramework/#teranodetestenv","title":"TeranodeTestEnv","text":"<p>Test environment management:</p> <ul> <li>Node client access</li> <li>Service health checks</li> <li>Shared storage for test coordination</li> <li>Network configuration</li> </ul>"},{"location":"topics/understandingTheTestingFramework/#helper-functions","title":"Helper Functions","text":"<p>Common test operations:</p> <ul> <li><code>GetBlockHeight()</code> - Retrieve current blockchain height</li> <li><code>CallRPC()</code> - Make RPC calls with retry logic</li> <li><code>GetMiningCandidate()</code> - Get mining candidate from block assembly</li> <li>Transaction creation and signing utilities</li> </ul>"},{"location":"topics/understandingTheTestingFramework/#other-resources","title":"Other Resources","text":"<ul> <li>Functional Requirement Tests Guide - Detailed guide on how functional tests map to Teranode requirements</li> <li>Teranode Daemon Reference - Guide to using the test daemon for integration testing</li> <li>Automated Testing How-To</li> <li>Testing Technical Reference</li> </ul>"},{"location":"topics/architecture/stateManagement/","title":"\ud83d\uddc2\ufe0f\ufe0f State Management in Teranode","text":"<p>Last Modified: 3-February-2025</p> <ol> <li>Introduction</li> <li>State Machine in Teranode</li> <li>Functionality<ul> <li>3.1. State Machine Initialization</li> <li>3.2. Accessing the State Machine</li> <li>3.2.1. Access via Command-Line Interface</li> <li>3.2.2. Access via HTTP</li> <li>3.2.3. Access via gRPC</li> <li>3.3. State Machine States</li> <li>3.3.1. FSM: Idle State</li> <li>3.3.2. FSM: Legacy Syncing State</li> <li>3.3.3. FSM: Running State</li> <li>3.3.4. FSM: Catching Blocks State</li> <li>3.4. State Machine Events</li> <li>3.4.1. FSM Event: Legacy Sync</li> <li>3.4.2. FSM Event: Run</li> <li>3.4.3. FSM Event: Catch up Blocks</li> <li>3.4.4. FSM Event: Stop</li> <li>3.5. Waiting on State Machine Transitions</li> </ul> </li> </ol>"},{"location":"topics/architecture/stateManagement/#1-introduction","title":"1. Introduction","text":"<p>A Finite State Machine is a model used in computer science that describes a system which can be in one of a finite number of states at any given time. The machine can transition between these predefined states based on inputs or conditions (an \"event\").</p> <p>Finite State Machines:</p> <ul> <li>have a finite set of states.</li> <li>can only be in one state at a time.</li> <li>transition between states based on inputs or events.</li> <li>have a defined initial state.</li> <li>may have one or more final states.</li> </ul>"},{"location":"topics/architecture/stateManagement/#2-state-machine-in-teranode","title":"2. State Machine in Teranode","text":"<p>The Teranode blockchain service uses a Finite State Machine (FSM) to manage the various states and transitions of the node. The FSM is responsible for controlling the node's behavior based on the current state and incoming events.</p> <p>The FSM has the following states:</p> <ul> <li>Idle</li> <li>LegacySyncing</li> <li>Running</li> <li>CatchingBlocks</li> </ul> <p>The FSM responds to the following events:</p> <ul> <li>LegacySync</li> <li>Run</li> <li>CatchupBlocks</li> <li>Stop</li> </ul> <p>The diagram below represents the relationships between the states and events in the FSM (as defined in <code>services/blockchain/fsm.go</code>):</p> <p></p> <p>The FSM handles the following state transitions:</p> <ul> <li>LegacySync: Transitions to LegacySyncing from Idle</li> <li>Run: Transitions to Running from Idle, LegacySyncing or CatchingBlocks</li> <li>CatchupBlocks: Transitions to CatchingBlocks from Running</li> <li>Stop: Transitions to Idle from LegacySyncing, Running, or CatchingBlocks</li> </ul> <p>Teranode provides a visualizer tool to generate and visualize the state machine diagram. To run the visualizer, use the command <code>go run services/blockchain/fsm_visualizer/main.go</code>. The generated <code>docs/state-machine.diagram.md</code> can be visualized using https://mermaid.live/. fsm_visualizer main.go.</p>"},{"location":"topics/architecture/stateManagement/#3-functionality","title":"3. Functionality","text":""},{"location":"topics/architecture/stateManagement/#31-state-machine-initialization","title":"3.1. State Machine Initialization","text":"<p>As part of its own initialization, the Blockchain service initializes the FSM in the Idle state, before it transitions to a LegacySyncing or Running state.</p>"},{"location":"topics/architecture/stateManagement/#32-accessing-the-state-machine","title":"3.2. Accessing the State Machine","text":""},{"location":"topics/architecture/stateManagement/#321-access-via-command-line-interface-recommended","title":"3.2.1. Access via Command-Line Interface (Recommended)","text":"<p>The Teranode Command-Line Interface (teranode-cli) provides the most direct and recommended approach for interacting with the State Machine. The CLI abstracts the underlying API calls and offers a straightforward interface for both operators and developers.</p> <p>The CLI provides two primary commands for FSM interaction:</p> <ul> <li>getfsmstate - Queries and displays the current state of the FSM</li> <li>setfsmstate - Changes the FSM state by sending the appropriate event</li> </ul> <p>These commands interface with the same underlying mechanisms as the gRPC methods, but provide a more user-friendly experience with appropriate validation and feedback.</p>"},{"location":"topics/architecture/stateManagement/#322-access-via-http-asset-server","title":"3.2.2. Access via HTTP (Asset Server)","text":"<p>The Asset Server provides a RESTful HTTP interface to the State Machine, offering a web-friendly approach to FSM interaction. This interface is particularly useful for web applications and administrative dashboards that need to monitor or control node state.</p> <p>The Asset Server exposes the following endpoints for FSM interaction:</p> <ul> <li>GET /api/v1/fsm/state - Retrieves the current FSM state</li> <li>POST /api/v1/fsm/state - Sends a custom event to the FSM</li> <li>GET /api/v1/fsm/events - Lists all available FSM events</li> <li>GET /api/v1/fsm/states - Lists all possible FSM states</li> </ul> <p>These HTTP endpoints provide the same functionality as the CLI and gRPC methods but with a RESTful interface that can be accessed using standard HTTP clients.</p>"},{"location":"topics/architecture/stateManagement/#323-access-via-grpc","title":"3.2.3. Access via gRPC","text":"<p>The Blockchain service also exposes the following gRPC methods to interact with the FSM programmatically:</p> <ul> <li>GetFSMCurrentState - Returns the current state of the FSM</li> <li> <p>SendFSMEvent - Sends an event to the FSM to trigger a state transition</p> </li> <li> <p>LegacySync - Transitions the FSM to the LegacySyncing state (delegates on the SendFSMEvent method)</p> </li> <li>Run - Transitions the FSM to the Running state (delegates on the SendFSMEvent method)</li> <li>CatchUpBlocks - Transitions the FSM to the CatchingBlocks state (delegates on the SendFSMEvent method)</li> </ul>"},{"location":"topics/architecture/stateManagement/#33-state-machine-states","title":"3.3. State Machine States","text":""},{"location":"topics/architecture/stateManagement/#331-fsm-idle-state","title":"3.3.1. FSM: Idle State","text":"<p>The Blockchain service always starts in an <code>Idle</code> state. In this state:</p> <ul> <li>No operations are permitted</li> <li>All services are inactive</li> <li>The node is not participating in the network in any way</li> <li>Must be manually triggered to transition to another state</li> </ul> <p>Allowed Operations in Idle State:</p> <ul> <li>\u274c Process external transactions</li> <li>\u274c Legacy relay transactions</li> <li>\u274c Queue subtrees</li> <li>\u274c Process subtrees</li> <li>\u274c Queue blocks</li> <li>\u274c Process blocks</li> <li>\u274c Relay blocks</li> <li>\u274c Speedy process blocks</li> <li>\u274c Create subtrees (or propagate them)</li> <li>\u274c Create blocks (mine candidates)</li> </ul> <p>All services will wait for the FSM to transition to the <code>Running</code> state (either directly or after going through a <code>Legacy Sync</code> step) before starting their operations. As such, the node should see no activity until the FSM transitions to the <code>Running</code> state.</p> <p>The node can also return back to the <code>Idle</code> state from any other state, however this can only be triggered by a manual / external request.</p>"},{"location":"topics/architecture/stateManagement/#332-fsm-legacy-syncing-state","title":"3.3.2. FSM: Legacy Syncing State","text":"<p>When a node is starting up, it may need to perform a legacy sync. This is a full block sync performed against legacy BSV nodes. In this state:</p> <p>Allowed Operations in Legacy Syncing State:</p> <ul> <li>\u274c Process external transactions</li> <li>\u274c Legacy relay transactions</li> <li>\u274c Queue subtrees</li> <li>\u274c Process subtrees</li> <li>\u274c Queue blocks</li> <li>\u274c Process blocks</li> <li>\u274c Relay blocks</li> <li>\u2705 Speedy process blocks</li> <li>\u274c Create subtrees (or propagate them)</li> <li>\u274c Create blocks (mine candidates)</li> </ul>"},{"location":"topics/architecture/stateManagement/#333-fsm-running-state","title":"3.3.3. FSM: Running State","text":"<p>The <code>Running</code> state represents the node actively participating in the network. In this state:</p> <p>Allowed Operations in Running State:</p> <ul> <li>\u2705 Process external transactions</li> <li>\u2705 Legacy relay transactions</li> <li>\u2705 Queue subtrees</li> <li>\u2705 Process subtrees</li> <li>\u2705 Queue blocks</li> <li>\u2705 Process blocks</li> <li>\u2705 Relay blocks</li> <li>\u274c Speedy process blocks</li> <li>\u2705 Create subtrees (or propagate them)</li> <li>\u2705 Create blocks (mine candidates)</li> </ul> <p>If <code>fsm_state_restore</code> setting is enabled, and if the node was previously in the <code>Idle</code> state, all services will start their operations once the FSM transitions to the <code>Running</code> state.</p> <p>If the node was previously in any other state, the Block Assembler would now start mining blocks. The Block Assembler will never mine blocks under any other node state.</p>"},{"location":"topics/architecture/stateManagement/#334-fsm-catching-blocks-state","title":"3.3.4. FSM: Catching Blocks State","text":"<p>The <code>CatchingBlocks</code> state represents the node catching up on blocks. This state is triggered by BlockValidation when the node needs to catch up with the network. In this state:</p> <p>Allowed Operations in Catching Blocks State:</p> <ul> <li>\u2705 Process external transactions</li> <li>\u2705 Legacy relay transactions</li> <li>\u2705 Queue subtrees</li> <li>\u2705 Process subtrees</li> <li>\u2705 Queue blocks</li> <li>\u2705 Process blocks</li> <li>\u2705 Relay blocks</li> <li>\u274c Speedy process blocks</li> <li>\u274c Create subtrees (or propagate them)</li> <li>\u274c Create blocks (mine candidates)</li> </ul>"},{"location":"topics/architecture/stateManagement/#error-handling-in-catching-blocks-state","title":"Error Handling in Catching Blocks State","text":"<p>When an error occurs during the catchup process, the FSM behavior has been updated to maintain state consistency:</p> <p></p> <p>Key points about error handling:</p> <ul> <li>State Persistence: When an error occurs during catchup (e.g., validation failure, network error), the FSM remains in the <code>CatchingBlocks</code> state</li> <li>No Automatic Reversion: The FSM does not automatically revert to the <code>Running</code> state on error</li> <li>Explicit Recovery Required: Recovery from errors requires either:<ul> <li>Manual retry of the catchup process</li> <li>Automatic retry mechanism (if configured)</li> <li>Explicit state reset via operator intervention</li> </ul> </li> <li>Consistency: This behavior prevents inconsistent state transitions and ensures the node doesn't incorrectly resume normal operations while catchup is incomplete</li> </ul>"},{"location":"topics/architecture/stateManagement/#34-state-machine-events","title":"3.4. State Machine Events","text":""},{"location":"topics/architecture/stateManagement/#341-fsm-event-legacy-sync","title":"3.4.1. FSM Event: Legacy Sync","text":"<p>The gRPC <code>LegacySync</code> method triggers the FSM to transition to the <code>LegacySyncing</code> state. This event is used to indicate that the node is syncing from legacy BSV nodes.</p> <p>The Legacy service triggers this event when the node is starting up and needs to perform a legacy sync.</p> <p></p>"},{"location":"topics/architecture/stateManagement/#342-fsm-event-run","title":"3.4.2. FSM Event: Run","text":"<p>The gRPC <code>Run</code> method triggers the FSM to transition to the <code>Running</code> state. This event is used to indicate that the node is ready to start participating in the network and processing transactions and blocks.</p> <p></p>"},{"location":"topics/architecture/stateManagement/#343-fsm-event-catch-up-blocks","title":"3.4.3. FSM Event: Catch up Blocks","text":"<p>The gRPC <code>CatchUpBlocks</code> method triggers the FSM to transition to the <code>CatchingBlocks</code> state. This event is used to indicate that the node is catching up on blocks and needs to process the latest blocks before resuming full operations.</p> <p></p>"},{"location":"topics/architecture/stateManagement/#344-fsm-event-stop","title":"3.4.4. FSM Event: Stop","text":"<p>The gRPC <code>Idle</code> method sends a <code>Stop</code> event to the FSM, which triggers a transition to the <code>Idle</code> state. This event is used to stop the node from participating in the network and halt all operations.</p> <p>This method is not currently used.</p>"},{"location":"topics/architecture/stateManagement/#35-waiting-on-state-machine-transitions","title":"3.5. Waiting on State Machine Transitions","text":"<p>Through internal helper methods, services can wait for the FSM to transition to a specific state before proceeding with their operations. This method is used by various services to ensure that the node is in the correct state before starting their activities.</p> <p>The method blocks until the FSM transitions to the specified state or until a timeout occurs. This ensures that services are synchronized with the node's state changes and can respond accordingly.</p> <p>This method is widely used by all Teranode services to await for the blockchain to signal a transition to the <code>Running</code> state, after which all services can resume normal operations.</p> <p>Specifically, and subject to the <code>fsm_state_restore</code> setting being enabled, the following services wait for the FSM to transition to the <code>Running</code> state before starting their operations:</p> <ul> <li>Asset Server</li> <li>Block Assembly Service</li> <li>Block Persister</li> <li>Block Validation</li> <li>Coinbase</li> <li>Legacy P2P Gateway</li> <li>P2P</li> <li>Propagation</li> <li>Subtree Validation</li> <li>UTXO Persister</li> <li>Validator</li> </ul>"},{"location":"topics/architecture/stateManagement/#4-other-resources","title":"4. Other Resources","text":""},{"location":"topics/architecture/stateManagement/#how-to-guides","title":"How-to Guides","text":"<ul> <li>How to Interact with the FSM - Practical guide for managing FSM states in test and production environments (Docker and Kubernetes)</li> </ul>"},{"location":"topics/architecture/stateManagement/#api-references","title":"API References","text":"<ul> <li>Blockchain API Reference - Complete reference for the Blockchain service API, including FSM methods</li> <li>Asset Server API Reference - Reference for the Asset Server REST API, including FSM endpoints</li> </ul>"},{"location":"topics/architecture/teranode-architecture-brief/","title":"Teranode - Architecture Brief","text":"<p>Condensed Overview</p> <p>This is a condensed version of the Teranode architecture documentation. For comprehensive details, technical specifications, and in-depth explanations, we recommend reading the complete Architecture Overview.</p>"},{"location":"topics/architecture/teranode-architecture-brief/#1-overview","title":"1. Overview","text":"<p>The original design of the Bitcoin network imposed a constraint on block size to 1MB (1 Megabyte). This size limit inherently restricts the network to a throughput of approximately 3.3 to 7 transactions per second. As adoption has increased, this constraint has led to bottlenecks in transaction processing, resulting in delays and increased transaction fees, highlighting the need for a scalability solution.</p> <p>The Teranode Project, being developed by the BSV Association, addresses the challenges of vertical scaling by instead spreading the workload across multiple machines. This horizontal scaling approach, coupled with an unbound block size, enables network capacity to grow with increasing demand through the addition of cluster nodes, allowing for BSV scaling to be truly unbounded.</p> <p>Teranode provides a robust node processing system for BSV that can consistently handle over 1 million transactions per second, while strictly adhering to the Bitcoin whitepaper. The node has been designed as a collection of services that work together to provide a decentralized, scalable, and secure blockchain network. The node is designed to be modular, allowing for easy integration of new services and features.</p> <p>The diagram below shows the main microservices, together with their interactions, that make up Teranode.</p> <p> </p> <p></p> <p> </p>"},{"location":"topics/architecture/teranode-architecture-brief/#2-data-model-and-propagation","title":"2. Data Model and Propagation","text":"<p>Teranode strictly adheres to the Bitcoin Whitepaper, while introducing ground-breaking enhancements.  Let's examine the differences between the BTC data model and the Teranode data model.</p>"},{"location":"topics/architecture/teranode-architecture-brief/#21-comparison-of-btc-bsv-and-teranode-bsv","title":"2.1. Comparison of BTC, BSV, and Teranode BSV","text":"Feature BTC BSV (pre-Teranode) Teranode BSV Transactions Standard Bitcoin transaction model. Standard Bitcoin transaction model with restored original op_codes. Adopts an extended format with extra metadata, improving processing efficiency. SubTrees Not used. Not used. Traditional block propagation. A novel concept in Teranode, serving as an intermediary for holding transaction IDs and their Merkle roots.  Each subtree contains 1 million transactions. Subtrees are broadcast every second. Broadcast frequently for faster and continuous data propagation. Blocks Transactions are grouped into blocks. Direct transaction data is stored in the block. Each block is linked to the previous one by a cryptographic hash, forming a secure, chronological chain. Same as BTC, with increased block size capacity. In the BSV blockchain, Bitcoin blocks are stored and propagated using an abstraction using subtrees of transaction IDs. This method significantly streamlines the validation process and synchronization among miners, optimizing the overall efficiency of the network. Block Size Originally capped at 1MB (1 Megabyte), restricting transactions per block. Increased to 2GB, then to 4GB block size limit. Current BSV expands to 4GB (4 Gigabytes), increasing transaction capacity. Teranode removes the size limit, enabling limitless transactions per block. Processed Transactions per second 3.3 to 7 transactions per second. Up to several thousand transactions per second. Guaranteees a minimum of 1 million transactions per second (100,000 x faster than BTC). Mempool Maintains a memory pool of unconfirmed transactions waiting to be included in blocks. Size limited by node memory. Similar to BTC, but with larger capacity due to increased memory limits. No traditional mempool. Transactions are immediately processed and organized into subtrees. Continuous validation and subtree creation replaces mempool functionality."},{"location":"topics/architecture/teranode-architecture-brief/#22-advantages-of-the-teranode-model","title":"2.2. Advantages of the Teranode Model","text":"<p>Enhances validation speed and scalability. The continuous broadcasting of subtrees allows for more consistent and efficient data validation and network behavior.</p>"},{"location":"topics/architecture/teranode-architecture-brief/#23-network-behavior","title":"2.3. Network Behavior","text":"<p>The Teranode network behaviour is characterized by its proactive approach, with nodes broadcasting and validating subtrees regularly, leading to expedited block validation and higher transaction throughput.</p>"},{"location":"topics/architecture/teranode-architecture-brief/#3-node-workflow","title":"3. Node Workflow","text":"<ul> <li>Transaction Submission: Managed via a dedicated Submission Service, ensuring efficient entry of transactions into the network.</li> <li>Transaction Validation: Conducted by the TX Validation Service, which rigorously checks transaction compliance with network rules.</li> <li>Subtree Assembly: Involves organizing validated transactions into subtrees, a critical step for efficient block assembly.</li> <li>Block Assembly: Focuses on compiling subtrees into block templates, which are then used for mining.</li> <li>Block Validation: Ensures each block adheres to the network's consensus rules, maintaining blockchain integrity.</li> </ul>"},{"location":"topics/architecture/teranode-architecture-brief/#4-services","title":"4. Services","text":"<ul> <li>Transaction Propagation Service: Handles the receipt and forwarding of transactions for validation and distribution to other nodes.</li> <li>Transaction Validator: Validates each transaction against network rules and updates their status in the system.</li> <li>Block Assembly Service: Responsible for creating subtrees and preparing block templates for mining.</li> <li>Subtree and Block Validator: Plays a pivotal role in confirming the integrity and validity of both subtrees and blocks.</li> <li>Blockchain Service: Manages the addition of new blocks to the blockchain and maintains the blockchain database.</li> <li>Asset Service: Serves as a gateway to various data elements, facilitating interactions with transactions, UTXOs, etc.</li> <li>Bootstrap: Assists new nodes in integrating into the Teranode network by discovering peers.</li> <li>Legacy Service: Ensures compatibility and communication between BSV nodes and Teranodes.</li> <li>UTXO Store: Focuses on tracking all spendable UTXOs, essential for validating new transactions.</li> </ul> <p>The Teranode's architecture revolutionizes Bitcoin's scalability through a combination of unbounded block size, innovative data models (such as SubTrees), and a modular node system. These advancements facilitate high transaction throughput and efficient network operations, positioning Teranode as a scalable solution for future blockchain demands.</p> <p>\u00a9 2024 BSV Blockchain Org.</p>"},{"location":"topics/architecture/teranode-microservices-overview/","title":"Teranode Microservices Overview","text":""},{"location":"topics/architecture/teranode-microservices-overview/#index","title":"Index","text":"<ul> <li>Teranode Microservices Overview<ul> <li>Index</li> </ul> </li> <li>1. Introduction</li> <li>2. Core Services<ul> <li>2.1 Asset Server</li> <li>2.2 Propagation Service</li> <li>2.3 Validator Service</li> <li>2.4 Subtree Validation Service</li> <li>2.5 Block Validation Service</li> <li>2.6 Block Assembly Service</li> <li>2.7 Blockchain Service</li> <li>2.8 Alert Service</li> </ul> </li> <li>3. Overlay Services<ul> <li>3.1 Block Persister Service</li> <li>3.2 UTXO Persister Service</li> <li>3.3 P2P Service</li> <li>3.4 Legacy Service</li> <li>3.5 RPC Service</li> </ul> </li> <li>4. Stores<ul> <li>4.1 TX and Subtree Store (Blob Server)</li> <li>4.2 UTXO Store</li> </ul> </li> <li>5. Other Components<ul> <li>5.1 Kafka Message Broker</li> <li>5.2 Miners</li> </ul> </li> <li>6. Interaction Patterns</li> <li>7. Related Resources</li> </ul>"},{"location":"topics/architecture/teranode-microservices-overview/#1-introduction","title":"1. Introduction","text":"<p>Teranode is designed as a collection of microservices that work together to provide a scalable and efficient blockchain network. This document provides an overview of each microservice, its responsibilities, and how it interacts with other components in the system.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#2-core-services","title":"2. Core Services","text":""},{"location":"topics/architecture/teranode-microservices-overview/#21-asset-server","title":"2.1 Asset Server","text":"<p>The Asset Server acts as an interface to various data stores, handling transactions, subtrees, blocks, and UTXOs. It uses the HTTP protocol for communication.</p> <p>Key Responsibilities:</p> <ul> <li>Provide access to blockchain data</li> <li>Handle data retrieval requests from other services and external clients</li> <li>Serve as a facade for various data stores</li> </ul> <p>Data Models:</p> <ul> <li>Blocks</li> <li>Block Headers</li> <li>Subtrees</li> <li>Extended Transactions</li> <li>UTXOs</li> </ul> <p>Key Interactions:</p> <p></p> <ul> <li>Interacts with UTXO Store, Blob Store (Subtree and TX Store), and Blockchain Server</li> <li>Provides data to other Teranode components and external clients over HTTP/WebSockets</li> </ul> <p></p> <p>HTTP Endpoints:</p> <ul> <li>getTransaction() and getTransactions()</li> <li>GetTransactionMeta()</li> <li>GetSubtree()</li> <li>GetBlockHeaders(), GetBlockHeader() and GetBestBlockHeader()</li> <li>GetBlock() and GetLastNBlocks()</li> <li>GetUTXO() and GetUTXOsByTXID()</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#22-propagation-service","title":"2.2 Propagation Service","text":"<p>The Propagation Service is responsible for receiving and forwarding transactions across the network.</p> <p>Key Responsibilities:</p> <ul> <li>Receive transactions from the network through multiple communication channels (gRPC, UDP over IPv6 and HTTP)</li> <li>Perform initial sanity checks on transactions</li> <li>Forward valid transactions to the TX Validator Service</li> </ul> <p>Key Interactions:</p> <p></p> <ul> <li>Receives transactions from other nodes via IPv6 multicast, gRPC or HTTP</li> <li>Forwards transactions to the TX Validator Service</li> </ul> <p></p> <p>Technology Stack:</p> <ul> <li>Go programming language</li> <li>UDP and HTTP for network communication</li> <li>gRPC and Protocol Buffers for service communication</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#23-validator-service","title":"2.3 Validator Service","text":"<p>The TX Validator Service checks transactions against network rules and updates their status in the UTXO store.</p> <p>Key Responsibilities:</p> <p></p> <ul> <li>Validate transactions against network rules and Bitcoin consensus rules</li> <li>Update transaction status in the UTXO store</li> <li>Forward validated transaction IDs to the Block Assembly Service</li> <li>Notify P2P subscribers about rejected transactions</li> </ul> <p></p> <p>Key Processes:</p> <ul> <li>Receiving transaction validation requests</li> <li>Validating transactions (including checks for double-spending)</li> <li>Updating UTXO store with new transaction data</li> <li>Propagating validated transactions to Block Assembly and Subtree Validation services</li> </ul> <p>Data Model:</p> <ul> <li>Extended Transaction format</li> </ul> <p>Technology Stack:</p> <ul> <li>Go programming language</li> <li>gRPC for service communication</li> <li>Kafka for message queuing (optional)</li> <li>BSV libraries for transaction validation</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#24-subtree-validation-service","title":"2.4 Subtree Validation Service","text":"<p>This service validates newly received subtrees, adds metadata, and persists them in the Subtree Store.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Validate subtrees received from other nodes</li> <li>Add metadata to subtrees for block validation</li> <li>Store validated subtrees in the Subtree Store</li> </ul> <p></p> <p>Key Processes:</p> <ul> <li>Real-time validation of subtrees</li> <li>UTXO validation for transactions within subtrees</li> <li>Handling unvalidated transactions within subtrees</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#25-block-validation-service","title":"2.5 Block Validation Service","text":"<p>The Block Validation Service processes new blocks, checking their validity before they are added to the blockchain.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Validate new blocks</li> <li>Coordinate with Subtree Validation Service for missing subtrees</li> <li>Update the blockchain with validated blocks</li> </ul> <p></p> <p>Key Processes:</p> <ul> <li>Receiving blocks for validation</li> <li>Validating block structure, Merkle root, and block header</li> <li>Catching up after a parent block is not found</li> <li>Marking transactions as mined</li> </ul> <p>Data Models:</p> <ul> <li>Blocks</li> <li>Subtrees</li> <li>Extended Transactions</li> <li>UTXOs</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#26-block-assembly-service","title":"2.6 Block Assembly Service","text":"<p>This service is responsible for creating subtrees and assembling block templates for miners.</p> <p>Key Responsibilities:</p> <p></p> <ul> <li>Organize transactions into subtrees</li> <li>Create block templates from subtrees</li> <li>Broadcast new subtrees and blocks to the network</li> <li>Handle blockchain reorganizations and forks</li> </ul> <p></p> <p>Key Processes:</p> <ul> <li>Receiving transactions from the TX Validator Service</li> <li>Grouping transactions into subtrees</li> <li>Creating mining candidates</li> <li>Processing subtrees and blocks from other nodes</li> <li>Handling forks and conflicts</li> </ul> <p>Data Models:</p> <ul> <li>Blocks</li> <li>Subtrees</li> <li>UTXOs</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#27-blockchain-service","title":"2.7 Blockchain Service","text":"<p>The Blockchain Service manages block updates and maintains the node's copy of the blockchain.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Add new blocks to the blockchain</li> <li>Manage block headers and subtree lists</li> <li>Provide blockchain state information to other services</li> <li>Handle block invalidation and chain reorganization</li> </ul> <p></p> <p>Key Processes:</p> <ul> <li>Adding new blocks to the blockchain</li> <li>Retrieving blocks and block headers</li> <li>Invalidating blocks</li> <li>Managing subscriptions for blockchain events</li> </ul> <p>Data Model:</p> <ul> <li>Blocks (including block header, coinbase TX, and block merkle root)</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#28-alert-service","title":"2.8 Alert Service","text":"<p>The Alert Service handles system-wide alerts and notifications, including UTXO freezing and block invalidation.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Distribute important network alerts</li> <li>Manage alert prioritization and dissemination</li> <li>Handle UTXO freezing, unfreezing, and reassignment</li> <li>Manage peer banning and unbanning</li> <li>Handle block invalidation requests</li> </ul> <p></p> <p>Key Processes:</p> <ul> <li>UTXO freezing and unfreezing</li> <li>UTXO reassignment</li> <li>Block invalidation</li> <li>Peer management</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#3-overlay-services","title":"3. Overlay Services","text":""},{"location":"topics/architecture/teranode-microservices-overview/#31-block-persister-service","title":"3.1 Block Persister Service","text":"<p>This service post-processes blocks, adding transaction metadata and storing them as files.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Decorate transactions in blocks with metadata</li> <li>Store processed blocks in a block data storage system</li> <li>Create and store UTXO addition and deletion files</li> </ul> <p></p> <p>Key Processes:</p> <ul> <li>Receiving and processing new block notifications</li> <li>Decorating transactions with UTXO metadata</li> <li>Creating and storing block, subtree, and UTXO files</li> </ul> <p>Data Models:</p> <ul> <li>Blocks</li> <li>Subtrees</li> <li>UTXOs (additions and deletions)</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#32-utxo-persister-service","title":"3.2 UTXO Persister Service","text":"<p>The UTXO Persister maintains an up-to-date record of all unspent transaction outputs.</p> <p>Key Responsibilities:</p> <ul> <li>Process new blocks to update the UTXO set</li> <li>Maintain UTXO set files for each block</li> <li>Create and maintain an up-to-date UTXO file set for each block in the blockchain</li> </ul> <p></p> <p>Key Processes:</p> <ul> <li>Monitoring for new block files</li> <li>Processing UTXO additions and deletions</li> <li>Generating UTXO set files</li> <li>Tracking progress of processed blocks</li> </ul> <p>Data Model:</p> <ul> <li>UTXO set (collection of unspent transaction outputs)</li> <li>UTXO components: TxID, Index, Value, Height, Script, Coinbase flag</li> </ul> <p>Technology Stack:</p> <ul> <li>Go programming language</li> <li>Blob store for file storage</li> <li>Bitcoin SV libraries for blockchain operations</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#33-p2p-service","title":"3.3 P2P Service","text":"<p>The P2P Service manages peer-to-peer communications within the network.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Handle peer discovery and connection management</li> <li>Facilitate message passing between nodes</li> <li>Manage subscriptions for blockchain events</li> <li>Handle WebSocket connections for real-time notifications</li> </ul> <p></p> <p>Key Processes:</p> <ul> <li>Peer discovery and connection</li> <li>Managing best block messages</li> <li>Handling blockchain messages (blocks, subtrees, mining)</li> <li>Processing TX validator messages</li> <li>Managing WebSocket notifications</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#34-legacy-service","title":"3.4 Legacy Service","text":"<p>The Legacy Service facilitates communication between Teranode and traditional Bitcoin SV nodes.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Receive blocks and transactions from legacy nodes</li> <li>Disseminate new blocks to legacy nodes</li> <li>Transform data between BSV and Teranode formats</li> </ul> <p>Key Processes:</p> <ul> <li>Receiving inventory notifications from BSV nodes</li> <li>Processing new blocks and converting them to Teranode format</li> <li>Handling requests from Teranode components for legacy data</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#35-rpc-service","title":"3.5 RPC Service","text":"<p>The RPC Service provides compatibility with the Bitcoin RPC interface, allowing clients to interact with the Teranode node using standard Bitcoin RPC commands.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Handle incoming RPC requests</li> <li>Process and validate RPC commands</li> <li>Interact with core Teranode services to fulfill requests</li> <li>Provide responses in Bitcoin-compatible format</li> </ul> <p>Supported RPC Commands:</p> <ul> <li>createrawtransaction, generate, getbestblockhash, getblock, sendrawtransaction, stop, version, getminingcandidate, submitminingsolution, getblockchaininfo, getinfo, getpeerinfo</li> </ul> <p>Key Processes:</p> <ul> <li>Authenticating RPC requests</li> <li>Routing requests to appropriate handlers</li> <li>Executing commands and interacting with other Teranode services</li> <li>Formatting and returning responses</li> </ul> <p>Technology Stack:</p> <ul> <li>Go programming language</li> <li>HTTP/HTTPS for RPC communication</li> <li>JSON for request/response formatting</li> </ul> <p>You can read more about this service here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#4-stores","title":"4. Stores","text":""},{"location":"topics/architecture/teranode-microservices-overview/#41-tx-and-subtree-store-blob-server","title":"4.1 TX and Subtree Store (Blob Server)","text":"<p>The Blob Server is a generic datastore used for storing transactions (extended tx) and subtrees.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Store and retrieve transaction data</li> <li>Store and retrieve subtree data</li> <li>Provide a common interface for various storage backends</li> </ul> <p>Supported Storage Backends:</p> <ul> <li>File System</li> <li>Google Cloud Storage (GCS)</li> <li>Amazon S3</li> <li>MinIO</li> <li>SeaweedFS</li> <li>SQL (PostgreSQL)</li> <li>In-memory storage</li> </ul> <p>Key Interactions:</p> <ul> <li>Used by Asset Server for retrieving transaction and subtree data</li> <li>Utilized by Block Assembly for storing and retrieving subtrees</li> <li>Accessed by Block Validation for transaction and subtree verification</li> </ul> <p>Data Models:</p> <ul> <li>Extended Transaction Data Model</li> <li>Subtree Data Model</li> </ul> <p>You can read more about this store here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#42-utxo-store","title":"4.2 UTXO Store","text":"<p>The UTXO Store is responsible for tracking spendable UTXOs based on the longest honest chain-tip in the network.</p> <p></p> <p>Key Responsibilities:</p> <ul> <li>Maintain the current UTXO set</li> <li>Handle UTXO creation, spending, and deletion</li> <li>Manage block height for determining UTXO spendability</li> <li>Support freezing, unfreezing, and reassigning UTXOs</li> </ul> <p></p> <p>Supported Storage Backends:</p> <ul> <li>Aerospike (primary production datastore)</li> <li>In-memory store</li> <li>SQL (PostgreSQL and SQLite)</li> <li>Nullstore (for testing)</li> </ul> <p>Key Interactions:</p> <ul> <li>Used by Asset Server for UTXO data retrieval</li> <li>Accessed by Block Persister for UTXO metadata</li> <li>Utilized by Block Assembly for coinbase UTXO management</li> <li>Interacts with Block Validation for UTXO verification</li> <li>Supports Transaction Validator for UTXO operations</li> </ul> <p>Data Model:</p> <ul> <li>UTXO Meta Data, including transaction details, parent transaction hashes, block IDs, fees, and other metadata</li> </ul> <p>You can read more about this store here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#5-other-components","title":"5. Other Components","text":""},{"location":"topics/architecture/teranode-microservices-overview/#51-kafka-message-broker","title":"5.1 Kafka Message Broker","text":"<p>Kafka serves as the messaging middleware for inter-service communication in Teranode.</p> <p>Key Responsibilities:</p> <ul> <li>Facilitate asynchronous communication between services</li> <li>Ensure reliable message delivery</li> <li>Support high-throughput data streaming</li> </ul> <p>Key Topics and Use Cases:</p> <ul> <li><code>kafka_validatortxsConfig</code>: Used for transmitting new transaction notifications from Propagation to Validator</li> <li><code>kafka_txsConfig</code>: Used for forwarding valid transactions from Validator to Block Assembly</li> <li><code>kafka_txmetaConfig</code>: Used for sending new UTXO metadata from Validator to Subtree Validation</li> <li><code>kafka_rejectedTxConfig</code>: Used for notifying P2P about rejected transactions</li> <li><code>kafka_blocksConfig</code>: Used for propagating new blocks from P2P to Block Validation</li> <li><code>kafka_subtreesConfig</code>: Used for sending new subtrees from P2P to Subtree Validation</li> <li><code>kafka_blocksFinalConfig</code>: Used for sending finalized blocks from Blockchain to Block Persister</li> </ul> <p>Key Features:</p> <ul> <li>Supports high-throughput data streaming</li> <li>Provides fault-tolerance and durability</li> <li>Allows for scalable message consumption</li> </ul> <p>You can read more about how Kafka is used here.</p>"},{"location":"topics/architecture/teranode-microservices-overview/#52-miners","title":"5.2 Miners","text":"<p>Miners are responsible for the computational work of finding valid blocks.</p> <p>Key Responsibilities:</p> <ul> <li>Perform proof-of-work calculations</li> <li>Broadcast newly found blocks</li> </ul>"},{"location":"topics/architecture/teranode-microservices-overview/#6-interaction-patterns","title":"6. Interaction Patterns","text":"<ul> <li>Propagation Service receives transactions and forwards them to TX Validator via Kafka</li> <li>TX Validator validates transactions, updates UTXO Store, and forwards to Block Assembly and Subtree Validation via Kafka</li> <li>Block Assembly receives validated transactions from TX Validator via Kafka</li> <li>Subtree Validation Service validates subtrees and interacts with TX Validator for missing transactions</li> <li>Block Validation Service coordinates with Subtree Validation Service for block processing</li> <li>P2P Service propagates new blocks and subtrees to Block Validation and Subtree Validation via Kafka</li> <li>Blockchain Service sends finalized blocks to Block Persister via Kafka</li> <li>UTXO Store interacts with multiple services for UTXO management and validation</li> </ul>"},{"location":"topics/architecture/teranode-microservices-overview/#7-related-resources","title":"7. Related Resources","text":"<ul> <li> <p>Teranode Architecture Overview</p> </li> <li> <p>Core Services:</p> <ul> <li>Asset Server</li> <li>Propagation Service</li> <li>Validator Service</li> <li>Subtree Validation Service</li> <li>Block Validation Service</li> <li>Block Assembly Service</li> <li>Blockchain Service</li> <li>Alert Service</li> </ul> </li> <li> <p>Overlay Services:</p> <ul> <li>Block Persister Service</li> <li>UTXO Persister Service</li> <li> <p>P2P Service</p> </li> <li> <p>Legacy Service</p> </li> <li>RPC Server</li> </ul> </li> <li> <p>Stores:</p> <ul> <li>Blob Server</li> <li>UTXO Store</li> </ul> </li> <li> <p>Messaging:</p> <ul> <li>Kafka</li> </ul> </li> </ul>"},{"location":"topics/architecture/teranode-overall-system-design/","title":"Teranode Overall System Design","text":""},{"location":"topics/architecture/teranode-overall-system-design/#index","title":"Index","text":"<ol> <li>Introduction</li> <li>Key Concepts and Innovations<ul> <li>2.1 Horizontal Scalability</li> <li>2.2 Subtrees</li> <li>2.3 Extended Transactions</li> <li>2.4 Unbounded Block Size</li> <li>2.5 Comparison with BTC</li> </ul> </li> <li>System Architecture Overview</li> <li>Data Model and Propagation<ul> <li>4.1 Bitcoin Data Model</li> <li>4.2 Teranode Data Model</li> <li>4.3 Network Behavior</li> </ul> </li> <li>Node Workflow</li> <li>Scalability and Performance</li> <li>Impact on End-Users and Developers</li> <li>Glossary of Terms</li> <li>Related Resources</li> </ol>"},{"location":"topics/architecture/teranode-overall-system-design/#1-introduction","title":"1. Introduction","text":"<p>In the early stages of Bitcoin's development, a block size limit of 1 megabyte per block was introduced as a temporary measure. This limit effectively restricts the network's capacity to approximately 3.3 to 7 transactions per second. As Bitcoin's adoption has expanded, this constraint has increasingly led to transaction processing bottlenecks, causing delays and higher transaction fees. These issues have highlighted the critical need for scalable solutions within the Bitcoin network.</p> <p>Teranode, the next evolution of the BSV node software, and developed by the BSV Association, addresses the challenges of vertical scaling by instead spreading the workload across multiple machines. This horizontal scaling approach, coupled with an unbound block size, enables network capacity to grow with increasing demand through the addition of cluster nodes, allowing for Bitcoin scaling to be truly unbounded.</p> <p>Teranode provides a robust node processing system for Bitcoin that can consistently handle over 1M transactions per second, while strictly adhering to the Bitcoin whitepaper.</p> <p>Teranode is responsible for:</p> <ul> <li> <p>Validating and accepting or rejecting received transactions.</p> </li> <li> <p>Building and assembling new subtrees and blocks.</p> </li> <li> <p>Validating and accepting or rejecting received or found subtrees and blocks.</p> </li> <li> <p>Adding found blocks to the Blockchain.</p> </li> <li> <p>Managing Coinbase transactions and their spendability.</p> </li> </ul>"},{"location":"topics/architecture/teranode-overall-system-design/#2-key-concepts-and-innovations","title":"2. Key Concepts and Innovations","text":""},{"location":"topics/architecture/teranode-overall-system-design/#21-horizontal-scalability","title":"2.1 Horizontal Scalability","text":"<p>While BTC relies on vertical scaling\u2014increasing the power of individual nodes\u2014Teranode embraces horizontal scalability through its microservices architecture. This fundamental difference allows Teranode to overcome BTC's inherent limitations:</p> <ol> <li> <p>Scalability Approach:</p> <ul> <li>BTC: Increases processing power of single nodes (vertical scaling).</li> <li>Teranode: Distributes workload across multiple machines (horizontal scaling).</li> </ul> </li> <li> <p>Transaction Processing:</p> <ul> <li>BTC: Limited to ~7 transactions per second due to 1MB block size and 10-minute block time.</li> <li>Teranode: Capable of processing over 1 million transactions per second, with potential for further increase.</li> </ul> </li> <li> <p>Resource Utilization:</p> <ul> <li>BTC: Requires increasingly powerful (and expensive) hardware for each node.</li> <li>Teranode: Can add multiple commodity machines to increase capacity cost-effectively.</li> </ul> </li> <li> <p>Flexibility:</p> <ul> <li>BTC: Monolithic architecture makes updates and improvements challenging.</li> <li>Teranode: Microservices allow independent scaling and updating of specific functions (e.g., transaction validation, block assembly).</li> </ul> </li> <li> <p>Network Resilience:</p> <ul> <li>BTC: Failure of a node can significantly impact network capacity.</li> <li>Teranode: Distributed architecture ensures continued operation even if some nodes fail.</li> </ul> </li> </ol>"},{"location":"topics/architecture/teranode-overall-system-design/#22-subtrees","title":"2.2 Subtrees","text":"<p>Subtrees are an innovation aimed at improving scalability and real-time processing capabilities of the blockchain system. A subtree acts as an intermediate data structure to hold batches of transaction IDs (including metadata) and their corresponding Merkle root. Each subtree computes its own Merkle root, which is a single hash representing the entire set of transactions within that subtree.</p> <p>Subtrees are broadcast every second (assuming a baseline throughput of 1M transactions per second), making data propagation more continuous. Broadcasting subtrees at this high frequency allows receiving nodes to validate batches quickly and continuously, essentially \"pre-approving\" them for inclusion in a block. When a block is found, its validation is expedited due to the continuous processing of subtrees.</p> <p>This proactive approach with subtrees enables the network to handle a significantly higher volume of transactions while maintaining quick validation times. It also allows nodes to utilize their processing power more evenly over time, rather than experiencing idle times between blocks.</p>"},{"location":"topics/architecture/teranode-overall-system-design/#23-extended-transactions","title":"2.3 Extended Transactions","text":"<p>Teranode uses the BSV Extended Transaction format, which includes additional metadata to facilitate processing. This format adds a marker to the transaction format and extends the input structure to include the previous locking script and satoshi outputs.</p>"},{"location":"topics/architecture/teranode-overall-system-design/#24-unbounded-block-size","title":"2.4 Unbounded Block Size","text":"<p>Unlike the fixed 1MB block size in the original Bitcoin implementation, Teranode BSV features an unbounded block size. This allows for potentially unlimited transactions per block, increasing throughput and reducing transaction fees.</p>"},{"location":"topics/architecture/teranode-overall-system-design/#25-comparison-with-btc","title":"2.5 Comparison with BTC","text":"Feature BTC Teranode BSV Transactions Standard Bitcoin transaction model. Adopts an extended format with extra metadata, improving processing efficiency. SubTrees Not used. A novel concept in Teranode, serving as an intermediary for holding transaction IDs and their Merkle roots.   Each subtree contains 1 million transactions. Subtrees are broadcast every second. Broadcast frequently for faster and continuous data propagation. Blocks Transactions are grouped into blocks. Direct transaction data is stored in the block. Each block is linked to the previous one by a cryptographic hash, forming a secure, chronological chain. In the BSV blockchain, Bitcoin blocks are stored and propagated using an abstraction using subtrees of transaction IDs. This method significantly streamlines the validation process and synchronization among miners, optimizing the overall efficiency of the network. Block Size Originally capped at 1MB (1 Megabyte), restricting transactions per block. Current BSV expands to 4GB (4 Gigabytes), increasing transaction capacity. Teranode removes the size limit, enabling limitless transactions per block. Processed Transactions per second 3.3 to 7 transactions per second. Guaranteees a minimum of 1 million transactions per second (100,000 x faster than BTC). Scalability Approach Vertical scaling: Increases processing power of individual nodes. Limited by hardware capabilities of single machines. Monolithic architecture makes updates challenging. Horizontal scaling: Distributes workload across multiple machines using a microservices architecture. Allows independent scaling of specific functions (e.g., transaction validation, block assembly). More cost-effective and flexible, with higher resilience to node failures."},{"location":"topics/architecture/teranode-overall-system-design/#3-system-architecture-overview","title":"3. System Architecture Overview","text":"<p>The Teranode architecture is designed as a collection of microservices that work together to provide a decentralized, scalable, and secure blockchain network. The node is modular, allowing for easy integration of new services and features.</p> <p></p> <p>Key components of the Teranode architecture include:</p> <ol> <li> <p>Teranode Core Services:</p> <ul> <li>Asset Server</li> <li>Propagation Service</li> <li>Validator Service</li> <li>Subtree Validation Service</li> <li>Block Validation Service</li> <li>Block Assembly Service</li> <li>Blockchain Service</li> <li>Alert Service</li> </ul> </li> <li> <p>Overlay Services:</p> <ul> <li>Block Persister Service</li> <li>UTXO Persister Service</li> <li>P2P Service</li> <li>P2P Bootstrap Service</li> <li>Legacy Service</li> </ul> </li> <li> <p>Stores:</p> <ul> <li> <p>Stores:</p> <ul> <li>TX and Subtree Store]</li> <li>UTXO Store</li> </ul> </li> </ul> </li> <li> <p>Other Components:</p> <ul> <li>Kafka Message Broker</li> <li>Miners</li> </ul> </li> </ol> <p></p> <p>For an introduction to each service, please check the Teranode Microservices Overview.</p>"},{"location":"topics/architecture/teranode-overall-system-design/#4-data-model-and-propagation","title":"4. Data Model and Propagation","text":""},{"location":"topics/architecture/teranode-overall-system-design/#41-bitcoin-data-model","title":"4.1 Bitcoin Data Model","text":"<p>In the original Bitcoin model:</p> <ul> <li>Transactions are broadcast and included in blocks as they are found.</li> <li>Blocks contain all transaction data for the transactions included.</li> </ul>"},{"location":"topics/architecture/teranode-overall-system-design/#42-teranode-data-model","title":"4.2 Teranode Data Model","text":"<p>The Teranode data model introduces:</p> <ul> <li>Extended Transactions: Include additional metadata to facilitate processing.</li> <li>Subtrees: Contain lists of transaction IDs and their Merkle root.</li> <li>Blocks: Contain lists of subtree identifiers, not transactions.</li> </ul> <p>The main differences can be seen in the table below:</p> Feature BTC BSV (pre-Teranode) Teranode BSV Transactions Standard Bitcoin transaction model. Standard Bitcoin transaction model with restored original op_codes. Adopts an extended format with extra metadata, improving processing efficiency. SubTrees Not used. Not used. Traditional block propagation. A novel concept in Teranode, serving as an intermediary for holding transaction IDs and their Merkle roots.  Each subtree contains 1 million transactions. Subtrees are broadcast every second. Broadcast frequently for faster and continuous data propagation. Blocks Transactions are grouped into blocks. Direct transaction data is stored in the block. Each block is linked to the previous one by a cryptographic hash, forming a secure, chronological chain. Same as BTC, with increased block size capacity. In the BSV blockchain, Bitcoin blocks are stored and propagated using an abstraction using subtrees of transaction IDs. This method significantly streamlines the validation process and synchronization among miners, optimizing the overall efficiency of the network. Block Size Originally capped at 1MB (1 Megabyte), restricting transactions per block. Increased to 2GB, then to 4GB block size limit. Current BSV expands to 4GB (4 Gigabytes), increasing transaction capacity. Teranode removes the size limit, enabling limitless transactions per block. Processed Transactions per second 3.3 to 7 transactions per second. Up to several thousand transactions per second. Guaranteees a minimum of 1 million transactions per second (100,000 x faster than BTC). Mempool Maintains a memory pool of unconfirmed transactions waiting to be included in blocks. Size limited by node memory. Similar to BTC, but with larger capacity due to increased memory limits. No traditional mempool. Transactions are immediately processed and organized into subtrees. Continuous validation and subtree creation replaces mempool functionality. <p> </p>"},{"location":"topics/architecture/teranode-overall-system-design/#43-network-behavior","title":"4.3 Network Behavior","text":"<ul> <li>Transactions are broadcast network-wide, and each node further propagates the transactions.</li> <li>Nodes broadcast subtrees to indicate prepared batches of transactions for block inclusion.</li> <li>When a block is found, its validation is expedited due to the continuous processing of subtrees.</li> </ul>"},{"location":"topics/architecture/teranode-overall-system-design/#5-node-workflow","title":"5. Node Workflow","text":"<ol> <li>Transaction Submission: Transactions are received by all nodes via a broadcast service.</li> <li>Transaction Validation: The TX Validator Service checks each transaction against network rules.</li> <li>Subtree Assembly: The Block Assembly Service organizes transactions into subtrees.</li> <li>Subtree Validation: The Block Validation Service validates received subtrees.</li> <li>Block Assembly: The Block Assembly Service compiles block templates consisting of subtrees.</li> <li>Block Validation: When a valid block is found, it's sent to the Block Validation Service for final checks before being appended to the blockchain.</li> </ol>"},{"location":"topics/architecture/teranode-overall-system-design/#6-scalability-and-performance","title":"6. Scalability and Performance","text":"<p>Teranode achieves high throughput through:</p> <ul> <li>Horizontal scaling: Spreading workload across multiple machines.</li> <li>Unbounded block size: Allowing for potentially unlimited transactions per block.</li> <li>Subtree processing: Enabling continuous validation of transaction batches.</li> <li>Extended transaction format: Facilitating more efficient transaction processing.</li> </ul>"},{"location":"topics/architecture/teranode-overall-system-design/#7-impact-on-end-users-and-developers","title":"7. Impact on End-Users and Developers","text":"<p>The Teranode architecture offers several benefits:</p> <ul> <li>Higher transaction throughput: Enabling more transactions per second.</li> <li>Lower fees: Due to increased capacity and efficiency.</li> <li>Faster transaction confirmation: Through continuous subtree processing.</li> <li>Improved scalability: Allowing the network to grow with demand.</li> </ul>"},{"location":"topics/architecture/teranode-overall-system-design/#8-glossary-of-terms","title":"8. Glossary of Terms","text":"<ul> <li>Subtree: An intermediate data structure holding batches of transaction IDs and their Merkle root.</li> <li>Extended Transaction: A transaction format that includes additional metadata for efficient processing.</li> <li>UTXO: Unspent Transaction Output, representing spendable coins in the Bitcoin system.</li> <li>Merkle root: A single hash representing a set of transactions in a subtree or block.</li> </ul> <p>Please check the Teranode BSV Glossary for more terms and definitions.</p>"},{"location":"topics/architecture/teranode-overall-system-design/#9-related-resources","title":"9. Related Resources","text":"<ul> <li>Teranode Microservices Overview</li> <li>Extended Transactions: Include additional metadata to facilitate processing.</li> <li>Subtrees: Contain lists of transaction IDs and their Merkle root.</li> <li>Blocks: Contain lists of subtree identifiers, not transactions.</li> </ul>"},{"location":"topics/architecture/understandingDoubleSpends/","title":"Understanding Double Spend Handling and Conflict Resolution in Teranode","text":""},{"location":"topics/architecture/understandingDoubleSpends/#index","title":"Index","text":"<ol> <li>Introduction<ul> <li>1.1 The Double Spend Problem</li> <li>1.2 Teranode's Approach to Double Spend Prevention</li> <li>1.3 Key Concepts in Teranode's Implementation</li> </ul> </li> <li>Core Concepts<ul> <li>2.1 Double Spend Detection</li> <li>2.1.1 First-Seen Rule Implementation</li> <li>2.1.2 Detection During Transaction Validation</li> <li>2.1.3 Detection During Block Validation</li> <li>2.1.4 Subtree Validation Handling</li> <li>2.1.5 Detection Outcomes</li> <li>2.2 Transaction States</li> <li>2.2.1 Non-conflicting Transactions</li> <li>2.2.2 Conflicting Transactions</li> <li>2.2.3 Child Transactions</li> <li>2.3 Conflict Storage and Tracking</li> <li>2.3.1 UTXO Store</li> <li>2.3.2 Subtree Storage</li> <li>2.3.3 Parent-Child Relationship</li> </ul> </li> <li>Chain Reorganization Handling<ul> <li>Phase 1: Mark Original as Conflicting</li> <li>Phase 2: Unspend Original</li> <li>Phase 3: Process Double Spend</li> <li>Phase 4: Update Double Spend Status</li> <li>Phase 5: Cleanup</li> </ul> </li> <li>Other Resources</li> </ol>"},{"location":"topics/architecture/understandingDoubleSpends/#1-introduction","title":"1. Introduction","text":"<p>A double spend occurs when someone attempts to spend the same Bitcoin UTXO (Unspent Transaction Output) more than once. This is one of the fundamental problems that Bitcoin was designed to solve, as preventing double spending is crucial for maintaining the integrity of a digital currency system.</p>"},{"location":"topics/architecture/understandingDoubleSpends/#11-the-double-spend-problem","title":"1.1 The Double Spend Problem","text":"<p>Consider a simple scenario:</p> <ol> <li>\"User A\" has 1 BSV in a UTXO</li> <li>\"User A\" creates transaction A sending that 1 BSV to \"User B\"</li> <li>\"User A\" also creates transaction B sending the same 1 BSV to \"User D\"</li> <li>Both transactions attempt to spend the same UTXO</li> </ol> <p></p> <p>This is a double spend attempt - the same UTXO cannot be validly spent twice. The network must have a consistent way to determine which transaction is valid and which should be rejected.</p>"},{"location":"topics/architecture/understandingDoubleSpends/#12-teranodes-approach-to-double-spend-prevention","title":"1.2 Teranode's Approach to Double Spend Prevention","text":"<p>Teranode implements a double spend prevention mechanism based on several key principles:</p> <ol> <li> <p>First-Seen Rule: The first valid transaction seen by the network that spends a particular UTXO is considered the \"original\" transaction. Any subsequent transaction attempting to spend the same UTXO is marked as a \"conflicting\" transaction.</p> </li> <li> <p>Proof of Work Override: While double spends are outright rejected during normal transaction validation, if a double spend appears in a block with valid proof of work, it receives special handling. In this scenario, the \"conflicting\" transaction is saved, but marked as \"conflicting\" in the UTXO storage. This is necessary because:</p> <ul> <li>The block represents network consensus</li> <li>The block's chain might become the longest chain of work</li> <li>The network must be able to handle reorganizations consistently</li> <li>To be able to reorganize, we want to track all transactions in blocks with valid proof of work</li> </ul> </li> <li> <p>Conflict Propagation: When conflicts are detected, the information is:</p> <ul> <li>Stored in the UTXO store (conflict status is tracked for the specific UTXO)</li> <li>Marked in the subtree stored on disk (subtrees track conflicting transactions within it)</li> <li>Propagated to child transactions (see next point)</li> </ul> </li> <li> <p>Child Transaction Handling: Any transaction that depends on (spends from) a conflicting transaction is also marked as conflicting. This \"poison pill\" effect ensures that entire chains of invalid transactions are properly tracked.</p> </li> </ol>"},{"location":"topics/architecture/understandingDoubleSpends/#13-key-concepts-in-teranodes-implementation","title":"1.3 Key Concepts in Teranode's Implementation","text":"<p>Teranode's double spend handling involves several important concepts:</p> <ol> <li> <p>Transaction States:</p> <ul> <li>Non-conflicting: Normal transactions following the first-seen rule</li> <li>Conflicting: Transactions attempting to double spend UTXOs</li> <li>Child conflicts: Transactions spending outputs from conflicting transactions</li> </ul> </li> <li> <p>Storage Mechanisms:</p> <ul> <li>UTXO store maintains conflict status and relationships</li> <li>Subtrees track conflicts for block validation</li> <li>All conflicting states have associated TTL (Time To Live) - conflicting transactions are eventually removed (once clear a reorganization is not possible)</li> </ul> </li> <li> <p>Processing Phases:</p> <ul> <li>Detection during transaction validation (double spends outright rejected)</li> <li>Special handling during block processing (third party blocks with conflicting transactions are processed, and the conflicting transactions stored and marked as such)</li> <li>Chain reorganization handling - should a reorganization occur, conflicting transactions are reprocessed, with the original ones now marked as \"conflicting\"</li> <li> <p>Five-phase commit process for resolving conflicts:</p> </li> <li> <p>Mark original transaction and children as conflicting</p> </li> <li>Unspend original transaction and children, temporarily marking its parent txs as not spendable (to prevent re-spending)</li> <li>Mark double-spend as non-conflicting</li> <li>Mark parents as spendable again (remove temporary marking)</li> </ul> </li> </ol> <p>The following services are key to Teranode's double spend handling:</p> <ul> <li>Propagation / TX Validator: Detects and rejects double spends during transaction validation</li> <li>Subtree Validation: Rejects subtrees with double spends during block validation</li> <li>Block Validation: Handles double spends in blocks with proof of work, storing the transaction in the UTXO store and marking it as \"conflicting\"</li> <li>UTXO Store: Stores conflict status and relationships</li> <li>Block Assembly: Handles chain reorganizations</li> </ul>"},{"location":"topics/architecture/understandingDoubleSpends/#2-core-concepts","title":"2. Core Concepts","text":""},{"location":"topics/architecture/understandingDoubleSpends/#21-double-spend-detection","title":"2.1 Double Spend Detection","text":"<p>Teranode implements different handling mechanisms depending on where the double spend is detected. There are two primary scenarios: detection during transaction validation and detection during block validation.</p>"},{"location":"topics/architecture/understandingDoubleSpends/#211-first-seen-rule-implementation","title":"2.1.1 First-Seen Rule Implementation","text":"<p>The first-seen rule is Teranode's primary mechanism for handling double spends during normal transaction processing. Under this rule:</p> <ul> <li>The first valid transaction that spends a particular UTXO is considered the \"original\" transaction</li> <li>Any subsequent transaction from other nodes attempting to spend the same UTXO is considered a \"conflicting\" transaction.</li> </ul>"},{"location":"topics/architecture/understandingDoubleSpends/#212-detection-during-transaction-validation","title":"2.1.2 Detection During Transaction Validation","text":"<p>When transactions arrive through the Validator component (Propagation Service):</p> <ol> <li> <p>UTXO Check:</p> <ul> <li>The validator checks if the UTXOs being spent are available</li> <li>If a UTXO is already spent, the transaction is identified as a potential double spend</li> </ul> </li> <li> <p>Immediate Rejection:</p> <ul> <li>Double spends detected at this stage are rejected</li> <li>They are NOT propagated further in the network</li> <li>They are NOT added to subtrees or blocks being assembled in the node</li> </ul> </li> </ol> <p></p>"},{"location":"topics/architecture/understandingDoubleSpends/#213-detection-during-block-validation","title":"2.1.3 Detection During Block Validation","text":"<p>Double spend detection behaves differently when the transactions are detected as part of the validation of a block with proof of work. In this scenario, Teranode understands that the conflicting transaction has been treated as valid by the network and included in a block. A remote node has invested work in creating the block, and it can become part of the longest honest chain. The conflicting transaction can no longer be ignored, and it must be processed - but flagged as \"conflicting\".</p> <ol> <li> <p>Block-Level Processing:</p> <ul> <li>Double spends in valid blocks must be processed.</li> <li>They are stored in the UTXO store and marked as \"conflicting\" (<code>conflicting</code>)</li> <li>The tx parent is modified to include a <code>conflictingChildren</code> field, which lists both the original and the conflicting \"child\" transactions</li> <li>The transactions are also marked as conflicting in the subtree store (<code>ConflictingNodes</code> field)</li> </ul> </li> <li> <p>Parent-Child Relationships:</p> <ul> <li>All transactions that spend outputs from a conflicting transaction are also marked as conflicting</li> <li>This creates a chain of conflicts that must be tracked</li> <li>The conflict status is stored in the first \"common\" parent (the first parent transaction that is not conflicting) using the <code>conflictingChildren</code> field</li> </ul> </li> <li> <p>Conflict Tracking:</p> <ul> <li>For conflicting transactions, a UTXO Store TTL is set - conflicting transactions are removed from the store once the TTL expires (indicating that a reorganization is no longer possible and the data is no longer needed)</li> </ul> </li> </ol> <p></p>"},{"location":"topics/architecture/understandingDoubleSpends/#214-subtree-validation-handling","title":"2.1.4 Subtree Validation Handling","text":"<p>The SubtreeValidation service provides an additional layer of double spend detection:</p> <ol> <li> <p>Subtree Checks:</p> <ul> <li>Subtrees containing double spends are rejected during normal validation</li> <li>They are NOT \"blessed\" (approved) for inclusion in blocks</li> <li>Exception: When the subtree is part of a block with proof of work</li> </ul> </li> <li> <p>Block Context:</p> <ul> <li> <p>When validating subtrees that are part of a block with proof of work:</p> <ul> <li>Double spends are allowed, but relevant txs are marked as \"conflicting\"</li> <li>The subtree includes a <code>ConflictingNodes</code> field, indicating the txs under contention</li> </ul> </li> </ul> </li> </ol> <p></p>"},{"location":"topics/architecture/understandingDoubleSpends/#215-detection-outcomes","title":"2.1.5 Detection Outcomes","text":"<p>The outcome of double spend detection varies based on the context:</p> <ol> <li> <p>Transaction Validation:</p> <ul> <li>Double spends are rejected</li> <li>Original transaction remains valid</li> </ul> </li> <li> <p>Block-Included Transactions:</p> <ul> <li>Double spends are processed</li> <li>Conflicts are marked and tracked</li> <li>Chain reorganization logic determines final validity</li> </ul> </li> <li> <p>Child Transactions:</p> <ul> <li>Automatically inherit conflict status</li> <li>Cannot be processed by validator or subtree validation</li> <li>Are tracked for potential chain reorganization</li> </ul> </li> </ol>"},{"location":"topics/architecture/understandingDoubleSpends/#22-transaction-states","title":"2.2 Transaction States","text":""},{"location":"topics/architecture/understandingDoubleSpends/#221-non-conflicting-transactions","title":"2.2.1 Non-conflicting Transactions","text":"<ul> <li>These are transactions that follow the first-seen rule (first valid transaction that spends a particular UTXO). They are considered the \"original\" transactions in case of double spends.</li> <li>Processed normally through validation and block assembly</li> </ul>"},{"location":"topics/architecture/understandingDoubleSpends/#222-conflicting-transactions","title":"2.2.2 Conflicting Transactions","text":"<ul> <li>Transactions attempting to spend UTXOs that are already spent by another transaction</li> <li>Only processed if seen in a block with valid proof of work</li> <li> <p>Marked as conflicting in:</p> <ul> <li>The UTXO store</li> <li>The subtree store</li> <li>Have an associated TTL (Time To Live), after which they are removed from the store</li> </ul> </li> </ul>"},{"location":"topics/architecture/understandingDoubleSpends/#223-child-transactions","title":"2.2.3 Child Transactions","text":"<ul> <li>Transactions that spend outputs from other transactions</li> <li>Inherit conflict status from their parents</li> <li> <p>If a parent transaction is marked as conflicting:</p> <ul> <li>All child transactions are automatically marked as conflicting</li> </ul> </li> </ul>"},{"location":"topics/architecture/understandingDoubleSpends/#23-conflict-storage-and-tracking","title":"2.3 Conflict Storage and Tracking","text":""},{"location":"topics/architecture/understandingDoubleSpends/#231-utxo-store","title":"2.3.1 UTXO Store","text":"<ul> <li>Primary storage for conflict information</li> <li> <p>Stores conflict status in two ways:</p> <ul> <li><code>conflicting</code> flag on transactions</li> <li><code>conflictingChildren</code> list for parent transactions</li> </ul> </li> </ul>"},{"location":"topics/architecture/understandingDoubleSpends/#232-subtree-storage","title":"2.3.2 Subtree Storage","text":"<ul> <li>Subtrees maintain a <code>ConflictingNodes</code> array</li> <li>Contains hashes of transactions marked as conflicting</li> <li>Used during chain reorganization</li> <li>Only transactions present in the subtree can be marked as conflicting</li> </ul>"},{"location":"topics/architecture/understandingDoubleSpends/#233-parent-child-relationship","title":"2.3.3 Parent-Child Relationship","text":"<ul> <li>Parent transactions track their conflicting children</li> <li>When marking a transaction as conflicting:</li> <li>Parent transactions are updated first</li> <li>Parent's <code>conflictingChildren</code> list is updated</li> </ul>"},{"location":"topics/architecture/understandingDoubleSpends/#3-chain-reorganization-handling","title":"3. Chain Reorganization Handling","text":"<p>During chain reorganization, when a block containing a double spend becomes part of the longest chain, Teranode must transition the UTXO set from the original transaction to the double spend (the \"conflicting\" transaction).</p> <p>This is handled through a five-phase commit process:</p> <p></p>"},{"location":"topics/architecture/understandingDoubleSpends/#phase-1-mark-original-as-conflicting","title":"Phase 1: Mark Original as Conflicting","text":"<ul> <li>The original transaction and all its children are marked as conflicting. This is done recursively through the transaction chain, ensuring all dependent transactions are properly marked.</li> </ul> <pre><code>Technical changes:\n\n- UTXO Store:\n\n    - Sets `conflicting = true` on tx_original\n    - Sets TTL for cleanup\n    - Updates `conflictingChildren[]` array in parent transactions\n    - Recursively marks all child transactions\n- Subtree Store:\n\n    - Adds transaction hashes to `ConflictingNodes[]` array\n    - Persists updated subtree\n</code></pre>"},{"location":"topics/architecture/understandingDoubleSpends/#phase-2-unspend-original","title":"Phase 2: Unspend Original","text":"<ul> <li>The original transaction is unspent, releasing its parent UTXOs</li> <li>All child transactions are also unspent</li> <li>Parent UTXOs are marked as \"not spendable\" to prevent other transactions from spending them during the reorganization process (this is a temporariry measure, remove the last step of the process)</li> </ul> <pre><code>Technical changes:\n\n- UTXO Store:\n\n    - Clears `spendingTxID` from parent UTXOs\n    - Sets `locked = true` on parent UTXOs\n    - Removes spend markers from tx_original's outputs\n    - Maintains conflict flags and TTLs\n</code></pre>"},{"location":"topics/architecture/understandingDoubleSpends/#phase-3-process-double-spend","title":"Phase 3: Process Double Spend","text":"<ul> <li>The double spend transaction is processed, spending its inputs even though some UTXOs are marked as not spendable. This is allowed specifically for this phase of the reorg process.</li> </ul> <pre><code>Technical changes:\n\n- UTXO Store:\n\n    - Sets `spendingTxID` to double spend transaction\n    - Creates new UTXOs for double spend outputs\n    - Ignores `locked` flag during spend\n    - No TTL set on double spend transaction\n</code></pre>"},{"location":"topics/architecture/understandingDoubleSpends/#phase-4-update-double-spend-status","title":"Phase 4: Update Double Spend Status","text":"<ul> <li>The double spend transaction is marked as non-conflicting, establishing it as the valid transaction in the new chain.</li> </ul> <pre><code>Technical changes:\n\n- UTXO Store:\n\n    - Sets `conflicting = false` on double spend\n    - Removes any TTL\n- Subtree Store:\n\n    - Removes hash from `ConflictingNodes[]` if present\n</code></pre>"},{"location":"topics/architecture/understandingDoubleSpends/#phase-5-cleanup","title":"Phase 5: Cleanup","text":"<ul> <li>Parent UTXOs are made \"spendable\" again, removing the temporary restriction.</li> </ul> <pre><code>Technical changes:\n\n- UTXO Store:\n\n    - Sets `locked = false` on parent UTXOs\n</code></pre> <p>At the end of the process, the original transaction is now marked as conflicting, and the double spend transaction is considered valid. The UTXO set has been successfully transitioned to the new chain.</p> <p></p> <p>Note how the reorganization process does not unspend regular transactions, provided they are present in both chains. Only double spent transactions are affected by the reorganization handling described above.</p>"},{"location":"topics/architecture/understandingDoubleSpends/#4-other-resources","title":"4. Other Resources","text":"<ul> <li>UTXO Store</li> <li>Tx Validator</li> <li>Subtree Validation Service</li> <li>Block Validation Service</li> <li>Block Assembly Service</li> </ul>"},{"location":"topics/commands/seeder/","title":"\ud83c\udf31 Seeder Command","text":""},{"location":"topics/commands/seeder/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1 Command Initialization</li> <li>2.2 Processing Headers</li> <li>2.3 Processing UTXOs</li> </ul> </li> <li>Data Model</li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>How to Run</li> <li>Configuration Options</li> </ol>"},{"location":"topics/commands/seeder/#1-description","title":"1. Description","text":"<p>The Seeder is a command-line tool designed to populate the UTXO store with data up to and including a specific block. It reads a UTXO set (as produced by the <code>UTXO Persister</code> service) and writes all the necessary records to the configured UTXO and Blockchain storage system (e.g., Aerospike and Postgres). This tool is crucial for initializing or updating the UTXO state of a Bitcoin node.</p> <p>Key features:</p> <ol> <li>Processes block headers and stores them in the blockchain store.</li> <li>Reads and processes UTXO data from a file.</li> <li>Populates the UTXO store with processed data.</li> <li>Supports parallel processing of UTXOs for improved performance.</li> <li>Provides options to skip header or UTXO processing if needed.</li> </ol> <p></p>"},{"location":"topics/commands/seeder/#2-functionality","title":"2. Functionality","text":""},{"location":"topics/commands/seeder/#21-command-initialization","title":"2.1 Command Initialization","text":"<ol> <li>The command starts by parsing command-line flags to determine the input directory, target hash, and processing options.</li> <li>It verifies the existence of required input files (headers and UTXO set).</li> <li>The command sets up signal handling for graceful shutdown.</li> <li>An HTTP server is started for profiling purposes.</li> </ol>"},{"location":"topics/commands/seeder/#22-processing-headers","title":"2.2 Processing Headers","text":"<p>If header processing is not skipped:</p> <ol> <li>The command opens the headers file and verifies its magic number.</li> <li>It reads headers sequentially from the file.</li> <li> <p>For each header:</p> <ul> <li>A <code>model.Block</code> object is created.</li> <li>The block is stored in the blockchain store.</li> <li>The command keeps track of the number of headers processed and total transaction count.</li> </ul> </li> </ol>"},{"location":"topics/commands/seeder/#23-processing-utxos","title":"2.3 Processing UTXOs","text":"<p>If UTXO processing is not skipped:</p> <ol> <li>The service initializes a connection to the UTXO store.</li> <li>It sets up a channel for UTXO processing and spawns multiple worker goroutines.</li> <li>The UTXO file is opened and its header is verified.</li> <li>UTXOs are read from the file and sent to the processing channel.</li> <li> <p>Worker goroutines process UTXOs in parallel:</p> <ul> <li>Each UTXO is converted to a Bitcoin transaction format.</li> <li>The transaction is stored in the UTXO store.</li> <li>The service tracks the number of transactions and UTXOs processed.</li> </ul> </li> </ol>"},{"location":"topics/commands/seeder/#3-data-model","title":"3. Data Model","text":"<p>The Seeder works with the following key data structures:</p> <ol> <li> <p>Block Header:    Represented by <code>utxopersister.BlockIndex</code>, containing:</p> <ul> <li>Block header information</li> <li>Transaction count</li> <li>Block height</li> </ul> </li> <li> <p>UTXO Wrapper:    Represented by <code>utxopersister.UTXOWrapper</code>, containing:</p> <ul> <li>Transaction ID</li> <li>Block height</li> <li>Coinbase flag</li> <li>List of UTXOs</li> </ul> </li> <li> <p>UTXO:    Represented within the UTXO Wrapper, containing:</p> <ul> <li>Output index</li> <li>Value (in satoshis)</li> <li>Locking script</li> </ul> </li> </ol> <p>The service reads these structures from input files and converts them to the format required by the blockchain and UTXO stores.</p>"},{"location":"topics/commands/seeder/#4-technology","title":"4. Technology","text":"<ol> <li> <p>Go (Golang): The primary programming language.</p> </li> <li> <p>Bitcoin SV Libraries:</p> <ul> <li><code>github.com/bsv-blockchain/go-bt/v2</code>: For handling Bitcoin transactions and scripts.</li> <li> <p>Custom TERANODE Libraries:</p> </li> <li> <p><code>github.com/bsv-blockchain/teranode</code>: For blockchain and UTXO store operations, error handling, and logging.</p> </li> </ul> </li> <li> <p>Concurrent Processing:</p> <ul> <li><code>golang.org/x/sync/errgroup</code>: For managing concurrent UTXO processing.</li> </ul> </li> <li> <p>Configuration Management:</p> <ul> <li><code>github.com/ordishs/gocore</code>: For reading configuration values.</li> </ul> </li> </ol>"},{"location":"topics/commands/seeder/#5-directory-structure-and-main-files","title":"5. Directory Structure and Main Files","text":"<pre><code>./cmd/seeder/\n\u2502\n\u251c\u2500\u2500 seeder.go\n\u2502   Entry point for the Seeder command-line tool.\n\u2502\n\u251c\u2500\u2500 seeder/\n\u2502   \u251c\u2500\u2500 start.go\n\u2502   \u2502   Main implementation of the Seeder service, including\n\u2502   \u2502   command-line parsing, header processing, and UTXO processing.\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 README.md\n\u2502       Documentation for the Seeder service.\n</code></pre>"},{"location":"topics/commands/seeder/#6-how-to-run","title":"6. How to Run","text":"<p>To run the Seeder command, use the following command:</p> <pre><code>teranode-cli seeder -inputDir &lt;folder&gt; -hash &lt;hash&gt; [-skipHeaders] [-skipUTXOs]\n</code></pre> <p>Options:</p> <ul> <li><code>-inputDir</code>: Specifies the input directory containing UTXO set and headers files.</li> <li><code>-hash</code>: Hash of the UTXO set / headers to process.</li> <li><code>-skipHeaders</code>: (Optional) Skip processing of headers.</li> <li><code>-skipUTXOs</code>: (Optional) Skip processing of UTXOs.</li> </ul>"},{"location":"topics/commands/seeder/#7-configuration-options","title":"7. Configuration Options","text":"<p>The Seeder uses various configuration options, which can be set through a configuration system:</p> <ul> <li><code>blockchain_store</code>: URL for the blockchain store.</li> <li><code>utxostore</code>: URL for the UTXO store.</li> <li><code>channelSize</code>: Size of the channel for UTXO processing (default: 1000).</li> <li><code>workerCount</code>: Number of worker goroutines for UTXO processing (default: 500).</li> <li><code>skipStore</code>: Boolean flag to skip storing UTXOs (for testing purposes).</li> </ul>"},{"location":"topics/datamodel/block_data_model/","title":"Teranode Data Model - Blocks","text":"<p>The Teranode BSV model introduces a novel approach to block propagation, optimizing the network for high transaction throughput.</p> <p>Teranode Blocks contain lists of subtree identifiers, not transactions. This is practical for nodes because they have been processing subtrees continuously, this allows for quick validation of blocks.</p> <p></p> <p>Each block is an abstraction which is a container of a group of subtrees. A block contains a variable number of subtrees, a coinbase transaction, and a header, called a block header, which includes the block ID of the previous block, effectively creating a chain.</p> Field Type Description Header *BlockHeader The Block Header CoinbaseTx *bt.Tx The coinbase transaction. Subtrees []*chainhash.Hash An array of hashes, representing the subtrees of the block. <p>This table provides an overview of each field in the <code>Block</code> struct, including the data type and a brief description of its purpose or contents.</p>"},{"location":"topics/datamodel/block_data_model/#block-storage-metadata","title":"Block Storage Metadata","text":"<p>In addition to the core block data, Teranode maintains additional metadata for block management:</p> Field Type Description ID uint64 Unique identifier for the block in storage Invalid bool Flag indicating if the block has been marked as invalid MinedSet bool Flag indicating if the block has been marked as mined SubtreesSet bool Flag indicating if subtrees have been processed"},{"location":"topics/datamodel/block_data_model/#block-id-pre-allocation","title":"Block ID Pre-allocation","text":"<p>Teranode supports pre-allocation of block IDs through the blockchain service's <code>GetNextBlockID</code> method. This feature enables:</p> <ul> <li>Parallel block processing with guaranteed unique IDs</li> <li>Quick validation scenarios where blocks need IDs before full processing</li> <li>Recovery scenarios requiring coordinated ID assignment across services</li> </ul>"},{"location":"topics/datamodel/block_data_model/#invalid-block-tracking","title":"Invalid Block Tracking","text":"<p>Blocks can be marked as invalid during validation, particularly during catchup operations. Invalid blocks:</p> <ul> <li>Are tracked in the blockchain store to prevent reprocessing</li> <li>Inherit their invalid status to child blocks (invalid parent = invalid children)</li> <li>Help identify problematic peers providing bad blocks</li> <li>Can be tracked for potential revalidation in recovery scenarios</li> </ul> <p>For details about the block header, please check the <code>references</code> section below.</p> <p>Please note that the use of subtrees within blocks represents a data abstraction for a more optimal propagation of transactions. The data model is still the same as Bitcoin, with blocks containing transactions. The subtrees are used to optimize the propagation of transactions and blocks.</p>"},{"location":"topics/datamodel/block_data_model/#advantages-of-the-teranode-bsv-model","title":"Advantages of the Teranode BSV Model","text":"<ul> <li> <p>Faster Validation: Since nodes process subtrees continuously, validating a block is quicker because it involves validating the presence and correctness of subtree identifiers rather than individual transactions.</p> </li> <li> <p>Scalability: The model supports a much higher transaction throughput (&gt; 1M transactions per second).</p> </li> </ul>"},{"location":"topics/datamodel/block_data_model/#network-behavior","title":"Network Behavior","text":"<ul> <li> <p>Transactions: They are broadcast network-wide, and each node further propagates the transactions.</p> </li> <li> <p>Subtrees: Nodes broadcast subtrees to indicate prepared batches of transactions for block inclusion, allowing other nodes to perform preliminary validations.</p> </li> <li> <p>Block Propagation: When a block is found, its validation is expedited due to the continuous processing of subtrees. If a node encounters a subtree within a new block that it is unaware of, it can request the details from the node that submitted the block.</p> </li> </ul> <p>This proactive approach with subtrees enables the network to handle a significantly higher volume of transactions while maintaining quick validation times. It also allows nodes to utilize their processing power more evenly over time, rather than experiencing idle times between blocks. This model ensures that Teranode BSV can scale effectively to meet high transaction demands without the bottlenecks experienced by the BTC network.</p>"},{"location":"topics/datamodel/block_data_model/#historical-bitcoin-block-model","title":"Historical Bitcoin Block Model","text":"<p>Historically, Bitcoin blocks have contained transactions, with each block linked to the previous one by a cryptographic hash.</p> <p></p> <p>Note how the Bitcoin block contains all transactions (including ALL transaction data) for each transaction it contains, not just the transaction Id. This means that the block size will be very large if many transactions were included. At scale, this is not practical, as the block size would be too large to propagate across the network in a timely manner.</p> <p>The Teranode BSV model optimizes this by using subtrees to represent transactions in a block, allowing for more efficient propagation and validation of blocks.</p>"},{"location":"topics/datamodel/block_data_model/#additional-resources","title":"Additional Resources","text":"<ul> <li>Overall System Design</li> <li>Block Header</li> <li>Subtree</li> </ul>"},{"location":"topics/datamodel/block_header_data_model/","title":"Teranode Data Model - Block Header","text":"<p>The block header is a data structure that contains metadata about a block. It is used to connect blocks together in a blockchain. The block header is a structure that is hashed as part of the proof-of-work algorithm for mining. It contains the following fields:</p> Field Type Description Version uint32 Version of the block, different from the protocol version. Represented as 4 bytes in little endian when built into block header bytes. HashPrevBlock *chainhash.Hash Reference to the hash of the previous block header in the blockchain. HashMerkleRoot *chainhash.Hash Reference to the Merkle tree hash of all subtrees in the block. Timestamp uint32 The time when the block was created, in Unix time. Represented as 4 bytes in little endian when built into block header bytes. Bits NBit Difficulty target for the block. Represented as a target threshold in little endian, the format used in a Bitcoin block. Nonce uint32 Nonce used in generating the block. Represented as 4 bytes in little endian when built into block header bytes."},{"location":"topics/datamodel/block_header_data_model/#additional-resources","title":"Additional Resources","text":"<ul> <li>Block Data Model</li> </ul>"},{"location":"topics/datamodel/subtree_data_model/","title":"Teranode Data Model - Subtrees","text":"<p>The Subtrees are an innovation aimed at improving the scalability and real-time processing capabilities of the blockchain system.</p>"},{"location":"topics/datamodel/subtree_data_model/#structure","title":"Structure","text":"<p>The concept of subtrees is a distinct feature not found in the BTC design.</p> <ol> <li> <p>A subtree acts as an intermediate data structure to hold batches of transaction IDs (including metadata) and their corresponding Merkle root.</p> <ul> <li>The size of a subtree can be any number of transactions, as long as it is a power of 2 (16, 32, 64, etc.). The only requirement is that all subtrees in a block must be the same size. At peak throughput, subtrees will contain millions of transaction IDs.</li> </ul> </li> <li> <p>Each subtree computes its own Merkle root, which is a single hash representing the entire set of transactions within that subtree.</p> </li> </ol> <p></p> <p>Here's a table documenting the structure of the <code>Subtree</code> type:</p> Field Type Description Height int The height of the subtree within the blockchain. Fees uint64 Total fees associated with the transactions in the subtree. SizeInBytes uint64 The size of the subtree in bytes. FeeHash chainhash.Hash Hash representing the combined fees of the subtree. Nodes []SubtreeNode An array of <code>SubtreeNode</code> objects, representing individual \"nodes\" within the subtree. ConflictingNodes []chainhash.Hash List of hashes representing nodes that conflict, requiring checks during block assembly. <p>Here, a `SubtreeNode is a data structure representing a transaction hash, a fee, and the size in bytes of said TX.</p> <p>Note - For subtree files in the <code>subtree-store</code> S3 buckets, each subtree has a size of 48MB.</p>"},{"location":"topics/datamodel/subtree_data_model/#subtree-composition","title":"Subtree Composition","text":"<p>Each subtree consists of:</p> <ul> <li>root hash: 32 bytes</li> <li>fees: 8 bytes (uint64)</li> <li>sizeInBytes: 8 bytes (uint64)</li> <li>numberOfNodes: 8 bytes (uint64)</li> <li>nodes: 48 bytes per node (hash:32 + fee:8 + size:8)</li> <li>numberOfConflictingNodes: 8 bytes (uint64)</li> <li>conflictingNodes: 32 bytes per conflicting node</li> </ul>"},{"location":"topics/datamodel/subtree_data_model/#calculation","title":"Calculation:","text":"<pre><code>Fixed header: 32 + 8 + 8 + 8 + 8 = 64 bytes\n\nAdditional - Per transaction node: 48 bytes\n</code></pre>"},{"location":"topics/datamodel/subtree_data_model/#data-transfer-between-nodes","title":"Data Transfer Between Nodes","text":"<p>However - only 32MB is transferred between the nodes. Each subtree transfer includes:</p> <ul> <li>hash: 32 bytes <pre><code>1024 * 1024 * (32) = 32MB\n</code></pre></li> </ul>"},{"location":"topics/datamodel/subtree_data_model/#efficiency","title":"Efficiency","text":"<p>Subtrees are broadcast every second (assuming a baseline throughput of 1M transactions per second), making data propagation more continuous rather than batched every 10 minutes. Although blocks are still created every 10 minutes, subtrees are broadcast every second.</p> <ol> <li> <p>Broadcasting subtrees at this high frequency allows receiving nodes to validate batches quickly and continuously, essentially \"pre-approving\" them for inclusion in a block.</p> </li> <li> <p>This contrasts with the BTC design, where a new block and its transactions are broadcast approximately every 10 minutes after being confirmed by miners.</p> </li> </ol>"},{"location":"topics/datamodel/subtree_data_model/#lightweight","title":"Lightweight","text":"<p>Subtrees only include transaction IDs, not full transaction data, since all nodes already possess the transactions, reducing the size of the propagated data.</p> <ol> <li> <p>All network nodes are assumed to already have the full transaction data (which they receive and store as transactions are created and spread through the network). Therefore, it's unnecessary to rebroadcast full details with each subtree.</p> </li> <li> <p>Subtrees allow nodes to confirm they have all relevant transactions and update their state accordingly without having to process large amounts of data repeatedly.</p> </li> </ol> <p></p>"},{"location":"topics/datamodel/subtree_data_model/#additional-resources","title":"Additional Resources","text":"<ul> <li>Overall System Design</li> <li>Block</li> <li>Transaction</li> </ul>"},{"location":"topics/datamodel/transaction_data_model/","title":"Teranode Data Model - Transaction Format Support","text":"<p>Teranode supports both standard Bitcoin transaction format and Extended Transaction Format (as defined in BIP-239). The Extended Format includes additional metadata to facilitate processing and validation.</p>"},{"location":"topics/datamodel/transaction_data_model/#transaction-format-flexibility","title":"Transaction Format Flexibility","text":"<p>Teranode accepts transactions in either format:</p> <ol> <li>Standard Bitcoin Format: Traditional Bitcoin transactions without extended input data</li> <li>Extended Format (BIP-239): Transactions with additional input metadata (previous output satoshis and locking scripts)</li> </ol> <p>When a transaction arrives in standard format, Teranode automatically extends it during validation by looking up the required input data from the UTXO store. This on-demand extension approach provides:</p> <ul> <li>Backward compatibility with existing Bitcoin wallets and applications</li> <li>Storage efficiency by storing transactions in compact non-extended format</li> <li>Validation flexibility by accepting both formats seamlessly</li> </ul> <p>For wallet developers: You may send either format to Teranode. Extended format may provide slightly faster initial validation (skips UTXO lookup), but both are fully supported.</p>"},{"location":"topics/datamodel/transaction_data_model/#transaction-format-specification","title":"Transaction Format Specification","text":"<p>Bitcoin Transaction format:</p> Field Description Size Version no currently 2 4 bytes In-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of inputs Transaction Input  Structure  qty with variable length per input Out-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of outputs Transaction Output Structure  qty with variable length per output nLocktime if non-zero and sequence numbers are &lt; 0xFFFFFFFF: block height or timestamp when transaction is final 4 bytes <p>The Extended Format adds a marker to the transaction format:</p> Field Description Size Version no currently 2 4 bytes EF marker marker for extended format 0000000000EF In-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of inputs Extended Format transaction Input Structure  qty with variable length per input Out-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of outputs Transaction Output Structure  qty with variable length per output nLocktime if non-zero and sequence numbers are &lt; 0xFFFFFFFF: block height or timestamp when transaction is final 4 bytes <p>The Extended Format marker allows a library that supports the format to recognize that it is dealing with a transaction in extended format, while a library that does not support extended format will read the transaction as having 0 inputs, 0 outputs and a future nLock time. This has been done to minimize the possible problems a legacy library will have when reading the extended format. It can in no way be recognized as a valid transaction.</p> <p>The input structure is the only additional thing that is changed in the Extended Format. The original input structure looks like this:</p> Field Description Size Previous Transaction hash TXID of the transaction the output was created in 32 bytes Previous Txout-index Index of the output (Non negative integer) 4 bytes Txin-script length Non negative integer VI = VarInt 1 - 9 bytes Txin-script / scriptSig Script -many bytes Sequence_no Used to iterate inputs inside a payment channel. Input is final when nSequence = 0xFFFFFFFF 4 bytes <p>In the Extended Format, we extend the input structure to include the previous locking script and satoshi outputs:</p> Field Description Size Previous Transaction hash TXID of the transaction the output was created in 32 bytes Previous Txout-index Index of the output (Non negative integer) 4 bytes Txin-script length Non negative integer VI = VarInt 1 - 9 bytes Txin-script / scriptSig Script -many bytes Sequence_no Used to iterate inputs inside a payment channel. Input is final when nSequence = 0xFFFFFFFF 4 bytes Previous TX satoshi output Output value in satoshis of previous input 8 bytes Previous TX script length Non negative integer VI = VarInt 1 - 9 bytes Previous TX locking script Script script length - many bytes <p>The Extended Format is not backwards compatible, but has been designed in such a way that existing software should not read a transaction in Extend Format as a valid (partial) transaction. The Extended Format header (0000000000EF) will be read as an empty transaction with a future nLock time in a library that does not support the Extended Format.</p>"},{"location":"topics/datamodel/transaction_data_model/#comparison-with-the-historical-bitcoin-transactions","title":"Comparison with the historical Bitcoin transactions","text":"<p>Bitcoin Transactions are broadcast and included in blocks as they are found.</p> <p>Current Transaction format:</p> Field Description Size Version no currently 2 4 bytes In-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of inputs Transaction Input  Structure  qty with variable length per input Out-counter positive integer VI = [[VarInt]] 1 - 9 bytes list of outputs Transaction Output Structure  qty with variable length per output nLocktime if non-zero and sequence numbers are &lt; 0xFFFFFFFF: block height or timestamp when transaction is final 4 bytes <p>As opposed to that, the Extended Format includes additional metadata, and is bundled within subtrees containers for efficient processing and propagation.</p>"},{"location":"topics/datamodel/transaction_data_model/#how-teranode-handles-transaction-formats","title":"How Teranode Handles Transaction Formats","text":""},{"location":"topics/datamodel/transaction_data_model/#ingress-receiving-transactions","title":"Ingress (Receiving Transactions)","text":"<p>The Propagation Service accepts transactions in any valid Bitcoin format:</p> <ul> <li>Standard Bitcoin transaction format</li> <li>Extended Format (BIP-239) with the <code>0000000000EF</code> marker</li> </ul> <p>No validation or rejection occurs based on format at ingress. Transactions are stored in their received form to the blob store, and format handling is delegated to the Validator Service.</p>"},{"location":"topics/datamodel/transaction_data_model/#validation-process","title":"Validation Process","text":"<p>During validation, the Validator Service automatically handles format conversion when necessary:</p> <ol> <li>Check if Extended: The validator checks <code>tx.IsExtended()</code> status on the transaction</li> <li>Automatic Extension: If not extended, the validator:</li> <li>Queries the UTXO store for each input's parent transaction</li> <li>Extracts the referenced output's satoshi value and locking script</li> <li>Decorates the transaction inputs with this data in-memory</li> <li>Marks the transaction as extended for the validation process</li> <li>Validation: Proceeds with full validation including script verification using the extended data</li> </ol> <p>This extension process happens transparently at multiple checkpoints throughout the validation pipeline:</p> <ul> <li><code>Validator.Validate()</code> in <code>services/validator/Validator.go</code> - Before transaction format validation</li> <li><code>Validator.validateConsensusRules()</code> in <code>services/validator/Validator.go</code> - Before consensus rule checks</li> <li><code>Validator.validateScripts()</code> in <code>services/validator/Validator.go</code> - Before script validation</li> <li><code>quickValidate()</code> in <code>services/blockvalidation/quick_validate.go</code> - During block validation for historical blocks</li> </ul> <p>Key implementation details:</p> <ul> <li>Parent transactions are looked up in parallel batches using Go's errgroup for optimal performance</li> <li>UTXO store queries are batched based on configuration (<code>UtxoStore.GetBatcherSize</code>)</li> <li>Already-decorated inputs are skipped (idempotent operation)</li> <li>Extension is performed entirely in-memory with no disk writes</li> </ul>"},{"location":"topics/datamodel/transaction_data_model/#storage-format","title":"Storage Format","text":"<p>Transactions are stored in their received format, preserving the format in which they arrived. This design decision provides:</p> <ul> <li>Space efficiency: Eliminates duplicate data since previous outputs are already stored in the UTXO set</li> <li>Compatibility: Standard format is universally readable by all Bitcoin tools</li> <li>Performance: Smaller transaction sizes improve I/O performance across the system</li> </ul> <p>The storage layer uses the go-bt library's <code>SerializeBytes()</code> method, which preserves the received format (extended or standard). When standard format transactions need to be extended (during validation or when serving API requests), the extension happens on-demand in memory.</p>"},{"location":"topics/datamodel/transaction_data_model/#performance-considerations","title":"Performance Considerations","text":"<p>Extended Format Advantages:</p> <ul> <li>Slightly faster initial validation (skips UTXO lookup step for input decoration)</li> <li>Useful for offline validation scenarios where UTXO store access is not available</li> <li>Beneficial for high-throughput transaction submission where every millisecond counts</li> </ul> <p>Standard Format Advantages:</p> <ul> <li>Smaller network transmission size (no duplicate previous output data)</li> <li>Compatible with all existing Bitcoin tools and libraries</li> <li>No change required for existing wallet implementations</li> <li>Widely supported across the Bitcoin ecosystem</li> </ul> <p>Recommendation: Use whichever format is most convenient for your application. The performance difference is negligible in Teranode's architecture due to highly optimized UTXO lookups. Teranode's Aerospike and SQL-based UTXO stores, combined with the txmeta cache, make parent transaction lookups extremely fast (typically sub-millisecond).</p>"},{"location":"topics/datamodel/transaction_data_model/#teranodes-bip-239-implementation","title":"Teranode's BIP-239 Implementation","text":"<p>Teranode implements BIP-239 (Extended Transaction Format) with several enhancements specific to its high-performance, distributed architecture:</p>"},{"location":"topics/datamodel/transaction_data_model/#flexible-format-acceptance","title":"Flexible Format Acceptance","text":"<p>Unlike a strict BIP-239 implementation that might require extended format, Teranode:</p> <ul> <li>Accepts both standard and extended formats at all ingress points (HTTP, gRPC, UDP)</li> <li>Does not reject transactions based on format at the network edge</li> <li>Handles format conversion transparently during the validation pipeline</li> <li>Maintains compatibility with standard Bitcoin transaction processing</li> </ul>"},{"location":"topics/datamodel/transaction_data_model/#automatic-extension-mechanism","title":"Automatic Extension Mechanism","text":"<p>When Teranode receives a standard format transaction, it automatically extends it during validation:</p> <ol> <li>The validator detects non-extended transactions using the <code>IsExtended()</code> check</li> <li>For each input, the system queries the UTXO store to retrieve parent transaction data</li> <li>Input decoration happens in-memory using the parent output's satoshi value and locking script</li> <li>The extended transaction is used for validation but never written back to storage</li> </ol> <p>This automatic extension is highly optimized:</p> <ul> <li>Parallel batch queries minimize latency</li> <li>In-memory operation with no disk I/O overhead</li> <li>Idempotent design allows safe retries</li> <li>Negligible performance impact compared to native extended format</li> </ul>"},{"location":"topics/datamodel/transaction_data_model/#storage-optimization-strategy","title":"Storage Optimization Strategy","text":"<p>Teranode's storage layer differs from a naive BIP-239 implementation:</p> <ul> <li>Transactions stored in received format regardless of ingress format</li> <li>Format flexibility allows clients to choose optimal format for their use case</li> <li>Extended format transactions avoid UTXO lookup during validation</li> <li>Standard format transactions save storage space</li> <li>Extension performed on-demand when needed for validation (if transaction arrives in standard format)</li> </ul>"},{"location":"topics/datamodel/transaction_data_model/#error-handling","title":"Error Handling","text":"<p>If parent transactions cannot be found during extension:</p> <ul> <li>Returns <code>ErrTxMissingParent</code> error to the client</li> <li>Transaction validation fails gracefully</li> <li>Provides clear error message indicating which parent is missing</li> <li>Client can retry after ensuring parent transactions are confirmed</li> </ul> <p>Common scenarios requiring parent transactions:</p> <ul> <li>Child-pays-for-parent (CPFP) transaction patterns</li> <li>Transaction chains where parent hasn't been validated yet</li> <li>Block validation where transactions reference outputs from the same block</li> </ul>"},{"location":"topics/datamodel/transaction_data_model/#comparison-with-bip-239-specification","title":"Comparison with BIP-239 Specification","text":"Aspect BIP-239 Spec Teranode Implementation Format requirement Extended format recommended Both formats accepted Storage Not specified Always standard format Extension Manual by client Automatic by validator Performance Faster validation Negligible difference Compatibility Limited to BIP-239 aware clients Full Bitcoin ecosystem compatibility"},{"location":"topics/datamodel/transaction_data_model/#implementation-references","title":"Implementation References","text":"<p>For developers interested in the implementation details:</p> <ul> <li>Extension logic: <code>Validator.Validate()</code> method in <code>services/validator/Validator.go</code></li> <li>UTXO decoration: <code>PreviousOutputsDecorate()</code> method in <code>stores/utxo/*/</code> implementations</li> <li>Storage serialization: Uses go-bt library's <code>SerializeBytes()</code> method (preserves received format)</li> </ul>"},{"location":"topics/datamodel/transaction_data_model/#additional-resources","title":"Additional Resources","text":"<p>To know more about the Extended Transaction Format, please refer to the Bitcoin Improvement Proposal 239 (09 November 2022).</p> <p>Other:</p> <ul> <li>Overall System Design</li> <li>Block</li> <li>Subtree</li> </ul>"},{"location":"topics/datamodel/utxo_data_model/","title":"Teranode Data Model - UTXO","text":""},{"location":"topics/datamodel/utxo_data_model/#utxo-data-model","title":"UTXO Data Model","text":"<p>For every transaction, a UTXO record is stored in the database. The record contains the following fields:</p> Field Name Data Type Description utxos <code>Array of Byte[32] or Byte[64]</code> A list of UTXOs, where each UTXO is either 32 bytes (unspent) or 64 bytes (spent). utxoSpendableIn <code>Map&lt;Integer, Integer&gt;</code> A map where the key is the UTXO offset and the value is the block height after which the UTXO is spendable. recordUtxos <code>Integer</code> Total number of UTXOs in this record. spentUtxos <code>Integer</code> Number of UTXOs that have been spent in this record. frozen <code>Boolean</code> Indicates whether the UTXO or transaction is frozen. locked <code>Boolean</code> Indicates whether the transaction outputs can be spent. Set to true during initial creation when the transaction is validated. Set to false in two scenarios: (1) immediately after successful addition to block assembly, or (2) when the transaction is mined in a block through the <code>SetMinedMulti</code> operation. conflicting <code>Boolean</code> Indicates whether this transaction is a double spend. conflictingChildren <code>Array&lt;chainhash.Hash&gt;</code> List of transaction hashes that spend from this transaction and are also marked as conflicting. unminedSince <code>Integer (Block Height)</code> When set to a block height value, indicates the transaction is unmined and tracks when it was first stored. When nil/not set, indicates the transaction has been mined. This field enables transaction recovery after service restarts. createdAt <code>Integer (Timestamp)</code> The timestamp when the unmined transaction was first added to the store. Used for ordering transactions during recovery. preserveUntil <code>Integer (Block Height)</code> Specifies a block height until which a transaction should be preserved from deletion. Used to protect parent transactions of unmined transactions from being deleted during cleanup operations. spendingHeight <code>Integer</code> If the UTXO is from a coinbase transaction, it stores the block height after which it can be spent. blockIDs <code>Array&lt;Integer&gt;</code> List of block IDs that reference this UTXO. blockHeights <code>Array&lt;uint32&gt;</code> List of block heights where this transaction appears. Used by the validator to identify the height at which a UTXO was mined. subtreeIdxs <code>Array&lt;int&gt;</code> List of subtree indexes where this transaction appears within blocks. external <code>Boolean</code> Flag indicating whether the transaction is stored externally (used for fetching external raw transaction data). totalExtraRecs <code>Integer (Optional)</code> The number of UTXO records associated with the transaction, used for pagination. reassignments <code>Array&lt;Map&gt;</code> Tracks UTXO reassignments. Contains maps with keys such as <code>offset</code>, <code>utxoHash</code>, <code>newUtxoHash</code>, and <code>blockHeight</code>. tx <code>bt.Tx Object</code> Raw transaction data containing inputs, outputs, version, and locktime. fee <code>Integer</code> Transaction fee associated with this UTXO. sizeInBytes <code>Integer</code> The size of the transaction in bytes. txInpoints <code>TxInpoints</code> Transaction input outpoints containing parent transaction hashes and their corresponding output indices. isCoinbase <code>Boolean</code> Indicates whether this UTXO is from a coinbase transaction. <p>Within it, each UTXO in the <code>utxos</code> array has the following fields:</p> Field Name Data Type Description utxoHash <code>Byte[32]</code> 32-byte little-endian hash representing the UTXO. spendingTxID <code>Byte[32]</code> 32-byte little-endian hash representing the transaction ID that spent this UTXO (only present for spent UTXOs). existingUTXOHash <code>Byte[32]</code> Extracted from a UTXO to validate that the UTXO matches the provided <code>utxoHash</code>. existingSpendingTxID <code>Byte[32]</code> If the UTXO has been spent, this field stores the spending transaction ID. <p>Additionally, note how the raw transaction data (tx (bt.Tx Object)) is stored in the record. This includes:</p> <ul> <li>Version: The transaction version.</li> <li>LockTime: The transaction lock time.</li> <li>Inputs: Array of inputs used in the transaction.</li> <li>Outputs: Array of outputs (UTXOs) created by the transaction.</li> </ul>"},{"location":"topics/datamodel/utxo_data_model/#time-to-live-ttl-fields","title":"Time to Live (TTL) Fields","text":"<p>Typically, UTXO records are kept with a time-to-live value that is set when all UTXOs in a record are spent or reassigned.</p> Field Name Data Type Description TTL <code>Integer</code> Time-to-live value for the record. Set when:- All UTXOs in a record are spent or reassigned- Transaction is marked as conflicting"},{"location":"topics/datamodel/utxo_data_model/#aerospike-storage","title":"Aerospike Storage","text":"<p>If storing in Aerospike, the UTXO record is stored as a bin in the Aerospike database. The bin contains the UTXO data in a serialized format, containing up to 1024 bytes.</p> <p></p> <p>For more information, please refer to the official Aerospike documentation: https://aerospike.com.</p>"},{"location":"topics/datamodel/utxo_data_model/#utxo-metadata","title":"UTXO MetaData","text":"<p>For convenience, the UTXO can be decorated using the <code>UTXO MetaData</code> format, widely used in Teranode:</p> Field Name Description Data Type Tx The raw transaction data. *bt.Tx Object Hash Unique identifier for the transaction. String/Hexadecimal Fee The fee associated with the transaction. Decimal Size in Bytes The size of the transaction in bytes. Integer TxInpoints Transaction input outpoints containing parent transaction hashes and their corresponding output indices. TxInpoints Object BlockIDs List of IDs of the blocks that include this transaction. Array of Integers BlockHeights List of block heights where this transaction appears. Array of Integers SubtreeIdxs List of subtree indexes where this transaction appears within blocks. Array of Integers LockTime The earliest time or block number that this transaction can be included in the blockchain. Integer/Timestamp or Block Number IsCoinbase Indicates whether the transaction is a coinbase transaction. Boolean Locked Flag indicating whether the transaction outputs can be spent. Part of the two-phase commit process for block assembly. Boolean Conflicting Indicates whether this transaction is a double spend. Boolean ConflictingChildren List of transaction hashes that spend from this transaction and are also marked as conflicting. Array of Strings/Hexadecimals"},{"location":"topics/datamodel/utxo_data_model/#txinpoints-structure","title":"TxInpoints Structure","text":"<p>The <code>TxInpoints</code> field contains complete outpoint information for all transaction inputs, providing precise identification of which UTXOs are being consumed by a transaction.</p>"},{"location":"topics/datamodel/utxo_data_model/#structure-definition","title":"Structure Definition","text":"<pre><code>type TxInpoints struct {\n    ParentTxHashes []chainhash.Hash  // Array of parent transaction hashes\n    Idxs           [][]uint32        // Array of arrays containing output indices for each parent transaction\n    nrInpoints     int               // Internal variable tracking total number of inpoints\n}\n</code></pre>"},{"location":"topics/datamodel/utxo_data_model/#field-descriptions","title":"Field Descriptions","text":"Field Type Description <code>ParentTxHashes</code> <code>[]chainhash.Hash</code> Array of unique parent transaction hashes from which this transaction consumes UTXOs <code>Idxs</code> <code>[][]uint32</code> Parallel array to <code>ParentTxHashes</code>. Each element contains the output indices being consumed from the corresponding parent transaction <code>nrInpoints</code> <code>int</code> Internal counter tracking the total number of individual outpoints across all parent transactions"},{"location":"topics/datamodel/utxo_data_model/#key-features","title":"Key Features","text":"<ol> <li> <p>Complete Outpoint Information: Each input is precisely identified by both the parent transaction hash and the specific output index being consumed.</p> </li> <li> <p>Efficient Storage: Uses parallel arrays to avoid duplicating parent transaction hashes when multiple outputs from the same transaction are consumed.</p> </li> <li> <p>Validation Support: Enables validators to quickly determine exactly which UTXOs are being spent without additional lookups.</p> </li> <li> <p>Chained Transaction Support: Facilitates handling of complex transaction chains where multiple outputs from the same parent transaction are consumed.</p> </li> </ol>"},{"location":"topics/datamodel/utxo_data_model/#example","title":"Example","text":"<p>For a transaction consuming:</p> <ul> <li>Output 0 of transaction A</li> <li>Output 2 of transaction A</li> <li>Output 1 of transaction B</li> </ul> <p>The TxInpoints structure would contain:</p> <pre><code>ParentTxHashes: [hashA, hashB]\nIdxs: [[0, 2], [1]]\n</code></pre> <p>This structure efficiently represents that the transaction consumes three UTXOs total: two from transaction A (outputs 0 and 2) and one from transaction B (output 1).</p> <p>Note:</p> <ul> <li> <p>Blocks: 1 or more block hashes. Each block represents a block that mined the transaction.</p> </li> <li> <p>Typically, a tx should only belong to one block. i.e. a) a tx is created (and its meta is stored in the UTXO store) and b) the tx is mined, and the mined block hash is tracked in the UTXO store for the given transaction.</p> </li> <li> <p>However, in the case of a fork, a tx can be mined in multiple blocks by different nodes. In this case, the UTXO store will track multiple block hashes for the given transaction, until such time that the fork is resolved and only one block is considered valid.</p> </li> <li> <p>Block Heights and Subtree Indexes: These fields track the exact location of transactions within the blockchain.</p> </li> <li>The block heights array is particularly important for validation, as it gives visibility on what height a UTXO was mined. While most UTXOs are mined at the same height across parallel chains or forks, this is not always the case. Storing this information enables the validator to efficiently determine the height of UTXOs being spent without performing expensive lookups. Block heights indicate how deep in the chain a transaction is, which is important for maturity checks.</li> <li>The subtree indexes are primarily informational, allowing for future features that might need to locate exactly where a transaction was placed within a block's structure, enabling potential parallel processing and efficient lookups.</li> </ul>"},{"location":"topics/features/two_phase_commit/","title":"Two-Phase Transaction Commit Process","text":""},{"location":"topics/features/two_phase_commit/#index","title":"Index","text":"<ol> <li>Overview</li> <li>Purpose and Benefits</li> <li>Implementation Details<ul> <li>3.1. Phase 1: Initial Transaction Creation with Locked Flag</li> <li>3.2. Phase 2: Unsetting the Locked Flag</li> <li>3.3. Special Case: Transactions from Blocks</li> </ul> </li> <li>Service Interaction<ul> <li>4.1. Validator Service Role</li> <li>4.2. Block Validation Service Role</li> </ul> </li> <li>Data Model Impact</li> <li>Configuration Options</li> <li>Flow Diagrams</li> <li>Related Documentation</li> </ol>"},{"location":"topics/features/two_phase_commit/#1-overview","title":"1. Overview","text":"<p>The Two-Phase Transaction Commit process is a mechanism implemented in Teranode to ensure atomicity and data consistency during transaction processing across the system's microservice architecture. This process uses an \"locked\" flag to temporarily lock transaction outputs until they are safely included in block assembly, and then makes them available for spending after confirmation.</p> <p>When a Bitcoin transaction is processed, it both spends existing outputs (inputs) and creates new outputs that can be spent in the future. In Teranode's distributed architecture, transaction processing spans multiple services - particularly the UTXO store (where transaction data is persisted) and Block Assembly (where transactions are prepared for inclusion in blocks). The two-phase commit process ensures atomicity across these services, preventing scenarios where a transaction might exist in one component but not another.</p> <p>This approach ensures transaction consistency across the entire system, preventing situations where a transaction might be included in a block but not properly recorded in the UTXO store, or vice versa. It maintains system integrity in a distributed environment by coordinating state changes across microservices.</p>"},{"location":"topics/features/two_phase_commit/#2-purpose-and-benefits","title":"2. Purpose and Benefits","text":"<p>The Two-Phase Transaction Commit process addresses several critical concerns that arise in a distributed transaction processing system:</p> <ul> <li> <p>Atomicity in a Microservice Architecture: The process ensures that a transaction is either fully processed across all components or effectively doesn't exist at all. This prevents serious inconsistencies between the UTXO store and Block Assembly.</p> </li> <li> <p>Prevention of Data Corruption Scenarios:</p> </li> <li> <p>Scenario 1 - Block Assembly Without UTXO Storage: Without two-phase commit, if a transaction were added to Block Assembly but failed to be stored in the UTXO store, when that block is mined, future transactions trying to spend its outputs would be rejected because the outputs don't exist in the UTXO store. This could cause a chain fork.</p> </li> <li> <p>Scenario 2 - UTXO Storage Without Block Assembly: If a transaction is stored in the UTXO store but not added to Block Assembly, it creates \"ghost money\" that exists in the database but isn't in the blockchain.</p> </li> <li> <p>Safe Failure Mode: In case of system failures, the worst case is that money temporarily can't be spent (rather than creating invalid money or losing funds). This preserves the integrity of the monetary system.</p> </li> <li> <p>System Consistency: Ensures that the UTXO database state and block assembly state are synchronized, maintaining a consistent view of transactions across the system.</p> </li> <li> <p>Transaction Dependency Management: Ensures proper handling of transaction dependencies in a distributed processing environment.</p> </li> <li> <p>Improved Validation: Provides a clear state transition mechanism that helps track the status of transactions throughout the system.</p> </li> </ul>"},{"location":"topics/features/two_phase_commit/#21-critical-assumptions","title":"2.1. Critical Assumptions","text":"<p>The Two-Phase Transaction Commit process is built on a critical assumption that must be understood to grasp how the system prevents double-spending and maintains consistency:</p> <p>Critical Assumption: Parent transactions are either mined, in block template, or in previous block. No other option is possible.</p> <p>This means that for any transaction being processed,  all input transactions (parent transactions) must be in one of these states:</p> <pre><code>- Already mined in a confirmed block (transactions in blocks with multiple confirmations)\n</code></pre> <ul> <li>Currently in a block template (pending mining, not yet in a block)</li> <li>In the immediately previous block (transactions in the most recently mined block)</li> </ul> <p>Note: The distinction between \"already mined in a confirmed block\" and \"in the immediately previous block\" is important. The former refers to transactions with multiple confirmations that are considered stable, while the latter refers specifically to transactions in the most recently added block that have only one confirmation and may still be subject to reorganization.</p> <p>Implications:</p> <pre><code>- This assumption ensures that all parent transactions are either confirmed or in the process of being confirmed\n</code></pre> <ul> <li>It prevents the system from processing transactions that refer to parent transactions that are still in an intermediate state</li> <li>It creates a clean dependency chain where transactions build upon others that are already securely in the system</li> </ul> <p>Security Benefits:</p> <pre><code>- Prevents transaction graph inconsistencies\n</code></pre> <ul> <li>Eliminates scenarios where transaction outputs could be spent before their parent transactions are fully committed</li> <li>Supports the effectiveness of the locked flag mechanism</li> </ul> <p>This assumption is foundational to the system's security model and crucial for the proper functioning of the two-phase commit process.</p>"},{"location":"topics/features/two_phase_commit/#3-implementation-details","title":"3. Implementation Details","text":""},{"location":"topics/features/two_phase_commit/#31-phase-1-initial-transaction-creation-with-locked-flag","title":"3.1. Phase 1: Initial Transaction Creation with Locked Flag","text":"<p>When a transaction is validated and accepted by the system:</p> <ol> <li> <p>The Validator service processes and validates the transaction according to Bitcoin SV consensus rules.</p> </li> <li> <p>Upon successful validation, new UTXOs generated by the transaction are created in the UTXO store with the \"locked\" flag set to <code>true</code>.</p> </li> <li> <p>This flag prevents these UTXOs from being spent in subsequent transactions while they're still being processed through the system.</p> </li> <li> <p>The transaction is marked as valid and forwarded to Block Assembly for inclusion in a block.</p> </li> </ol>"},{"location":"topics/features/two_phase_commit/#32-phase-2-unsetting-the-locked-flag","title":"3.2. Phase 2: Unsetting the Locked Flag","text":"<p>The locked flag is unset in two key scenarios:</p>"},{"location":"topics/features/two_phase_commit/#321-scenario-1-after-successful-addition-to-block-assembly","title":"3.2.1. Scenario 1: After Successful Addition to Block Assembly","text":"<ol> <li> <p>When a transaction is successfully validated and added to block assembly:</p> </li> <li> <p>The Validator service detects the successful addition and immediately unsets the \"locked\" flag (changes it to <code>false</code>).</p> </li> <li> <p>This makes the transaction outputs available for spending in subsequent transactions, even before the transaction is mined in a block.</p> </li> </ol>"},{"location":"topics/features/two_phase_commit/#322-scenario-2-when-mined-in-a-block","title":"3.2.2. Scenario 2: When Mined in a Block","text":"<p>As a fallback mechanism, the locked flag is also unset when a transaction is mined in a block:</p> <ol> <li> <p>When a block containing the transaction is validated, the Block Validation service processes the block and identifies all transactions within it.</p> </li> <li> <p>As part of the <code>SetMinedMulti</code> operation during block processing, the Block Validation service updates the transaction's metadata in the UTXO store, which includes unsetting the \"locked\" flag (changing it to <code>false</code>) if it hasn't been unset already.</p> </li> <li> <p>The transaction is now fully committed and integrated into the blockchain.</p> </li> </ol>"},{"location":"topics/features/two_phase_commit/#33-special-case-transactions-from-blocks","title":"3.3. Special Case: Transactions from Blocks","text":"<p>For transactions that are received as part of a block (rather than through the transaction validation process):</p> <ol> <li> <p>The Validator can be configured to ignore the locked flag when validating transactions that are part of a received block.</p> </li> <li> <p>This is controlled via the <code>WithIgnoreLocked</code> option in the Validator service.</p> </li> <li> <p>This approach allows the system to accept transactions that are already part of a valid block, even if they would spend outputs that are marked as locked in the local UTXO store.</p> </li> <li> <p>This mechanism is essential for handling block synchronization and reorgs properly.</p> </li> </ol>"},{"location":"topics/features/two_phase_commit/#4-service-interaction","title":"4. Service Interaction","text":""},{"location":"topics/features/two_phase_commit/#41-validator-service-role","title":"4.1. Validator Service Role","text":"<p>The Validator service is responsible for:</p> <ul> <li>Validating incoming transactions against consensus rules</li> <li>Setting the locked flag on newly created UTXOs (Phase 1)</li> <li>Forwarding valid transactions to Block Assembly (while keeping them marked as locked)</li> <li>Handling the special case of ignoring the locked flag for transactions in blocks</li> </ul>"},{"location":"topics/features/two_phase_commit/#42-block-validation-service-role","title":"4.2. Block Validation Service Role","text":"<p>The Block Validation service is responsible for:</p> <ul> <li>Validating blocks and their transactions</li> <li>Updating transaction metadata when marking transactions as mined</li> <li>Unsetting the locked flag as part of the SetMinedMulti operation when a transaction is mined in a block (Phase 2)</li> <li>Ensuring that the second phase of the commit process is completed</li> </ul>"},{"location":"topics/features/two_phase_commit/#5-data-model-impact","title":"5. Data Model Impact","text":"<p>The Two-Phase Transaction Commit process impacts the UTXO data model by using the \"locked\" flag field in the UTXO records. This flag is stored in the UTXO metadata and is used to track the state of transaction outputs throughout the system.</p> <p>UTXO Table: <pre><code>| Field Name         | Type    | Description                                             |\n|--------------------|---------|---------------------------------------------------------|\n| locked             | boolean | Indicates if the UTXO is temporarily locked             |\n</code></pre></p> <p>UTXO MetaData Table: <pre><code>| Field Name         | Type    | Description                                             |\n|--------------------|---------|---------------------------------------------------------|\n| locked             | boolean | Flag to prevent spending during the two-phase commit    |\n</code></pre></p>"},{"location":"topics/features/two_phase_commit/#6-configuration-options","title":"6. Configuration Options","text":"<p>The behavior of the Two-Phase Transaction Commit process can be configured through the following options:</p> <p>Validator Service: <pre><code>WithIgnoreLocked(bool) - When set to true, the validator will ignore the locked flag when processing transactions that are part of a block\n</code></pre></p> <p>In the Validator Options struct: <pre><code>type Options struct {\n    IgnoreLocked bool\n}\n</code></pre></p>"},{"location":"topics/features/two_phase_commit/#7-flow-diagrams","title":"7. Flow Diagrams","text":""},{"location":"topics/features/two_phase_commit/#transaction-validation-phase-1","title":"Transaction Validation (Phase 1)","text":""},{"location":"topics/features/two_phase_commit/#transaction-mining-phase-2","title":"Transaction Mining (Phase 2)","text":""},{"location":"topics/features/two_phase_commit/#8-related-documentation","title":"8. Related Documentation","text":"<ul> <li>Validator Service Documentation</li> <li>Block Validation Service Documentation</li> <li>UTXO Data Model Documentation</li> </ul>"},{"location":"topics/kafka/kafka/","title":"\ud83d\udc18\ufe0f Kafka in Teranode","text":""},{"location":"topics/kafka/kafka/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Description</li> <li>Use Cases<ul> <li>Propagation Service</li> <li>Validator Component</li> <li>P2P Service</li> <li>Blockchain</li> </ul> </li> <li>Reliability and Recoverability</li> <li>Configuration</li> <li>Operational Guidelines<ul> <li>Performance Tuning</li> <li>Reliability Considerations</li> <li>Monitoring</li> </ul> </li> <li>Kafka URL Configuration Parameters<ul> <li>Consumer Configuration Parameters</li> <li>Producer Configuration Parameters</li> </ul> </li> <li>Service-Specific Kafka Settings<ul> <li>Auto-Commit Behavior by Service Criticality</li> <li>Service-Specific Performance Settings</li> <li>Configuration Examples by Service</li> </ul> </li> <li>Other Resources</li> </ol>"},{"location":"topics/kafka/kafka/#1-description","title":"1. Description","text":"<p>Kafka is a high-throughput, distributed messaging system designed to store and process large volumes of data. Its key features include scalability, fault-tolerance, and high availability, making it an ideal choice for real-time data processing and analytics in complex systems like Teranode.</p> <p>In the Teranode ecosystem, Kafka plays a crucial role in facilitating communication between various components, such as the Validator, BlockValidation, and Blockchain. It enables these components to exchange messages, notifications, and data reliably and efficiently, ensuring smooth operation of the entire system.</p> <p>It's important to note that Kafka is a third-party dependency in Teranode. As such, there is no specific installation or configuration process provided within the Teranode framework. Users are expected to have a properly configured Kafka setup running before initiating the Teranode services. This approach allows for flexibility in Kafka configuration based on specific deployment needs and existing infrastructure.</p>"},{"location":"topics/kafka/kafka/#2-use-cases","title":"2. Use Cases","text":""},{"location":"topics/kafka/kafka/#propagation-service","title":"Propagation Service","text":"<p>After initial sanity check tests, the propagation service endorses transactions to the validator. This is done by sending transaction notifications to the validator via the <code>kafka_validatortxsConfig</code> topic.</p> <p></p> <ul> <li>kafka_validatortxsConfig: This Kafka topic is used to transmit new transaction notifications from the Propagation component to the Validator.</li> </ul>"},{"location":"topics/kafka/kafka/#validator-component","title":"Validator Component","text":"<p>This diagram illustrates the central role of the Validator in processing new transactions, and how it uses Kafka:</p> <ol> <li> <p>The Validator receives new transactions from the Propagation component via the <code>kafka_validatortxsConfig</code> topic.</p> </li> <li> <p>Valid transactions are forwarded to the Block Assembly component using direct gRPC calls (not Kafka). The Validator uses the <code>blockAssembler.Store()</code> method for synchronous transaction processing required for mining candidate generation.</p> </li> <li> <p>The Validator sends new UTXO (Unspent Transaction Output) metadata to the Subtree Validation component through the <code>kafka_txmetaConfig</code> topic for inclusion in new subtrees. Should a reversal be required, the same topic is  used to notify a deletion (\"delete\" command).</p> </li> <li> <p>If a transaction is rejected, the Validator notifies the P2P component via the <code>kafka_rejectedTxConfig</code> topic, allowing the network (other peers) to be informed about invalid transactions.</p> </li> </ol>"},{"location":"topics/kafka/kafka/#p2p-service","title":"P2P Service","text":"<p>The P2P (Peer-to-Peer) service is responsible from peer-to-peer communication, receiving and sending data to other nodes in the network. Here's how it interacts with other components using Kafka:</p> <ol> <li> <p>It receives notifications about rejected transactions from the Validator through the <code>kafka_rejectedTxConfig</code> topic, allowing it to inform other nodes in the network.</p> </li> <li> <p>The P2P component propagates new blocks (as received from other peers in the network) to the Block Validation component via the <code>kafka_blocksConfig</code> topic, initiating the block validation process.</p> </li> <li> <p>New subtrees (as received from other peers in the network) are sent from the P2P component to the Subtree Validation component using the <code>kafka_subtreesConfig</code> topic, enabling efficient validation of large transaction sets.</p> </li> </ol>"},{"location":"topics/kafka/kafka/#blockchain","title":"Blockchain","text":"<p>This diagram shows the final stage of block processing:</p> <ul> <li>The Blockchain component sends newly finalized blocks to the Blockpersister component using the <code>kafka_blocksFinalConfig</code> topic. This ensures that validated and accepted blocks are permanently stored in the blockchain.</li> </ul>"},{"location":"topics/kafka/kafka/#3-reliability-and-recoverability","title":"3. Reliability and Recoverability","text":"<p>Kafka's role as a critical component in the Teranode system cannot be overstated. Its central position in facilitating the communication of new transactions, remote subtrees, and blocks makes it indispensable for the node's operation.</p> <p>To maintain system integrity, Teranode is designed to pause operations when Kafka is in an unreliable state. This means:</p> <ol> <li>The system will not process new transactions, blocks, or subtrees until Kafka is available and functioning correctly.</li> <li>During Kafka downtime or unreliability, the node enters a safe state, preventing potential data inconsistencies or processing errors.</li> <li>Once Kafka is reported as healthy again, the node automatically resumes normal operation without manual intervention.</li> </ol>"},{"location":"topics/kafka/kafka/#4-configuration","title":"4. Configuration","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the Kafka Settings Reference.</p>"},{"location":"topics/kafka/kafka/#5-operational-guidelines","title":"5. Operational Guidelines","text":""},{"location":"topics/kafka/kafka/#performance-tuning","title":"Performance Tuning","text":"<ol> <li> <p>Partition Optimization</p> <ul> <li>Each partition can only be consumed by one consumer in a consumer group</li> <li>Increase partitions to increase parallelism, but avoid over-partitioning</li> <li>General guideline: Start with partitions = number of consumers * 2</li> </ul> </li> <li> <p>Resource Allocation</p> <ul> <li>Kafka is memory-intensive; ensure sufficient RAM</li> <li>Disk I/O is critical; use fast storage (SSDs recommended)</li> <li>Network bandwidth should be sufficient for peak message volumes</li> </ul> </li> <li> <p>Producer Tuning</p> <ul> <li>Batch messages when possible by adjusting <code>flush_*</code> parameters</li> <li>Monitor producer queue size and adjust if messages are being dropped</li> </ul> </li> </ol>"},{"location":"topics/kafka/kafka/#reliability-considerations","title":"Reliability Considerations","text":"<ol> <li> <p>Replication Factor</p> <ul> <li>Minimum recommended for production: 3</li> <li>Ensures data survives broker failures</li> </ul> </li> <li> <p>Consumer Group Design</p> <ul> <li>Critical services should use dedicated consumer groups</li> <li>Monitor consumer lag to detect processing issues</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Services have different retry policies based on criticality</li> <li>Block and subtree validation use manual commits to ensure exactly-once processing</li> </ul> </li> </ol>"},{"location":"topics/kafka/kafka/#monitoring","title":"Monitoring","text":"<p>Key metrics to monitor:</p> <ol> <li> <p>Broker Metrics</p> <ul> <li>CPU, memory, disk usage</li> <li>Network throughput</li> </ul> </li> <li> <p>Topic Metrics</p> <ul> <li>Message rate</li> <li>Byte throughput</li> <li>Partition count</li> </ul> </li> <li> <p>Consumer Metrics</p> <ul> <li>Consumer lag</li> <li>Processing time</li> <li>Error rate</li> </ul> </li> <li> <p>Producer Metrics</p> <ul> <li>Send success rate</li> <li>Retry rate</li> <li>Queue size</li> </ul> </li> </ol>"},{"location":"topics/kafka/kafka/#6-kafka-url-configuration-parameters","title":"6. Kafka URL Configuration Parameters","text":""},{"location":"topics/kafka/kafka/#consumer-configuration-parameters","title":"Consumer Configuration Parameters","text":"<p>When configuring Kafka consumers via URL, the following query parameters are supported:</p> Parameter Type Default Description <code>partitions</code> int 1 Number of topic partitions to consume from <code>consumer_ratio</code> int 1 Ratio for scaling consumer count (partitions/consumer_ratio) <code>replay</code> int 1 Whether to replay messages from beginning (1=true, 0=false) <code>group_id</code> string - Consumer group identifier for coordination <p>Example Consumer URL:</p> <pre><code>kafka://localhost:9092/transactions?partitions=4&amp;consumer_ratio=2&amp;replay=0&amp;group_id=validator-group\n</code></pre>"},{"location":"topics/kafka/kafka/#producer-configuration-parameters","title":"Producer Configuration Parameters","text":"<p>When configuring Kafka producers via URL, the following query parameters are supported:</p> Parameter Type Default Description <code>partitions</code> int 1 Number of topic partitions to create <code>replication</code> int 1 Replication factor for topic <code>retention</code> string \"600000\" Message retention period (ms) <code>segment_bytes</code> string \"1073741824\" Segment size in bytes (1GB) <code>flush_bytes</code> int varies Flush threshold in bytes (1MB async, 1KB sync) <code>flush_messages</code> int 50000 Number of messages before flush <code>flush_frequency</code> string \"10s\" Time-based flush frequency <p>Example Producer URL:</p> <pre><code>kafka://localhost:9092/blocks?partitions=2&amp;replication=3&amp;retention=3600000&amp;flush_frequency=5s\n</code></pre>"},{"location":"topics/kafka/kafka/#7-service-specific-kafka-settings","title":"7. Service-Specific Kafka Settings","text":""},{"location":"topics/kafka/kafka/#auto-commit-behavior-by-service-criticality","title":"Auto-Commit Behavior by Service Criticality","text":"<p>Auto-commit in Kafka is a consumer configuration that determines when and how message offsets are committed (marked as processed) back to Kafka. When auto-commit is enabled, Kafka automatically commits message offsets at regular intervals (default is every 5 seconds). When auto-commit is disabled, it is the responsibility of the application to manually commit offsets after successfully processing messages.</p> <p>Kafka consumer auto-commit behavior varies by service based on processing criticality:</p>"},{"location":"topics/kafka/kafka/#auto-commit-enabled-services","title":"Auto-Commit Enabled Services","text":"<p>These services can tolerate potential message loss for performance:</p> <ul> <li>TxMeta Cache (Subtree Validation): <code>autoCommit=true</code></li> <li>Rationale: Metadata can be regenerated if lost</li> <li> <p>Performance priority over strict delivery guarantees</p> </li> <li> <p>Rejected Transactions (P2P): <code>autoCommit=true</code></p> </li> <li>Rationale: Rejection notifications are not critical for consistency</li> <li>Network efficiency prioritized</li> </ul>"},{"location":"topics/kafka/kafka/#auto-commit-disabled-services","title":"Auto-Commit Disabled Services","text":"<p>These services require exactly-once processing guarantees:</p> <ul> <li>Subtree Validation: <code>autoCommit=false</code></li> <li>Rationale: Transaction processing must be atomic</li> <li> <p>Manual commit after successful processing</p> </li> <li> <p>Block Persister: <code>autoCommit=false</code></p> </li> <li>Rationale: Block finalization is critical for blockchain integrity</li> <li> <p>Manual commit ensures durability</p> </li> <li> <p>Block Validation: <code>autoCommit=false</code></p> </li> <li>Rationale: Block processing affects consensus</li> <li>Manual commit prevents duplicate processing</li> </ul>"},{"location":"topics/kafka/kafka/#kafka-consumer-concurrency","title":"Kafka Consumer Concurrency","text":"<p>Important: Unlike what the service-specific <code>kafkaWorkers</code> settings might suggest, Kafka consumer concurrency in Teranode is actually controlled through the <code>consumer_ratio</code> URL parameter for each topic. The actual number of consumers is calculated as:</p> <pre><code>consumerCount = partitions / consumer_ratio\n</code></pre> <p>Common consumer ratios in use:</p> <ul> <li><code>consumer_ratio=1</code>: One consumer per partition (maximum parallelism)</li> <li><code>consumer_ratio=4</code>: One consumer per 4 partitions (balanced approach)</li> </ul>"},{"location":"topics/kafka/kafka/#service-specific-performance-settings","title":"Service-Specific Performance Settings","text":""},{"location":"topics/kafka/kafka/#propagation-service-settings","title":"Propagation Service Settings","text":"<ul> <li><code>validator_kafka_maxMessageBytes</code>: Size threshold for routing decisions</li> <li>Purpose: Determines when to use HTTP fallback vs Kafka</li> <li>Default: 1048576 (1MB)</li> <li>Usage: Large transactions routed via HTTP to avoid Kafka message size limits</li> </ul>"},{"location":"topics/kafka/kafka/#validator-service-settings","title":"Validator Service Settings","text":"<ul> <li><code>validator_kafkaWorkers</code>: Number of concurrent Kafka processing workers</li> <li>Purpose: Controls parallel transaction processing capacity</li> <li>Tuning: Should match CPU cores and expected transaction volume</li> <li>Integration: Works with Block Assembly via direct gRPC (not Kafka)</li> </ul>"},{"location":"topics/kafka/kafka/#block-validation-service-settings","title":"Block Validation Service Settings","text":"<p>Note: Kafka consumer concurrency is actually controlled via the <code>consumer_ratio</code> URL parameter, not through service-specific worker settings. The formula is:</p> <pre><code>consumerCount = partitions / consumer_ratio\n</code></pre> <p>For example, with 8 partitions and <code>consumer_ratio=4</code>, you get 2 consumers.</p>"},{"location":"topics/kafka/kafka/#configuration-examples-by-service","title":"Configuration Examples by Service","text":""},{"location":"topics/kafka/kafka/#high-throughput-service-propagation","title":"High-Throughput Service (Propagation)","text":"<pre><code>kafka_validatortxsConfig=kafka://localhost:9092/validator-txs?partitions=8&amp;consumer_ratio=2&amp;flush_frequency=1s\nvalidator_kafka_maxMessageBytes=1048576  # 1MB threshold\n</code></pre>"},{"location":"topics/kafka/kafka/#critical-processing-service-block-validation","title":"Critical Processing Service (Block Validation)","text":"<pre><code>kafka_blocksConfig=kafka://localhost:9092/blocks?partitions=4&amp;consumer_ratio=1&amp;replay=0\nblockvalidation_kafkaWorkers=4\nautoCommit=false  # Manual commit for reliability\n</code></pre>"},{"location":"topics/kafka/kafka/#metadata-service-subtree-validation","title":"Metadata Service (Subtree Validation)","text":"<pre><code>kafka_txmetaConfig=kafka://localhost:9092/txmeta?partitions=2&amp;consumer_ratio=1&amp;replay=1\nautoCommit=true   # Performance over strict guarantees\n</code></pre>"},{"location":"topics/kafka/kafka/#8-other-resources","title":"8. Other Resources","text":"<ul> <li>Kafka Message Format</li> <li>Block Data Model: Contain lists of subtree identifiers.</li> <li>Subtree Data Model: Contain lists of transaction IDs and their Merkle root.</li> <li>Extended Transaction Data Model: Includes additional metadata to facilitate processing.</li> </ul>"},{"location":"topics/services/alert/","title":"\ud83d\udea8 Alert Service","text":""},{"location":"topics/services/alert/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1. Initialization</li> <li>2.2. UTXO Freezing</li> <li>2.3. UTXO Unfreezing</li> <li>2.4. UTXO Reassignment</li> <li>2.5. Block Invalidation</li> </ul> </li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>How to run</li> <li>Configuration</li> <li>Other Resources</li> </ol>"},{"location":"topics/services/alert/#1-description","title":"1. Description","text":"<p>The Teranode Alert Service reintroduces the alert system functionality that was removed from Bitcoin in 2016. This service is designed to enhance control by allowing specific actions on UTXOs and peer management.</p> <p>The Service features are:</p>"},{"location":"topics/services/alert/#utxo-freezing","title":"UTXO Freezing","text":"<ul> <li>Ability to freeze a set of UTXOs at a specific block height + 1.</li> <li>Frozen UTXOs are classified as such and attempts to spend them are rejected.</li> </ul>"},{"location":"topics/services/alert/#utxo-unfreezing","title":"UTXO Unfreezing","text":"<ul> <li>Capability to unfreeze a set of UTXOs at a specified block height.</li> </ul>"},{"location":"topics/services/alert/#utxo-reassignment","title":"UTXO Reassignment","text":"<ul> <li>Ability to reassign UTXOs to another specified address at a given block height.</li> </ul>"},{"location":"topics/services/alert/#peer-management","title":"Peer Management","text":"<ul> <li>Ban a peer based on IP address (with optional netmask), cutting off communications.</li> <li>Unban a previously banned peer, re-enabling communications.</li> </ul>"},{"location":"topics/services/alert/#block-invalidation","title":"Block Invalidation","text":"<ul> <li>Manually invalidate a block based on its hash.</li> <li>Retrieve and re-validate all valid transactions from the invalidated block and subsequent blocks.</li> <li>Include valid transactions in the next block(s) to be built.</li> <li>Start building the new longest honest chain from the block height -1 of the invalidated block.</li> </ul> <p>Note: For information about how the Alert service is initialized during daemon startup and how it interacts with other services, see the Teranode Daemon Reference.</p> <p>The Alert Service uses the third party <code>github.com/bitcoin-sv/alert-system</code> library. This library provides the ability to subscribe to a private P2P network where other BSV nodes participate, and subscribes to topics where Alert related messages are received. Based on the received messages, the Alert Service handles the UTXO freezing, unfreezing, reassignment, block invalidation and peer management operations.</p> <p></p> <p>The Alert Service interacts with several core components of Teranode:</p> <ul> <li>Blockchain Service: For block invalidation and chain management</li> <li>UTXO Store: For freezing, unfreezing, and reassigning UTXOs</li> <li>Block Assembly: For including re-validated transactions after block invalidation</li> </ul> <p></p> <p>Additionally, a P2P private network is used for peer management, allowing the Alert Service to ban and unban peers based on IP addresses.</p> <p>The following diagram provides a deeper level of detail into the Alert Service's internal components and their interactions:</p> <p></p>"},{"location":"topics/services/alert/#2-functionality","title":"2. Functionality","text":""},{"location":"topics/services/alert/#21-initialization","title":"2.1. Initialization","text":"<p>The Alert Service initializes the necessary components and services to start processing alerts.</p> <p></p> <ol> <li> <p>The Teranode Main function creates a new Alert Service instance, passing necessary dependencies (logger, blockchain client, UTXO store, and block assembly client).</p> </li> <li> <p>The Alert Service initializes the Prometheus metrics.</p> </li> <li> <p>The Main function calls the <code>Init</code> method on the Alert Service:</p> <ul> <li>The service loads its configuration.</li> <li>It initializes the datastore (database connection). This is a dependency for the alert library, which uses the datastore to store alert data. <p>Note: For detailed information about the alert datastore structure, data models, and configuration options, see the Alert Service Datastore Reference.</p> </li> <li>It creates and stores a genesis alert in the database.</li> <li>If enabled, it verifies the RPC connection to the Bitcoin node.</li> </ul> </li> <li> <p>After initialization, the Main function calls the <code>Start</code> method:</p> <ul> <li>The Alert Service creates a new P2P Server instance.</li> <li>It starts the P2P Server.</li> </ul> </li> <li> <p>The Alert Service is now fully initialized and running.</p> </li> </ol>"},{"location":"topics/services/alert/#22-utxo-freezing","title":"2.2. UTXO Freezing","text":"<ol> <li> <p>The P2P Alert library initiates the process by calling <code>AddToConsensusBlacklist</code> with a list of funds to freeze.</p> </li> <li> <p>The Alert Service iterates through each fund:</p> <ul> <li>It retrieves the transaction data from the UTXO Store.</li> <li>Calculates the UTXO hash.</li> <li>Calls the UTXO Store to freeze the UTXO.</li> </ul> </li> <li> <p>The UTXO Store interacts with the database to mark the UTXO as frozen.</p> </li> <li> <p>Depending on the success of the freeze operation, the Alert Service adds the result to either the processed or notProcessed list.</p> </li> <li> <p>Finally, the Alert Service returns a BlacklistResponse to the P2P network.</p> </li> </ol>"},{"location":"topics/services/alert/#23-utxo-unfreezing","title":"2.3. UTXO Unfreezing","text":"<ol> <li> <p>The P2P Alert library initiates the process by calling <code>AddToConsensusBlacklist</code> with a list of funds to potentially unfreeze.</p> </li> <li> <p>The Alert Service iterates through each fund:</p> <ul> <li>It retrieves the transaction data from the UTXO Store.</li> <li>Calculates the UTXO hash.</li> <li>Checks if the fund is eligible for unfreezing by comparing the EnforceAtHeight.Stop with the current block height.</li> </ul> </li> <li> <p>If the fund is eligible for unfreezing:</p> <ul> <li>The Alert Service calls the UTXO Store to unfreeze the UTXO.</li> <li>The UTXO Store interacts with the database to mark the UTXO as unfrozen.</li> <li>Depending on the success of the unfreeze operation, the Alert Service adds the result to either the processed or notProcessed list.</li> </ul> </li> <li> <p>If the fund is not eligible for unfreezing:</p> <ul> <li>The Alert Service adds it to the notProcessed list with a reason.</li> </ul> </li> <li> <p>Finally, the Alert Service returns a BlacklistResponse to the P2P network.</p> </li> </ol>"},{"location":"topics/services/alert/#24-utxo-reassignment","title":"2.4. UTXO Reassignment","text":"<ol> <li>The P2P Alert library initiates the process by calling <code>AddToConfiscationTransactionWhitelist</code> with a list of transactions.</li> <li> <p>The Alert Service iterates through each transaction:</p> <ul> <li>It parses the transaction from the provided hex string.</li> </ul> </li> <li> <p>For each input in the transaction:</p> <ul> <li>The Alert Service retrieves the parent transaction data from the UTXO Store.</li> <li>It calculates the old UTXO hash based on the parent transaction output.</li> <li>It extracts the public key from the input's unlocking script.</li> <li>It creates a new locking script using the extracted public key.</li> <li>It calculates a new UTXO hash based on the new locking script.</li> </ul> </li> <li> <p>The Alert Service calls the UTXO Store to reassign the UTXO:</p> <ul> <li>The UTXO Store updates the database to reflect the new UTXO assignment.</li> </ul> </li> <li> <p>Depending on the success of the reassignment operation:</p> <ul> <li>The Alert Service adds the result to either the processed or notProcessed list.</li> </ul> </li> <li> <p>After processing all inputs of all transactions, the Alert Service returns an AddToConfiscationTransactionWhitelistResponse to the P2P network.</p> </li> </ol>"},{"location":"topics/services/alert/#25-block-invalidation","title":"2.5. Block Invalidation","text":"<ol> <li> <p>The P2P Alert library initiates the process by calling <code>InvalidateBlock</code> with the hash of the block to be invalidated.</p> </li> <li> <p>The Alert Service forwards this request to the Blockchain Client.</p> </li> <li> <p>The Blockchain Client interacts with the Blockchain Store to:</p> <ul> <li>Mark the specified block as invalid.</li> <li>Retrieve all transactions from the invalidated block.</li> </ul> </li> <li> <p>For each transaction in the invalidated block:</p> <ul> <li>The Blockchain Client re-validates the transaction.</li> <li>If the transaction is still valid, it's added back to the Block Assembly service, for re-inclusion in the next mined block.</li> </ul> </li> <li> <p>The Blockchain Client then:</p> <ul> <li>Retrieves the block immediately preceding the invalidated block.</li> <li>Sets the chain tip to this previous block, effectively removing the invalidated block from the main chain.</li> </ul> </li> <li> <p>The Blockchain Client confirms the invalidation process to the Alert Service.</p> </li> <li> <p>Finally, the Alert Service returns the invalidation result to the P2P network.</p> </li> </ol>"},{"location":"topics/services/alert/#3-technology","title":"3. Technology","text":"<ol> <li> <p>Go Programming Language:</p> <ul> <li>The Alert service is implemented in Go (Golang).</li> </ul> </li> <li> <p>gRPC and Protocol Buffers:</p> <ul> <li>Uses gRPC for inter-service communication.</li> <li>Protocol Buffers (<code>.proto</code> files in <code>alert_api/</code>) define the service API and data structures.</li> </ul> </li> <li> <p>Database Technologies:</p> <ul> <li> <p>Supports both SQLite and PostgreSQL:</p> <ul> <li>SQLite for development and lightweight deployments.</li> <li>PostgreSQL for production environments.</li> </ul> </li> <li> <p>GORM ORM is used for database operations, with a custom logger (<code>gorm_logger.go</code>).</p> </li> </ul> </li> <li> <p>gocore Library:</p> <ul> <li> <p>Utilized for managing application configurations.</p> </li> <li> <p>Handles statistics gathering and operational settings.</p> </li> </ul> </li> <li> <p>P2P Networking:</p> <ul> <li>Implements peer-to-peer communication for alert distribution.</li> <li>Uses libp2p library for P2P network stack.</li> <li>Includes custom topic name and protocol ID for Bitcoin alert system.</li> </ul> </li> <li> <p>Prometheus for Metrics:</p> <ul> <li>Metrics collection and reporting implemented in <code>metrics.go</code>.</li> <li>Used for monitoring the performance and health of the Alert service.</li> </ul> </li> <li> <p>Bitcoin-specific Libraries:</p> <ul> <li>Uses <code>github.com/bsv-blockchain/go-bt/v2</code> for Bitcoin transaction handling.</li> <li>Integrates with <code>github.com/bitcoin-sv/alert-system</code> for core alert functionality.</li> </ul> </li> </ol>"},{"location":"topics/services/alert/#4-directory-structure-and-main-files","title":"4. Directory Structure and Main Files","text":"<pre><code>/services/alert/\n\u251c\u2500\u2500 alert_api/\n\u2502   \u251c\u2500\u2500 alert_api.pb.go\n\u2502   \u2502   Description: Auto-generated Go code from the Protocol Buffers definition.\n\u2502   \u2502   Purpose: Defines structures and interfaces for the Alert API.\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 alert_api.proto\n\u2502   \u2502   Description: Protocol Buffers definition file for the Alert API.\n\u2502   \u2502   Purpose: Defines the service and message structures for the Alert system.\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 alert_api_grpc.pb.go\n\u2502       Description: Auto-generated gRPC Go code from the Protocol Buffers definition.\n\u2502       Purpose: Provides gRPC server and client implementations for the Alert API.\n\u2502\n\u251c\u2500\u2500 gorm_logger.go\n\u2502   Description: Custom logger implementation for GORM.\n\u2502   Purpose: Provides logging functionality specifically tailored for GORM database operations.\n\u2502\n\u251c\u2500\u2500 logger.go\n\u2502   Description: Custom logger implementation for the Alert service.\n\u2502   Purpose: Defines logging methods and interfaces used throughout the Alert service.\n\u2502\n\u251c\u2500\u2500 metrics.go\n\u2502   Description: Metrics collection and reporting for the Alert service.\n\u2502   Purpose: Initializes and manages Prometheus metrics for monitoring the Alert service.\n\u2502\n\u251c\u2500\u2500 node.go\n\u2502   Description: Implementation of the Node interface for the Alert system.\n\u2502   Purpose: Provides methods for interacting with the blockchain and managing alerts.\n\u2502\n\u2514\u2500\u2500 server.go\n</code></pre>"},{"location":"topics/services/alert/#5-how-to-run","title":"5. How to run","text":"<p>To run the Alert Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -Alert=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Alert Service locally.</p>"},{"location":"topics/services/alert/#6-configuration","title":"6. Configuration","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the Alert Service Settings Reference.</p>"},{"location":"topics/services/alert/#7-other-resources","title":"7. Other Resources","text":"<p>Alert Reference</p>"},{"location":"topics/services/assetServer/","title":"\ud83d\uddc2\ufe0f Asset Server","text":""},{"location":"topics/services/assetServer/#index","title":"Index","text":"<ol> <li>Description</li> <li>Architecture</li> <li>Data Model</li> <li>Use Cases<ul> <li>4.1. HTTP<ul> <li>4.1.1. getTransaction() and getTransactions()</li> <li>4.1.2. GetTransactionMeta()</li> <li>4.1.3. GetSubtree()</li> <li>4.1.4. GetBlockHeaders(), GetBlockHeader() and GetBestBlockHeader()</li> <li>4.1.5. GetBlockByHash(), GetBlocks and GetLastNBlocks()</li> <li>4.1.6. GetUTXO() and GetUTXOsByTXID()</li> <li>4.1.7. Search()</li> <li>4.1.8. GetBlockStats()</li> <li>4.1.9. GetBlockGraphData()</li> <li>4.1.10. GetBlockForks()</li> <li>4.1.11. GetBlockSubtrees()</li> <li>4.1.12. GetLegacyBlock()</li> <li>4.1.13. GetBlockHeadersToCommonAncestor()</li> <li>4.1.14. FSM State Management</li> <li>4.1.15. Block Validation Management</li> </ul> </li> </ul> </li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>How to run<ul> <li>7.1 How to run</li> <li>7.2 Configuration Options (Settings Flags)</li> <li>7.3 Configuration Examples</li> <li>7.4 FSM Configuration</li> <li>7.5 Coinbase Configuration</li> <li>7.6 Dashboard Configuration</li> <li>7.7 Block Validation</li> </ul> </li> <li>Other Resources</li> </ol>"},{"location":"topics/services/assetServer/#1-description","title":"1. Description","text":"<p>The Asset Service acts as an interface (\"Front\" or \"Facade\") to various data stores. It deals with several key data elements:</p> <ul> <li> <p>Transactions (TX).</p> </li> <li> <p>SubTrees.</p> </li> <li> <p>Blocks and Block Headers.</p> </li> <li> <p>Unspent Transaction Outputs (UTXO).</p> </li> </ul> <p>The server uses HTTP as communication protocol:</p> <ul> <li>HTTP: A ubiquitous protocol that allows the server to be accessible from the web, enabling other nodes or clients to interact with the server using standard web requests.</li> </ul> <p>The server being externally accessible implies that it is designed to communicate with other nodes and external clients across the network, to share blockchain data or synchronize states.</p> <p>The various micro-services typically write directly to the data stores, but the asset service fronts them as a common interface.</p> <p>Finally, the Asset Service also offers a WebSocket interface, allowing clients to receive real-time notifications when new subtrees and blocks are added to the blockchain.</p>"},{"location":"topics/services/assetServer/#2-architecture","title":"2. Architecture","text":"<p>Using HTTP, the Asset Server provides data both to other Teranode components, and to remote Teranodes. It also provides data to external clients over HTTP / Websockets, such as the Teranode UI Dashboard.</p> <p>All data is retrieved from other Teranode services / stores.</p> <p>Here we can see the Asset Server's relationship with other Teranode components in more detail:</p> <p></p> <p>The Asset Server is composed of the following components:</p> <p></p> <p>The detailed internal component architecture of the Asset Server shows how the various handlers, clients, and data access layers interact:</p> <p></p> <ul> <li>UTXO Store: Provides UTXO data to the Asset Server.</li> <li>Blob Store: Provides Subtree and Extended TX data to the Asset Server, referred here as Subtree Store and TX Store.</li> <li>Blockchain Server: Provides blockchain data (blocks and block headers) to the Asset Server.</li> </ul> <p>Finally, note that the Asset Server benefits of the use of Lustre Fs (filesystem). Lustre is a type of parallel distributed file system, primarily used for large-scale cluster computing. This filesystem is designed to support high-performance, large-scale data storage and workloads. Specifically for Teranode, these volumes are meant to be temporary holding locations for short-lived file-based data that needs to be shared quickly between various services Teranode microservices make use of the Lustre file system in order to share subtree and tx data, eliminating the need for redundant propagation of subtrees over grpc or message queues. The services sharing Subtree data through this system can be seen here:</p> <p></p>"},{"location":"topics/services/assetServer/#3-data-model","title":"3. Data Model","text":"<p>The following data types are provided by the Asset Server:</p> <ul> <li>Block Data Model: Contain lists of subtree identifiers.</li> <li>Block Header Data Model: a block header includes the block ID of the previous block.</li> <li>Subtree Data Model: Contain lists of transaction IDs and their Merkle root.</li> <li>Extended Transaction Data Model: Include additional metadata to facilitate processing.</li> <li>UTXO Data Model: Include additional metadata to facilitate processing.</li> </ul>"},{"location":"topics/services/assetServer/#4-use-cases","title":"4. Use Cases","text":""},{"location":"topics/services/assetServer/#41-http","title":"4.1. HTTP","text":"<p>The Asset Service exposes the following HTTP methods:</p>"},{"location":"topics/services/assetServer/#411-gettransaction-and-gettransactions","title":"4.1.1. getTransaction() and getTransactions()","text":"<ul> <li>URL: <code>/tx/:hash</code> (single transaction), <code>/txs</code> (multiple transactions via POST)</li> <li>Method: GET (single), POST (multiple)</li> <li>Response Format: JSON</li> <li>Content: Transaction data with extended metadata</li> </ul>"},{"location":"topics/services/assetServer/#412-gettransactionmeta","title":"4.1.2. GetTransactionMeta()","text":"<ul> <li>URL: <code>/tx/:hash/meta</code></li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Transaction metadata including UTXO information</li> </ul>"},{"location":"topics/services/assetServer/#413-getsubtree","title":"4.1.3. GetSubtree()","text":"<ul> <li>URL: <code>/subtree/:hash</code></li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Subtree data with transaction IDs and Merkle root</li> </ul>"},{"location":"topics/services/assetServer/#414-getblockheaders-getblockheader-and-getbestblockheader","title":"4.1.4. GetBlockHeaders(), GetBlockHeader() and GetBestBlockHeader()","text":"<ul> <li>URL: <code>/block/:hash/header</code> (single), <code>/blocks/headers</code> (multiple), <code>/block/best/header</code> (best)</li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Block header data including previous block ID and metadata</li> </ul>"},{"location":"topics/services/assetServer/#415-getblockbyhash-getblocks-and-getlastnblocks","title":"4.1.5. GetBlockByHash(), GetBlocks and GetLastNBlocks()","text":"<ul> <li>URL: <code>/block/:hash</code> (single), <code>/blocks</code> (multiple), <code>/blocks/last/:count</code> (last N blocks)</li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Block data with subtree identifiers and metadata</li> </ul>"},{"location":"topics/services/assetServer/#416-getutxo-and-getutxosbytxid","title":"4.1.6. GetUTXO() and GetUTXOsByTXID()","text":"<ul> <li>URL: <code>/utxo/:hash</code> (single UTXO), <code>/utxos/:hash/json</code> (UTXOs by transaction ID)</li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: UTXO data with additional metadata for processing</li> </ul> <ul> <li> <p>For specific UTXO by hash requests (/utxo/:hash), the HTTP Server requests UTXO data from the UtxoStore using a hash.</p> </li> <li> <p>For getting UTXOs by a transaction ID (/utxos/:hash/json), the HTTP Server requests transaction meta data from the UTXO Store using a transaction hash. Then for each output in the transaction, it queries the UtxoStore to get UTXO data for the corresponding output hash.</p> </li> </ul>"},{"location":"topics/services/assetServer/#417-search","title":"4.1.7. Search()","text":"<p>Generic hash search. The server searches for a hash in the Blockchain, the UTXO store and the subtree store.</p> <ul> <li>URL: <code>/search/:hash</code></li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Search results from blockchain, UTXO store, and subtree store</li> </ul> <p></p>"},{"location":"topics/services/assetServer/#418-getblockstats","title":"4.1.8. GetBlockStats()","text":"<p>Retrieves block statistics.</p> <ul> <li>URL: <code>/block/:hash/stats</code></li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Block statistics and performance metrics</li> </ul> <p></p>"},{"location":"topics/services/assetServer/#419-getblockgraphdata","title":"4.1.9. GetBlockGraphData()","text":"<p>Retrieves block graph data for a given period</p> <ul> <li>URL: <code>/blocks/graph/:period</code></li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Block graph data and visualization metrics for specified time period</li> </ul> <p></p>"},{"location":"topics/services/assetServer/#4110-getblockforks","title":"4.1.10. GetBlockForks()","text":"<p>Retrieves information about block forks</p> <ul> <li>URL: <code>/blocks/forks</code></li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Information about blockchain forks and alternative chains</li> </ul> <p></p>"},{"location":"topics/services/assetServer/#4111-getblocksubtrees","title":"4.1.11. GetBlockSubtrees()","text":"<p>Retrieves subtrees for a block in JSON format</p> <ul> <li>URL: <code>/block/:hash/subtrees</code></li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Subtrees data for a specific block with transaction IDs and Merkle roots</li> </ul> <p></p>"},{"location":"topics/services/assetServer/#4112-getlegacyblock","title":"4.1.12. GetLegacyBlock()","text":"<p>Retrieves a block in legacy format, and as a binary stream.</p> <ul> <li>URL: <code>/block_legacy/:hash</code></li> <li>Method: GET</li> <li>Response Format: Binary stream (application/octet-stream)</li> <li>Content: Block in legacy Bitcoin protocol format</li> </ul> <p></p>"},{"location":"topics/services/assetServer/#4113-getblockheaderstocommonancestor","title":"4.1.13. GetBlockHeadersToCommonAncestor()","text":"<p>Retrieves block headers up to a common ancestor point between two chains. This is useful for chain reorganization and fork resolution.</p> <ul> <li>URL: <code>/blocks/headers/ancestor/:hash1/:hash2</code></li> <li>Method: GET</li> <li>Response Format: JSON</li> <li>Content: Block headers from two chains up to their common ancestor point</li> </ul> <p></p>"},{"location":"topics/services/assetServer/#4114-fsm-state-management","title":"4.1.14. FSM State Management","text":"<p>The Asset Server provides an interface to the Finite State Machine (FSM) of the blockchain service. These endpoints allow for monitoring and controlling the blockchain state:</p> <p></p> <ul> <li>GET /api/v1/fsm/state: Retrieves the current FSM state</li> <li>POST /api/v1/fsm/state: Sends a custom event to the FSM</li> <li>GET /api/v1/fsm/events: Lists all available FSM events</li> <li>GET /api/v1/fsm/states: Lists all possible FSM states</li> </ul>"},{"location":"topics/services/assetServer/#4115-block-validation-management","title":"4.1.15. Block Validation Management","text":"<p>The Asset Server offers endpoints for block validation control:</p> <p></p> <ul> <li>POST /api/v1/block/invalidate: Invalidates a specified block</li> <li>POST /api/v1/block/revalidate: Revalidates a previously invalidated block</li> <li>GET /api/v1/blocks/invalid: Retrieves a list of invalid blocks</li> </ul>"},{"location":"topics/services/assetServer/#5-technology","title":"5. Technology","text":"<p>Key technologies involved:</p> <ol> <li> <p>Go Programming Language (Golang):</p> <ul> <li>A statically typed, compiled language known for its simplicity and efficiency, especially in concurrent operations and networked services.</li> <li>The primary language used for implementing the service's logic.</li> </ul> </li> <li> <p>HTTP/HTTPS Protocols:</p> <ul> <li>HTTP for transferring data over the web. HTTPS adds a layer of security with SSL/TLS encryption.</li> <li>Used for communication between clients and the server, and for serving web pages or APIs.</li> </ul> </li> <li> <p>Echo Web Framework:</p> <ul> <li>A high-performance, extensible, minimalist Go web framework.</li> <li>Used for handling HTTP requests and routing, including upgrading HTTP connections to WebSocket connections.</li> <li>Library: github.com/labstack/echo</li> </ul> </li> <li> <p>JSON (JavaScript Object Notation):</p> <ul> <li>A lightweight data-interchange format, easy for humans to read and write, and easy for machines to parse and generate.</li> <li>Used for structuring data sent to and from clients, especially in contexts where HTTP is used.</li> </ul> </li> </ol>"},{"location":"topics/services/assetServer/#6-directory-structure-and-main-files","title":"6. Directory Structure and Main Files","text":"<pre><code>./services/asset\n\u251c\u2500\u2500 Server.go                  # Server logic for the Asset Service.\n\u251c\u2500\u2500 Server_test.go             # Tests for the server functionality.\n\u251c\u2500\u2500 asset_api\n\u2502   \u251c\u2500\u2500 asset_api.pb.go        # Generated protobuf code for the asset API.\n\u2502   \u2514\u2500\u2500 asset_api.proto        # Protobuf definitions for the asset API.\n\u251c\u2500\u2500 centrifuge_impl            # Implementation using Centrifuge for real-time updates.\n\u2502   \u251c\u2500\u2500 centrifuge.go          # Core Centrifuge implementation.\n\u2502   \u251c\u2500\u2500 client\n\u2502   \u2502   \u251c\u2500\u2500 client.go          # Client-side implementation for Centrifuge.\n\u2502   \u2502   \u2514\u2500\u2500 index.html         # HTML template for client-side rendering.\n\u2502   \u2514\u2500\u2500 websocket.go           # WebSocket implementation for real-time communication.\n\u251c\u2500\u2500 httpimpl                   # HTTP implementation of the asset service.\n\u2502   \u251c\u2500\u2500 GetBestBlockHeader.go  # Logic to retrieve the best block header.\n\u2502   \u251c\u2500\u2500 GetBlock.go            # Logic to retrieve a specific block.\n\u2502   \u251c\u2500\u2500 GetBlockForks.go       # Logic to retrieve information about block forks.\n\u2502   \u251c\u2500\u2500 GetBlockGraphData.go   # Logic to retrieve block graph data.\n\u2502   \u251c\u2500\u2500 GetBlockHeader.go      # Logic to retrieve a block header.\n\u2502   \u251c\u2500\u2500 GetBlockHeaders.go     # Logic to retrieve multiple block headers.\n\u2502   \u251c\u2500\u2500 GetBlockHeadersToCommonAncestor.go # Logic to retrieve headers to common ancestor.\n\u2502   \u251c\u2500\u2500 GetBlockStats.go       # Logic to retrieve block statistics.\n\u2502   \u251c\u2500\u2500 GetBlockSubtrees.go    # Logic to retrieve block subtrees.\n\u2502   \u251c\u2500\u2500 GetBlocks.go           # Logic to retrieve multiple blocks.\n\u2502   \u251c\u2500\u2500 GetLastNBlocks.go      # Logic to retrieve the last N blocks.\n\u2502   \u251c\u2500\u2500 GetLegacyBlock.go      # Logic to retrieve legacy block format.\n\u2502   \u251c\u2500\u2500 GetNBlocks.go          # Logic to retrieve N blocks from a specific point.\n\u2502   \u251c\u2500\u2500 GetSubtree.go          # Logic to retrieve a subtree.\n\u2502   \u251c\u2500\u2500 GetSubtreeTxs.go       # Logic to retrieve transactions in a subtree.\n\u2502   \u251c\u2500\u2500 GetTransaction.go      # Logic to retrieve a specific transaction.\n\u2502   \u251c\u2500\u2500 GetTransactionMeta.go  # Logic to retrieve transaction metadata.\n\u2502   \u251c\u2500\u2500 GetTransactions.go     # Logic to retrieve multiple transactions.\n\u2502   \u251c\u2500\u2500 GetTxMetaByTXID.go     # Logic to retrieve transaction metadata by TXID.\n\u2502   \u251c\u2500\u2500 GetUTXO.go             # Logic to retrieve UTXO data.\n\u2502   \u251c\u2500\u2500 GetUTXOsByTXID.go      # Logic to retrieve UTXOs by a transaction ID.\n\u2502   \u251c\u2500\u2500 Readmode.go            # Manages read-only mode settings.\n\u2502   \u251c\u2500\u2500 Search.go              # Implements search functionality.\n\u2502   \u251c\u2500\u2500 block_handler.go       # Handles block validation operations.\n\u2502   \u251c\u2500\u2500 blockHeaderResponse.go # Formats block header responses.\n\u2502   \u251c\u2500\u2500 fsm_handler.go         # Handles FSM state and event operations.\n\u2502   \u251c\u2500\u2500 helpers.go             # Helper functions for HTTP implementation.\n\u2502   \u251c\u2500\u2500 http.go                # Core HTTP implementation.\n\u2502   \u251c\u2500\u2500 metrics.go             # HTTP-specific metrics.\n\u2502   \u251c\u2500\u2500 sendError.go           # Utility for sending error responses.\n\u2502   \u2514\u2500\u2500 *_test.go files        # Various test files for each component.\n\u2514\u2500\u2500 repository                 # Repository layer managing data interactions.\n    \u251c\u2500\u2500 GetLegacyBlock.go      # Repository logic for retrieving legacy blocks.\n    \u251c\u2500\u2500 GetLegacyBlock_test.go # Tests for GetLegacyBlock functionality.\n    \u251c\u2500\u2500 repository.go          # Core repository implementation.\n    \u2514\u2500\u2500 repository_test.go     # Tests for the repository implementation.\n</code></pre>"},{"location":"topics/services/assetServer/#7-how-to-run","title":"7. How to run","text":""},{"location":"topics/services/assetServer/#71-how-to-run","title":"7.1 How to run","text":"<p>To run the Asset Server locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -Asset=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Asset Server locally.</p>"},{"location":"topics/services/assetServer/#72-configuration-options-settings-flags","title":"7.2 Configuration Options (Settings Flags)","text":"<p>The Asset Server can be configured using various settings that control its behavior, network connectivity, security features, and performance characteristics. This section provides a comprehensive reference of all configuration options and their interactions.</p>"},{"location":"topics/services/assetServer/#721-core-asset-server-configuration","title":"7.2.1 Core Asset Server Configuration","text":"<p>HTTP Server Settings:</p> <ul> <li> <p>Asset HTTP Listen Address (<code>asset_httpListenAddress</code>): Address for the Asset Service HTTP server to listen for requests.</p> <ul> <li>Type: <code>string</code></li> <li>Default: <code>\":8090\"</code></li> <li>Environment Variable: <code>TERANODE_ASSET_HTTPLISTENADDRESS</code></li> <li>Impact: Critical - Service will not start without this setting (<code>\"no asset_httpListenAddress setting found\"</code>)</li> <li>Code Usage: Required for HTTP server initialization (Server.go line 184-187)</li> </ul> </li> <li> <p>Asset HTTP Address (<code>asset_httpAddress</code>): Base URL of the Asset Service HTTP server.</p> <ul> <li>Type: <code>string</code></li> <li>Default: <code>\"http://localhost:8090/api/v1\"</code></li> <li>Environment Variable: <code>TERANODE_ASSET_HTTPADDRESS</code></li> <li>Impact: Critical for Centrifuge - Required when Centrifuge is enabled; URL validation performed</li> <li>Code Usage: Used for Centrifuge server initialization; validates URL format</li> </ul> </li> <li> <p>Asset HTTP Public Address (<code>asset_httpPublicAddress</code>): Public-facing URL configuration.</p> <ul> <li>Type: <code>string</code></li> <li>Default: <code>\"\"</code> (empty string)</li> <li>Environment Variable: <code>TERANODE_ASSET_HTTPPUBLICADDRESS</code></li> <li>Impact: Configuration placeholder - not actively used in traced Asset Server code paths</li> </ul> </li> <li> <p>Asset API Prefix (<code>asset_apiPrefix</code>): URL prefix for API routes.</p> <ul> <li>Type: <code>string</code></li> <li>Default: <code>\"/api/v1\"</code></li> <li>Environment Variable: <code>TERANODE_ASSET_APIPREFIX</code></li> <li>Impact: Determines URL structure for all API endpoints</li> <li>Code Usage: Applied to API route groups in HTTP server initialization</li> </ul> </li> <li> <p>Asset HTTP Port (<code>ASSET_HTTP_PORT</code>): HTTP port configuration.</p> <ul> <li>Type: <code>int</code></li> <li>Default: <code>8090</code></li> <li>Environment Variable: <code>ASSET_HTTP_PORT</code></li> <li>Impact: Configuration placeholder - not actively used in traced Asset Server code paths</li> </ul> </li> </ul> <p>Response Signing Settings:</p> <ul> <li>Asset Sign HTTP Responses (<code>asset_sign_http_responses</code>): Enables cryptographic signing of HTTP responses.<ul> <li>Type: <code>bool</code></li> <li>Default: <code>false</code></li> <li>Environment Variable: <code>TERANODE_ASSET_SIGN_HTTP_RESPONSES</code></li> <li>Impact: When enabled, requires valid P2P private key; logs errors if key is invalid</li> <li>Code Usage: Controls response signing initialization using P2P private key</li> </ul> </li> </ul> <p>Debug Settings:</p> <ul> <li>Echo Debug (<code>ECHO_DEBUG</code>): Enables Echo framework debug mode.<ul> <li>Type: <code>bool</code></li> <li>Default: <code>false</code></li> <li>Environment Variable: <code>ECHO_DEBUG</code></li> <li>Impact: Enables debug logging and custom logging middleware for HTTP requests</li> <li>Code Usage: Controls Echo debug mode and custom request logging</li> </ul> </li> </ul>"},{"location":"topics/services/assetServer/#722-centrifuge-real-time-updates-configuration","title":"7.2.2 Centrifuge Real-time Updates Configuration","text":"<ul> <li> <p>Asset Centrifuge Disable (<code>asset_centrifuge_disable</code>): Controls Centrifuge server initialization.</p> <ul> <li>Type: <code>bool</code></li> <li>Default: <code>false</code></li> <li>Environment Variable: <code>TERANODE_ASSET_CENTRIFUGE_DISABLE</code></li> <li>Impact: When <code>true</code>, disables real-time WebSocket functionality entirely</li> <li>Code Usage: Controls conditional Centrifuge server creation (Server.go line 204)</li> </ul> </li> <li> <p>Asset Centrifuge Listen Address (<code>asset_centrifugeListenAddress</code>): WebSocket server listen address.</p> <ul> <li>Type: <code>string</code></li> <li>Default: <code>\":8892\"</code></li> <li>Environment Variable: <code>TERANODE_ASSET_CENTRIFUGELISTENADDRESS</code></li> <li>Impact: Determines Centrifuge WebSocket server listening address when enabled</li> <li>Code Usage: Used for Centrifuge server address configuration</li> </ul> </li> </ul> <p>Centrifuge Subscription Channels:</p> <p>Centrifuge supports the following subscription channels:</p> <ul> <li><code>ping</code>: For connection health checks</li> <li><code>block</code>: For new block notifications</li> <li><code>subtree</code>: For Merkle tree updates</li> <li><code>mining_on</code>: For mining status updates</li> </ul>"},{"location":"topics/services/assetServer/#723-security-configuration-global-settings","title":"7.2.3 Security Configuration (Global Settings)","text":"<p>HTTPS Settings:</p> <ul> <li> <p>Security Level HTTP (<code>securityLevelHTTP</code>): Determines HTTP vs HTTPS mode.</p> <ul> <li>Type: <code>int</code></li> <li>Default: <code>0</code></li> <li>Environment Variable: <code>TERANODE_SECURITYLEVELHTTP</code></li> <li>Impact: <code>0</code> = HTTP mode, non-zero = HTTPS mode</li> <li>Code Usage: Controls server startup mode selection (HTTP vs HTTPS)</li> </ul> </li> <li> <p>Server Certificate File (<code>server_certFile</code>): TLS certificate file path.</p> <ul> <li>Type: <code>string</code></li> <li>Default: <code>\"\"</code> (empty string)</li> <li>Environment Variable: <code>TERANODE_SERVER_CERTFILE</code></li> <li>Impact: Required for HTTPS - Service returns configuration error if missing when HTTPS enabled</li> <li>Code Usage: Used for HTTPS server startup; validated when securityLevelHTTP is non-zero</li> </ul> </li> <li> <p>Server Key File (<code>server_keyFile</code>): TLS private key file path.</p> <ul> <li>Type: <code>string</code></li> <li>Default: <code>\"\"</code> (empty string)</li> <li>Environment Variable: <code>TERANODE_SERVER_KEYFILE</code></li> <li>Impact: Required for HTTPS - Service returns configuration error if missing when HTTPS enabled</li> <li>Code Usage: Used for HTTPS server startup; validated when securityLevelHTTP is non-zero</li> </ul> </li> </ul> <p>P2P Settings for Response Signing:</p> <ul> <li> <p>P2P HTTP Address (<code>p2p_httpAddress</code>): P2P server HTTP address for WebSocket connection.</p> <ul> <li>Type: <code>string</code></li> <li>Default: <code>\"\"</code> (empty string)</li> <li>Environment Variable: <code>TERANODE_P2P_HTTPADDRESS</code></li> <li>Impact: Required for Centrifuge - Centrifuge server cannot start without valid P2P connection</li> <li>Code Usage: Used for P2P WebSocket listener initialization</li> </ul> </li> <li> <p>P2P Private Key (<code>p2p_private_key</code>): Private key for HTTP response signing.</p> <ul> <li>Type: <code>string</code></li> <li>Default: <code>\"\"</code> (empty string)</li> <li>Environment Variable: <code>TERANODE_P2P_PRIVATE_KEY</code></li> <li>Impact: Used for HTTP response signing when <code>asset_sign_http_responses</code> is enabled</li> <li>Code Usage: Decoded and used for cryptographic signing; errors logged if invalid</li> </ul> </li> </ul>"},{"location":"topics/services/assetServer/#724-configuration-dependencies-and-interactions","title":"7.2.4 Configuration Dependencies and Interactions","text":"<p>HTTP Server Operation:</p> <ul> <li>Primary Setting: <code>asset_httpListenAddress</code> (required)</li> <li>HTTPS Dependencies: <code>securityLevelHTTP</code>, <code>server_certFile</code>, <code>server_keyFile</code></li> <li>Interaction: HTTPS mode requires both certificate and key files; missing files cause configuration errors</li> </ul> <p>Centrifuge Real-time Updates:</p> <ul> <li>Primary Setting: <code>asset_centrifuge_disable</code> (controls feature)</li> <li>Dependencies: <code>asset_centrifugeListenAddress</code>, <code>asset_httpAddress</code></li> <li>Interaction: When enabled, requires valid HTTP address and listen address; URL validation performed</li> </ul> <p>Centrifuge supports the following subscription channels:</p> <ul> <li><code>ping</code>: For connection health checks</li> <li><code>block</code>: For new block notifications</li> <li><code>subtree</code>: For Merkle tree updates</li> <li><code>mining_on</code>: For mining status updates</li> </ul> <p>HTTP Response Signing:</p> <ul> <li>Primary Setting: <code>asset_sign_http_responses</code> (enables feature)</li> <li>Dependency: <code>p2p_private_key</code></li> <li>Interaction: Requires valid P2P private key; invalid keys logged as errors but don't prevent startup</li> </ul>"},{"location":"topics/services/assetServer/#725-error-conditions-and-validation","title":"7.2.5 Error Conditions and Validation","text":"<p>Configuration Errors from Code:</p> <pre><code>Error: \"no asset_httpListenAddress setting found\"\nCause: Missing or empty asset_httpListenAddress setting\n</code></pre> <pre><code>Error: \"asset_httpAddress not found in config\"\nCause: Missing asset_httpAddress when Centrifuge is enabled\n</code></pre> <pre><code>Error: \"asset_httpAddress is not a valid URL\"\nCause: Invalid URL format in asset_httpAddress setting\n</code></pre> <pre><code>Error: \"server_certFile is required for HTTPS\"\nCause: Missing server_certFile when securityLevelHTTP is non-zero\n</code></pre> <pre><code>Error: \"server_keyFile is required for HTTPS\"\nCause: Missing server_keyFile when securityLevelHTTP is non-zero\n</code></pre>"},{"location":"topics/services/assetServer/#726-environment-variables","title":"7.2.6 Environment Variables","text":"<p>Standard Environment Variables:</p> <ul> <li><code>TERANODE_ASSET_HTTPLISTENADDRESS</code> - HTTP server listen address</li> <li><code>TERANODE_ASSET_HTTPADDRESS</code> - Base HTTP server URL</li> <li><code>TERANODE_ASSET_HTTPPUBLICADDRESS</code> - Public-facing URL</li> <li><code>TERANODE_ASSET_APIPREFIX</code> - API URL prefix</li> <li><code>TERANODE_ASSET_CENTRIFUGE_DISABLE</code> - Disable Centrifuge service</li> <li><code>TERANODE_ASSET_CENTRIFUGELISTENADDRESS</code> - Centrifuge listen address</li> <li><code>TERANODE_ASSET_SIGN_HTTP_RESPONSES</code> - Enable response signing</li> <li><code>TERANODE_SECURITYLEVELHTTP</code> - HTTP security level</li> <li><code>TERANODE_SERVER_CERTFILE</code> - TLS certificate file</li> <li><code>TERANODE_SERVER_KEYFILE</code> - TLS private key file</li> <li><code>TERANODE_P2P_HTTPADDRESS</code> - P2P server HTTP address</li> <li><code>TERANODE_P2P_PRIVATE_KEY</code> - P2P private key for signing</li> </ul> <p>Special Environment Variables:</p> <ul> <li><code>ECHO_DEBUG</code> - Echo framework debug mode</li> <li><code>ASSET_HTTP_PORT</code> - HTTP port (configuration placeholder)</li> </ul>"},{"location":"topics/services/assetServer/#727-dependency-configuration","title":"7.2.7 Dependency Configuration","text":"<p>The Asset Server depends on several services for data access. These must be properly configured for the Asset Server to function:</p> Service Setting Description Required UTXO Store <code>utxostore</code> Connection URL for UTXO data Yes Transaction Store <code>txstore</code> Connection URL for transaction data Yes Subtree Store <code>subtreestore</code> Connection URL for Merkle subtree data Yes Block Persister Store <code>block_persisterStore</code> Connection URL for persisted block data Yes Blockchain Client <code>blockchain_grpcAddress</code> gRPC connection for blockchain service Yes <p>Example Dependency Configuration:</p> <pre><code>utxostore=aerospike://localhost:3000/test?set=utxo\ntxstore=blob://localhost:8080/tx\nsubtreestore=blob://localhost:8080/subtree\nblock_persisterStore=blob://localhost:8080/blocks\nblockchain_grpcAddress=localhost:8082\n</code></pre>"},{"location":"topics/services/assetServer/#728-environment-variable-examples","title":"7.2.8 Environment Variable Examples","text":"<p>All configuration options can be set using environment variables with the prefix <code>TERANODE_</code>. For example:</p> <pre><code>export TERANODE_ASSET_HTTPLISTENADDRESS=:8090\nexport TERANODE_SECURITYLEVELHTTP=1\nexport TERANODE_SERVER_CERTFILE=/path/to/cert.pem\n</code></pre>"},{"location":"topics/services/assetServer/#73-configuration-examples","title":"7.3 Configuration Examples","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the asset Server Settings Reference.</p>"},{"location":"topics/services/assetServer/#74-fsm-configuration","title":"7.4 FSM Configuration","text":"<ul> <li>fsm_state_restore: Enables or disables the restore state for the Finite State Machine.<ul> <li>Example: <code>fsm_state_restore=false</code></li> </ul> </li> <li>FSM Functionality: The FSM provides state management for the blockchain system with endpoints for querying and manipulating states.</li> </ul>"},{"location":"topics/services/assetServer/#75-coinbase-configuration","title":"7.5 Coinbase Configuration","text":"<ul> <li>coinbase_grpcAddress: gRPC address for coinbase-related operations.<ul> <li>Example: <code>coinbase_grpcAddress=localhost:50051</code></li> </ul> </li> </ul>"},{"location":"topics/services/assetServer/#76-dashboard-configuration","title":"7.6 Dashboard Configuration","text":"<ul> <li>dashboard_enabled: Enables or disables the Teranode dashboard UI.<ul> <li>Example: <code>dashboard_enabled=true</code></li> </ul> </li> <li>Dashboard Features: Dashboard-related settings control authentication and user interface features.</li> </ul>"},{"location":"topics/services/assetServer/#77-block-validation","title":"7.7 Block Validation","text":"<ul> <li>Block Management: The Asset Server provides endpoints to invalidate and revalidate blocks, which is useful for managing forks and recovering from errors.</li> </ul>"},{"location":"topics/services/assetServer/#8-other-resources","title":"8. Other Resources","text":"<p>Asset Reference</p>"},{"location":"topics/services/blockAssembly/","title":"\ud83d\udce6 Block Assembly Service","text":""},{"location":"topics/services/blockAssembly/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1. Starting the Block Assembly Service</li> <li>2.2. Receiving Transactions from the TX Validator Service</li> <li>2.3. Grouping Transactions into Subtrees</li> <li>2.3.1 Dynamic Subtree Size Adjustment</li> <li>2.4. Creating Mining Candidates</li> <li>2.5. Submit Mining Solution</li> <li>2.6. Processing Subtrees and Blocks from other Nodes and Handling Forks and Conflicts</li> <li>2.6.1. The block received is the same as the current chaintip (i.e. the block we have already seen).</li> <li>2.6.2. The block received is a new block, and it is the new chaintip.</li> <li>2.6.3. The block received is a new block, but it represents a fork.</li> <li>Fork Detection and Assessment</li> <li>Chain Selection and Reorganization Process</li> <li>2.8. Resetting the Block Assembly</li> </ul> </li> <li>Data Model</li> <li>gRPC Protobuf Definitions</li> <li>Technology</li> <li>Error Handling<ul> <li>6.1. Error Handling Patterns</li> <li>6.2. Performance Monitoring</li> </ul> </li> <li>Directory Structure and Main Files</li> <li>How to run</li> <li>Configuration options (settings flags)</li> <li>Other Resources</li> </ol>"},{"location":"topics/services/blockAssembly/#1-description","title":"1. Description","text":"<p>The Block Assembly Service is responsible for assembling new blocks and adding them to the blockchain.  The block assembly process involves the following steps:</p> <ol> <li> <p>Receiving Transactions from the TX Validator Service:</p> <ul> <li>The block assembly module receives new transactions from the transaction validator service.</li> </ul> </li> <li> <p>Grouping Transactions into Subtrees:</p> <ul> <li>The received transactions are grouped into subtrees.</li> <li>Subtrees represent a hierarchical structure that organizes transactions for more efficient processing and inclusion in a block.</li> </ul> </li> <li> <p>Broadcasting Subtrees to Other Nodes:</p> <ul> <li>Once subtrees are formed, they are broadcasted to other nodes in the network. This is initiated by the block assembly service, which sends a notification to the P2P service (via the Blockchain Service).</li> <li>This step is crucial for maintaining network synchronization and ensuring all nodes have the latest set of subtrees, prior to receiving a block with those subtrees in them. The nodes can validate the subtrees and ensure that they are valid before they are included in a block.</li> </ul> </li> <li> <p>Creating Mining Candidates:</p> <ul> <li>The block assembly continuously creates mining candidates.</li> <li>A mining candidate is essentially a potential block that includes all the subtrees known up to that time, built on top of the longest honest chain.</li> <li>This candidate block is then submitted to the mining module of the node.</li> </ul> </li> <li> <p>Mining Process:</p> <ul> <li>The mining module attempts to find a solution to the cryptographic challenge (proof of work) associated with the mining candidate.</li> <li>If the miner successfully solves the puzzle before other nodes in the network, the block is considered valid and ready to be added to the blockchain.</li> </ul> </li> <li> <p>Adding the Block to the Blockchain:</p> <ul> <li>Once a mining solution is found, the new block is added to the blockchain.</li> </ul> </li> <li> <p>Notifying Other Nodes of the New Block:</p> <ul> <li>After successfully adding the block, other nodes in the network are notified of the new block.</li> </ul> </li> <li> <p>Handling Forks and Conflicts:</p> <ul> <li>The node also handles the resolution of forks in the blockchain and conflicting subtrees or blocks mined by other nodes.</li> <li>This involves choosing between different versions of the blockchain (in case of forks) and resolving conflicts in transactions and subtrees included in other nodes' blocks.</li> </ul> </li> </ol> <p>Note: For information about how the Block Assembly service is initialized during daemon startup and how it interacts with other services, see the Teranode Daemon Reference.</p> <p>A high level diagram:</p> <p></p> <p>Based on its settings, the Block Assembly receives TX notifications from the validator service via direct gRPC calls.</p> <p>The Block Assembly service also subscribes to the Blockchain service, and receives notifications when a new subtree or block is received from another node.</p> <p></p>"},{"location":"topics/services/blockAssembly/#detailed-component-diagram","title":"Detailed Component Diagram","text":"<p>The detailed component diagram below shows the internal architecture of the Block Assembly Service with code-verified connections:</p> <p></p> <p>Finally, note that the Block Assembly benefits of the use of Lustre Fs (filesystem). Lustre is a type of parallel distributed file system, primarily used for large-scale cluster computing. This filesystem is designed to support high-performance, large-scale data storage and workloads.</p> <p>Specifically for Teranode, these volumes are meant to be temporary holding locations for short-lived file-based data that needs to be shared quickly between various services.</p> <p>Teranode microservices make use of the Lustre file system in order to share subtree and tx data, eliminating the need for redundant propagation of subtrees over grpc or message queues. The services sharing Subtree data through this system can be seen here:</p> <p></p>"},{"location":"topics/services/blockAssembly/#2-functionality","title":"2. Functionality","text":""},{"location":"topics/services/blockAssembly/#21-starting-the-block-assembly-service","title":"2.1. Starting the Block Assembly Service","text":"<p>The Block Assembly service initialisation involves setting up internal communication channels and external communication channels, and instantiating the Subtree Processor and Job Store.</p> <p>The SubTree Processor is the component that groups transactions into subtrees.</p> <p>The Job Store is a temporary in-memory map that tracks information about the candidate blocks that the miners are attempting to find a solution for.</p>"},{"location":"topics/services/blockAssembly/#211-loading-unmined-transactions-on-startup","title":"2.1.1. Loading Unmined Transactions on Startup","text":"<p>When the Block Assembly service starts, it automatically recovers unmined transactions from the UTXO store to ensure continuity across service restarts. This functionality is crucial for maintaining transaction processing reliability.</p> <p></p> <p>Process Overview:</p> <ol> <li>Wait for Pending Blocks: The service first ensures all pending blocks are processed to avoid conflicts</li> <li>Load Unmined Transactions: Uses the <code>UnminedTxIterator</code> to retrieve all transactions marked with the <code>unminedSince</code> flag</li> <li>Order by Creation Time: Transactions are sorted topologically by their <code>createdAt</code> timestamp to maintain proper dependencies</li> <li>Re-add to Processing: Each unmined transaction is added back to the subtree processor using <code>AddDirectly()</code></li> <li>Unlock Transactions: Previously locked transactions are unlocked to allow processing</li> </ol> <p>This recovery mechanism ensures that:</p> <ul> <li>Transactions accepted but not yet mined persist across restarts</li> <li>Network participants don't need to resubmit transactions after node restarts</li> <li>The transaction processing pipeline maintains continuity</li> </ul>"},{"location":"topics/services/blockAssembly/#22-receiving-transactions-from-the-tx-validator-service","title":"2.2. Receiving Transactions from the TX Validator Service","text":"<ul> <li>The TX Validator interacts with the Block Assembly Client. Based on configuration, we send either transactions in batches or individually. This communication is performed over gRPC.</li> <li>The Block Assembly client then delegates to the Server, which adds the transactions to the Subtree Processor.</li> <li>At a later stage, the Subtree Processor will group the transactions into subtrees, which will be used to create mining candidates.</li> </ul>"},{"location":"topics/services/blockAssembly/#23-grouping-transactions-into-subtrees","title":"2.3. Grouping Transactions into Subtrees","text":"<ul> <li>The Subtree Processor dequeues any transaction request (txReq) received in the previous section, and adds it to the latest (current) subtree.</li> <li>If the current subtree is complete (i.e. if it has reached the target length, say 1M transactions), it sends the subtree to the server through an internal Go channel (newSubtreeChan).</li> <li>The server then checks if the subtree already exists in the Subtree Store. Otherwise, the server persists the new subtree in the store with a specified (and settings-driven) TTL (Time-To-Live).</li> <li>Finally, the server sends a notification to the BlockchainClient to announce the new subtree. This will be propagated to other nodes via the P2P service.</li> </ul>"},{"location":"topics/services/blockAssembly/#231-dynamic-subtree-size-adjustment","title":"2.3.1 Dynamic Subtree Size Adjustment","text":"<p>The Block Assembly service can dynamically adjust the subtree size based on real-time performance metrics when enabled via configuration:</p> <ul> <li>The system targets a rate of approximately one subtree per second under high throughput conditions</li> <li>If subtrees are being created too quickly, the size is automatically increased</li> <li>If subtrees are being created too slowly, the size is decreased</li> <li>Adjustments are always made to a power of 2 and constrained by minimum and maximum bounds</li> <li>Size increases are capped at 2x per block to prevent wild oscillations</li> </ul> <p>Importantly, the system maintains a minimum subtree size threshold, configured via <code>minimum_merkle_items_per_subtree</code>. In low transaction volume scenarios, subtrees will only be created once enough transactions have accumulated to meet this minimum size requirement. This means that during periods of low network usage, the target rate of one subtree per second may not be achieved, as the system prioritizes reaching the minimum subtree size before sending.</p> <p></p> <p>This self-tuning mechanism helps maintain consistent processing rates and optimal resource utilization during block assembly, automatically adapting to the node's current processing capabilities and transaction volumes.</p>"},{"location":"topics/services/blockAssembly/#24-creating-mining-candidates","title":"2.4. Creating Mining Candidates","text":"<ul> <li>The \"Miner\" initiates the process by requesting a mining candidate (a block to mine) from the Block Assembly.</li> <li>The \"Block Assembler\" sub-component interacts with the Subtree Processor to obtain completed subtrees that can be included in the mining candidate. It must be noted that there is no subtree limit, Teranode has no restrictions on the maximum block size (hence, neither on the number of subtrees).</li> <li>The Block Assembler then calculates the coinbase value and merkle proof for the candidate block.</li> <li>The mining candidate, inclusive of the list of subtrees, a coinbase TX, a merkle proof, and associated fees, is returned back to the miner.</li> <li>The Block Assembly Server makes status announcements, using the Status Client, about the mining candidate's height and previous hash.</li> <li>Finally, the Server tracks the current candidate in the JobStore within a new \"job\" and its TTL. This information will be retrieved at a later stage, if and when the miner submits a solution to the mining challenge for this specific mining candidate.</li> </ul>"},{"location":"topics/services/blockAssembly/#25-submit-mining-solution","title":"2.5. Submit Mining Solution","text":"<p>Once a miner solves the mining challenge, it submits a solution to the Block Assembly Service. The solution includes the nonce required to solve the mining challenge.</p> <p></p> <ul> <li>The \"Mining\" service submits a mining solution (based on a previously provided \"mining candidate\") to the Block Assembly Service.</li> <li>The Block Assembly server adds the submission to a channel (blockSubmissionCh) and processes the submission (submitMiningSolution).</li> <li>The job item details are retrieved from the JobStore, and a new block is created with the miner's proof of work.</li> <li> <p>The block is validated, and if valid, the coinbase transaction is persisted in the Tx Store.</p> </li> <li> <p>The block is added to the blockchain via the Blockchain Client. This will be propagated to other nodes via the P2P service.</p> </li> <li> <p>Subtree TTLs are removed, effectively setting the subtrees for removal from the Subtree Store.</p> </li> <li>All jobs in the Job Store are deleted.</li> <li>In case of an error at any point in the process, the block is invalidated through the Blockchain Client.</li> </ul>"},{"location":"topics/services/blockAssembly/#26-processing-subtrees-and-blocks-from-other-nodes-and-handling-forks-and-conflicts","title":"2.6. Processing Subtrees and Blocks from other Nodes and Handling Forks and Conflicts","text":"<p>The block assembly service subscribes to the Blockchain service, and receives notifications (<code>model.NotificationType_Block</code>) when a new block is received from another node. The logic for processing these blocks can be found in the <code>BlockAssembler.go</code> file, <code>startChannelListeners</code> function.</p> <p>Once a new block has been received from another node, there are 4 scenarios to consider:</p>"},{"location":"topics/services/blockAssembly/#261-the-block-received-is-the-same-as-the-current-chaintip-ie-the-block-we-have-already-seen","title":"2.6.1. The block received is the same as the current chaintip (i.e. the block we have already seen)","text":"<p>In this case, the block notification is redundant, and refers to a block that the service already considers the current chaintip. The service does nothing.</p>"},{"location":"topics/services/blockAssembly/#262-the-block-received-is-a-new-block-and-it-is-the-new-chaintip","title":"2.6.2. The block received is a new block, and it is the new chaintip","text":"<p>In this scenario, another node has mined a new block that is now the new chaintip.</p> <p>The service needs to \"move up\" the block. By this, we mean the process to identify transactions included in the new block (so we do not include them in future blocks) and to process the coinbase UTXOs (so we can include them in future blocks).</p> <p></p> <ol> <li> <p>Checking for the Best Block Header:</p> <ul> <li>The <code>BlockAssembler</code> logs information indicating that the best block header (the header of the most recent block in the chain) is the same as the previous one. It then attempts to \"move up\" to this new block.</li> </ul> </li> <li> <p>Getting the Block from Blockchain Client:</p> <ul> <li><code>b.blockchainClient.GetBlock(ctx, bestBlockchainBlockHeader.Hash())</code> fetches the block corresponding to the best blockchain block header.</li> </ul> </li> <li> <p>Processing the Block in SubtreeProcessor:</p> <ul> <li><code>b.subtreeProcessor.MoveForwardBlock(block)</code> is called, which initiates the process of updating the subtree processor with the new block.</li> </ul> </li> <li> <p>SubtreeProcessor Handling:</p> <ul> <li>In <code>MoveForwardBlock</code>, a channel for error handling is set up and a <code>moveBlockRequest</code> is sent to <code>moveForwardBlockChan</code>.</li> <li>This triggers the <code>case moveForwardReq := &lt;-stp.moveForwardBlockChan</code> in <code>SubtreeProcessor</code>, which handles the request to move up a block.</li> <li><code>stp.moveForwardBlock(ctx, moveForwardReq.block, false)</code> is called, which is where the main logic of handling the new block is executed.</li> </ul> </li> <li> <p>MoveForwardBlock Functionality:</p> <p>The <code>moveForwardBlock</code> function is a comprehensive process that handles the integration of a new block into the Block Assembly service. This function performs seven distinct processing steps to ensure proper blockchain state management and transaction processing.</p> <p>Initial Validation and Setup:</p> <ul> <li>When <code>moveForwardBlock</code> is invoked, it receives a <code>block</code> object as a parameter and performs validation checks to ensure the block is not nil and is valid for processing.</li> <li>Error handling is implemented throughout the process, with proper error propagation and cleanup mechanisms.</li> </ul> <p>Step 1: Process Coinbase UTXOs (<code>processCoinbaseUtxos</code>):</p> <ul> <li>Handles the coinbase transaction (the first transaction in a block, used to reward miners)</li> <li>Creates and stores coinbase UTXOs in the UTXO store with proper block height and mined block information</li> <li>Implements duplicate coinbase transaction handling for the two known duplicate coinbase transactions on the network</li> <li>Includes comprehensive error handling for UTXO creation failures</li> </ul> <p>Step 2: Process Block Subtrees (<code>processBlockSubtrees</code>):</p> <ul> <li>Creates a reverse lookup map (<code>blockSubtreesMap</code>) of all subtrees contained in the received block</li> <li>Filters and separates chained subtrees that were not included in the block</li> <li>This separation is crucial for distinguishing between blocks from other nodes vs. blocks from own mining</li> </ul> <p>Step 3: Create Transaction Map (<code>createTransactionMapIfNeeded</code>):</p> <ul> <li>For blocks from other nodes: Retrieves subtree data from the Subtree Store for each subtree in the block<ul> <li>Deserializes subtree data and creates a comprehensive transaction map</li> <li>Extracts conflicting nodes that may cause transaction conflicts</li> </ul> </li> <li>For blocks from own mining: Uses existing chained subtrees without store retrieval<ul> <li>Implements error handling for subtree retrieval failures</li> </ul> </li> </ul> <p>Step 4: Process Conflicting Transactions (<code>processConflictingTransactions</code>):</p> <ul> <li>Identifies transactions that conflict with those in the received block</li> <li>Creates a map of losing transaction hashes that need to be marked as conflicting</li> <li>Ensures proper conflict resolution to maintain blockchain integrity</li> </ul> <p>Step 5: Reset Subtree State (<code>resetSubtreeState</code>):</p> <ul> <li>Resets the current subtree and transaction map to prepare for new transaction processing</li> <li>Clears previous state to ensure clean processing of remaining transactions</li> <li>Returns the reset current subtree and transaction map for further processing</li> </ul> <p>Step 6: Process Remainder Transactions (<code>processRemainderTransactions</code>):</p> <ul> <li>Processes transactions that were not included in the received block</li> <li>Handles transactions from both chained subtrees and the current subtree</li> <li>Manages transaction conflicts and updates subtree state accordingly</li> <li>Ensures that pending transactions are properly carried over for inclusion in future blocks</li> </ul> <p>Step 7: Finalize Block Processing (<code>finalizeBlockProcessing</code>):</p> <ul> <li>Updates internal state variables and performs necessary cleanup operations</li> <li>Ensures the Block Assembly service is ready for the next block processing cycle</li> <li>Completes the integration of the new block into the service's state</li> </ul> <p>Error Handling and Recovery:</p> <ul> <li>Each step includes comprehensive error handling with proper error propagation</li> <li>Failed operations result in appropriate error messages and cleanup procedures</li> <li>The function ensures that partial processing failures don't leave the service in an inconsistent state</li> </ul> <p>Performance Considerations:</p> <ul> <li>The function uses concurrent processing where appropriate to optimize performance</li> <li>Subtree retrieval and processing are optimized to handle large blocks efficiently</li> <li>Memory management is carefully handled to prevent resource leaks during processing</li> </ul> </li> </ol>"},{"location":"topics/services/blockAssembly/#263-the-block-received-is-a-new-block-but-it-represents-a-fork","title":"2.6.3. The block received is a new block, but it represents a fork","text":"<p>In this scenario, the function needs to handle a reorganization. A blockchain reorganization occurs when a node discovers a longer or more difficult chain different from the current local chain. This can happen due to network delays or forks in the blockchain network.</p> <p>It is the responsibility of the block assembly to always build on top of the longest chain of work. For clarity, it is not the Block Validation or Blockchain services's responsibility to resolve forks. The Block Assembly is notified of the ongoing chains of work, and it makes sure to build on the longest one. If the longest chain of work is different from the current local chain the block assembly was working on, a reorganization will take place.</p>"},{"location":"topics/services/blockAssembly/#fork-detection-and-assessment","title":"Fork Detection and Assessment","text":"<p>The Block Assembly service implements real-time fork detection through the following mechanisms:</p> <ul> <li>Real-time Block Monitoring: Implemented via <code>blockchainSubscriptionCh</code> in the Block Assembler, which continuously monitors for new blocks and chain updates.</li> </ul> <p>Fork Detection Criteria: The Block Assembly service uses three main criteria to detect and handle forks:</p> <ol> <li> <p>Chain Height Tracking:</p> <ul> <li>Maintains current blockchain height through <code>bestBlockHeight</code></li> <li>Compares incoming block heights with current chain tip</li> <li>Used to determine if incoming blocks represent a longer chain</li> </ul> </li> <li> <p>Block Hash Verification:</p> <ul> <li>Uses <code>HashPrevBlock</code> to verify block connectivity</li> <li>Ensures each block properly references its predecessor</li> <li>Helps identify where chains diverge</li> </ul> </li> <li> <p>Reorganization Size Protection:</p> <ul> <li>Monitors the size of potential chain reorganizations</li> <li>If a reorganization would require moving more than 5 blocks either backwards or forwards</li> <li>AND the current chain height is greater than 1000 blocks</li> <li>Triggers a full reset of the block assembler as a safety measure against deep reorganizations</li> </ul> </li> </ol> <p>The <code>BlockAssembler</code> keeps the node synchronized with the network by identifying and switching to the strongest chain (the one with the most accumulated proof of work), ensuring all nodes in the network converge on the same transaction history.</p>"},{"location":"topics/services/blockAssembly/#chain-selection-and-reorganization-process","title":"Chain Selection and Reorganization Process","text":"<p>During a reorganization, the <code>BlockAssembler</code> performs two key operations:</p> <ol> <li>Removes (rolls back) transactions from blocks in the current chain, starting from where the fork occurred</li> <li>Applies transactions from the new chain's blocks, ensuring the node switches to the stronger chain</li> </ol> <p>The service automatically manages chain selection through:</p> <ol> <li> <p>Best Chain Detection:</p> <ul> <li>Continuously monitors for new best block headers</li> <li>Compares incoming blocks against current chain tip</li> <li>Automatically triggers reorganization when a better chain is detected</li> </ul> </li> <li> <p>Chain Selection Process:</p> <ul> <li>Accepts the chain with the most accumulated proof of work</li> <li> <p>Performs a safety check on reorganization depth:</p> <ul> <li>If the reorganization involves more than 5 blocks in either direction</li> <li>And the current chain height is greater than 1000</li> <li>The block assembler will reset rather than attempt the reorganization</li> </ul> </li> <li> <p>Block validation and transaction verification are handled by other services, not the Block Assembly</p> </li> </ul> </li> <li> <p>Chain Switching Process:</p> <ul> <li>Identifies common ancestor between competing chains</li> <li>Rolls back the current chain to a common point with the competing (and stronger) chain</li> <li>Applies new blocks from the competing chain</li> <li>Updates UTXO set and transaction pools accordingly</li> </ul> </li> </ol> <p></p> <p>The following diagram illustrates how the Block Assembly service handles a chain reorganization:</p> <ul> <li> <p><code>err = b.handleReorg(ctx, bestBlockchainBlockHeader)</code>:</p> </li> <li> <p>Calls the <code>handleReorg</code> method, passing the current context (<code>ctx</code>) and the new best block header from the blockchain network.</p> </li> <li> <p>The reorg process involves rolling back to the last common ancestor block and then adding the new blocks from the network to align the <code>BlockAssembler</code>'s blockchain state with the network's state.</p> </li> <li> <p>Getting Reorg Blocks:</p> </li> <li> <p><code>moveBackBlocks, moveForwardBlocks, err := b.getReorgBlocks(ctx, header)</code>:</p> <ul> <li>Calls <code>getReorgBlocks</code> to determine the blocks to move down (to revert) and move up (to apply) for aligning with the network's consensus chain.</li> <li><code>header</code> is the new block header that triggered the reorg.</li> <li>This step involves finding the common ancestor and getting the blocks from the current chain (move down) and the new chain (move up).</li> </ul> </li> <li> <p>Performing Reorg in Subtree Processor:</p> <ul> <li> <p><code>b.subtreeProcessor.Reorg(moveBackBlocks, moveForwardBlocks)</code>:</p> </li> <li> <p>Executes the actual reorg process in the <code>SubtreeProcessor</code>, responsible for managing the blockchain's data structure and state.</p> </li> <li>The function reverts the coinbase Txs associated to invalidated blocks (deleting their UTXOs).<ul> <li>This step involves reconciling the status of transactions from reverted and new blocks, and coming to a curated new current subtree(s) to include in the next block to mine.</li> </ul> </li> </ul> </li> </ul> <p>Note: If other nodes propose blocks containing a transaction that Teranode has identified as a double-spend (based on the First-Seen rule), Teranode will only build on top of such blocks when the network has reached consensus on which transaction to accept, even if it differs from Teranode's initial first-seen assessment. For more information, please review the Double Spend Detection documentation.</p>"},{"location":"topics/services/blockAssembly/#27-unmined-transaction-cleanup","title":"2.7. Unmined Transaction Cleanup","text":"<p>The Block Assembly service periodically cleans up old unmined transactions to prevent unbounded growth of the UTXO store. This cleanup process is essential for maintaining system performance and preventing resource exhaustion.</p> <p></p> <p>Cleanup Process:</p> <ol> <li>Periodic Trigger: A background ticker (<code>unminedCleanupTicker</code>) runs at configured intervals</li> <li>Age-Based Selection: Identifies unmined transactions older than the retention period using <code>QueryOldUnminedTransactions</code></li> <li>Parent Preservation: Protects parent transactions of younger unmined transactions from deletion</li> <li>Batch Deletion: Removes eligible transactions in batches to minimize performance impact</li> </ol> <p>Configuration:</p> <ul> <li>Retention Period: Configured via <code>UnminedTxRetention</code> settings</li> <li>Cleanup Interval: Controlled by the cleanup ticker configuration</li> <li>Parent Protection: Uses <code>PreserveTransactions</code> to mark parents that should be retained</li> </ul> <p>This cleanup mechanism ensures the UTXO store remains performant while preserving transaction dependencies.</p>"},{"location":"topics/services/blockAssembly/#28-resetting-the-block-assembly","title":"2.8. Resetting the Block Assembly","text":"<p>The Block Assembly service can be reset to the best block by calling the <code>ResetBlockAssembly</code> gRPC method.</p> <ol> <li> <p>State Storage and Retrieval:</p> <ul> <li><code>bestBlockchainBlockHeader, meta, err = b.blockchainClient.GetBestBlockHeader(ctx)</code>: Retrieves the best block header from the blockchain along with its metadata.</li> </ul> </li> <li> <p>Resetting Block Assembly:</p> <ul> <li>The block assembler resets to the new best block header with its height and details.</li> <li>It then calculates which blocks need to be moved down or up to align with the new best block header (<code>getReorgBlocks</code>).</li> </ul> </li> <li> <p>Processing the Reorganization:</p> <ul> <li>It attempts to reset the <code>subtreeProcessor</code> with the new block headers. If there's an error during this reset, it logs the error, and the block header is re-set to match the <code>subtreeProcessor</code>'s current block header.</li> </ul> </li> <li> <p>Updating Assembly State:</p> <ul> <li>Updates internal state with the new best block header and adjusts the height of the best block based on how many blocks were moved up and down.</li> <li>Attempts to set the new state and current blockchain chain.</li> </ul> </li> </ol> <p></p>"},{"location":"topics/services/blockAssembly/#3-data-model","title":"3. Data Model","text":"<ul> <li>Block Data Model: Contain lists of subtree identifiers.</li> <li>Subtree Data Model: Contain lists of transaction IDs and their Merkle root.</li> <li>UTXO Data Model: Include additional metadata to facilitate processing.</li> </ul>"},{"location":"topics/services/blockAssembly/#4-grpc-protobuf-definitions","title":"4. gRPC Protobuf Definitions","text":"<p>The Block Assembly Service uses gRPC for communication between nodes. The protobuf definitions used for defining the service methods and message formats can be seen in the Block Assembly Protobuf Reference.</p>"},{"location":"topics/services/blockAssembly/#5-technology","title":"5. Technology","text":"<ul> <li> <p>Go (Golang): The service is written in Go.</p> </li> <li> <p>gRPC: For communication between different services, gRPC is commonly used.</p> </li> <li> <p>Kafka: Used for tx message queuing and streaming, Kafka can efficiently handle the high throughput of transaction data in a distributed manner.</p> </li> <li> <p>Configuration Management (gocore): Uses <code>gocore</code> for configuration management, allowing dynamic configuration of service parameters.</p> </li> <li> <p>Networking and Protocol Buffers: Handles network communications and serializes structured data using Protocol Buffers, a language-neutral, platform-neutral, extensible mechanism for serializing structured data.</p> </li> </ul>"},{"location":"topics/services/blockAssembly/#6-error-handling","title":"6. Error Handling","text":""},{"location":"topics/services/blockAssembly/#61-error-handling-patterns","title":"6.1. Error Handling Patterns","text":"<p>The Block Assembly service implements robust error handling across multiple layers:</p>"},{"location":"topics/services/blockAssembly/#block-validation-errors","title":"Block Validation Errors","text":"<ul> <li>Mining Solution Validation: When <code>SubmitMiningSolution</code> is called, the service performs comprehensive block validation including difficulty target verification</li> <li>Invalid Block Handling: Failed validations are logged with detailed error messages and the mining solution is rejected</li> <li>Recovery: The service continues processing other mining candidates without interruption</li> </ul>"},{"location":"topics/services/blockAssembly/#subtree-storage-failures","title":"Subtree Storage Failures","text":"<ul> <li>Retry Mechanism: Failed subtree storage operations are automatically retried using <code>subtreeRetryChan</code></li> <li>Error Propagation: Storage errors are communicated back through error channels to prevent data loss</li> <li>Graceful Degradation: The service can continue operating with reduced functionality if storage issues persist</li> </ul>"},{"location":"topics/services/blockAssembly/#reorganization-conflicts","title":"Reorganization Conflicts","text":"<ul> <li>Conflicting Transaction Detection: During reorgs, the service identifies and marks conflicting transactions in affected subtrees</li> <li>Transaction Recovery: Non-conflicting transactions are automatically re-added to new subtrees</li> <li>Deep Reorg Protection: Reorganizations affecting more than 5 blocks trigger a full service reset for safety</li> </ul>"},{"location":"topics/services/blockAssembly/#utxo-store-unavailability","title":"UTXO Store Unavailability","text":"<ul> <li>Connection Monitoring: The service monitors UTXO store connectivity and handles temporary unavailability</li> <li>Operation Queuing: Transactions are queued when UTXO operations fail temporarily</li> <li>State Consistency: The service ensures consistent state even during UTXO store recovery</li> </ul>"},{"location":"topics/services/blockAssembly/#62-performance-monitoring","title":"6.2. Performance Monitoring","text":"<p>The service integrates comprehensive Prometheus metrics for operational monitoring:</p>"},{"location":"topics/services/blockAssembly/#subtree-processing-metrics","title":"Subtree Processing Metrics","text":"<ul> <li><code>teranode_subtreeprocessor_add_tx</code>: Counter for transaction additions</li> <li><code>teranode_subtreeprocessor_dynamic_subtree_size</code>: Current dynamic subtree size</li> <li><code>teranode_subtreeprocessor_move_forward_block</code>: Block processing operations</li> <li><code>teranode_subtreeprocessor_move_back_block</code>: Block rollback operations</li> </ul>"},{"location":"topics/services/blockAssembly/#block-assembly-metrics","title":"Block Assembly Metrics","text":"<ul> <li><code>teranode_blockassembly_get_mining_candidate</code>: Mining candidate requests</li> <li><code>teranode_blockassembly_submit_mining_solution</code>: Mining solution submissions</li> <li>Duration histograms for critical operations with microsecond precision</li> </ul>"},{"location":"topics/services/blockAssembly/#state-monitoring","title":"State Monitoring","text":"<ul> <li>Current service state (starting, running, resetting, etc.)</li> <li>Transaction queue lengths and processing rates</li> <li>Subtree counts and completion rates</li> </ul>"},{"location":"topics/services/blockAssembly/#7-directory-structure-and-main-files","title":"7. Directory Structure and Main Files","text":"<pre><code>/services/blockassembly\n\u251c\u2500\u2500 BlockAssembler.go              - Main logic for assembling blocks.\n\u251c\u2500\u2500 BlockAssembler_test.go         - Tests for BlockAssembler.go.\n\u251c\u2500\u2500 Client.go                      - Client-side logic for block assembly.\n\u251c\u2500\u2500 Interface.go                   - Interface definitions for block assembly.\n\u251c\u2500\u2500 Server.go                      - Server-side logic for block assembly.\n\u251c\u2500\u2500 Server_test.go                 - Tests for Server.go.\n\u251c\u2500\u2500 blockassembly_api              - Directory for block assembly API.\n\u2502   \u251c\u2500\u2500 blockassembly_api.pb.go    - Generated protobuf code.\n\u2502   \u251c\u2500\u2500 blockassembly_api.proto    - Protobuf definitions.\n\u2502   \u251c\u2500\u2500 blockassembly_api_grpc.pb.go - gRPC generated code.\n\u251c\u2500\u2500 blockassembly_system_test.go   - System-level integration tests.\n\u251c\u2500\u2500 data.go                        - Data structures used in block assembly.\n\u251c\u2500\u2500 data_test.go                   - Tests for data.go.\n\u251c\u2500\u2500 metrics.go                     - Metrics collection for block assembly.\n\u251c\u2500\u2500 mining                         - Directory for mining-related functionality.\n\u251c\u2500\u2500 remotettl.go                   - Management of remote TTL (Time To Live) values.\n\u2514\u2500\u2500 subtreeprocessor               - Directory for subtree processing.\n    \u251c\u2500\u2500 SubtreeProcessor.go        - Main logic for processing subtrees.\n    \u251c\u2500\u2500 SubtreeProcessor_test.go   - Tests for SubtreeProcessor.go.\n    \u251c\u2500\u2500 metrics.go                 - Metrics specific to subtree processing.\n    \u251c\u2500\u2500 options.go                 - Configuration options for subtree processing.\n    \u251c\u2500\u2500 queue.go                   - Queue implementation for subtree processing.\n    \u251c\u2500\u2500 queue_test.go              - Tests for queue.go.\n    \u251c\u2500\u2500 testdata                   - Test data for subtree processor tests.\n    \u2514\u2500\u2500 txIDAndFee.go              - Handling transaction IDs and fees.\n</code></pre>"},{"location":"topics/services/blockAssembly/#8-how-to-run","title":"8. How to run","text":"<p>To run the Block Assembly Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -BlockAssembly=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Block Assembly Service locally.</p>"},{"location":"topics/services/blockAssembly/#9-configuration-options-settings-flags","title":"9. Configuration options (settings flags)","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the block Assembly Settings Reference.</p>"},{"location":"topics/services/blockAssembly/#10-other-resources","title":"10. Other Resources","text":"<ul> <li>Block Assembly Reference</li> <li>Handling Double Spends</li> </ul>"},{"location":"topics/services/blockPersister/","title":"\ud83d\udd0d Block Persister Service","text":""},{"location":"topics/services/blockPersister/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1 Service Initialization</li> <li>2.2 Block Discovery and Processing</li> <li>2.3 Subtree Processing Details</li> </ul> </li> <li>Data Model</li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>How to run</li> <li>Configuration options (settings flags)</li> <li>Other Resources</li> </ol>"},{"location":"topics/services/blockPersister/#1-description","title":"1. Description","text":"<p>The Block Persister service functions as an overlay microservice, designed to post-process subtrees after their integration into blocks and persisting them to a separate storage.</p> <p>The Block Persister service operates as a background processing service that continuously polls the blockchain service for new blocks. Unlike other Teranode services, it does not expose a gRPC API and functions purely as a data persistence layer. When new blocks are detected through polling, the Block Persister service decorates (enriches) all transactions within the block's subtrees, ensuring the inclusion of transaction metadata (UTXO meta data). Then, it will save a number of files into a file storage system (such as S3):</p> <ul> <li>A file for the block (<code>.block</code> extension).</li> <li>A file for each subtree in a block (<code>.subtree</code> extension), containing the decorated transactions.</li> <li>A file for the UTXO Additions (<code>.utxo-additions</code> extension) per block, containing the newly created UTXOs.</li> <li>A file for the UTXO Deletions ( <code>.utxo-deletions</code> extension) per block, containing the spent UTXOs.</li> </ul> <p>This service plays a key role within the Teranode network, guaranteeing that txs are accurately post-processed and stored with the essential metadata for necessary audit and traceability purposes. The decorated subtrees remain invaluable for future data analysis and inspection.</p> <p>The Block Persister files are optionally post-processed by the UTXO Persister, which maintains a UTXO Set in a similar disk format.</p> <p>Note: For information about how the Block Persister service is initialized during daemon startup and how it interacts with other services, see the Teranode Daemon Reference.</p> <p></p> <ul> <li>The Block Persister polls the Blockchain service for new blocks and stores the decorated block in a data store (such as S3).</li> </ul> <p></p> <ul> <li> <p>The Block Persister service operates independently by polling the Blockchain service directly for new blocks, rather than relying on external notification systems.</p> </li> <li> <p>The Blockchain client is directly accessed to wait for the node State Management to change to <code>RUNNING</code> state before beginning block processing operations. For more information on this, please refer to the State Management documentation.</p> </li> </ul>"},{"location":"topics/services/blockPersister/#detailed-component-view","title":"Detailed Component View","text":"<p>The following diagram provides a deeper level of detail into the Block Persister Service's internal components and their interactions:</p> <p></p>"},{"location":"topics/services/blockPersister/#2-functionality","title":"2. Functionality","text":""},{"location":"topics/services/blockPersister/#21-service-initialization","title":"2.1 Service Initialization","text":"<p>The service initializes through the following sequence:</p> <ol> <li>Loads configuration settings</li> <li>Initializes state management</li> <li>Establishes connection with blockchain client</li> <li>Waits for FSM transition from IDLE state</li> <li>Starts block processing loop</li> </ol>"},{"location":"topics/services/blockPersister/#22-block-discovery-and-processing","title":"2.2 Block Discovery and Processing","text":"<p>The service processes blocks through a continuous polling mechanism:</p> <ol> <li> <p>Block Discovery</p> <ul> <li>Retrieves last persisted block height from the local state</li> <li>Polls the Blockchain service to get the current best block header</li> <li>Determines if new blocks need processing based on <code>BlockPersisterPersistAge</code>. Blocks are only processed when <code>(currentTip - lastPersistedHeight) &gt; BlockPersisterPersistAge</code>. This means the service intentionally stays at least <code>BlockPersisterPersistAge</code> blocks behind the tip to avoid reorgs and ensure block finality.</li> </ul> </li> <li> <p>Processing Flow</p> <ul> <li>Retrieves the next block to process from the Blockchain service</li> <li>Converts block to bytes</li> <li>Persists block data to storage</li> <li> <p>Creates and stores the associated files:</p> </li> <li> <p>Block file (.block)</p> </li> <li>A Subtree file for each subtree in the block (.subtree), including the number of transactions in the subtree, and the decorated transactions (with UTXO meta data).</li> <li>UTXO additions (.utxo-additions), containing the UTXOs created in the block. This represents a list of new transaction outputs, including the Coinbase transaction outputs.</li> <li>UTXO deletions (.utxo-deletions), containing the UTXOs spent in the block. This represents a list of transaction inputs that reference and spend previous outputs.</li> <li>Updates the local state with the new block height</li> </ul> </li> <li> <p>Sleep Mechanisms</p> <ul> <li>On error: the service sleeps for a 1-minute period before retrying</li> <li>If no new blocks: The service sleeps for a configurable period (<code>BlockPersisterPersistSleep</code>) before polling again</li> </ul> </li> <li> <p>Service Dependencies</p> <ul> <li>FSM State Dependency: The service waits for the blockchain FSM (Finite State Machine) to transition from the IDLE state to RUNNING before beginning block processing operations</li> <li>No gRPC API: Unlike other Teranode services, the Block Persister does not expose a gRPC API and operates purely as a background processing service</li> </ul> </li> </ol>"},{"location":"topics/services/blockPersister/#23-subtree-processing-details","title":"2.3 Subtree Processing Details","text":"<p>The detailed subtree processing workflow includes:</p> <ol> <li> <p>Subtree Retrieval</p> <ul> <li>Retrieves subtree data from the blockchain service</li> <li>Deserializes transaction data within the subtree</li> </ul> </li> <li> <p>Transaction Metadata Loading</p> <ul> <li>Loads transaction metadata in batches for efficiency</li> <li>Decorates transactions with UTXO metadata</li> <li>Processes transactions individually if batch processing fails</li> </ul> </li> <li> <p>File Creation</p> <ul> <li>Stores decorated subtree files (.subtree extension)</li> <li>Calculates and stores UTXO changes (additions and deletions)</li> <li>Updates the UTXO difference tracking</li> </ul> </li> <li> <p>Concurrency</p> <ul> <li>Processes multiple subtrees concurrently using configurable concurrency limits</li> <li>Uses error groups to handle concurrent processing errors</li> </ul> </li> </ol>"},{"location":"topics/services/blockPersister/#3-data-model","title":"3. Data Model","text":"<p>The Block Persister service data model is identical in scope to the Block Validation model. Please refer to the Block Validation documentation for more information.</p> <p>In addition to blocks and subtrees, utxo additions and deletions files are created for each block, containing the newly created and spent UTXOs, respectively.</p>"},{"location":"topics/services/blockPersister/#utxo-files","title":"UTXO Files","text":""},{"location":"topics/services/blockPersister/#utxo-additions-file","title":".utxo-additions file","text":"<p>Content: A series of UTXO records, each containing:</p> <ul> <li>TxID (32 bytes)</li> <li>Output Index (4 bytes)</li> <li>Value (8 bytes)</li> <li>Block Height (4 bytes)</li> <li>Locking Script Length (4 bytes)</li> <li>Locking Script (variable length)</li> <li>Coinbase flag (1 bit, packed with block height)</li> </ul> <p>Format: Binary encoded, as per the UTXO struct:</p> <pre><code>type UTXO struct {\n    TxID     *chainhash.Hash\n    Index    uint32\n    Value    uint64\n    Height   uint32\n    Script   []byte\n    Coinbase bool\n}\n</code></pre>"},{"location":"topics/services/blockPersister/#utxo-deletions-file","title":".utxo-deletions file","text":"<p>Content: A series of UTXO deletion records, each containing:</p> <ul> <li>TxID (32 bytes)</li> <li>Output Index (4 bytes)</li> </ul> <p>Format: Binary encoded, as per the UTXODeletion struct:</p> <pre><code>type UTXODeletion struct {\n    TxID  *chainhash.Hash\n    Index uint32\n}\n</code></pre>"},{"location":"topics/services/blockPersister/#31-storage-architecture","title":"3.1 Storage Architecture","text":"<p>The Block Persister service uses two distinct storage buckets:</p>"},{"location":"topics/services/blockPersister/#block-store","title":"Block Store","text":"<p>The block-store bucket contains all the persistent data needed for blockchain reconstruction:</p> <ul> <li>Block files (<code>.block</code>) - Complete serialized block data including all transactions</li> <li>Detailed subtree files (<code>.subtree</code>) - Complete transaction data for each subtree, containing full transaction contents</li> <li>UTXO files (<code>.utxo-additions</code>, <code>.utxo-deletions</code>, <code>.utxo-set</code>) - UTXO state changes and complete sets</li> </ul>"},{"location":"topics/services/blockPersister/#subtree-store","title":"Subtree Store","text":"<p>The subtree-store bucket contains lightweight subtree information:</p> <ul> <li>Lightweight subtree files (no extension) - Minimal transaction metadata (hash, fee, size)</li> <li>Optional meta files (<code>.meta</code>) - Additional transaction metadata created by block validation</li> </ul> <p>This dual-storage approach serves different purposes:</p> <ol> <li> <p>The subtree-store acts as a shared, lightweight transaction reference used by multiple services (block validation, subtree validation, block assembly, and asset services)</p> </li> <li> <p>The block-store contains comprehensive blockchain data with complete transaction details needed for audit, analysis and blockchain reconstruction</p> </li> </ol> <p>If you need all transaction information in a subtree, you should access the <code>.subtree</code> file in the block-store bucket. If you only need basic transaction references (hashes, fees), you can use the more efficient files in the subtree-store bucket.</p>"},{"location":"topics/services/blockPersister/#4-technology","title":"4. Technology","text":"<ol> <li> <p>Go (Golang): The primary programming language used for developing the service.</p> </li> <li> <p>Bitcoin SV (BSV) Libraries:</p> <ul> <li>Data Models and Utilities: For handling BSV blockchain data structures and operations, including transaction and block processing.</li> </ul> </li> <li> <p>Storage Libraries:</p> <ul> <li>Blob Store: For retrieving the subtree blobs.</li> <li>UTXO Store: To access and store transaction metadata.</li> <li>File Storage: For saving the decorated block files.</li> </ul> </li> <li> <p>Configuration and Logging:</p> <ul> <li>Dynamic Configuration: For managing service settings, including transaction metadata caching configurations and worker settings.</li> <li>Logging: For monitoring service operations, error handling, and debugging.</li> </ul> </li> </ol>"},{"location":"topics/services/blockPersister/#5-directory-structure-and-main-files","title":"5. Directory Structure and Main Files","text":"<p>The Block Persister service is located in the <code>services/blockpersister</code> directory.</p> <pre><code>services/blockpersister/\n\u251c\u2500\u2500 state/          # State management\n\u251c\u2500\u2500 server.go       # Main service implementation\n\u2514\u2500\u2500 metrics.go      # Prometheus metrics\n</code></pre>"},{"location":"topics/services/blockPersister/#6-how-to-run","title":"6. How to run","text":"<p>To run the Block Persister Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -BlockPersister=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Block Persister Service locally.</p>"},{"location":"topics/services/blockPersister/#7-configuration-options-settings-flags","title":"7. Configuration options (settings flags)","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the block Persister Settings Reference.</p>"},{"location":"topics/services/blockPersister/#8-other-resources","title":"8. Other Resources","text":"<p>Block Persister Reference</p>"},{"location":"topics/services/blockValidation/","title":"\ud83d\udd0d Block Validation Service","text":""},{"location":"topics/services/blockValidation/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1. Receiving blocks for validation</li> <li>2.2. Validating blocks</li> <li>2.2.1. Overview</li> <li>2.2.2. Catching up after a parent block is not found</li> <li>2.2.3. Quick Validation for Checkpointed Blocks</li> <li>2.2.4. Validating the Subtrees</li> <li>2.2.5. Block Data Validation</li> <li>2.2.6. Transaction Re-presentation Detection</li> <li>2.3. Marking Txs as mined</li> </ul> </li> <li>gRPC Protobuf Definitions</li> <li>Data Model</li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>How to run</li> <li>Configuration options (settings flags)</li> <li>Other Resources</li> </ol>"},{"location":"topics/services/blockValidation/#1-description","title":"1. Description","text":"<p>The Block Validator is responsible for ensuring the integrity and consistency of each block before it is added to the blockchain. It performs several key functions:</p> <ol> <li> <p>Validation of Block Structure: Verifies that each block adheres to the defined structure and format, and that their subtrees are known and valid.</p> </li> <li> <p>Merkle Root Verification: Confirms that the Merkle root in the block header correctly represents the subtrees in the block, ensuring data integrity.</p> </li> <li> <p>Block Header Verification: Validates the block header, including the proof of work , timestamp, and reference to the previous block, maintaining the blockchain's unbroken chain.</p> </li> </ol> <p></p> <p>The Block Validation Service:</p> <ul> <li>Receives new blocks from the Legacy Service. The Legacy Service has received them from other nodes on the network.</li> <li>Validates the blocks, after fetching them from the remote asset server.</li> <li>Updates stores, and notifies the blockchain service of the new block.</li> </ul> <p>The Legacy Service communicates with the Block Validation over the gRPC protocol.</p> <p></p>"},{"location":"topics/services/blockValidation/#detailed-component-diagram","title":"Detailed Component Diagram","text":"<p>The detailed component diagram below shows the internal architecture of the Block Validation Service:</p> <p></p> <p>Note: For information about how the Block Validation service is initialized during daemon startup and how it interacts with other services, see the Teranode Daemon Reference.</p> <p>Finally, note that the Block Validation service benefits of the use of Lustre Fs (filesystem). Lustre is a type of parallel distributed file system, primarily used for large-scale cluster computing. This filesystem is designed to support high-performance, large-scale data storage and workloads. Specifically for Teranode, these volumes are meant to be temporary holding locations for short-lived file-based data that needs to be shared quickly between various services Teranode microservices make use of the Lustre file system in order to share subtree and tx data, eliminating the need for redundant propagation of subtrees over grpc or message queues. The services sharing Subtree data through this system can be seen here:</p> <p></p>"},{"location":"topics/services/blockValidation/#2-functionality","title":"2. Functionality","text":"<p>The block validator is a service that validates blocks. After validating them, it will update the relevant stores and blockchain accordingly.</p>"},{"location":"topics/services/blockValidation/#21-receiving-blocks-for-validation","title":"2.1. Receiving blocks for validation","text":"<ul> <li>The Legacy Service is responsible for receiving new blocks from the network. When a new block is found, it will notify the block validation service via the <code>BlockFound()</code> gRPC endpoint.</li> <li>The block validation service will then check if the block is already known. If not, it will start the validation process.</li> <li>The block is added to a channel for processing. The channel is used to ensure that the block validation process is asynchronous and non-blocking.</li> </ul>"},{"location":"topics/services/blockValidation/#22-validating-blocks","title":"2.2. Validating blocks","text":""},{"location":"topics/services/blockValidation/#221-overview","title":"2.2.1. Overview","text":"<ul> <li>As seen in the section 2.1, a new block is queued for validation in the blockFoundCh. The block validation server will pick it up and start the validation process.</li> <li>The server will request the block data from the remote node (<code>DoHTTPRequest()</code>).</li> <li>If the parent block is not known, it will be added to the catchupCh channel for processing. We stop at this point, as we can no longer proceed. The catchup process will be explained in the next section (section 2.2.2).</li> <li>If the parent is known, the block will be validated.<ul> <li>First, the service validates all the block subtrees.<ul> <li>For each subtree, we check if it is known. If not, we kick off a subtree validation process (see section 2.2.3 for more details).</li> </ul> </li> <li>The validator retrieves the last 100 block headers, which are used to validate the block data. We can see more about this specific step in the section 2.2.4.</li> <li>The validator stores the coinbase Tx in the UTXO Store and the Tx Store.</li> <li>The validator adds the block to the Blockchain.</li> <li>For each Subtree in the block, the validator updates the TTL (Time To Live) to zero for the subtree. This allows the Store to clear out data the services will no longer use.</li> <li>For each Tx for each Subtree, we set the Tx as mined in the UTXO Store. This allows the UTXO Store to know which block(s) the Tx is in.</li> <li>Should an error occur during the validation process, the block will be invalidated and removed from the blockchain.</li> </ul> </li> </ul> <p>Note - there is a <code>optimisticMining</code> setting that allows to reverse the block validation and block addition to the blockchain steps.</p> <ul> <li>In the regular mode, the block is validated first, and, if valid, added to the block.</li> <li>If <code>optimisticMining</code> is on, the block is optimistically added to the blockchain right away, and then validated in the background next. If it was to be found invalid after validation, it would be removed from the blockchain. This mode is not recommended for production use, as it can lead to a temporary fork in the blockchain. It however can be useful for performance testing purposes.</li> </ul>"},{"location":"topics/services/blockValidation/#222-catching-up-after-a-parent-block-is-not-found","title":"2.2.2. Catching up after a parent block is not found","text":"<p>When a block parent is not found in the local blockchain, the node will start a catchup process. The catchup process will iterate through the parent blocks until it finds a known block in the blockchain.</p> <p>When a block is not known, it will be requested from the remote node. Once received, it will be queued for validation (effectively starting the process of validation for the parent block from the beginning, as seen in 2.3.1).</p> <p>Notice that, when catching up, the Block Validator will set the machine state of the node to <code>CATCHING_UP</code>. This is done to prevent the node from processing new blocks while it is still catching up. The node will only assemble or process new blocks once it has caught up with the blockchain. For more information on this, please refer to the State Management  documentation.</p> <p>During the catchup process, the system tracks invalid blocks. If a block fails validation during catchup, it is marked as invalid in the blockchain store. This prevents invalid blocks from corrupting the chain state and allows the system to avoid reprocessing known invalid blocks. The system also maintains metrics on peer quality to identify and avoid peers that provide invalid blocks.</p>"},{"location":"topics/services/blockValidation/#223-quick-validation-for-checkpointed-blocks","title":"2.2.3. Quick Validation for Checkpointed Blocks","text":"<p>For blocks that are below known checkpoints in the blockchain, the Block Validation service employs an optimized quick validation path that significantly improves synchronization performance. This mechanism is particularly effective during initial blockchain synchronization.</p> <p></p>"},{"location":"topics/services/blockValidation/#quick-validation-process","title":"Quick Validation Process","text":"<p>The quick validation system operates in two distinct phases:</p> <ol> <li> <p>UTXO Creation Phase: All UTXOs for the block's transactions are created in parallel. The system:</p> <ul> <li>Processes transactions concurrently using configurable parallelism (<code>blockvalidation_concurrency_createAllUTXOs</code>)</li> <li>Creates UTXOs with the block's ID and mined status pre-set</li> <li>Handles existing UTXO conflicts gracefully for recovery scenarios</li> </ul> </li> <li> <p>Transaction Spending Phase: Validates transactions by spending their inputs in parallel. The system:</p> <ul> <li>Uses optimized validator options that skip expensive policy checks for mined blocks</li> <li>Processes transactions with configurable concurrency (<code>blockvalidation_concurrency_spendAllTransactions</code>)</li> <li>Validates transaction relationships and dependencies efficiently</li> </ul> </li> </ol>"},{"location":"topics/services/blockValidation/#checkpoint-based-optimization","title":"Checkpoint-Based Optimization","text":"<p>The quick validation path is only applied to blocks below verified checkpoints:</p> <ul> <li>Checkpoints are known valid block heights that serve as trust anchors</li> <li>Blocks below checkpoints skip full script validation since they are known to be valid</li> <li>The checkpoint height is configurable via <code>blockvalidation_quickValidationCheckpointHeight</code></li> </ul>"},{"location":"topics/services/blockValidation/#performance-benefits","title":"Performance Benefits","text":"<p>Quick validation provides substantial performance improvements:</p> <ul> <li>Approximately 10x faster processing for historical blocks below checkpoints</li> <li>Parallel UTXO operations maximize throughput</li> <li>Elimination of redundant policy checks for known valid blocks</li> <li>Reduced memory footprint through on-demand transaction extension</li> </ul>"},{"location":"topics/services/blockValidation/#subtree-and-transaction-processing","title":"Subtree and Transaction Processing","text":"<p>During quick validation, the system also:</p> <ul> <li>Creates subtree metadata files (<code>.subtreeMeta</code>) automatically</li> <li>Reconstructs full subtree data (<code>.subtree</code>) with proper fee and size information</li> <li>Stores transactions in non-extended format to reduce storage overhead</li> <li>Extends transaction data on-demand only when needed for validation<ul> <li>Transactions received in standard Bitcoin format are automatically extended in-memory</li> <li>Parent transaction data is retrieved from the UTXO store as needed</li> <li>Extension is transparent and does not affect storage format</li> </ul> </li> </ul> <p>If quick validation encounters any errors, the system automatically falls back to normal validation to ensure correctness.</p>"},{"location":"topics/services/blockValidation/#224-validating-the-subtrees","title":"2.2.4. Validating the Subtrees","text":"<p>Should the validation process for a block encounter a subtree it does not know about, it can request its processing off the Subtree Validation service.</p> <p></p> <p>If any transaction under the subtree is also missing, the subtree validation process will kick off a recovery process for those transactions.</p>"},{"location":"topics/services/blockValidation/#225-block-data-validation","title":"2.2.5. Block Data Validation","text":"<p>As part of the overall block validation, the service will validate the block data, ensuring the format and integrity of the data, as well as confirming that coinbase tx, subtrees and transactions are valid. This is done in the <code>Valid()</code> method under the <code>Block</code> struct.</p> <p></p> <p>Effectively, the following validations are performed:</p> <ul> <li> <p>The hash of the previous block must be known and valid. Teranode must always build a block on a previous block that it recognizes as the longest chain.</p> </li> <li> <p>The Proof of Work of a block must satisfy the difficulty target (Proof of Work higher than nBits in block header).</p> </li> <li> <p>The Merkle root of all transactions in a block must match the value of the Merkle root in the block header.</p> </li> <li> <p>A block must include at least one transaction, which is the Coinbase transaction.</p> </li> <li> <p>A block timestamp must not be too far in the past or the future.</p> <ul> <li>The block time specified in the header must be larger than the Median-Time-Past (MTP) calculated from the previous block index. MTP is calculated by taking the timestamps of the last 11 blocks and finding the median (More details in BIP113).</li> <li>The block time specified in the header must not be larger than the adjusted current time plus two hours (\"maximum future block time\").</li> </ul> </li> <li> <p>The first transaction in a block must be Coinbase. The transaction is Coinbase if the following requirements are satisfied:</p> <ul> <li>The Coinbase transaction has exactly one input.</li> <li>The input is null, meaning that the input's previous hash is 0000\u20260000 and the input's previous index is 0xFFFFFFFF.</li> <li>The Coinbase transaction must start with the serialized block height, to ensure block and transaction uniqueness.</li> </ul> </li> <li> <p>The Coinbase transaction amount may not exceed block subsidy and all transaction fees (block reward).</p> </li> </ul>"},{"location":"topics/services/blockValidation/#226-transaction-re-presentation-detection","title":"2.2.6. Transaction Re-presentation Detection","text":"<p>The Block Validation service implements a robust mechanism for detecting re-presented transactions using bloom filters. This mechanism, implemented in the <code>validOrderAndBlessed</code> function, is critical for preventing double-spending and ensuring transaction integrity in the blockchain.</p> <p></p>"},{"location":"topics/services/blockValidation/#bloom-filter-implementation","title":"Bloom Filter Implementation","text":"<p>Teranode maintains bloom filters for recent blocks to efficiently detect re-presented transactions:</p> <ul> <li>Creation: Each validated block generates a bloom filter containing all of its transaction hashes</li> <li>Storage: Bloom filters are stored in both memory (for active validation) and in the subtree store (for persistence)</li> <li>Retention: Filters are maintained for a configurable number of recent blocks (<code>blockvalidation_bloom_filter_retention_size</code>)</li> <li>TTL Ordering: The system enforces a strict TTL (Time-To-Live) ordering: txmetacache &lt; utxo store &lt; bloom filter<ul> <li>This ensures that even if a transaction is pruned from txmetacache, the bloom filter can still detect its re-presentation</li> <li>The longer retention period for bloom filters provides an extended window for detecting re-presented transactions</li> </ul> </li> </ul>"},{"location":"topics/services/blockValidation/#the-validorderandblessed-mechanism","title":"The validOrderAndBlessed Mechanism","text":"<p>The <code>validOrderAndBlessed</code> function performs several critical validations during block processing:</p> <ol> <li> <p>Transaction Ordering Validation:</p> <ul> <li>Ensures child transactions appear after their parent transactions within the same block</li> <li>For each transaction, verifies that all of its parent transactions either appear earlier in the same block or exist in a previous block on the current chain</li> </ul> </li> <li> <p>Re-presented Transaction Detection:</p> <ul> <li>Efficiently checks if transactions have already been mined in the current chain using bloom filters</li> <li>For potential matches in the bloom filter (which may include false positives), performs definitive verification against the txMetaStore</li> <li>Rejects blocks containing transactions that have already been mined in the current chain</li> </ul> </li> <li> <p>Duplicate Input Prevention:</p> <ul> <li>Tracks all inputs being spent within the block to detect duplicate spends</li> <li>If a transaction is found to be already mined in another block on the same chain, the new block is marked as invalid</li> <li>Ensures no two transactions in the block spend the same input</li> </ul> </li> <li> <p>Orphaned Transaction Prevention:</p> <ul> <li>Verifies that parent transactions of each transaction either exist in the current block (before the child) or in a previous block on the current chain</li> <li>Prevents situations where transactions depend on parents that don't exist or aren't accessible</li> </ul> </li> </ol> <p>This comprehensive validation mechanism operates with high concurrency (configurable via <code>block_validOrderAndBlessedConcurrency</code>) to maintain performance while ensuring the integrity of the blockchain by preventing double-spends and transaction re-presentations.</p>"},{"location":"topics/services/blockValidation/#23-marking-txs-as-mined","title":"2.3. Marking Txs as mined","text":"<p>When a block is validated, the transactions in the block are marked as mined in the UTXO store. This process includes:</p> <ol> <li> <p>Updating Transaction Status: The Block Validation service marks each transaction as mined by setting its block information.</p> </li> <li> <p>Unsetting the Locked Flag: For any transaction that still has the \"locked\" flag set, the flag is unset during the mined transaction update process.</p> </li> <li> <p>Storing Subtree Information: The service also stores the subtree index in the block where the transaction was located, enabling more efficient transaction lookups.</p> </li> </ol> <p>The Block Validation service is exclusively responsible for marking block transactions as mined and ensuring their flags are properly updated, regardless of whether the transaction was mined by the local Block Assembly service or by another node in the network.</p> <p>As a first step, either the <code>Block Validation</code> (after a remotely mined block is validated) or the <code>Block Assembly</code> (if a block is locally mined) marks the block subtrees as \"set\", by invoking the <code>Blockchain</code> <code>SetBlockSubtreesSet</code> gRPC call, as shown in the diagram below.</p> <p></p> <p>The <code>Blockchain</code> client then notifies subscribers (in this case, the <code>BlockValidation</code> service) of a new <code>NotificationType_BlockSubtreesSet</code> event. The <code>BlockValidation</code> proceeds to mark all transactions within the block as \"mined\" in the <code>UTXOStore</code>. This allows to identify in which block a given tx was mined. See diagram below:</p> <p></p> <p>For a comprehensive explanation of the two-phase commit process across the entire system, including how Block Validation plays a role in the second phase, see the Two-Phase Transaction Commit Process documentation. </p>"},{"location":"topics/services/blockValidation/#3-grpc-protobuf-definitions","title":"3. gRPC Protobuf Definitions","text":"<p>The Block Validation Service uses gRPC for communication between nodes. The protobuf definitions used for defining the service methods and message formats can be seen in the Block Validation protobuf documentation.</p>"},{"location":"topics/services/blockValidation/#4-data-model","title":"4. Data Model","text":"<ul> <li>Block Data Model: Contain lists of subtree identifiers.</li> <li>Subtree Data Model: Contain lists of transaction IDs and their Merkle root.</li> <li>Transaction Data Model: Comprehensive documentation covering both standard Bitcoin format and Extended Format (BIP-239), including automatic format conversion during block validation.</li> <li>UTXO Data Model: UTXO and UTXO Metadata data models for managing unspent transaction outputs.</li> </ul>"},{"location":"topics/services/blockValidation/#5-technology","title":"5. Technology","text":"<ol> <li> <p>Go Programming Language (Golang).</p> </li> <li> <p>gRPC (Google Remote Procedure Call):</p> <ul> <li>Used for implementing server-client communication. gRPC is a high-performance, open-source framework that supports efficient communication between services.</li> </ul> </li> <li> <p>Blockchain Data Stores:</p> <ul> <li>Integration with various stores such as UTXO (Unspent Transaction Output) store, blob store, and transaction metadata store.</li> </ul> </li> <li> <p>Caching Mechanisms (ttlcache):</p> <ul> <li>Uses <code>ttlcache</code>, a Go library for in-memory caching with time-to-live settings, to avoid redundant processing and improve performance.</li> </ul> </li> <li> <p>Configuration Management (gocore):</p> <ul> <li>Uses <code>gocore</code> for configuration management, allowing dynamic configuration of service parameters.</li> </ul> </li> <li> <p>Networking and Protocol Buffers:</p> <ul> <li>Handles network communications and serializes structured data using Protocol Buffers, a language-neutral, platform-neutral, extensible mechanism for serializing structured data.</li> </ul> </li> <li> <p>Synchronization Primitives (sync):</p> <ul> <li>Utilizes Go's <code>sync</code> package for synchronization primitives like mutexes, aiding in managing concurrent access to shared resources.</li> </ul> </li> </ol>"},{"location":"topics/services/blockValidation/#6-directory-structure-and-main-files","title":"6. Directory Structure and Main Files","text":"<pre><code>./services/blockvalidation\n\u2502\n\u251c\u2500\u2500 BlockValidation.go             - Contains the core logic for block validation.\n\u251c\u2500\u2500 BlockValidation_test.go        - Unit tests for the `BlockValidation.go` functionalities.\n\u251c\u2500\u2500 Client.go                      - Client-side logic or API for interacting with the block validation service.\n\u251c\u2500\u2500 Interface.go                   - Defines an interface for the block validation service, outlining the methods that any implementation of the service should provide.\n\u251c\u2500\u2500 Server.go                      - Contains the server-side implementation for the block validation service, handling incoming requests and providing validation services.\n\u251c\u2500\u2500 Server_test.go                 - Unit tests for the `Server.go` functionalities,\n\u251c\u2500\u2500 blockvalidation_api\n\u2502   \u251c\u2500\u2500 blockvalidation_api.pb.go         - Auto-generated file from protobuf definitions, containing Go bindings for the API.\n\u2502   \u251c\u2500\u2500 blockvalidation_api.proto         - Protocol Buffers definition file for the block validation API.\n\u2502   \u2514\u2500\u2500 blockvalidation_api_grpc.pb.go    - gRPC (Google's RPC framework) specific implementation file for the block validation API.\n\u251c\u2500\u2500 metrics.go                     - Metrics collection and monitoring of the block validation service's performance.\n\u251c\u2500\u2500 ttl_queue.go                   - Implements a time-to-live (TTL) queue, for managing caching within the service.\n\u251c\u2500\u2500 txmetacache.go                 - Transaction metadata cache, used to improve performance and efficiency in transaction data access.\n\u2514\u2500\u2500 txmetacache_test.go            - Unit tests for the `txmetacache.go` functionalities.\n</code></pre>"},{"location":"topics/services/blockValidation/#7-how-to-run","title":"7. How to run","text":"<p>To run the Block Validation Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -BlockValidation=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Block Validation Service locally.</p>"},{"location":"topics/services/blockValidation/#8-configuration-options-settings-flags","title":"8. Configuration options (settings flags)","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the block Validation Settings Reference.</p>"},{"location":"topics/services/blockValidation/#9-other-resources","title":"9. Other Resources","text":"<p>Block Validation Reference</p>"},{"location":"topics/services/blockchain/","title":"\ud83c\udf10 Blockchain Service","text":""},{"location":"topics/services/blockchain/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1. Service initialization</li> <li>2.2. Adding a new block to the blockchain</li> <li>2.3. Sending new block notifications to the Block Persister</li> <li>2.4. Getting a block from the blockchain</li> <li>2.5. Getting the last N blocks from the blockchain</li> <li>2.6. Checking if a Block Exists in the Blockchain</li> <li>2.7. Getting the Best Block Header</li> <li>2.8. Getting the Block Headers</li> <li>2.9. Invalidating a Block</li> <li>2.10. Subscribing to Blockchain Events</li> <li>2.11. Triggering a Subscription Notification</li> </ul> </li> <li>gRPC Protobuf Definitions</li> <li>Data Model</li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>How to run</li> <li>Configuration</li> <li>Additional Technical Details<ul> <li>9.1. Complete gRPC Method Coverage</li> <li>9.2. Finite State Machine Implementation</li> <li>9.3. Kafka Integration Details</li> <li>9.4. Error Handling Strategies</li> </ul> </li> <li>Other Resources</li> </ol>"},{"location":"topics/services/blockchain/#1-description","title":"1. Description","text":"<p>This service implements a local Bitcoin SV (BSV) Blockchain service, maintaining the blockchain as understood by the node.</p> <p>The service exposes various RPC methods such as <code>AddBlock</code>, <code>GetBlock</code>, <code>InvalidateBlock</code> and <code>Subscribe</code>.</p> <p>The main features of the service are:</p> <ol> <li> <p>Subscription Management: The service can handle live subscriptions. Clients can subscribe to blockchain events, and the service will send them notifications. It manages new and dead subscriptions and sends out notifications accordingly.</p> </li> <li> <p>Adding a new Block to the Blockchain: Allows adding a new block to the blockchain. It accepts a block request, processes it, and stores it in the blockchain store. Notifications are sent out about this new block.</p> </li> <li> <p>Block Retrieval: Provides various methods to retrieve block information (<code>GetBlock</code>, <code>GetLastNBlocks</code>, <code>GetBlockExists</code> functions).</p> </li> <li> <p>Block Invalidation: It allows to invalidate blocks (<code>InvalidateBlock</code> function), as part of a rollback process.</p> </li> </ol> <p>Note: For information about how the Blockchain service is initialized during daemon startup and how it interacts with other services, see the Teranode Daemon Reference.</p> <p></p> <p>To fulfill its purpose, the service interfaces with a blockchain store for data persistence and retrieval.</p> <p></p>"},{"location":"topics/services/blockchain/#2-functionality","title":"2. Functionality","text":""},{"location":"topics/services/blockchain/#21-service-initialization","title":"2.1. Service initialization","text":"<p>Explanation of the sequence:</p> <ol> <li> <p>New Method:</p> <ul> <li>The <code>Main</code> requests a new instance of the <code>Blockchain</code> service by calling the <code>New</code> function with a logger.</li> <li>Inside the <code>New</code> method, the <code>Blockchain Service</code> performs initialization tasks including setting up the blockchain store, initializing various channels, and preparing the context and subscriptions.</li> </ul> </li> <li> <p>Start Method:</p> <ul> <li>The <code>Main</code> calls the <code>Start</code> method on the <code>Blockchain</code> service instance.</li> <li>The <code>Blockchain Service</code> starts server operations, including channel listeners and the GRPC server.</li> <li>The service enters a loop handling notifications and subscriptions.</li> </ul> </li> </ol>"},{"location":"topics/services/blockchain/#22-adding-a-new-block-to-the-blockchain","title":"2.2. Adding a new block to the blockchain","text":"<p>There are 2 clients invoking this endpoint:</p> <ol> <li>The <code>Block Assembly</code> service:<ul> <li>The <code>Block Assembly</code> service calls the <code>AddBlock</code> method on the <code>Blockchain Service</code> to add a new mined block to the blockchain.</li> </ul> </li> </ol> <p>The sequence diagram for the Block Assembly to add a new block to the blockchain is as follows:</p> <p> 2. The <code>Block Validation</code> service:     - The <code>Block Validation</code> service calls the <code>AddBlock</code> method on the <code>Blockchain Service</code> to add a new block (received from another node) to the blockchain.</p> <p>The sequence diagram for the Block Validation to add a new block to the blockchain is as follows:</p> <p> Explanation of the sequences:</p> <ol> <li> <p>Client Request:</p> <ul> <li>The <code>Client</code> calls the <code>AddBlock</code> method on the <code>Blockchain Service</code>, passing the block request.</li> </ul> </li> <li> <p>Parse Block Header:</p> <ul> <li>The <code>Blockchain Service</code> parses the block header from the request. If there's an error, it returns the error to the client and the process stops.</li> </ul> </li> <li> <p>Parse Coinbase Transaction:</p> <ul> <li>If the header parsing is successful, the service then parses the coinbase transaction. Again, if there's an error, it returns the error to the client.</li> </ul> </li> <li> <p>Parse Subtree Hashes:</p> <ul> <li>If the coinbase transaction parsing is successful, the service then parses the subtree hashes. If there's an error in parsing, it returns the error to the client.</li> </ul> </li> <li> <p>Store Block:</p> <ul> <li>If all parsing steps are successful, the <code>Blockchain Service</code> stores the block using the <code>Block Store</code>.</li> </ul> </li> <li> <p>Handle Storage Response:</p> <ul> <li>If there's an error in storing the block, the <code>Blockchain Service</code> returns the error to the client.</li> <li>If the block is stored successfully, the <code>Blockchain Service</code> proceeds to send notifications.</li> </ul> </li> <li> <p>Send Notifications:</p> <ul> <li>The service sends notifications for the block.</li> </ul> </li> </ol>"},{"location":"topics/services/blockchain/#23-sending-new-block-notifications-to-the-block-persister","title":"2.3. Sending new block notifications to the Block Persister","text":"<p>The Blockchain service, after adding a new block, emits a Kafka notification which is received by the Block Persister service. The Block Persister service is responsible for post-processing the block and storing it in a file format, in a persistent data store (such as S3).</p> <p></p> <p>The Blockchain service, based on standard practices, will retry sending the message until Kafka receives it. In case of Kafka downtime, the service will keep retrying for as long as the message is not sent.</p>"},{"location":"topics/services/blockchain/#24-getting-a-block-from-the-blockchain","title":"2.4. Getting a block from the blockchain","text":"<p>Explanation of the sequence:</p> <ol> <li> <p>Client Request:</p> <ul> <li>The <code>Client</code> calls the <code>GetBlock</code> method on the <code>Blockchain Service</code>, passing the context and the request.</li> </ul> </li> <li> <p>Parse Block Hash:</p> <ul> <li>The <code>Blockchain Service</code> uses the <code>Model</code> to parse the block hash from the request. If there's an error, it returns the error to the client.</li> </ul> </li> <li> <p>Retrieve Block from Store:</p> <ul> <li>If the hash parsing is successful, the service then retrieves the block from the <code>Store</code> using the parsed hash.</li> </ul> </li> <li> <p>Handle Store Response:</p> <ul> <li>If there's an error in retrieving the block, the service returns the error to the client.</li> <li>If the block is successfully retrieved, the service proceeds to prepare the response.</li> </ul> </li> <li> <p>Prepare Response:</p> <ul> <li>The service loops through each subtree hash in the block, converting them to bytes.</li> <li>The service creates a <code>GetBlockResponse</code> with the block's header, height, coinbase transaction, subtree hashes, transaction count, and size in bytes.</li> </ul> </li> <li> <p>Return Response:</p> <ul> <li>The <code>Blockchain Service</code> returns the prepared <code>GetBlockResponse</code> to the <code>Client Code</code>.</li> </ul> </li> </ol> <p>There are 2 clients invoking this endpoint:</p> <ol> <li> <p>The <code>Asset Server</code> service:</p> <ul> <li>The <code>Asset Server</code> service calls the <code>GetBlock</code> method on the <code>Blockchain Service</code> to retrieve a block from the blockchain.</li> </ul> </li> <li> <p>The <code>Block Assembly</code> service:</p> </li> <li>The <code>Block Assembly</code> service calls the <code>GetBlock</code> method on the <code>Blockchain Service</code> to retrieve a block from the blockchain.</li> </ol>"},{"location":"topics/services/blockchain/#25-getting-the-last-n-blocks-from-the-blockchain","title":"2.5. Getting the last N blocks from the blockchain","text":"<p>Explanation of the sequence:</p> <ol> <li> <p>Client Request:</p> <ul> <li>The <code>Client</code> initiates a call to the <code>GetLastNBlocks</code> method on the <code>Blockchain Service</code>, passing the context and the request.</li> </ul> </li> <li> <p>Retrieve Blocks from Store:</p> <ul> <li>The <code>Blockchain Service</code> then calls the <code>GetLastNBlocks</code> method on the <code>Store</code>, passing the number of blocks, orphan inclusion flag, and the starting height from the request.</li> </ul> </li> <li> <p>Handle Store Response:</p> <ul> <li>If there's an error in retrieving the last N blocks, the service returns the error to the client.</li> <li>If the blocks are successfully retrieved, the service prepares the response.</li> </ul> </li> <li> <p>Prepare and Return Response:</p> <ul> <li>The <code>Blockchain Service</code> creates a <code>GetLastNBlocksResponse</code> containing the retrieved <code>blockInfo</code>.</li> <li>It then returns this response to the <code>Client</code>.</li> </ul> </li> </ol> <p>The <code>Asset Server</code> service is the only client invoking this endpoint. It calls the <code>GetLastNBlocks</code> method on the <code>Blockchain Service</code> to retrieve the last N blocks from the blockchain.</p>"},{"location":"topics/services/blockchain/#26-checking-if-a-block-exists-in-the-blockchain","title":"2.6. Checking if a Block Exists in the Blockchain","text":"<p>Explanation of the sequence:</p> <ol> <li> <p>Client Request:</p> <ul> <li>The <code>Client</code> initiates a call to the <code>GetBlockExists</code> method on the <code>Blockchain Service</code>, providing the context and request (which includes the block hash).</li> </ul> </li> <li> <p>Retrieve Block Existence from Store:</p> <ul> <li>The <code>Blockchain Service</code> processes the request by first converting the provided hash in the request to a <code>chainhash.Hash</code> object.</li> <li>It then queries the <code>Store</code> to check if the block exists, using the adjusted context (<code>ctx1</code>) and the block hash.</li> </ul> </li> <li> <p>Handle Store Response:</p> <ul> <li>If there's an error in checking the existence of the block, the service returns the error to the client.</li> <li>If the existence check is successful, the service prepares the response.</li> </ul> </li> <li> <p>Prepare and Return Response:</p> <ul> <li>The <code>Blockchain Service</code> creates a <code>GetBlockExistsResponse</code>, indicating whether the block exists or not.</li> <li>It then returns this response to the <code>Client</code>.</li> </ul> </li> </ol> <p>The <code>Block Validation</code> service is the only client invoking this endpoint. It calls the <code>GetBlockExists</code> method on the <code>Blockchain Service</code> to check if a block exists in the blockchain.</p>"},{"location":"topics/services/blockchain/#27-getting-the-best-block-header","title":"2.7. Getting the Best Block Header","text":"<p>Explanation of the sequence:</p> <ol> <li> <p>Client Request:</p> <ul> <li>The <code>Client</code> initiates a call to the <code>GetBestBlockHeader</code> method on the <code>Blockchain Service</code>, providing the context and an empty message.</li> </ul> </li> <li> <p>Retrieve Best Block Header from Store:</p> <ul> <li>The <code>Blockchain Service</code> processes the request by querying the <code>Store</code> for the best block header, using the adjusted context (<code>ctx1</code>).</li> </ul> </li> <li> <p>Handle Store Response:</p> <ul> <li>If there's an error in retrieving the best block header, the service returns the error to the client.</li> <li>If the retrieval is successful, the service prepares the response.</li> </ul> </li> <li> <p>Prepare and Return Response:</p> <ul> <li>The <code>Blockchain Service</code> creates a <code>GetBlockHeaderResponse</code> with the block header, height, transaction count, size in bytes, and miner information.</li> <li>It then returns this response to the <code>Client</code>.</li> </ul> </li> </ol> <p>Multiple services make use of this endpoint, including the <code>Block Assembly</code>, <code>P2P Server</code>, and <code>Asset Server</code> services, as well as the <code>UTXO Store</code>.</p>"},{"location":"topics/services/blockchain/#28-getting-the-block-headers","title":"2.8. Getting the Block Headers","text":"<p>The methods <code>GetBlockHeader</code>, <code>GetBlockHeaders</code>, and <code>GetBlockHeaderIDs</code> in the <code>Blockchain</code> service provide different ways to retrieve information about blocks in the blockchain.</p> <p></p> <ol> <li> <p>GetBlockHeader:</p> <ul> <li>Purpose: Retrieves a single block header.</li> <li>Process:<ul> <li>It takes a <code>GetBlockHeaderRequest</code> containing the hash of the desired block.</li> <li>Converts the hash into a <code>chainhash.Hash</code> object.</li> <li>Calls <code>GetBlockHeader</code> on the store, providing the context and the hash, to fetch the block header and associated metadata.</li> <li>If successful, it creates and returns a <code>GetBlockHeaderResponse</code> containing the block header's byte representation and metadata like height, transaction count, size, and miner.</li> </ul> </li> </ul> </li> <li> <p>GetBlockHeaders:</p> <ul> <li>Purpose: Fetches multiple block headers starting from a given hash.</li> <li>Process:<ul> <li>Accepts a <code>GetBlockHeadersRequest</code> with the starting block hash and the number of headers to retrieve.</li> <li>Converts the starting hash into a <code>chainhash.Hash</code> object.</li> <li>Calls <code>GetBlockHeaders</code> on the store to obtain a list of block headers and their heights.</li> <li>Assembles the headers into a byte array and returns them in a <code>GetBlockHeadersResponse</code>.</li> </ul> </li> </ul> </li> <li> <p>GetBlockHeaderIDs:</p> <ul> <li>Purpose: Retrieves the IDs (hashes) of a range of block headers.</li> <li>Process:<ul> <li>Receives a <code>GetBlockHeadersRequest</code> similar to <code>GetBlockHeaders</code>.</li> <li>Converts the start hash to a <code>chainhash.Hash</code> object.</li> <li>Uses the store's <code>GetBlockHeaderIDs</code> method to fetch the IDs of the requested block headers.</li> <li>Returns the IDs in a <code>GetBlockHeaderIDsResponse</code>.</li> </ul> </li> </ul> </li> </ol> <p>Each of these methods serves a specific need:</p> <ul> <li><code>GetBlockHeader</code> is for fetching detailed information about a single block.</li> <li><code>GetBlockHeaders</code> is useful for getting information about a sequence of blocks.</li> <li><code>GetBlockHeaderIDs</code> provides a lighter way to retrieve just the IDs of a range of block headers without the additional metadata.</li> </ul> <p>Multiple services make use of these endpoints, including the <code>Block Assembly</code>, <code>Block Validation</code>, and <code>Asset Server</code> services.</p>"},{"location":"topics/services/blockchain/#29-invalidating-a-block","title":"2.9. Invalidating a Block","text":"<ol> <li>The <code>Block Assembly</code> sends an <code>InvalidateBlock</code> request to the <code>Blockchain Service</code>.</li> <li>The <code>Blockchain Service</code> processes the request and calls the <code>InvalidateBlock</code> method on the <code>Store</code>, passing the block hash.</li> <li>The <code>Store</code> performs the invalidation operation and returns the result (success or error) back to the <code>Blockchain Service</code>.</li> <li>Finally, the <code>Blockchain Service</code> returns a response to the <code>Client</code>, which is either an empty response (indicating success) or an error message.</li> </ol>"},{"location":"topics/services/blockchain/#210-subscribing-to-blockchain-events","title":"2.10. Subscribing to Blockchain Events","text":"<p>The Blockchain service provides a subscription mechanism for clients to receive notifications about blockchain events.</p> <p></p> <p>In this diagram, the sequence of operations is as follows:</p> <ol> <li>The <code>Client</code> sends a <code>Subscribe</code> request to the <code>Blockchain Client</code>.</li> <li>The <code>Blockchain Server</code> receives the subscription request and adds it to the <code>Subscription Store</code> (a map of subscriber channels).</li> <li>The server then enters a loop where it waits for either the client's context to be done (indicating disconnection) or the subscription to end.</li> <li>If the client's context is done, the server logs the disconnection and breaks out of the loop. If the subscription ends, the loop is also exited.</li> <li>The server then sends back a response to the client, indicating that the subscription has been established or ended.</li> <li>On the client side, after establishing the subscription, it manages the subscription by continuously receiving stream notifications from the server and processing them as they arrive.</li> </ol> <p>Multiple services make use of the subscription service, including the <code>Block Assembly</code>, <code>Block Validation</code>, <code>P2P</code>, and <code>Asset Server</code> services, and <code>UTXO</code> store. To know more, check the documentation of those services.</p>"},{"location":"topics/services/blockchain/#211-triggering-a-subscription-notification","title":"2.11. Triggering a Subscription Notification","text":"<p>There are two distinct paths for sending notifications, notifications originating from the <code>Blockchain Server</code> and notifications originating from a <code>Blockchain Client</code> gRPC client.</p> <p></p> <ol> <li> <p>Path 1: Notification Originating from Blockchain Server</p> <ul> <li>The <code>Blockchain Server</code> processes an <code>AddBlock</code> or a <code>SetBlockSubtreesSet</code> call. The <code>AddBlock</code> sequence can be seen in the diagram above.</li> <li>Inside this method, it creates a notification of type <code>MiningOn</code> or <code>Block</code>.</li> <li>The server then calls its own <code>SendNotification</code> method to disseminate this notification.</li> <li>The <code>Subscription Store</code> is queried to send the notification to all relevant subscribers.</li> </ul> </li> <li> <p>Path 2: Notification from Block Assembly Through Blockchain Client</p> <ul> <li>The <code>Block Assembly</code> component requests the <code>Blockchain Client</code> to send a notification, of type <code>model.NotificationType_Subtree</code>.</li> <li>The <code>Blockchain Client</code> then communicates with the <code>Blockchain Server</code> to invoke the <code>SendNotification</code> method.</li> <li>Similar to Path 1, the server uses the <code>Subscription Store</code> to distribute the notification to all subscribers.</li> </ul> </li> </ol> <p>In both scenarios, the mechanism for reaching the subscribers through the <code>Subscription Store</code> remains consistent.</p> <p>For further detail, we show here the sequence for the <code>SetBlockSubtreesSet</code> call, not detailed in the diagram above.</p> <p></p>"},{"location":"topics/services/blockchain/#3-grpc-protobuf-definitions","title":"3. gRPC Protobuf Definitions","text":"<p>The Blockchain Service uses gRPC for communication between nodes. The protobuf definitions used for defining the service methods and message formats can be seen here.</p>"},{"location":"topics/services/blockchain/#4-data-model","title":"4. Data Model","text":"<p>The Blockchain works with the Block Data Model.</p> <p>The blockchain database stores the block header, coinbase TX, and block merkle root. The following is the structure of the <code>blocks</code> data:</p> Field Type Constraints Description id BIGSERIAL PRIMARY KEY Unique identifier for each block. parent_id BIGSERIAL REFERENCES blocks(id) Identifier of the parent block. version INTEGER NOT NULL Version of the block. hash BYTEA NOT NULL Hash of the block. previous_hash BYTEA NOT NULL Hash of the previous block. merkle_root BYTEA NOT NULL Merkle root of the block. block_time BIGINT NOT NULL Timestamp of when the block was created. n_bits BYTEA NOT NULL Compact form of the block's target difficulty. nonce BIGINT NOT NULL Nonce used during the mining process. height BIGINT NOT NULL Height of the block in the blockchain. chain_work BYTEA NOT NULL Cumulative proof of work of the blockchain up to this block. tx_count BIGINT NOT NULL Number of transactions in the block. size_in_bytes BIGINT NOT NULL Size of the block in bytes. subtree_count BIGINT NOT NULL Number of subtrees in the block. subtrees BYTEA NOT NULL Serialized data of the subtrees. coinbase_tx BYTEA NOT NULL Serialized data of the coinbase transaction. invalid BOOLEAN NOT NULL DEFAULT FALSE Flag to mark the block as valid or invalid. peer_id VARCHAR(64) NOT NULL Identifier of the peer that provided the block. inserted_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP Timestamp of when the block was inserted in the database. <p>The table structure is designed to store comprehensive information about each block in the blockchain, including its relationships with other blocks, its contents, and metadata.</p>"},{"location":"topics/services/blockchain/#5-technology","title":"5. Technology","text":"<ol> <li> <p>PostgreSQL Database:</p> <ul> <li>The primary store technology for the blockchain service.</li> <li>Used for persisting blockchain data such as blocks, block headers, and state information.</li> <li>SQL scripts and functions (<code>/stores/blockchain/sql</code>) facilitate querying and manipulating blockchain data within the PostgreSQL database.</li> </ul> </li> <li> <p>Go Programming Language:</p> <ul> <li>The service is implemented in Go (Golang).</li> </ul> </li> <li> <p>gRPC and Protocol Buffers:</p> <ul> <li>The service uses gRPC for inter-service communication.</li> <li>Protocol Buffers (<code>.proto</code> files) are used for defining the service API and data structures, ensuring efficient and strongly-typed data exchange.</li> </ul> </li> <li> <p>gocore Library:</p> <ul> <li>Utilized for managing application configurations and statistics gathering.</li> </ul> </li> <li> <p>Model Layer (in <code>/model</code>):</p> <ul> <li>Represents the data structures and business logic related to blockchain operations.</li> <li>Contains definitions for blocks and other blockchain components.</li> </ul> </li> <li> <p>Prometheus for Metrics:</p> <ul> <li>Client in <code>metrics.go</code>.</li> <li>Used for monitoring the performance and health of the service.</li> </ul> </li> </ol>"},{"location":"topics/services/blockchain/#6-directory-structure-and-main-files","title":"6. Directory Structure and Main Files","text":"<p>The Blockchain service is located in the <code>./services/blockchain</code> directory. The following is the directory structure of the service:</p> <pre><code>services/blockchain\n\u251c\u2500\u2500 Client.go - Implements the client-side logic for interacting with the Blockchain service.\n\u251c\u2500\u2500 Difficulty.go - Manages difficulty adjustment logic for the blockchain.\n\u251c\u2500\u2500 Interface.go - Defines the interface for the Blockchain store, outlining required methods for implementation.\n\u251c\u2500\u2500 LocalClient.go - Provides a local client implementation for the Blockchain service, for internal or in-process use.\n\u251c\u2500\u2500 Server.go - Contains server-side logic for the Blockchain service, handling requests and processing blockchain operations.\n\u251c\u2500\u2500 blockchain_api\n\u2502   \u251c\u2500\u2500 blockchain_api.pb.go - Auto-generated Go bindings from the `.proto` file, used for implementing the Blockchain service API.\n\u2502   \u251c\u2500\u2500 blockchain_api.proto - The Protocol Buffers definition file for the Blockchain service API.\n\u2502   \u251c\u2500\u2500 blockchain_api_extra.go - Supplemental code extending or enhancing the auto-generated API code.\n\u2502   \u251c\u2500\u2500 blockchain_api_grpc.pb.go - Auto-generated gRPC bindings from the `.proto` file, specifically for gRPC communication.\n\u2502   \u2514\u2500\u2500 fsm_extra.go - Additional logic related to the Finite State Machine (FSM) functionality.\n\u251c\u2500\u2500 data\n|\n\u251c\u2500\u2500 fsm.go\n\u2502   - Implements the Finite State Machine logic for managing blockchain states.\n\u2502\n\u251c\u2500\u2500 fsm_visualizer\n\u2502   \u2514\u2500\u2500 main.go  - A tool for visualizing the Finite State Machine structure.\n\u2502\n\u251c\u2500\u2500 metrics.go\n\u2502   - Manages and implements functionality related to operational metrics of the Blockchain service.\n\u2502\n\u2514\u2500\u2500 work\n    \u2514\u2500\u2500 work.go\n        - Used to compute the cumulative chain work.\n</code></pre> <p>Further to this, the store part of the service is kept under <code>stores/blockchain</code>. The following is the directory structure of the store:</p> <pre><code>stores/blockchain\n\u251c\u2500\u2500 Interface.go\n    - Defines the interface for blockchain storage, outlining the methods for blockchain data manipulation and retrieval.\n\u251c\u2500\u2500 README.md\n    - Contains documentation and information about the blockchain store.\n\u251c\u2500\u2500 mock.go\n    - Likely contains mock implementations for testing purposes.\n\u251c\u2500\u2500 new.go\n    - Contains the constructor or factory methods for creating new instances of the blockchain store.\n\u251c\u2500\u2500 options\n\u2502   \u2514\u2500\u2500 Options.go\n    - Defines options or configurations for the blockchain store.\n\u2514\u2500\u2500 sql\n    \u251c\u2500\u2500 CheckBlockIsInCurrentChain.go\n    \u251c\u2500\u2500 CheckBlockIsInCurrentChain_test.go\n    \u251c\u2500\u2500 ExportBlocksDB.go\n    \u251c\u2500\u2500 GetBestBlockHeader.go\n    \u251c\u2500\u2500 GetBestBlockHeader_test.go\n    \u251c\u2500\u2500 GetBlock.go\n    \u251c\u2500\u2500 GetBlockByHeight.go\n    \u251c\u2500\u2500 GetBlockByHeight_test.go\n    \u251c\u2500\u2500 GetBlockExists.go\n    \u251c\u2500\u2500 GetBlockGraphData.go\n    \u251c\u2500\u2500 GetBlockHeader.go\n    \u251c\u2500\u2500 GetBlockHeaderIDs.go\n    \u251c\u2500\u2500 GetBlockHeaderIDs_test.go\n    \u251c\u2500\u2500 GetBlockHeaders.go\n    \u251c\u2500\u2500 GetBlockHeadersByHeight.go\n    \u251c\u2500\u2500 GetBlockHeadersFromHeight.go\n    \u251c\u2500\u2500 GetBlockHeaders_test.go\n    \u251c\u2500\u2500 GetBlockHeight.go\n    \u251c\u2500\u2500 GetBlockHeight_test.go\n    \u251c\u2500\u2500 GetBlockStats.go\n    \u251c\u2500\u2500 GetBlock_test.go\n    \u251c\u2500\u2500 GetBlocks.go\n    \u251c\u2500\u2500 GetBlocksByTime.go\n    \u251c\u2500\u2500 GetBlocksMinedNotSet.go\n    \u251c\u2500\u2500 GetBlocksSubtreesNotSet.go\n    \u251c\u2500\u2500 GetForkedBlockHeaders.go\n    \u251c\u2500\u2500 GetHashOfAncestorBlock.go\n    \u251c\u2500\u2500 GetHashOfAncestorBlock_test.go\n    \u251c\u2500\u2500 GetHeader.go\n    \u251c\u2500\u2500 GetHeader_test.go\n    \u251c\u2500\u2500 GetLastNBlocks.go\n    \u251c\u2500\u2500 GetSuitableBlock.go\n    \u251c\u2500\u2500 GetSuitableBlock_test.go\n    \u251c\u2500\u2500 InvalidateBlock.go\n    \u251c\u2500\u2500 InvalidateBlock_test.go\n    \u251c\u2500\u2500 LocateBlockHeaders.go\n    \u251c\u2500\u2500 LocateBlockHeaders_test.go\n    \u251c\u2500\u2500 RevalidateBlock.go\n    \u251c\u2500\u2500 RevalidateBlock_test.go\n    \u251c\u2500\u2500 SetBlockMinedSet.go\n    \u251c\u2500\u2500 SetBlockSubtreesSet.go\n    \u251c\u2500\u2500 State.go\n    \u251c\u2500\u2500 State_test.go\n    \u251c\u2500\u2500 StoreBlock.go\n    \u251c\u2500\u2500 StoreBlock_test.go\n    \u251c\u2500\u2500 sql.go\n    \u2514\u2500\u2500 sql_test.go\n</code></pre>"},{"location":"topics/services/blockchain/#7-how-to-run","title":"7. How to run","text":"<p>To run the Blockchain Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -Blockchain=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Blockchain Service locally.</p>"},{"location":"topics/services/blockchain/#8-configuration","title":"8. Configuration","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the Blockchain Settings Reference.</p>"},{"location":"topics/services/blockchain/#9-additional-technical-details","title":"9. Additional Technical Details","text":""},{"location":"topics/services/blockchain/#91-complete-grpc-method-coverage","title":"9.1. Complete gRPC Method Coverage","text":"<p>In addition to the core methods described in the Functionality section, the Blockchain Service provides the following API endpoints:</p>"},{"location":"topics/services/blockchain/#fsm-management-methods","title":"FSM Management Methods","text":"<ul> <li>SendFSMEvent: Sends an event to the blockchain FSM to trigger state transitions.</li> <li>GetFSMCurrentState: Retrieves the current state of the FSM.</li> <li>WaitFSMToTransitionToGivenState: Waits for FSM to reach a specific state.</li> <li>WaitUntilFSMTransitionFromIdleState: Waits for FSM to transition from IDLE state.</li> <li>Run, CatchUpBlocks, LegacySync, Idle: Transitions the service to specific operational modes.</li> </ul>"},{"location":"topics/services/blockchain/#state-management","title":"State Management","text":"<ul> <li>GetState: Retrieves a value from the blockchain state storage by its key.</li> <li>SetState: Stores a value in the blockchain state storage with the specified key.</li> </ul>"},{"location":"topics/services/blockchain/#block-mining-status-methods","title":"Block Mining Status Methods","text":"<ul> <li>GetBlockIsMined: Checks if a block has been marked as mined.</li> <li>SetBlockMinedSet: Marks a block as mined in the blockchain.</li> <li>GetBlocksMinedNotSet: Retrieves blocks that haven't been marked as mined.</li> <li>SetBlockSubtreesSet: Marks a block's subtrees as set.</li> <li>GetBlocksSubtreesNotSet: Retrieves blocks whose subtrees haven't been set.</li> </ul>"},{"location":"topics/services/blockchain/#legacy-synchronization-methods","title":"Legacy Synchronization Methods","text":"<ul> <li>GetBlockLocator: Creates block locators for chain synchronization.</li> <li>LocateBlockHeaders: Finds block headers using a locator.</li> <li>GetBestHeightAndTime: Retrieves the current best height and median time.</li> </ul>"},{"location":"topics/services/blockchain/#92-finite-state-machine-implementation","title":"9.2. Finite State Machine Implementation","text":"<p>The Blockchain Service uses a Finite State Machine (FSM) to manage its operational states. This design allows the service to maintain a clear lifecycle and respond appropriately to different events.</p> <p>For a comprehensive understanding of the Blockchain Service's FSM implementation, please refer to the dedicated State Management in Teranode documentation, which covers:</p> <ul> <li>FSM states (Idle, Running, CatchingBlocks, LegacySyncing)</li> <li>State transitions and events</li> <li>Allowed operations in each state</li> <li>FSM initialization and access methods</li> <li>Waiting on state transitions</li> </ul> <p>The FSM implementation in the Blockchain Service exposes several gRPC methods for state management:</p> <ul> <li>GetFSMCurrentState: Returns the current state of the FSM</li> <li>WaitFSMToTransitionToGivenState: Waits for the FSM to reach a specific state</li> <li>SendFSMEvent: Sends events to trigger state transitions</li> <li>Run, CatchUpBlocks, LegacySync, Idle: Convenience methods that delegate to SendFSMEvent</li> </ul> <p>The FSM ensures that the service only performs operations appropriate for its current state, providing isolation and predictable behavior.</p>"},{"location":"topics/services/blockchain/#93-kafka-integration-details","title":"9.3. Kafka Integration Details","text":"<p>The Blockchain Service integrates with Kafka for block notifications and event streaming:</p>"},{"location":"topics/services/blockchain/#message-formats","title":"Message Formats","text":"<p>Block notifications are serialized using Protocol Buffers and contain:</p> <ul> <li>Block header</li> <li>Block height</li> <li>Hash</li> <li>Transaction count</li> <li>Size in bytes</li> <li>Timestamp</li> </ul>"},{"location":"topics/services/blockchain/#topics","title":"Topics","text":"<ul> <li>Blocks-Final: Used for finalized block notifications, consumed by the Block Persister service.</li> </ul>"},{"location":"topics/services/blockchain/#error-handling","title":"Error Handling","text":"<ul> <li>The service implements exponential backoff retry for Kafka publishing failures.</li> <li>Failed messages are logged and retried based on the <code>blockchain_maxRetries</code> and <code>blockchain_retrySleep</code> settings.</li> <li>Persistent failures, after the retries are exhausted, are reported through the health monitoring endpoints (<code>/health</code> HTTP endpoint and the <code>HealthGRPC</code> gRPC method).</li> </ul>"},{"location":"topics/services/blockchain/#94-error-handling-strategies","title":"9.4. Error Handling Strategies","text":"<p>The Blockchain Service employs several strategies to handle errors and maintain resilience:</p>"},{"location":"topics/services/blockchain/#network-and-communication-errors","title":"Network and Communication Errors","text":"<ul> <li>Uses timeouts and context cancellation to handle hanging network operations.</li> <li>Implements retry mechanisms for transient failures with configured backoff periods.</li> </ul>"},{"location":"topics/services/blockchain/#validation-errors","title":"Validation Errors","text":"<ul> <li>Blocks with invalid headers, merkle roots, or proofs are rejected with appropriate error codes.</li> <li>Invalid blocks can be explicitly marked using the InvalidateBlock method.</li> </ul>"},{"location":"topics/services/blockchain/#chain-reorganization-and-longest-chain-tracking","title":"Chain Reorganization and Longest Chain Tracking","text":"<p>The Blockchain Service implements sophisticated chain reorganization handling with optimized longest chain tracking:</p>"},{"location":"topics/services/blockchain/#automatic-detection","title":"Automatic Detection","text":"<ul> <li>Detects chain splits and reorganizations automatically through block header validation</li> <li>Uses rollback and catch-up operations to handle chain reorganizations</li> <li>Limits reorganization depth for security (configurable via <code>blockchain_maxReorgDepth</code>)</li> </ul>"},{"location":"topics/services/blockchain/#optimized-longest-chain-selection","title":"Optimized Longest Chain Selection","text":"<p>The service employs an optimized algorithm for tracking and selecting the longest valid chain:</p> <p>Key Features:</p> <ul> <li>Efficient Chain Comparison: Uses cumulative proof-of-work (chainwork) rather than simple block height for chain selection</li> <li>Fast Fork Detection: Maintains indexed fork points to quickly identify competing chains</li> <li>Minimal Database Queries: Caches chain tips and their accumulated work to reduce database load</li> <li>Parallel Validation: Can validate multiple competing chain tips simultaneously</li> </ul> <p>Implementation Details:</p> <ol> <li>Chainwork Tracking: Each block stores cumulative chainwork from genesis, allowing O(1) chain strength comparison</li> <li>Fork Point Cache: Maintains an in-memory cache of recent fork points for rapid reorganization detection</li> <li>Tip Management: Tracks multiple competing chain tips with their associated metadata:<ul> <li>Total chainwork</li> <li>Block height</li> <li>Last validation timestamp</li> <li>Fork depth from main chain</li> </ul> </li> </ol> <p>Performance Benefits:</p> <ul> <li>Reduced latency in chain selection during high fork activity</li> <li>Lower database load through intelligent caching</li> <li>Faster recovery from network partitions</li> <li>Improved resilience to chain split scenarios</li> </ul> <p>Configuration Options:</p> <ul> <li><code>blockchain_maxReorgDepth</code>: Maximum allowed reorganization depth (default: 6 blocks)</li> <li><code>blockchain_chainTipCacheSize</code>: Number of competing tips to track (default: 10)</li> <li><code>blockchain_forkPointCacheSize</code>: Size of fork point cache (default: 100)</li> </ul>"},{"location":"topics/services/blockchain/#storage-errors","title":"Storage Errors","text":"<ul> <li>Implements transaction-based operations with the store to maintain consistency.</li> <li>Reports persistent storage errors through health endpoints.</li> </ul>"},{"location":"topics/services/blockchain/#10-other-resources","title":"10. Other Resources","text":"<ul> <li>Blockchain Reference</li> <li>FSM Documentation</li> </ul>"},{"location":"topics/services/legacy/","title":"\ud83d\udd17 Legacy Service","text":""},{"location":"topics/services/legacy/#index","title":"Index","text":"<ol> <li>Introduction<ul> <li>1.1. Summary</li> <li>1.2. Data Transformation for Compatibility</li> <li>1.3. Phased Migration Towards Teranode</li> </ul> </li> <li>Architecture<ul> <li>2.1 Validator Integration</li> </ul> </li> <li>Data Model</li> <li> <p>Functionality</p> <ul> <li>4.1. BSV to Teranode Communication</li> <li>4.1.1. Receiving Inventory Notifications</li> </ul> </li> <li> <p>Technology</p> </li> <li>How to run</li> <li>Configuration options (settings flags)</li> </ol>"},{"location":"topics/services/legacy/#1-introduction","title":"1. Introduction","text":""},{"location":"topics/services/legacy/#11-summary","title":"1.1. Summary","text":"<p>The Legacy Service bridges the gap between the traditional BSV nodes and the advanced Teranode-BSV nodes, ensuring seamless communication and data translation between the two. By facilitating this integration, the Legacy Service enables both historical BSV nodes and modern Teranode nodes to operate concurrently, supporting a smooth and gradual transition to the Teranode infrastructure.</p> <p>The core functionality of the Legacy Service revolves around managing and translating communications between historical BSV nodes and Teranode-BSV nodes. This includes:</p> <ul> <li> <p>Receiving Blocks and Transactions: The service accepts blocks and transactions from legacy BSV nodes, ensuring that these can be efficiently propagated to Teranode nodes within the network.</p> </li> <li> <p>Disseminating Newly Mined Blocks: It also sends newly mined blocks from Teranode nodes back to the legacy nodes, maintaining the continuity and integrity of the blockchain across different node versions.</p> </li> </ul>"},{"location":"topics/services/legacy/#12-data-transformation-for-compatibility","title":"1.2. Data Transformation for Compatibility","text":"<p>The Legacy Service performs a major data transformation processes, designed to ensure full compatibility between the differing data structures of BSV and Teranode.</p> <p>Legacy blocks and their transactions are encapsulated into subtrees, aligning with the Teranode model's approach to data management. In a similar way, the service can transparently convert blocks, subtrees, and transactions from the Teranode format back into the conventional block and transaction format used by BSV. This bidirectional conversion capability allows to maintain operational continuity across the network.</p>"},{"location":"topics/services/legacy/#13-phased-migration-towards-teranode","title":"1.3. Phased Migration Towards Teranode","text":"<p>The Legacy Service allows for BSV and Teranode BSV nodes to operate side by side. This is intended as a temporary solution until all historical BSV nodes are phased out. As transaction volumes on the BSV network continue to grow, the demand for more scalable and feature-rich infrastructure will necessitate a complete migration to Teranode. The Legacy Service is a critical enabler of this transition, ensuring that the migration can occur gradually and without disrupting the network's ongoing operations.</p>"},{"location":"topics/services/legacy/#2-architecture","title":"2. Architecture","text":"<p>The Legacy Service acts as a BSV node, connecting to the BSV mainnet and receiving txs and blocks. It then processes these txs and blocks, converting them into the Teranode format and propagating them to the Teranode network. The service also receives blocks from the Teranode network and converts them back into the BSV format for dissemination to the BSV mainnet.</p> <p></p> <p>As it can be seen in the diagram above, the service maintains its own block database (in historical format).</p> <p>When the service receives a block message from the network, the service will propagate it to the Teranode Block Validator Service for validation and inclusion in the Teranode blockchain.</p> <p></p> <p>Both the Block Validation and the Subtree validation will query the Legacy Service (using the same endpoints the Asset Server offers) for the block and subtree data, respectively. The Legacy Service will respond with the requested data in the same format the Asset Server uses. The following endpoints are offered by the Legacy Service:</p> <pre><code> e.GET(\"/block/:hash\", tb.BlockHandler)\n e.GET(\"/subtree/:hash\", tb.SubtreeHandler)\n e.GET(\"/tx/:hash\", tb.TxHandler)\n e.POST(\"/txs\", tb.TxBatchHandler())\n</code></pre> <p>In addition to the block database, the service maintains an in-memory peer database, tracking peers it receives / sends messages from /to.</p> <p>Also, note how the Blockchain client is used in order to wait for the node State to change to <code>RUNNING</code> state. For more information on this, please refer to the State Management  documentation.</p> <p>The following diagram provides a deeper level of detail into the Legacy Service's internal components and their interactions:</p> <p></p>"},{"location":"topics/services/legacy/#21-validator-integration","title":"2.1 Validator Integration","text":"<p>The Legacy service interacts with the Validator service to validate incoming transactions from the BSV network. This interaction can happen in two different configurations:</p> <ol> <li> <p>Local Validator:</p> <ul> <li>When <code>useLocalValidator=true</code> (recommended for production)</li> <li>The Validator is instantiated directly within the Legacy service</li> <li>Direct method calls are used without network overhead</li> <li>This provides the best performance and lowest latency</li> </ul> </li> <li> <p>Remote Validator Service:</p> <ul> <li>When <code>useLocalValidator=false</code></li> <li>The Legacy service connects to a separate Validator service via gRPC</li> <li>Useful for development, testing, or specialized deployment scenarios</li> <li>Has higher latency due to additional network calls</li> </ul> </li> </ol> <p>This configuration is controlled by the settings passed to <code>GetValidatorClient()</code> in daemon.go.</p>"},{"location":"topics/services/legacy/#3-data-model","title":"3. Data Model","text":"<p>When we announce a teranode block, the format is:</p> <ul> <li>Blockheader</li> <li>Block meta data (size, tx count etc etc)</li> <li>Coinbase TX (the actual payload / bytes of the coinbase TX itself)</li> <li> <p>A slice of subtrees.</p> </li> <li> <p>Each subtree has a list of txid's, and to calculate the merkle root, you replace the coinbase placeholder with the txid of the coinbase transaction and then do the merkle tree calculation as normal.  The result should match the merkle root in the block header.</p> </li> </ul>"},{"location":"topics/services/legacy/#4-functionality","title":"4. Functionality","text":""},{"location":"topics/services/legacy/#41-bsv-to-teranode-communication","title":"4.1. BSV to Teranode Communication","text":"<p>The overall cycle is:</p> <p></p> <p>1) An inventory (inv) notification is received, indicating that a new block or transaction is available from a BSV node. The Legacy Service processes this notification and requests the block or transaction data from the BSV node.</p> <p>2) When a new block is received from the BSV node, it is transformed into a Teranode subtree and block propagation format. The block hash is then sent to the Teranode network for further processing.</p> <p>3) The Teranode network processes the block hash and requests the full block data from the Legacy Service.</p> <ul> <li>Upon receipt of the full block data, Teranode will notice that it contains subtrees it is not aware of, and request them from the Legacy Service.</li> <li>Upon receipt of the full subtree data, the list of transactions will be known, and will be requested from the Legacy Service.</li> <li>With all the information now available, the block is then added to the Teranode blockchain.</li> </ul> <p>In the next sections, we will detail the steps involved.</p>"},{"location":"topics/services/legacy/#411-receiving-inventory-notifications","title":"4.1.1. Receiving Inventory Notifications","text":"<p>In the Bitcoin network, an \"inv\" message is a component of the network's communication protocol, used to indicate inventory. The term \"inv\" stands for inventory, and these messages are utilized by nodes to inform other nodes about the existence of certain items, such as blocks and transactions, without transmitting the full data of those items. This mechanism is crucial for the efficient propagation of information across the network, helping to minimize bandwidth usage and improve the overall scalability of the system.</p> <p>The primary purpose of an inv message is to advertise the availability of specific data objects, like new transactions or blocks, to peer nodes. It allows a node to broadcast to its peers that it has something of interest, which the peers might not yet have. The peers can then decide whether they need this data based on the inventory vectors provided in the inv message and request the full data using a \"getdata\" message if necessary.</p> <p>An inv message consists of a list of inventory vectors, where each vector specifies the type and identifier of a particular item. The structure of an inventory vector is as follows:</p> <ul> <li>Type: This field indicates the type of item being advertised. It could represent a block, a transaction, or other data types defined in the protocol.</li> <li>Identifier: This is a hash that uniquely identifies the item, such as the hash of a block or a transaction.</li> </ul> <p>There are several types of inventory that can be advertised using inv messages, including but not limited to:</p> <ul> <li>Transaction: Indicates the presence of a new transaction.</li> <li>Block: Signifies the availability of a new block.</li> <li>Filtered Block: Used in connection with Bloom filters to indicate a block that matches the filter criteria, allowing lightweight clients to download only a subset of transactions.</li> </ul> <p>Inv messages are critical in the data dissemination process within the Bitcoin network.</p> <ol> <li>Broadcasting New Transactions: When a node receives or creates a new transaction, it broadcasts an inv message containing the transaction's hash to its peers, signaling that this transaction is available.</li> <li>Announcing New Blocks: Similarly, when a node learns about a new block (whether by mining it or receiving it from another node), it uses an inv message to announce this block to its peers.</li> <li>Data Request: Upon receiving an inv message, a node can send a \"getdata\" message to request the full data for any items it does not already have but is interested in obtaining.</li> </ol> <p>In the context of the Legacy Service, the reception of inv messages is a crucial step in the communication process between BSV nodes and Teranode nodes. These messages serve as the initial trigger for data exchange, allowing the service to identify new blocks or transactions and initiate the necessary actions to retrieve and process this data.</p> <p></p> <ol> <li> <p>Connection Establishment: The Legacy Overlay Service establishes a connection with the MainNet, setting the stage for data exchange.</p> </li> <li> <p>Block and Transaction Advertisement:</p> <ul> <li>The MainNet node sends an <code>OnInv</code> message to the Legacy Service, advertising available blocks or transactions.</li> <li>The Legacy Service checks with its database (<code>db</code>) to determine if the advertised item (block or transaction) is already known.</li> <li>If the item is new, the Legacy Service requests the full data using <code>OnGetData</code>.</li> </ul> </li> <li> <p>Receiving Full Data:</p> <ul> <li>For a new block, the MainNet responds with <code>OnBlock</code>, containing the block data. The Legacy Service processes this block and stores its details in the database.</li> <li>For a new transaction, the MainNet responds with <code>OnTx</code>, containing the transaction data. Similarly, the Legacy Service processes and stores this information.</li> </ul> </li> </ol>"},{"location":"topics/services/legacy/#412-processing-new-transactions","title":"4.1.2. Processing New Transactions","text":"<p>When the Legacy Service receives a new transaction from the BSV network, it undergoes several validation and processing steps before being accepted or rejected. The validator client plays a crucial role in this process.</p> <p></p> <p>The sequence of steps involved in processing a new transaction is as follows:</p> <ol> <li> <p>Receiving a Transaction:</p> <ul> <li>The MainNet sends a transaction to the Legacy Service using the <code>OnTx</code> message, which includes the peer identifier and the transaction data.</li> </ul> </li> <li> <p>Basic Transaction Validation:</p> <ul> <li>The Legacy Service adds the transaction to the known inventory for the peer.</li> <li>It then queues the transaction to be handled by the Sync Manager for more detailed processing.</li> </ul> </li> <li> <p>Transaction Validation with Validator Client:</p> <ul> <li>The transaction is converted to a binary format and passed to the Teranode Validator client.</li> <li> <p>The Teranode Validator client performs comprehensive validation on the transaction, including:</p> <ul> <li>Checking transaction syntax and structure</li> <li>Verifying that inputs reference valid UTXOs</li> <li>Validating transaction scripts</li> <li>Ensuring the transaction follows consensus rules</li> </ul> </li> </ul> </li> <li> <p>Handling Validation Results:</p> <ul> <li>If the validation fails due to missing parent transactions, the transaction is stored in an orphan pool to be processed when parent transactions arrive.</li> <li>If the validation fails for other reasons (like invalid scripts or consensus violations), the transaction is rejected, and a reject message is sent to the peer.</li> <li>If the validation succeeds, the transaction is accepted into the mempool and announced to other peers.</li> </ul> </li> <li> <p>Orphan Transaction Processing:</p> <ul> <li>After a transaction is accepted, the Legacy Service checks if any orphan transactions were waiting for this transaction.</li> <li>If found, those orphan transactions are recursively processed in the same manner.</li> </ul> </li> </ol> <p>The use of the validator client ensures that all transactions follow the protocol rules before they are propagated to the Teranode network, maintaining consistency across both networks.</p>"},{"location":"topics/services/legacy/#413-processing-new-blocks","title":"4.1.3. Processing New Blocks","text":"<p>Once a new block is received by the Legacy Service, it undergoes a series of transformations to align with the Teranode model. This process involves encapsulating the block into subtrees and converting the block data into a format that is compatible with the Teranode blockchain structure.</p> <p>The sequence of steps involved in processing a new block is as follows:</p> <p></p> <p>Process flow:</p> <ol> <li> <p>Receiving a Block:</p> <ul> <li>The MainNet sends a block to the Legacy Service using the <code>OnBlock</code> message, which includes the peer identifier, message metadata, and the block data in bytes.</li> </ul> </li> <li> <p>Block Processing:</p> <ul> <li>The Legacy Service constructs a block from the received bytes and initiates the <code>HandleBlock</code> function to process the block.</li> </ul> </li> <li> <p>Subtree Creation and Transaction Processing:</p> <ul> <li>The concept of subtrees is unique to Teranode, so, when processing legacy blocks, we must make sure to convert to the internal Teranode format. A new, empty subtree is created, and a placeholder coinbase transaction is added to this subtree.</li> <li> <p>For each transaction in the block, the Legacy Service:</p> <ul> <li>Converts the transaction into an Extended Transaction format.</li> <li>Adds the Extended Transaction to the Subtree.</li> <li>Caches the Extended Transaction in the \"Tx Cache\" (a short term Tx cache DB).</li> </ul> </li> </ul> <p>This process is detailed at the end of the current section in more detail.</p> </li> <li> <p>Caching and Block Building:</p> <ul> <li>The entire subtree is cached in the \"Subtree Cache\" (a short term subtree cache DB).</li> <li>A Teranode block is constructed using the processed block and the subtree, and this block is cached in the \"Block Cache\" (a short term block cache DB).</li> </ul> </li> <li> <p>Block Validation:</p> <ul> <li>The Legacy Service notifies the Block Validator (part of Teranode) about the new block through Kafka using the <code>kafka_blocksConfig</code> topic, providing the block hash and a base URL for accessing the block and its components.</li> <li>It must be noted that this base URL would typically represent a native Teranode Asset server. However, in this case, the Legacy Service will handle the endpoints, providing responses in the same format as the Teranode Asset server, to allow Teranode to request data in the same way they would do it with a remote asset server.</li> <li>Notice how the local Teranode does not understand that the new blocks are being added by the Legacy Service. As far as the Teranode is concerned, a notification for a new block is received via Kafka as if it came from an internal Teranode service. And when requesting data from the provided baseURL, it receives information as if it was a remote asset server. In all senses, the Legacy Service impersonates a remote asset server for the Teranode.</li> <li>The Block Validator requests the block using the provided URL, and the Legacy Service responds with the block data.</li> </ul> </li> <li> <p>Subtree Validation:</p> <ul> <li>The Block Validator notifies the Subtree Validator about the new subtree via Kafka using the <code>subtrees</code> topic, providing the subtree hash and base URL.</li> <li>The Subtree Validator requests the subtree using the provided URL, and the Legacy Service responds with the subtree data.</li> <li>For each transaction ID in the subtree, the Subtree Validator requests the corresponding transaction data using the base URL, and the Legacy Service provides the transaction data in response.</li> </ul> </li> <li> <p>Finalization:</p> <ul> <li>Upon successful validation, the Block Validator coordinates with other Teranode microservices and stores, and adds the block to the blockchain, completing the process.</li> </ul> </li> </ol> <p>Block to Subtree Conversion Process (detail):</p> <p>The Legacy Service converts standard BSV blocks (which don't have subtrees) into the Teranode format (which uses blocks + subtrees) through the following process:</p> <ol> <li> <p>Subtree Creation:</p> <ul> <li>A new subtree structure is created based on the number of transactions in the block (depending on the number of transactions, one or more subtrees are required to fit the transactions)</li> <li>A coinbase placeholder node is added to the first subtree</li> <li>Each transaction from the block is processed and added to the subtree</li> </ul> </li> <li> <p>Transaction Processing:</p> <ul> <li> <p>For each transaction, the service calculates:</p> </li> <li> <p>Transaction fee</p> </li> <li>Transaction size</li> <li>It then adds the transaction hash to the subtree(s) at the appropriate position</li> </ul> </li> <li> <p>Data Organization:</p> <ul> <li>Subtree Data: The service stores the full extended transaction data (inputs, outputs, scripts)</li> <li>Subtree Metadata: Parent transaction references are stored to maintain the transaction dependency structure</li> </ul> </li> <li> <p>Validation:</p> <ul> <li>In legacy sync mode, a quick validation is performed assuming blocks are valid</li> <li>In normal mode, more thorough validation is performed via the Teranode validation services</li> </ul> </li> <li> <p>Storage:</p> <ul> <li>The subtree, its data, and metadata are stored in the Subtree Store</li> <li>The subtree hash is then ready to be included in the Teranode block structure</li> </ul> </li> </ol>"},{"location":"topics/services/legacy/#42-teranode-to-bsv-communication","title":"4.2. Teranode to BSV Communication","text":"<p>The Legacy Service not only processes data from the BSV network to Teranode but also propagates data from Teranode back to the BSV network. This bidirectional flow is essential for maintaining consistency within the network.</p>"},{"location":"topics/services/legacy/#421-processing-new-subtrees-from-teranode","title":"4.2.1. Processing New Subtrees from Teranode","text":"<p>When new subtrees are created within the Teranode environment, the Legacy Service receives notifications via a blockchain subscription mechanism and propagates the associated transactions to the BSV network:</p> <p></p> <ol> <li> <p>Subscription to Blockchain Events:</p> <ul> <li>The Legacy Service's Sync Manager subscribes to blockchain events from Teranode using the <code>Subscribe</code> method.</li> <li>This subscription allows the Legacy Service to receive notifications about new blocks and subtrees created in the Teranode environment.</li> </ul> </li> <li> <p>Receiving Subtree Notifications:</p> <ul> <li>When a new subtree is created in Teranode, a notification is sent to the subscribed Legacy Service.</li> <li>The notification includes the hash of the new subtree.</li> </ul> </li> <li> <p>Retrieving Subtree Data:</p> <ul> <li>Upon receiving a subtree notification, the Legacy Service retrieves the subtree data from the Subtree Store.</li> <li>The subtree data contains information about all transactions included in the subtree.</li> </ul> <p>Note</p> <p>Subtrees are a unique feature of Teranode. We will discuss this Teranode-to-BSV data abstraction conversion process later in this section.</p> </li> <li> <p>Transaction Announcement:</p> <ul> <li>For each transaction in the subtree, the Legacy Service announces it to connected BSV peers.</li> <li>These announcements are managed by a transaction batcher that de-duplicates transactions that have been recently sent.</li> <li>BSV peers that receive these announcements can then request the full transaction data if needed.</li> </ul> </li> <li> <p>Propagation to BSV Network:</p> <ul> <li>The announced transactions are propagated throughout the BSV network, ensuring that blocks mined in Teranode are reflected in the BSV blockchain.</li> </ul> </li> </ol> <p>This bidirectional communication ensures that both networks remain synchronized, with transactions and blocks being properly reflected in both environments. The Legacy Service acts as a critical bridge, enabling seamless interoperability between the legacy BSV protocol and the more scalable Teranode architecture.</p> <p>Subtree to Transaction Conversion Process:</p> <p>The Legacy Service converts Teranode subtree container abstractions back to \"flat\" BSV transactions through the following process:</p> <ol> <li> <p>Notification Reception:</p> <ul> <li>The Legacy Service subscribes to Kafka-based blockchain events from Teranode</li> <li>When a new subtree is created, a notification with the subtree hash is received</li> </ul> </li> <li> <p>Subtree Retrieval:</p> <ul> <li>The service retrieves the subtree data from the Subtree Store using the hash in the notification</li> <li>The subtree is deserialized from its binary format into a structured subtree object</li> </ul> </li> <li> <p>Transaction Extraction:</p> <ul> <li>The service extracts each transaction hash from the subtree nodes</li> <li>For each transaction, it also retrieves the associated fee and size information</li> </ul> </li> <li> <p>Transaction Announcement:</p> <ul> <li>Each transaction hash is added to a transaction announcement batcher</li> <li>The batcher de-duplicates transactions that have been recently announced to prevent flooding</li> <li>Transactions are then announced to connected BSV peers</li> </ul> </li> <li> <p>Data Fulfillment:</p> <ul> <li>When BSV peers request the full transaction data, the Legacy Service retrieves it from the Subtree Data store</li> <li>Transactions are served to the BSV network in their standard format, with no reference to the subtree structure</li> </ul> </li> </ol> <p>This process effectively bridges the gap between Teranode's subtree-based architecture and the BSV network's traditional transaction model, ensuring that data originating in Teranode can be properly propagated to the BSV network.</p>"},{"location":"topics/services/legacy/#5-technology","title":"5. Technology","text":"<p>The entire codebase is written in Go (Golang), a statically typed, compiled programming language designed for simplicity and efficiency.</p> <p>Technology highlights:</p> <ul> <li>Bitcoin SV (BSV): An implementation of the Bitcoin protocol that follows the vision set out by Satoshi Nakamoto's original Bitcoin whitepaper. BSV focuses on scalability, aiming to provide high transaction throughput and enterprise-level capabilities.</li> <li> <p>Teranode: A high-performance implementation of the Bitcoin protocol designed to handle a massive scale of transactions.</p> </li> <li> <p>Echo Framework: The code uses Echo, a high-performance, extensible web framework for Go, to create an HTTP server. This server exposes endpoints for block, transaction, and subtree data retrieval.</p> </li> <li> <p>ExpiringMap: An expiring map is used for caching transactions, subtrees, and blocks with a time-based eviction policy.</p> </li> <li> <p>Chainhash: A library for handling SHA-256 double hash values, commonly used as identifiers in blockchain (e.g., transaction IDs, block hashes).</p> </li> <li> <p>Database Technologies (ffldb, leveldb, sqlite): Used as block data stores. Each has its use cases:</p> </li> <li> <p>ffldb: Optimized for blockchain data storage and retrieval. Default and recommended choice.</p> </li> <li>leveldb: A fast key-value storage library suitable for indexing blockchain data.</li> <li>sqlite: An embedded SQL database engine for simpler deployment and structured data queries.</li> </ul>"},{"location":"topics/services/legacy/#6-how-to-run","title":"6. How to run","text":"<p>To run the Legacy Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -Legacy=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Legacy Service locally.</p>"},{"location":"topics/services/legacy/#7-configuration-options-settings-flags","title":"7. Configuration options (settings flags)","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the legacy Settings Reference.</p>"},{"location":"topics/services/legacy/#8-other-resources","title":"8. Other Resources","text":"<p>Legacy Reference</p>"},{"location":"topics/services/p2p/","title":"\ud83c\udf10 P2P Service","text":""},{"location":"topics/services/p2p/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1. Creating, initializing and starting a new P2P Server</li> <li>2.1.1. Creating a New P2P Server</li> <li>2.1.2. Initializing the P2P Server</li> <li>2.1.3. Starting the P2P Server</li> <li>2.2. Peer Discovery and Connection</li> <li>2.3. Best Block Messages</li> <li>2.4. Blockchain Messages</li> <li>2.5. TX Validator Messages</li> <li>2.6. Websocket notifications</li> <li>2.7. Ban Management System<ul> <li>2.7.1. Ban List Management</li> <li>2.7.2. Ban Operations</li> <li>2.7.3. Ban Event Handling</li> <li>2.7.4. Configuration</li> </ul> </li> <li>3. Technology</li> <li>4. Data Model</li> <li>5. Directory Structure and Main Files</li> <li>6. How to run</li> <li>7. Configuration options (settings flags)</li> <li>8. Other Resources</li> </ul> </li> </ol>"},{"location":"topics/services/p2p/#1-description","title":"1. Description","text":"<p>The p2p package implements a peer-to-peer (P2P) server using <code>libp2p</code> (<code>github.com/libp2p/go-libp2p</code>, <code>https://libp2p.io</code>), a modular network stack that allows for direct peer-to-peer communication. The implementation follows an interface-based design pattern using the public <code>github.com/bsv-blockchain/go-p2p</code> package, with <code>p2p.NodeI</code> interface abstracting the concrete P2P node implementation to allow for better testability and modularity. This public package enables external developers to create custom P2P implementations that integrate with Teranode.</p> <p>The p2p service allows peers to subscribe and receive blockchain notifications, effectively allowing nodes to receive notifications about new blocks and subtrees in the network.</p> <p>The p2p peers are part of a private network. This private network is managed by the p2p bootstrap service, which is responsible for bootstrapping the network and managing the network topology.</p> <ol> <li> <p>Initialization and Configuration:</p> <ul> <li>The <code>Server</code> struct holds essential information for the P2P server, such as hosts, topics, subscriptions, clients for blockchain and validation, and logger for logging activities.</li> <li>The <code>NewServer</code> function creates a new instance of the P2P server, sets up the host with a private key, and defines various topic names for message publishing and subscribing.</li> <li>The <code>Init</code> function configures the server with necessary clients and settings.</li> </ul> </li> <li> <p>Message Types:</p> <ul> <li>There are several message types (<code>BestBlockMessage</code>, <code>MiningOnMessage</code>, <code>BlockMessage</code>, <code>SubtreeMessage</code>, <code>RejectedTxMessage</code>) used for communicating different types of data over the network.</li> <li>The <code>BlockMessage</code> includes a <code>Header</code> field containing the raw block header in hexadecimal format, allowing peers to validate block properties without downloading the full block.</li> </ul> </li> <li> <p>Networking and Communication:</p> <ul> <li>The server uses <code>libp2p</code> for network communication. It sets up a host with given IP and port and handles different topics for publishing and subscribing to messages.</li> <li>The server uses Kademlia for peer discovery, implementing both standard and private DHT (Distributed Hash Table) modes based on configuration.</li> <li>The server uses GossipSub for PubSub messaging, with automated topic subscription and management.</li> <li>Peers identify themselves using a user agent string in format <code>teranode/bitcoin/{version}</code> where the version is dynamically determined at build time from Git tags (e.g., \"v1.2.3\") or generated as a pseudo-version (e.g., \"v0.0.0-20250731141601-18714b9\").</li> <li><code>handleBlockchainMessage</code>, <code>handleBestBlockTopic</code>, <code>handleBlockTopic</code>, <code>handleSubtreeTopic</code>, and <code>handleMiningOnTopic</code> are functions to handle incoming messages for different topics.</li> <li><code>sendBestBlockMessage</code> and <code>sendPeerMessage</code> are used for sending messages to peers in the network.</li> <li>The implementation includes error handling and retry mechanisms for peer connections to enhance network resilience.</li> </ul> </li> <li> <p>Peer Discovery and Connection:</p> <ul> <li><code>discoverPeers</code> function is responsible for discovering peers in the network and attempting to establish connections with them.</li> <li>The system implements an intelligent retry mechanism that tracks failed connection attempts and manages reconnection policies based on error types.</li> <li>A dedicated mechanism for connecting to static peers runs in a separate goroutine, ensuring that mission-critical peers are always connected when available.</li> </ul> </li> <li> <p>HTTP Server and WebSockets:</p> <ul> <li>An HTTP server is started using <code>echo</code>, a Go web framework, providing routes for health checks and WebSocket connections.</li> <li><code>HandleWebSocket</code> is used to handle incoming WebSocket connections and manage the communication with connected clients.</li> </ul> </li> <li> <p>Subscription Listeners:</p> <ul> <li>The server listens for notifications from the blockchain and the tx validator services. It subscribes to these services and handles incoming notifications.</li> </ul> </li> <li> <p>Publish-Subscribe Mechanism:</p> <ul> <li>The server uses the publish-subscribe model over <code>libp2p</code> pubsub for message dissemination. It joins topics and subscribes to them to receive and broadcast messages related to blockchain events.</li> </ul> </li> <li> <p>Private Key Management:</p> <ul> <li>The server generates or reads a private key for secure communication (peerId and encryption) in the P2P network.</li> <li>The generated private key is persisted.</li> </ul> </li> </ol> <p>Note: For information about how the P2P service is initialized during daemon startup and how it interacts with other services, see the Teranode Daemon Reference.</p> <p></p> <p>In the diagram above:</p> <ul> <li>The node P2P service subscribes and listens to Blockchain and Tx Validator service notifications. When blocks, subtrees or rejected transactions are detected, the P2P service publishes these messages to the network.</li> <li>When the P2P service receives a message from the network (i.e. a Blockchain message from another node), it forwards it to the node Block Validation Service for processing.</li> </ul> <p>In more detail:</p> <p></p> <p>The following diagram provides a deeper level of detail into the P2P Service's internal components and their interactions:</p> <p></p>"},{"location":"topics/services/p2p/#2-functionality","title":"2. Functionality","text":""},{"location":"topics/services/p2p/#21-creating-initializing-and-starting-a-new-p2p-server","title":"2.1. Creating, initializing and starting a new P2P Server","text":""},{"location":"topics/services/p2p/#211-creating-a-new-p2p-server","title":"2.1.1. Creating a New P2P Server","text":"<p>The startup process of the node involves the <code>main.go</code> file calling the <code>p2p.NewServer</code> function from the P2P package (<code>services/p2p/Server.go</code>). This function is tasked with creating a new P2P server instance.</p> <ol> <li> <p>Private Key Management:</p> <ul> <li> <p>The server tries to read an existing private key from the blockchain store through the <code>readPrivateKey()</code> function:</p> <ul> <li><code>readPrivateKey</code> calls <code>blockchainClient.GetState(ctx, \"p2p.privateKey\")</code> to retrieve the serialized key data</li> <li> <p>If found, it deserializes the key using <code>crypto.UnmarshalPrivateKey()</code></p> <ul> <li>If no key is specified in the configuration and no key exists in the blockchain store, it automatically generates and stores a new one using the <code>generateAndStorePrivateKey()</code> function:</li> </ul> </li> <li> <p><code>generateAndStorePrivateKey</code> creates a new Ed25519 key pair using <code>crypto.GenerateEd25519Key()</code></p> </li> <li>It serializes the private key with <code>crypto.MarshalPrivateKey()</code></li> <li>It stores the serialized key in the blockchain store using <code>blockchainClient.SetState(ctx, \"p2p.privateKey\", privBytes)</code></li> <li>The generated key is automatically persisted to ensure the node maintains the same peer ID across restarts</li> </ul> </li> <li> <p>This blockchain store persistence mechanism works through gRPC calls to the blockchain service</p> </li> <li>Using the blockchain store ensures that the P2P private key (and therefore node identity) persists even if the container is destroyed, maintaining consistent peer relationships in the network</li> </ul> <p></p> </li> <li> <p>Configuration Retrieval and Topic Registration:</p> <ul> <li>Retrieves required configuration settings like <code>p2p_listen_addresses</code> and <code>p2p_port</code>.</li> <li>It registers specific topic names derived from the configuration, such as <code>p2p_block_topic</code>, <code>p2p_subtree_topic</code>, <code>p2p_bestblock_topic</code>, <code>p2p_mining_on_topic</code>, and <code>p2p_rejected_tx_topic</code>.</li> </ul> </li> <li> <p>P2P Node Initialization:</p> <ul> <li>Initializes a libp2p node (host) using the specified IP, port, and private key, which manages node communications and connections.</li> </ul> </li> </ol>"},{"location":"topics/services/p2p/#212-initializing-the-p2p-server","title":"2.1.2. Initializing the P2P Server","text":"<p>The P2P server's <code>Init</code> function is in charge of server setup:</p> <ol> <li> <p>Blockchain Client Initialization:</p> <ul> <li>Creates a new blockchain client with <code>blockchain.NewClient(ctx, s.logger, \"source\")</code>.</li> </ul> </li> <li> <p>Asset HTTP Address Configuration:</p> <ul> <li>Retrieves the Asset HTTP Address URL from the configuration using <code>gocore.Config().GetURL(\"asset_httpAddress\")</code>.</li> </ul> </li> <li> <p>Block Validation Client Initialization:</p> <ul> <li>Sets up a block validation client with <code>blockvalidation.NewClient(ctx)</code>.</li> </ul> </li> <li> <p>Validator Client Initialization:</p> <ul> <li>Initializes a transaction validator client with <code>validator.NewClient(ctx, s.logger)</code>.</li> </ul> </li> </ol>"},{"location":"topics/services/p2p/#213-starting-the-p2p-server","title":"2.1.3. Starting the P2P Server","text":"<p>The <code>Start</code> function is responsible for commencing the P2P service:</p> <ol> <li> <p>HTTP Server Setup:</p> <ul> <li>Utilizes the Echo framework to set up an HTTP server.</li> </ul> </li> <li> <p>HTTP Endpoints:</p> <ul> <li>Sets up a health check endpoint (<code>/health</code>) that responds with \"OK\".</li> <li>Adds a WebSocket endpoint (<code>/ws</code>) for real-time communication via <code>s.HandleWebSocket</code>.</li> </ul> </li> <li> <p>Start HTTP Server:</p> <ul> <li>Initiates the HTTP server using a goroutine, which is executed via <code>s.StartHttp</code>.</li> </ul> </li> <li> <p>PubSub Topics Setup:</p> <ul> <li>Initializes the GossipSub system for pub-sub messaging.</li> <li>Subscribes to various topics defined in the configuration.</li> </ul> </li> <li> <p>Peer-to-Peer Host Configuration:</p> <ul> <li>Assigns a stream handler to the P2P host to handle blockchain-related messages.</li> </ul> </li> <li> <p>Topic Handlers:</p> <ul> <li>Initiates goroutines for message handling on designated topics.</li> </ul> </li> <li> <p>Subscription Listeners:</p> <ul> <li>Begins subscription listeners for blockchain and validator services to manage incoming notifications about blocks and transactions.</li> </ul> </li> <li> <p>Best Block Message Broadcast:</p> <ul> <li>Sends a best block message to the network to solicit the current best block hash from peers.</li> </ul> </li> </ol> <p>Once these steps are completed, the server is ready to accept peer connections, handle messages, and monitor network activity.</p>"},{"location":"topics/services/p2p/#22-peer-discovery-and-connection","title":"2.2. Peer Discovery and Connection","text":"<p>In the previous section, the P2P Service created a `P2PClient as part of the initialization phase. This P2PClient is responsible for joining the network. The P2PClient utilizes a libp2p host and a Distributed Hash Table (DHT) for peer discovery and connection based on shared topics. This mechanism enables the client to become part of a decentralized network, facilitating communication and resource sharing among peers.</p> <p></p> <ol> <li> <p>Initialization of DHT and libp2p Host:</p> <ul> <li>The <code>P2PClient</code> struct, upon invocation of its <code>Start</code> method, initializes the DHT using either <code>initDHT</code> or <code>initPrivateDHT</code> methods depending on the configuration. This step sets up the DHT for the libp2p host (<code>s.host</code>), which allows for peer discovery and content routing within the P2P network. The DHT is bootstrapped with default or configured peers to integrate the client into the existing network.</li> </ul> </li> <li> <p>Setting Up Routing Discovery:</p> <ul> <li>Once the DHT is initialized, <code>P2PClient.Start</code> sets up <code>routingDiscovery</code> with the created DHT instance. This discovery service is responsible for locating peers within the network and advertising the client's own presence.</li> <li> <p>The DHT implementation supports two modes:</p> <ul> <li>Standard mode (<code>initDHT</code>): Uses the public IPFS bootstrap nodes for initial discovery</li> <li>Private mode (<code>initPrivateDHT</code>): Creates an isolated private network with custom bootstrap nodes, providing enhanced security for enterprise deployments</li> </ul> </li> </ul> </li> <li> <p>Advertising and Searching for Peers:</p> <ul> <li>The node then advertises itself for the configured topics and looks for peers associated with these topics. This is conducted through the <code>discoverPeers</code> method, which iterates over the topic names and uses the routing discovery to advertise and find peers interested in the same topics.</li> </ul> </li> <li> <p>Connecting to Discovered and Static Peers:</p> <ul> <li> <p>The <code>discoverPeers</code> method includes sophisticated filtering and error handling mechanisms:</p> <ul> <li><code>shouldSkipPeer</code>: Determines if connection attempts should be skipped based on various criteria</li> <li><code>shouldSkipBasedOnErrors</code>: Manages retry logic for previously failed connections</li> <li><code>shouldSkipNoGoodAddresses</code>: Special handling for peers with address resolution issues</li> </ul> </li> <li> <p>The <code>connectToStaticPeers</code> method runs in a dedicated goroutine to periodically attempt connections with a predefined list of peers (static peers), ensuring critical network infrastructure remains connected even after temporary failures.</p> </li> <li>Connection errors are carefully tracked to avoid network congestion from repeated failed connection attempts.</li> </ul> </li> <li> <p>Integration with P2P Network:</p> <ul> <li>The capabilities of the DHT for discovery and topic-based advertising enables the node to seamlessly integrate into the Teranode P2P network.</li> </ul> </li> </ol>"},{"location":"topics/services/p2p/#23-best-block-messages","title":"2.3. Best Block Messages","text":"<ol> <li> <p>Node (Peer 1) Starts:</p> <ul> <li>The server's <code>Start()</code> method is invoked. Within <code>Start()</code>, <code>s.sendBestBlockMessage(ctx)</code> is called to send the best block message.</li> <li>This message is published to a topic using <code>s.topics[bestBlockTopicName].Publish(ctx, msgBytes)</code>.</li> </ul> </li> <li> <p>Peer 2 Receives the Best Block Topic Message:</p> <ul> <li>Peer 2's server handles the best block topic through <code>s.handleBestBlockTopic(ctx)</code>.</li> <li>A peer message is sent using <code>s.sendPeerMessage(ctx, peer, msgBytes)</code>. A new stream to the Peer 1 LibP2P host is established with <code>s.host.NewStream(ctx, peer.ID, protocol.ID(s.bitcoinProtocolId))</code>.</li> </ul> </li> <li> <p>Node (Peer 1) Receives the Stream Response from Peer 2:</p> <ul> <li>The LibP2P host sends the response stream to Node (Peer 1).</li> <li>Node (Peer 1) handles the blockchain message using <code>s.handleBlockchainMessage(ctx, stream)</code>.</li> <li>The message received is logged with <code>s.logger.Debugf(\"Received block topic message: %s\", string(buf))</code> (at the time of writing, the <code>handleBlockchainMessage</code> function simply logs block topic messages).</li> </ul> </li> </ol>"},{"location":"topics/services/p2p/#24-blockchain-messages","title":"2.4. Blockchain Messages","text":"<p>When a node creates a new subtree, or finds a new block hashing solution, it will broadcast this information to the network. This is done by publishing a message to the relevant topic. The message is then received by all peers subscribed to that topic. Listening peers can then feed relevant messages to their own Block Validation Service.</p> <p></p> <ol> <li> <p>Blockchain Subscription:</p> <ul> <li>The server subscribes to the blockchain service using <code>s.blockchainClient.Subscribe(ctx, blockchain.SubscriptionType_Blockchain)</code>.</li> <li>The server listens for blockchain notifications (<code>Block</code>, <code>Subtree</code> or <code>MiningOn</code> notifications) using <code>s.blockchainSubscriptionListener(ctx)</code>.</li> </ul> </li> <li> <p>New Block Notification:</p> <ul> <li>Node 1 listens for blockchain notifications.</li> <li>If a new block notification is detected, it publishes the block message to the PubSub System.</li> <li>The block message includes the block hash, height, data hub URL, peer ID, and the raw block header in hexadecimal format.</li> <li>The PubSub System then delivers this message to Node 2.</li> <li>Node 2 receives the message on the block topic, can quickly validate the block header, then submits the block message to its own Block Validation Service, and notifies the block message on its notification channel.</li> <li>Note that the Block Validation Service might be configured to either receive gRPC notifications or listen to a Kafka producer. In the diagram above, the gRPC method is described. Please check the Block Validation Service documentation for more details</li> </ul> </li> <li> <p>New Mined Block Notification:</p> <ul> <li>Node 1 listens for blockchain notifications.</li> <li>If a new mined block notification is detected, it publishes the mining on message to the PubSub System.</li> <li>The PubSub System delivers this message to Node 2.</li> <li>Node 2 receives the mining message on the mining topic and notifies the \"mining on\" message on its notification channel.</li> </ul> </li> <li> <p>New Subtree Notification:</p> <ul> <li>Node 1 listens for blockchain notifications.</li> <li>If a new subtree notification is detected, it publishes the subtree message to the PubSub System.</li> <li>The PubSub System delivers this message to Node 2.</li> <li>Node 2 receives the subtree message on the subtree topic, submits the subtree message to its own Subtree Validation Service, and notifies the subtree message on its notification channel.<ul> <li>Note that the Subtree Validation Service might be configured to either receive gRPC notifications or listen to a Kafka producer. In the diagram above, the gRPC method is described. Please check the Subtree Validation Service  documentation for more details</li> </ul> </li> </ul> </li> </ol>"},{"location":"topics/services/p2p/#25-tx-validator-messages","title":"2.5. TX Validator Messages","text":"<p>Nodes will broadcast rejected transaction notifications to the network. This is done by publishing a message to the relevant topic. The message is then received by all peers subscribed to that topic.</p> <p></p> <ul> <li>The Node 1 listens for validator subscription events.</li> <li>When a new rejected transaction notification is detected, the Node 1 publishes this message to the PubSub System using the topic name <code>rejectedTxTopicName</code>, forwarding it to any subscribers of the <code>rejectedTxTopicName</code> topic.</li> </ul> <p>Note that the P2P service can only subscribe to these notifications if and when the TX Validator Service is available in the node. The service uses the <code>useLocalValidator</code> setting to determine whether a local validator or a validator service is in scope. If no TX validator runs in the node, the P2P will not attempt to subscribe.</p>"},{"location":"topics/services/p2p/#26-websocket-notifications","title":"2.6. Websocket notifications","text":"<p>All notifications collected from the Block and Validator listeners are sent over to the Websocket clients. The process can be seen below:</p> <p></p> <ul> <li> <p>WebSocket Request Handling:</p> </li> <li> <p>An HTTP request is upgraded to a WebSocket connection. A new client channel is associated to this Websocket client.</p> </li> <li>Data is sent over the WebSocket, using its dedicated client channel.</li> <li> <p>If there's an error in sending data, the channel is removed from the <code>clientChannels</code>.</p> </li> <li> <p>The server listens for various types of events in a concurrent process:</p> </li> <li> <p>The server tracks all active client channels (<code>clientChannels</code>).</p> </li> <li>When a new client connects, it is added to the <code>clientChannels</code>.</li> <li>If a client disconnects, it is removed from <code>clientChannels</code>.</li> <li>Periodically, we ping all connected clients. Any error would have the client removed from the list of clients.</li> <li>When a notification is received (from the block validation or transaction listeners described in the previous sections), it is sent to all connected clients.</li> </ul> <p>As a sequence:</p> <p></p> <ol> <li>A client requests a WebSocket connection to the server. The new client is added to the <code>newClientCh</code> queue, which then adds the client to the active client channels.</li> <li>The server enters a loop for WebSocket communication, where it can either receive new notifications or pings.</li> <li>For each new notification or ping, the server dispatches this data to all client channels.</li> <li>If there's a WebSocket error or the client disconnects, the client is added to the <code>deadClientCh</code> queue, which leads to its removal from the active client channels.</li> </ol>"},{"location":"topics/services/p2p/#27-ban-management-system","title":"2.7. Ban Management System","text":"<p>The P2P service includes a ban management system that allows nodes to maintain a list of banned peers and handle ban-related events across the network.</p> <p></p>"},{"location":"topics/services/p2p/#271-ban-list-management","title":"2.7.1. Ban List Management","text":"<p>The ban system consists of two main components:</p> <ol> <li>BanList: A thread-safe data structure that maintains banned IP addresses and subnets</li> <li>BanChan: A channel that broadcasts ban-related events to system components</li> </ol> <p>The ban list supports:</p> <ul> <li>Individual IP addresses</li> <li>Entire subnets using CIDR notation</li> <li>Temporary bans with expiration times</li> <li>Persistent storage of bans in a database</li> </ul>"},{"location":"topics/services/p2p/#272-ban-operations","title":"2.7.2. Ban Operations","text":"<p>While the banList is maintained by the P2P service, the ban operations are managed by the RPC Server. The RPC server receives request to add / remove peers, notifying the P2P service to update the ban list.</p>"},{"location":"topics/services/p2p/#273-ban-event-handling","title":"2.7.3. Ban Event Handling","text":"<p>When a ban event occurs:</p> <ol> <li>The ban is added to the persistent storage</li> <li>A ban event is broadcast through the ban channel</li> <li>The P2P service checks current connections against the ban</li> <li>Any matching connections are terminated</li> <li>Future connection attempts from banned IPs are rejected</li> </ol>"},{"location":"topics/services/p2p/#274-configuration","title":"2.7.4. Configuration","text":"<p>Ban-related settings in the configuration:</p> <ul> <li><code>banlist_db</code>: Database connection string for ban storage</li> <li><code>ban_default_duration</code>: Default duration for bans (24 hours if not specified)</li> <li><code>ban_max_entries</code>: Maximum number of banned entries to maintain</li> </ul>"},{"location":"topics/services/p2p/#3-technology","title":"3. Technology","text":"<ol> <li> <p>Go Programming Language:</p> <ul> <li>The entire package is written in Go (Golang).</li> </ul> </li> <li> <p>libp2p:</p> <ul> <li>A modular network framework that allows peers to communicate directly with each other. It's widely used in decentralized systems for handling various network protocols, peer discovery, transport, encryption, and stream multiplexing.</li> </ul> </li> <li> <p>pubsub (go-libp2p-pubsub):</p> <ul> <li>A library for publish-subscribe functionality in <code>libp2p</code>. It's used for messaging between nodes in a decentralized network, supporting various messaging patterns like broadcasting.</li> </ul> </li> <li> <p>crypto (go-libp2p-core/crypto):</p> <ul> <li>This library provides cryptographic functions for <code>libp2p</code>, including key generation, marshaling, and unmarshaling. It's crucial for maintaining secure communication channels in the P2P network.</li> </ul> </li> <li> <p>Echo (labstack/echo/v4):</p> <ul> <li>A high-performance, extensible web framework for Go.</li> </ul> </li> <li> <p>WebSockets:</p> <ul> <li>A communication protocol providing full-duplex channels over a single TCP connection. Used here for real-time, two-way interaction between the server and clients.</li> </ul> </li> <li> <p>JSON (JavaScript Object Notation):</p> <ul> <li>A lightweight data-interchange format, used here for serializing and transmitting structured data over the network (like the various message types).</li> </ul> </li> <li> <p>Distributed Hash Table (DHT) for Peer Discovery:</p> <ul> <li>A decentralized method of discovering peers in the network. It allows a node to efficiently find other nodes in the P2P network.</li> </ul> </li> <li> <p>HTTP/HTTPS Protocols:</p> <ul> <li>Used for setting up the web server and handling requests. The server can handle both HTTP and HTTPS requests.</li> </ul> </li> <li> <p>Middleware &amp; Logging (middleware, ulogger):</p> <ul> <li>Used for handling common tasks across requests (like logging, error handling, CORS settings) in the Echo web framework.</li> </ul> </li> <li> <p>gocore (ordishs/gocore):</p> <ul> <li>A utility library for configuration management and other core functionalities.</li> </ul> </li> <li> <p>go-p2p (github.com/bsv-blockchain/go-p2p):</p> <ul> <li>Public P2P networking package that provides the core P2P node implementation</li> <li>Offers standardized interfaces for P2P communication in Bitcoin SV applications</li> <li>Enables external developers to build compatible P2P solutions</li> </ul> </li> <li> <p>Environmental Configuration:</p> <ul> <li>Configuration management using environment variables, required for setting up network parameters, topic names, etc.</li> </ul> </li> </ol>"},{"location":"topics/services/p2p/#4-data-model","title":"4. Data Model","text":"<ul> <li>Block Data Model: Contain lists of subtree identifiers.</li> <li>Subtree Data Model: Contain lists of transaction IDs and their Merkle root.</li> </ul> <p>Within the P2P service, notifications are sent to the Websocket clients using the following data model:</p> <ul> <li>Block notifications:</li> </ul> <pre><code>   s.notificationCh &lt;- &amp;notificationMsg{\n    Timestamp: time.Now().UTC(),\n    Type:      \"block\",\n    Hash:      blockMessage.Hash,\n    BaseURL:   blockMessage.DataHubUrl,\n    PeerId:    blockMessage.PeerId,\n   }\n</code></pre> <ul> <li>Subtree notifications:</li> </ul> <pre><code>   s.notificationCh &lt;- &amp;notificationMsg{\n    Type:    \"subtree\",\n    Hash:    subtreeMessage.Hash,\n    BaseURL: subtreeMessage.DataHubUrl,\n    PeerId:  subtreeMessage.PeerId,\n   }\n</code></pre> <ul> <li>\"MiningOn\" notifications:</li> </ul> <pre><code>   s.notificationCh &lt;- &amp;notificationMsg{\n    Timestamp:    time.Now().UTC(),\n    Type:         \"mining_on\",\n    Hash:         miningOnMessage.Hash,\n    BaseURL:      miningOnMessage.DataHubUrl,\n    PeerId:       miningOnMessage.PeerId,\n    PreviousHash: miningOnMessage.PreviousHash,\n    Height:       miningOnMessage.Height,\n    Miner:        miningOnMessage.Miner,\n    SizeInBytes:  miningOnMessage.SizeInBytes,\n    TxCount:      miningOnMessage.TxCount,\n   }\n</code></pre>"},{"location":"topics/services/p2p/#5-directory-structure-and-main-files","title":"5. Directory Structure and Main Files","text":"<pre><code>./services/p2p\n\u2502\n\u251c\u2500\u2500 HandleWebsocket.go   - Manages WebSocket connections and communications.\n\u251c\u2500\u2500 Server.go            - Main server logic for the P2P service, including network handling and peer interactions.\n\u251c\u2500\u2500 client.html          - A client-side HTML file for testing or interacting with the WebSocket server.\n\u2514\u2500\u2500 dht.go               - Implements the Distributed Hash Table (DHT) functionality for the P2P network.\n</code></pre>"},{"location":"topics/services/p2p/#6-how-to-run","title":"6. How to run","text":"<p>To run the P2P Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -P2P=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the P2P Service locally.</p>"},{"location":"topics/services/p2p/#7-configuration-options-settings-flags","title":"7. Configuration options (settings flags)","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the p2p Settings Reference.</p>"},{"location":"topics/services/p2p/#8-other-resources","title":"8. Other Resources","text":"<p>P2P Reference</p>"},{"location":"topics/services/propagation/","title":"\ud83c\udf10 Propagation Service","text":""},{"location":"topics/services/propagation/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1. Starting the Propagation Service</li> <li>2.1.1 Validator Integration</li> <li>2.2. Propagating Transactions</li> <li>2.3. Transaction Processing Workflow</li> <li>2.4. Error Handling</li> </ul> </li> <li>gRPC Protobuf Definitions</li> <li>Data Model</li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>How to run</li> <li>Configuration options (settings flags)</li> <li>Other Resources</li> </ol>"},{"location":"topics/services/propagation/#1-description","title":"1. Description","text":"<p>The <code>Propagation Service</code> is designed to handle the propagation of transactions across a peer-to-peer Teranode network.</p> <p>At a glance, the Propagation service:</p> <ol> <li>Receives new transactions through various communication methods.</li> <li>Stores transactions in the tx store.</li> <li>Sends the transaction to the Validator service for further processing.</li> </ol> <p></p> <p>The gRPC protocol is the primary communication method, although HTTP is also accepted.</p> <ul> <li><code>StartHTTPServer</code>: This function is designed to start a network listener for the HTTP protocol. Each function configures and starts a server to listen for incoming connections and requests on specific network addresses and ports. For example, the HTTP endpoints are <code>/tx</code>, <code>/txs</code>, and <code>/health</code>.</li> </ul> <p>A node can start multiple parallel instances of the Propagation service. This translates into multiple pods within a Kubernetes cluster. Each instance will have its own gRPC server, and will be able to receive and propagate transactions independently. GRPC load balancing allows to distribute the load across the multiple instances.</p> <p></p> <p>Notice that the Validator, as shown in the diagram above, can be either a local validator or a remote validator service, depending on the Node configuration. To know more, please refer to the Transaction Validator documentation.</p> <p>Also, note how the Blockchain client is used in order to wait for the node State to change to <code>RUNNING</code> state. For more information on this, please refer to the State Management  documentation.</p>"},{"location":"topics/services/propagation/#2-functionality","title":"2. Functionality","text":""},{"location":"topics/services/propagation/#21-starting-the-propagation-service","title":"2.1. Starting the Propagation Service","text":"<p>Upon startup, the Propagation service starts the relevant communication channels, as configured via settings.</p>"},{"location":"topics/services/propagation/#211-validator-integration","title":"2.1.1 Validator Integration","text":"<p>The Propagation service can work with the Validator in two different configurations:</p> <ol> <li> <p>Local Validator:</p> <ul> <li>When <code>useLocalValidator=true</code> (recommended for production)</li> <li>The Validator is instantiated directly within the Propagation service</li> <li>Direct method calls are used without network overhead</li> <li>This provides the best performance and lowest latency</li> </ul> </li> <li> <p>Remote Validator Service:</p> <ul> <li>When <code>useLocalValidator=false</code></li> <li>The Propagation service connects to a separate Validator service via gRPC</li> <li>Useful for development, testing, or specialized deployment scenarios</li> <li>Has higher latency due to additional network calls</li> </ul> </li> </ol> <p>This configuration is controlled by the settings passed to <code>GetValidatorClient()</code> in daemon.go.</p> <p>Note: For detailed information about how services are initialized and connected during daemon startup, see the Teranode Daemon Reference.</p>"},{"location":"topics/services/propagation/#22-propagating-transactions","title":"2.2. Propagating Transactions","text":"<p>All communication channels receive txs and delegate them to the <code>ProcessTransaction()</code> function. The main communication channels are shown below.</p> <p>HTTP:</p> <p></p> <p>gRPC:</p> <p></p>"},{"location":"topics/services/propagation/#23-transaction-processing-workflow","title":"2.3. Transaction Processing Workflow","text":"<p>The transaction processing involves several steps to ensure proper validation and propagation:</p> <ol> <li>Initial Validation: Each transaction is validated for correct format and to ensure it's not a coinbase transaction.</li> <li>Storage: Valid transactions are stored in the transaction store using the transaction hash as the key.</li> <li> <p>Validation Submission: Transactions are submitted to the validator service through one of two channels:</p> <ul> <li>Kafka: Normal-sized transactions are sent to the validator through Kafka for asynchronous processing.</li> <li>HTTP Fallback: Large transactions exceeding Kafka message size limits are sent directly to the validator's HTTP endpoint.</li> </ul> </li> </ol>"},{"location":"topics/services/propagation/#format-handling","title":"Format Handling","text":"<p>The Propagation Service is format-agnostic and handles transaction formats flexibly:</p> <ul> <li>Accepts both formats: Standard Bitcoin format and Extended Format (BIP-239) transactions</li> <li>No format validation at ingress: Transactions are not rejected based on their format</li> <li>Storage preserves received format: Transactions are initially stored as received in the blob store</li> <li>Format handling delegation: The actual format conversion and extension is handled downstream by the Validator Service</li> </ul> <p>This design ensures:</p> <ul> <li>Maximum compatibility with diverse wallet implementations (from legacy to modern)</li> <li>No format-based rejection at the network edge</li> <li>Flexible deployment supporting both traditional Bitcoin clients and BIP-239 aware applications</li> <li>Backward compatibility with the broader Bitcoin ecosystem</li> </ul> <p>The Propagation Service focuses on efficient transaction ingress and distribution, delegating format-specific processing to the appropriate downstream services.</p>"},{"location":"topics/services/propagation/#24-error-handling","title":"2.4. Error Handling","text":"<p>The Propagation Service implements comprehensive error handling:</p> <ol> <li>Transaction Format Errors: Malformed transactions are rejected with appropriate error messages.</li> <li>Storage Failures: If transaction storage fails, the error is logged and propagated to the client.</li> <li>Validation Errors: Errors during validation are captured and returned to the client.</li> <li>Batch Processing: When processing transaction batches, each transaction is handled independently, allowing some transactions to succeed even if others fail.</li> <li>Request Limiting: Implements limits on transaction size and batch counts to prevent resource exhaustion.</li> </ol>"},{"location":"topics/services/propagation/#3-grpc-protobuf-definitions","title":"3. gRPC Protobuf Definitions","text":"<p>The Propagation Service uses gRPC for communication between nodes. The protobuf definitions used for defining the service methods and message formats can be seen in the propagationProto.md documentation.</p>"},{"location":"topics/services/propagation/#4-data-model","title":"4. Data Model","text":"<p>The Propagation Service accepts transactions in multiple formats:</p> <ul> <li>Transaction Data Model: Comprehensive guide covering both standard Bitcoin format and Extended Format (BIP-239), including how Teranode handles format conversion and storage.</li> </ul>"},{"location":"topics/services/propagation/#5-technology","title":"5. Technology","text":"<p>Main technologies involved:</p> <ol> <li> <p>Go Programming Language (Golang):</p> <ul> <li>The entire service is written in Go.</li> </ul> </li> <li> <p>Peer-to-Peer (P2P) Networking:</p> <ul> <li>The service is designed for a P2P network environment, where nodes (computers) in the network communicate directly with each other without central coordination.</li> <li><code>bsv-blockchain/go-p2p/wire</code> is used for P2P transaction propagation in the Teranode BSV network.</li> </ul> </li> <li> <p>Networking Protocols (HTTP)</p> </li> <li> <p>Cryptography:</p> <ul> <li>The use of <code>crypto</code> packages for RSA key generation and TLS (Transport Layer Security) configuration for secure communication.</li> </ul> </li> <li> <p>gRPC and Protocol Buffers:</p> <ul> <li>gRPC, indicated by the use of <code>google.golang.org/grpc</code>, is a high-performance, open-source universal RPC framework. It uses Protocol Buffers as its interface definition language.</li> </ul> </li> </ol>"},{"location":"topics/services/propagation/#6-directory-structure-and-main-files","title":"6. Directory Structure and Main Files","text":"<pre><code>./services/propagation\n\u2502\n\u251c\u2500\u2500 Client.go                            - Contains the client-side logic for interacting with the propagation service.\n\u251c\u2500\u2500 Client_test.go                       - Unit tests for the Client.go functionality.\n\u251c\u2500\u2500 Server.go                            - Contains the main server-side implementation for the propagation service.\n\u251c\u2500\u2500 Server_test.go                       - Unit tests for the Server.go functionality.\n\u251c\u2500\u2500 client_large_tx_fallback_test.go     - Tests the large transaction fallback mechanism in the client.\n\u251c\u2500\u2500 http_handlers_test.go                - Unit tests for HTTP handler functions.\n\u251c\u2500\u2500 large_tx_fallback_test.go            - Tests for the large transaction fallback mechanism.\n\u251c\u2500\u2500 metrics.go                           - Metrics collection and monitoring of the propagation service.\n\u251c\u2500\u2500 propagation_error_test.go            - Unit tests for error handling in the propagation service.\n\u2514\u2500\u2500 propagation_api                      - Directory containing various files related to the API definition and implementation of the propagation service.\n    \u251c\u2500\u2500 propagation_api.pb.go            - Auto-generated file from protobuf definitions, containing Go bindings for the API.\n    \u251c\u2500\u2500 propagation_api.proto            - Protocol Buffers definition file for the propagation API.\n    \u2514\u2500\u2500 propagation_api_grpc.pb.go       - gRPC (Google's RPC framework) specific implementation file for the propagation API.\n</code></pre>"},{"location":"topics/services/propagation/#7-how-to-run","title":"7. How to run","text":"<p>To run the Propagation Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -Propagation=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Propagation Service locally.</p>"},{"location":"topics/services/propagation/#8-configuration-options-settings-flags","title":"8. Configuration options (settings flags)","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the Propagation Settings Reference.</p>"},{"location":"topics/services/propagation/#9-other-resources","title":"9. Other Resources","text":"<p>Propagation Service Reference</p>"},{"location":"topics/services/rpc/","title":"\ud83c\udf10 RPC Service","text":""},{"location":"topics/services/rpc/#index","title":"Index","text":"<ol> <li>Introduction<ul> <li>Supported RPC Commands</li> <li>Unimplemented RPC Commands</li> <li>Command help</li> <li>Authentication</li> </ul> </li> <li>Architecture</li> <li>Functionality<ul> <li>3.1. RPC Service Initialization and Configuration</li> <li>3.2. Command: Create Raw Transaction</li> <li>3.3. Command: Freeze</li> <li>3.4. Command: Generate</li> <li>3.5. Command: Generate to Address</li> <li>3.6. Command: Get Best Block Hash</li> <li>3.7. Command: Get Block</li> <li>3.8. Command: Get Block By Height</li> <li>3.9. Command: Get Blockchain Info</li> <li>3.10. Command: Get Block Hash</li> <li>3.11. Command: Get Block Header</li> <li>3.12. Command: Get Difficulty</li> <li>3.13. Command: Get Info</li> <li>3.14. Command: Get Mining Info</li> <li>3.15. Command: Get Mining Candidate</li> <li>3.16. Command: Get Peer Info</li> <li>3.17. Command: Get Raw Transaction</li> <li>3.18. Command: Invalidate Block</li> <li>3.19. Command: Is Banned</li> <li>3.20. Command: Reassign</li> <li>3.21. Command: Reconsider Block</li> <li>3.22. Command: Send Raw Transaction</li> <li>3.23. Command: Set Ban</li> <li>3.24. Command: Stop</li> <li>3.25. Command: Submit Mining Solution</li> <li>3.26. Command: Unfreeze</li> <li>3.27. Command: Version</li> </ul> </li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>Configuration Settings</li> <li>How to run</li> <li>Other Resources</li> </ol>"},{"location":"topics/services/rpc/#1-introduction","title":"1. Introduction","text":"<p>Note: For information about how the RPC service is initialized during daemon startup and how it interacts with other services, see the Teranode Daemon Reference.</p> <p>The RPC server provides compatibility with the Bitcoin RPC interface, allowing clients to interact with the Teranode node using standard Bitcoin RPC commands. The RPC server listens for incoming requests and processes them, returning the appropriate responses.</p> <p>Teranode provides partial support, as required for its own services. Additional support for specific commands and features might be added over time.</p> <p>The below table summarises the services supported in the current version:</p>"},{"location":"topics/services/rpc/#supported-rpc-commands","title":"Supported RPC Commands","text":"RPC Command Status Description createrawtransaction Supported Creates a raw transaction without signing it freeze Supported Freezes a specific UTXO, preventing it from being spent generate Supported Generates blocks (for testing) generatetoaddress Supported Generates blocks to a specified address (for testing) getbestblockhash Supported Returns the hash of the best (tip) block in the longest blockchain getblock Supported Returns information about a block from the block hash getblockbyheight Supported Returns information about a block from the block height getblockchaininfo Supported Returns state information about blockchain processing getblockhash Supported Returns hash of block in best-block-chain at height getblockheader Supported Returns information about block header from hash getdifficulty Supported Returns the proof-of-work difficulty as a multiple of the minimum difficulty getinfo Supported Returns general information about the node and blockchain getmininginfo Supported Returns mining-related information getpeerinfo Supported Returns data about each connected network node getrawtransaction Supported Returns raw transaction data getminingcandidate Supported Returns data needed to construct a block to work on invalidateblock Supported Permanently marks a block as invalid isbanned Supported Checks if a network address is currently banned reassign Supported Reassigns ownership of a specific UTXO to a new Bitcoin address reconsiderblock Supported Removes invalidity status of a block sendrawtransaction Supported Submits raw transaction to local node and network setban Supported Attempts to add or remove an IP/Subnet from the banned list stop Supported Stops the node submitminingsolution Supported Submits a mining solution to the network unfreeze Supported Unfreezes a previously frozen UTXO, allowing it to be spent version Supported Returns version information about the server"},{"location":"topics/services/rpc/#unimplemented-rpc-commands","title":"Unimplemented RPC Commands","text":"RPC Command Status Description addnode Unimplemented Attempts to add or remove a node from the addnode list debuglevel Unimplemented Changes the debug level of the server decoderawtransaction Unimplemented Returns a JSON object representing the serialized transaction decodescript Unimplemented Decodes a hex-encoded script estimatefee Unimplemented Estimates the fee per kilobyte for a transaction getaddednodeinfo Unimplemented Returns information about added nodes getbestblock Unimplemented Returns the height and hash of the best block getblockcount Unimplemented Returns the number of blocks in the longest blockchain getblocktemplate Unimplemented Returns data needed to construct a block to work on getcfilter Unimplemented Returns a compact filter for a block getcfilterheader Unimplemented Returns the filter header of a block getconnectioncount Unimplemented Returns the number of connections to other nodes getcurrentnet Unimplemented Returns the name of the current network getgenerate Unimplemented Returns if the server is set to generate coins gethashespersec Unimplemented Returns a recent hashes per second performance measurement getheaders Unimplemented Returns block headers starting from a hash getmempoolinfo Unimplemented Returns information about the node's current transaction memory pool getnettotals Unimplemented Returns information about network traffic getnetworkhashps Unimplemented Returns the estimated network hashes per second getrawmempool Unimplemented Returns all transaction ids in memory pool gettxout Unimplemented Returns details about an unspent transaction output gettxoutproof Unimplemented Returns a hex-encoded proof that a transaction was included in a block help Unimplemented Lists all available commands, or gets help for a specified command node Unimplemented Attempts to add or remove a node from the addnode list ping Unimplemented Queues a ping to be sent to all connected peers searchrawtransactions Unimplemented Returns raw transactions matching given criteria setgenerate Unimplemented Sets if the server should generate coins submitblock Unimplemented Attempts to submit a new block to the network uptime Unimplemented Returns the total uptime of the server validateaddress Unimplemented Returns information about the given bitcoin address verifychain Unimplemented Verifies blockchain database verifymessage Unimplemented Verifies a signed message verifytxoutproof Unimplemented Verifies that a proof points to a transaction in a block"},{"location":"topics/services/rpc/#command-help","title":"Command help","text":"<p>A description of the commands can be found in the <code>rpcserverhelp.go</code> file in the <code>bsvd</code> repository:</p> <ul> <li>rpcserverhelp.go</li> </ul> <p>Teranode RPC server is designed to be compatible with the Bitcoin RPC interface, as implemented in the <code>bsvd</code> repository.</p>"},{"location":"topics/services/rpc/#authentication","title":"Authentication","text":"<p>All RPC commands require a valid username and password for authentication. The server listens on a specified port for incoming requests and processes them accordingly. The server could be opened up only for local (within the node) access, or it could be exposed to the public internet, depending on the deployment requirements. In either case, authentication is required to access the RPC server.</p>"},{"location":"topics/services/rpc/#grpc-api-key-authentication","title":"GRPC API Key Authentication","text":"<p>For GRPC services, certain administrative operations require additional API key authentication. The following methods require an API key:</p> <ul> <li>Ban/Unban Operations: <code>BanPeer</code> and <code>UnbanPeer</code> methods in both P2P and Legacy services require API key authentication for security.</li> </ul> <p>The API key is configured via the <code>grpc_admin_api_key</code> setting. If no API key is provided, the system will automatically generate a random 32-byte API key at startup, which will be logged for reference. The API key must be included in GRPC requests as metadata with the key <code>x-api-key</code>.</p>"},{"location":"topics/services/rpc/#2-architecture","title":"2. Architecture","text":"<p>The RPC server is a standalone service that listens for incoming requests and processes them based on the command type. The server starts by initializing the HTTP server and setting up the necessary configurations.</p> <p>It then listens for incoming requests and routes them to the appropriate handler based on the command type. The handler processes the request, executes the command, and returns the response to the client.</p> <p>In order to serve some of the requests, the RPC server interacts with the Teranode core services to fetch the required data.</p> <p>For example, when a <code>getblock</code> command is received, the server interacts with the blockchain service to fetch the block data.</p> <p>Also, when a <code>generate</code> command is received, the server interacts with the miner service to generate the requested number of blocks.</p>"},{"location":"topics/services/rpc/#3-functionality","title":"3. Functionality","text":""},{"location":"topics/services/rpc/#31-rpc-service-initialization-and-configuration","title":"3.1. RPC Service Initialization and Configuration","text":""},{"location":"topics/services/rpc/#32-command-create-raw-transaction","title":"3.2. Command: Create Raw Transaction","text":"<p>The <code>createrawtransaction</code> RPC method is used in Bitcoin to manually create a raw transaction. This transaction is not broadcast to the network but returned as a hex-encoded string. The created transaction could then be further modified, signed, and eventually broadcast using other RPC commands.</p> <p>The CreateRawTransaction method constructs a transaction that spends a given set of inputs and sends the outputs to specified addresses. It requires specific parameters about the inputs (which UTXOs to spend) and outputs (where to send the coins).</p> <p></p>"},{"location":"topics/services/rpc/#input-parameters","title":"Input Parameters","text":"<ul> <li>Inputs: A list of transaction inputs including the transaction ID (<code>txid</code>), output index (<code>vout</code>), and a sequence number if applicable.</li> <li>Amounts: A dictionary where each key is a Bitcoin address and the value is the amount of bitcoin to send to that address.</li> <li>LockTime (optional): Specifies the earliest time or block number that this transaction can be included in the blockchain.</li> </ul>"},{"location":"topics/services/rpc/#steps","title":"Steps","text":"<ol> <li> <p>Validate LockTime: Checks if the provided <code>LockTime</code> is within the valid range.</p> </li> <li> <p>Create Transaction: Initializes a new transaction (<code>mtx</code>).</p> </li> <li> <p>Process Inputs:</p> <ul> <li>For each input, it validates the transaction ID and constructs the transaction input structure.</li> <li>If a <code>LockTime</code> is set and not zero, it adjusts the sequence number to allow for the lock time to be effective.</li> </ul> </li> <li> <p>Process Outputs:</p> <ul> <li>Validates the amount for each output to ensure it's within the valid monetary range.</li> <li>Validates each output address, ensuring it's a supported type and appropriate for the network.</li> <li>Creates a payment script for each address and constructs the transaction output.</li> </ul> </li> <li> <p>Set Transaction LockTime: If provided, sets the transaction's lock time.</p> </li> <li> <p>Serialize Transaction: Converts the transaction to a hex-encoded string for output.</p> </li> </ol>"},{"location":"topics/services/rpc/#outputs","title":"Outputs","text":"<ul> <li>Success: Returns a hex-encoded string representing the raw transaction.</li> <li>Error: Returns an error if there are issues with the inputs, outputs, lock time, address decoding, or serialization.</li> </ul>"},{"location":"topics/services/rpc/#33-command-freeze","title":"3.3. Command: Freeze","text":"<p>The <code>freeze</code> command allows administrators to freeze a specific UTXO, preventing it from being spent in future transactions.</p>"},{"location":"topics/services/rpc/#function-overview","title":"Function Overview","text":"<ul> <li> <p>Purpose: To mark a specific transaction output (UTXO) as frozen, making it unavailable for spending</p> </li> <li> <p>Parameters:</p> <ul> <li><code>txid</code> (string, required): The transaction ID of the output to freeze</li> <li><code>vout</code> (numeric, required): The output index to freeze</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns <code>true</code> indicating the UTXO was successfully frozen</li> <li>On failure: Returns an error if the UTXO cannot be found or frozen</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with transaction ID and output index</li> <li>Validates input parameters</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>UTXO Validation:</p> <ul> <li>Checks if the specified UTXO exists</li> <li>Validates that the UTXO is not already frozen</li> </ul> </li> <li> <p>Freezing Operation:</p> <ul> <li>Marks the UTXO as frozen in the UTXO state database</li> <li>Records the freezing action for auditing purposes</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns <code>true</code> on successful freeze operation</li> <li>Returns error if validation fails or database update fails</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields","title":"(Success) Response Fields","text":"<ul> <li>Returns boolean <code>true</code> on successful freeze operation</li> </ul>"},{"location":"topics/services/rpc/#important-notes","title":"Important Notes","text":"<ul> <li>Freezing a UTXO is an administrative function that should be used with caution</li> <li>This operation is reversible using the <code>unfreeze</code> command</li> <li>Frozen UTXOs will be rejected if included in transaction inputs</li> </ul>"},{"location":"topics/services/rpc/#34-command-generate","title":"3.4. Command: Generate","text":"<p>The <code>generate</code> command is used to generate a specified number of blocks on the blockchain. The RPC server processes this command by interacting with the blockchain service to create the requested number of blocks.</p> <p>This command is commonly used in testing and development environments to artificially advance the blockchain by generating blocks immediately, rather than waiting for them to be mined in the usual way.</p> <p></p> <p>The function accepts a <code>GenerateCmd</code> which contains the number of blocks to generate, and sends an HTTP GET request to the miner's URL (e.g. <code>http://localhost:${MINER_HTTP_PORT}/mine?blocks=${numberOfBlocksToGenerate}</code>) to trigger block generation.</p> <ul> <li>Example Use:</li> </ul> <pre><code>// Sample usage within an RPC server setup\ncommand := &amp;bsvjson.GenerateCmd{NumBlocks: 10}\nresult, err := handleGenerate(rpcServerInstance, command, closeChannel)\nif err != nil {\n    log.Fatalf(\"Failed to generate blocks: %v\", err)\n}\n</code></pre> <ul> <li> <p>Settings: It requires a valid <code>MINER_HTTP_PORT</code> setting.</p> </li> <li> <p>Considerations:</p> <ul> <li>This function should not be exposed in production environments as it allows the generation of blocks outside of the normal consensus rules, which can be exploited or lead to unintended forks if used maliciously.</li> <li>Ensure the miner service is secured and only accessible by the RPC server to prevent unauthorized block generation.</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#success-response-fields_1","title":"(Success) Response Fields","text":"<ul> <li>Returns nil on success</li> </ul>"},{"location":"topics/services/rpc/#35-command-generate-to-address","title":"3.5. Command: Generate to Address","text":"<p>The <code>generatetoaddress</code> command mines blocks immediately to a specified address. This command is primarily used for testing purposes.</p>"},{"location":"topics/services/rpc/#function-overview_1","title":"Function Overview","text":"<ul> <li> <p>Purpose: To generate a specified number of blocks with coinbase rewards sent to a designated address</p> </li> <li> <p>Parameters:</p> <ul> <li><code>nblocks</code> (numeric, required): Number of blocks to generate</li> <li><code>address</code> (string, required): The address to send the newly generated bitcoin to</li> <li><code>maxtries</code> (numeric, optional): Maximum number of iterations to try</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns an array of block hashes that were generated</li> <li>On failure: Returns an error describing what went wrong</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_1","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with number of blocks, destination address, and optional max tries</li> <li>Validates input parameters</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>Address Validation:</p> <ul> <li>Validates the destination address format and network compatibility</li> <li>Creates a payment script for the address</li> </ul> </li> <li> <p>Block Generation Setup:</p> <ul> <li>Prepares the mining configuration with the specified address</li> <li>Sets up the maximum number of hash attempts (tries)</li> </ul> </li> <li> <p>Block Generation Loop:</p> <ul> <li> <p>For each requested block:</p> <ul> <li>Creates a new block template with the address script</li> <li>Performs proof-of-work calculation</li> <li>Processes and validates the new block</li> <li>Adds the block to the blockchain</li> </ul> </li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Collects the hashes of all successfully generated blocks</li> <li>Returns the array of block hashes</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_2","title":"(Success) Response Fields","text":"<ul> <li>Returns an array of strings, each containing the hex-encoded hash of a generated block</li> </ul>"},{"location":"topics/services/rpc/#important-notes_1","title":"Important Notes","text":"<ul> <li>This command is for testing purposes only and should not be used in production environments</li> <li>It bypasses normal mining difficulty by forcing block creation</li> <li>Can be used to test blockchain functionality or advance the chain state rapidly for testing</li> <li>All generated blocks will send the block reward to the specified address</li> </ul>"},{"location":"topics/services/rpc/#36-command-get-best-block-hash","title":"3.6. Command: Get Best Block Hash","text":"<p>The <code>getbestblockhash</code> command is used to retrieve the hash of the best (most recent) block on the blockchain. The RPC server processes this command by interacting with the blockchain service to fetch the hash of the best block.</p> <p></p>"},{"location":"topics/services/rpc/#success-response-fields_3","title":"(Success) Response Fields","text":"<ul> <li>hash: The hex-encoded hash of the best block in the main chain</li> </ul>"},{"location":"topics/services/rpc/#37-command-get-block","title":"3.7. Command: Get Block","text":"<p>The <code>getblock</code> command is used to retrieve information about a specific block on the blockchain. The RPC server processes this command by interacting with the blockchain service to fetch the block data based on the provided block hash.</p> <p></p>"},{"location":"topics/services/rpc/#success-response-fields_4","title":"(Success) Response Fields","text":"<p>When verbosity=0:</p> <ul> <li>Returns hex-encoded serialized block data</li> </ul> <p>When verbosity=1 or 2:</p> <ul> <li>hash: The block hash (same as provided)</li> <li>confirmations: Number of confirmations</li> <li>size: The block size in bytes</li> <li>height: The block height</li> <li>version: The block version</li> <li>versionHex: The block version in hexadecimal</li> <li>merkleroot: Root hash of the merkle tree</li> <li>time: Block time in Unix epoch</li> <li>nonce: The block nonce</li> <li>bits: The bits which represent the block difficulty</li> <li>difficulty: The proof-of-work difficulty</li> <li>previousblockhash: Hash of the previous block</li> <li>nextblockhash: Hash of the next block (if available)</li> </ul>"},{"location":"topics/services/rpc/#38-command-get-block-by-height","title":"3.8. Command: Get Block By Height","text":"<p>The <code>getblockbyheight</code> command returns information about a block at a specific height in the blockchain. The response format varies based on the verbosity parameter.</p>"},{"location":"topics/services/rpc/#function-overview_2","title":"Function Overview","text":"<ul> <li> <p>Purpose: To retrieve block information using block height instead of block hash</p> </li> <li> <p>Parameters:</p> <ul> <li><code>height</code>: The height of the block to retrieve</li> <li><code>verbosity</code>: Determines the format and detail level of the returned data<ul> <li>0: Returns serialized, hex-encoded block data</li> <li>1: Returns JSON object with block information</li> <li>2: Returns JSON object with block information including detailed transaction data</li> </ul> </li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns block data in the format specified by verbosity</li> <li>On failure: Returns an error if the block cannot be found or retrieved</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_2","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with block height and verbosity level</li> <li>Validates input parameters</li> </ul> </li> <li> <p>Block Retrieval:</p> <ul> <li>Queries blockchain service to get block at specified height</li> <li>If block not found, returns appropriate error</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li> <p>For verbosity=0:</p> <ul> <li>Returns hex-encoded serialized block data</li> </ul> </li> <li> <p>For verbosity=1:</p> <ul> <li>Returns JSON object with block header information and transaction IDs</li> </ul> </li> <li> <p>For verbosity=2:</p> <ul> <li>Returns JSON object with complete block information including full transaction data</li> </ul> </li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_5","title":"(Success) Response Fields","text":"<p>When verbosity=0:</p> <ul> <li>Returns hex-encoded serialized block data</li> </ul> <p>When verbosity=1 or 2:</p> <ul> <li>hash: The block hash</li> <li>confirmations: Number of confirmations</li> <li>size: Block size in bytes</li> <li>height: Block height</li> <li>version: Block version</li> <li>versionHex: Block version in hexadecimal</li> <li>merkleroot: Merkle root hash</li> <li>time: Block timestamp</li> <li>nonce: Block nonce</li> <li>bits: Target difficulty bits</li> <li>difficulty: Calculated difficulty</li> <li>previousblockhash: Hash of previous block</li> <li>nextblockhash: Hash of next block (if available)</li> <li>tx: Array of transaction IDs (verbosity=1) or full transaction data (verbosity=2)</li> </ul>"},{"location":"topics/services/rpc/#39-command-get-blockchain-info","title":"3.9. Command: Get Blockchain Info","text":"<p>The <code>getblockchaininfo</code> command returns information about the current blockchain state, including network name, block count, and other blockchain-related data.</p>"},{"location":"topics/services/rpc/#function-overview_3","title":"Function Overview","text":"<ul> <li> <p>Purpose: To retrieve comprehensive information about the current state of the blockchain.</p> </li> <li> <p>Parameters: None</p> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns a JSON object containing blockchain state information</li> <li>On failure: Returns an error describing what went wrong</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_3","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request for blockchain information</li> <li>No parameters to validate</li> </ul> </li> <li> <p>Blockchain Query:</p> <ul> <li>Retrieves the best block header and metadata from the blockchain service</li> <li>If retrieval fails, returns an error</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li> <p>Constructs response object containing:</p> <ul> <li>Chain name (main, test, regtest)</li> <li>Current block count</li> <li>Header count</li> <li>Best block hash</li> <li>Current difficulty</li> <li>Chain work</li> <li>Additional metadata (pruning status, soft fork information)</li> </ul> </li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_6","title":"(Success) Response Fields","text":"<ul> <li>chain: The name of the blockchain network (main, test, regtest)</li> <li>blocks: The current number of blocks in the chain</li> <li>headers: Number of headers in the chain (currently hardcoded to 863341)</li> <li>bestblockhash: Hash of the current best block</li> <li>difficulty: Current mining difficulty</li> <li>mediantime: Median time of the chain (currently 0)</li> <li>verificationprogress: Chain verification progress (currently 0)</li> <li>chainwork: Total accumulated work in the chain</li> <li>pruned: Whether the node is running in pruned mode (currently false)</li> <li>softforks: Array of active soft forks (currently empty)</li> </ul>"},{"location":"topics/services/rpc/#310-command-get-block-hash","title":"3.10. Command: Get Block Hash","text":"<p>The <code>getblockhash</code> command returns the hash of a block at a specific height in the blockchain.</p>"},{"location":"topics/services/rpc/#function-overview_4","title":"Function Overview","text":"<ul> <li> <p>Purpose: To retrieve the hash of a block at a given block height</p> </li> <li> <p>Parameters:</p> <ul> <li><code>index</code>: The height of the block in the blockchain</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns the block hash as a string</li> <li>On failure: Returns an error if the block cannot be found</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_4","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with block height (index)</li> <li>Validates input parameter</li> </ul> </li> <li> <p>Block Retrieval:</p> <ul> <li>Queries blockchain service to get block at specified height</li> <li>Extracts hash from retrieved block</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns the block hash as a string</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_7","title":"(Success) Response Fields","text":"<ul> <li>Returns string containing the hex-encoded hash of the block at the specified height</li> </ul>"},{"location":"topics/services/rpc/#311-command-get-block-header","title":"3.11. Command: Get Block Header","text":"<p>The <code>getblockheader</code> command returns information about a block's header given its hash. The response format varies based on the verbose parameter.</p>"},{"location":"topics/services/rpc/#function-overview_5","title":"Function Overview","text":"<ul> <li> <p>Purpose: To retrieve block header information using block hash</p> </li> <li> <p>Parameters:</p> <ul> <li><code>hash</code>: The hash of the block</li> <li><code>verbose</code>: Boolean determining the format of the returned data<ul> <li>false: Returns serialized, hex-encoded header data</li> <li>true: Returns JSON object with detailed header information</li> </ul> </li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns header data in the format specified by verbose parameter</li> <li>On failure: Returns an error if the block cannot be found or hash is invalid</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_5","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with block hash and verbose flag</li> <li>Validates block hash format</li> </ul> </li> <li> <p>Header Retrieval:</p> <ul> <li>Queries blockchain service to get block header and metadata</li> <li>If block not found, returns appropriate error</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li> <p>For verbose=false:</p> <ul> <li>Returns hex-encoded serialized header data</li> </ul> </li> <li> <p>For verbose=true:</p> <ul> <li>Calculates difficulty from bits</li> <li>Returns JSON object with detailed header information</li> </ul> </li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_8","title":"(Success) Response Fields","text":"<p>When verbose=false:</p> <ul> <li>Returns hex-encoded serialized header data</li> </ul> <p>When verbose=true:</p> <ul> <li>hash: The block hash (same as provided)</li> <li>version: Block version</li> <li>versionHex: Block version in hexadecimal</li> <li>previoushash: Hash of the previous block</li> <li>merkleroot: Root hash of the merkle tree</li> <li>time: Block timestamp</li> <li>nonce: Block nonce value</li> <li>bits: Target difficulty bits</li> <li>difficulty: Calculated difficulty</li> <li>height: Height of the block in the blockchain</li> </ul>"},{"location":"topics/services/rpc/#312-command-get-difficulty","title":"3.12. Command: Get Difficulty","text":"<p>The <code>getdifficulty</code> command returns the proof-of-work difficulty as a multiple of the minimum difficulty.</p>"},{"location":"topics/services/rpc/#function-overview_6","title":"Function Overview","text":"<ul> <li> <p>Purpose: To retrieve the current mining difficulty of the network</p> </li> <li> <p>Parameters: None</p> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns the current difficulty as a floating-point number</li> <li>On failure: Returns an error if the difficulty cannot be retrieved</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_6","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request for current difficulty</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>Difficulty Retrieval:</p> <ul> <li>Queries block assembly service for current difficulty</li> <li>If retrieval fails, returns appropriate error</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns the difficulty value directly to the client</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#313-command-get-info","title":"3.13. Command: Get Info","text":"<p>The <code>getinfo</code> command returns general information about the node's state, including version information, network status, and blockchain details.</p>"},{"location":"topics/services/rpc/#function-overview_7","title":"Function Overview","text":"<ul> <li> <p>Purpose: To retrieve general information about the node's current state and configuration.</p> </li> <li> <p>Parameters: None</p> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns a JSON object containing node state information</li> <li>On failure: Returns an error if blockchain height cannot be retrieved</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_7","title":"Process Flow","text":"<ol> <li> <p>Height Retrieval:</p> <ul> <li>Queries blockchain service for current best height and time</li> <li>If query fails, sets height to 0 and logs error</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li> <p>Builds response containing:</p> <ul> <li>Server version</li> <li>Protocol version</li> <li>Current block height</li> <li>Network time offset</li> <li>Connection count</li> <li>Network type (testnet/mainnet/stn)</li> <li>Minimum relay fee</li> </ul> </li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_9","title":"(Success) Response Fields","text":"<ul> <li>version: Server version (currently 1)</li> <li>protocolversion: Protocol version being used</li> <li>blocks: Current block height</li> <li>timeoffset: Network time offset (currently 1)</li> <li>connections: Number of peer connections (currently 1)</li> <li>proxy: Proxy being used, if any (currently \"host:port\")</li> <li>difficulty: Current mining difficulty (currently 1)</li> <li>testnet: Whether running on testnet</li> <li>stn: Whether running on the Scaling Test Network</li> <li>relayfee: Minimum relay fee for transactions (currently 100 sat/KB)</li> </ul>"},{"location":"topics/services/rpc/#314-command-get-mining-info","title":"3.14. Command: Get Mining Info","text":"<p>The <code>getmininginfo</code> command returns various mining-related information including block chain state, current block statistics, and network mining parameters.</p>"},{"location":"topics/services/rpc/#function-overview_8","title":"Function Overview","text":"<ul> <li> <p>Purpose: To retrieve comprehensive information about the current mining state</p> </li> <li> <p>Parameters: None</p> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns a JSON object containing mining-related information</li> <li>On failure: Returns an error if the mining information cannot be retrieved</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_8","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request for mining information</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>Data Retrieval:</p> <ul> <li>Queries blockchain service for best block header and metadata</li> <li>Calculates network hash rate based on difficulty and target block time</li> <li>If retrieval fails, returns appropriate error</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Compiles mining information into a structured response</li> <li>Returns comprehensive mining state data</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_10","title":"(Success) Response Fields","text":"<ul> <li>blocks: The current block height</li> <li>currentblocksize: Size of the latest best block in bytes</li> <li>currentblocktx: Number of transactions in the latest best block</li> <li>difficulty: Current mining difficulty</li> <li>errors: Current network error messages (empty string if none)</li> <li>networkhashps: Estimated network hashes per second</li> <li>chain: Current network name (main, test, regtest)</li> </ul>"},{"location":"topics/services/rpc/#315-command-get-mining-candidate","title":"3.15. Command: Get Mining Candidate","text":"<p>The <code>getminingcandidate</code> RPC command in Bitcoin RPC is used to retrieve a candidate block for mining. This command allows miners to obtain the necessary information to attempt mining a new block.</p>"},{"location":"topics/services/rpc/#function-overview_9","title":"Function Overview","text":"<ul> <li> <p>Purpose: To request a mining candidate, which represents a potential new block template, from the node.</p> </li> <li> <p>Parameters:</p> <ul> <li>The function doesn't take any specific parameters from the RPC call.</li> <li><code>closeChan</code>: A channel that signals the function to close and stop processing, used for graceful shutdowns and interruption handling (not directly used in this implementation).</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns a mining candidate object (<code>mc</code>) containing the necessary information for mining.</li> <li>On failure: Returns an error detailing why the mining candidate could not be retrieved.</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_9","title":"Process Flow","text":"<ol> <li> <p>Mining Candidate Retrieval:</p> <ul> <li>Calls the <code>GetMiningCandidate</code> method on the <code>blockAssemblyClient</code> to retrieve a mining candidate.</li> <li>The Block Assembly generates the latest block template from the blockchain's current state.</li> </ul> </li> <li> <p>Error Handling:</p> <ul> <li>If the retrieval fails, the function returns the error to the caller.</li> </ul> </li> <li> <p>Response:</p> <ul> <li>If successful, the function returns the mining candidate object (<code>mc</code>).</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_11","title":"(Success) Response Fields","text":"<ul> <li>id: Hex-encoded mining candidate ID</li> <li>prevhash: Previous block hash</li> <li>coinbaseValue: Value for coinbase transaction</li> <li>version: Block version</li> <li>nBits: Encoded current difficulty target</li> <li>time: Current timestamp</li> <li>height: Block height</li> <li>num_tx: Number of transactions</li> <li>sizeWithoutCoinbase: Block size excluding coinbase</li> <li>merkleProof: Array of merkle proof hashes</li> <li>coinbase: Hex-encoded coinbase transaction (if requested)</li> </ul>"},{"location":"topics/services/rpc/#316-command-get-peer-info","title":"3.16. Command: Get Peer Info","text":"<p>The <code>getpeerinfo</code> command returns data about each connected network peer as an array of JSON objects.</p> <ul> <li>Purpose: To retrieve detailed information about all connected peers, both legacy and new P2P connections.</li> <li>Parameters: None</li> <li> <p>Return Value:</p> <ul> <li>On success: Returns an array of peer information objects</li> <li>On failure: Returns partial information if at least one peer service responds</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_10","title":"Process Flow","text":"<ol> <li> <p>Legacy Peer Information:</p> <ul> <li>Queries legacy peer service for connected peer information</li> <li>If query fails, logs as non-critical error</li> <li>Processes legacy peer data if available</li> </ul> </li> <li> <p>P2P Information:</p> <ul> <li>Queries new P2P service for connected peer information</li> <li>If query fails, logs as error</li> <li>Processes P2P data if available</li> </ul> </li> <li> <p>Information Aggregation:</p> <ul> <li>Combines information from both sources</li> <li>Creates unified peer information objects</li> <li>Includes connection details, network stats, and state information</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns array of peer information objects</li> <li>Each object contains available peer details based on connection type</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_12","title":"(Success) Response Fields","text":"<p>For each peer:</p> <ul> <li>id: Peer ID</li> <li>addr: The IP address and port of the peer</li> <li>addrlocal: Local address (legacy peers only)</li> <li>servicesStr: Services supported by the peer</li> <li>lastsend: Time of last sent message (legacy peers only)</li> <li>lastrecv: Time of last received message (legacy peers only)</li> <li>bytessent: Total bytes sent (legacy peers only)</li> <li>bytesrecv: Total bytes received (legacy peers only)</li> <li>conntime: Connection time (legacy peers only)</li> <li>pingtime: Ping time in seconds (legacy peers only)</li> <li>timeoffset: Time offset with peer (legacy peers only)</li> <li>version: Protocol version (legacy peers only)</li> <li>subver: Peer subversion string (legacy peers only)</li> <li>inbound: Whether connection is inbound (legacy peers only)</li> <li>startingheight: Starting height of peer (legacy peers only)</li> <li>currentheight: Current height of peer (legacy peers only)</li> <li>banscore: Ban score (legacy peers only)</li> <li>whitelisted: Whether peer is whitelisted (legacy peers only)</li> <li>feefilter: Minimum fee rate for transactions to be announced (legacy peers only)</li> </ul> <p>The command aggregates peer information from both the legacy P2P service and the new P2P service, providing a comprehensive view of all connected peers.</p>"},{"location":"topics/services/rpc/#317-command-get-raw-transaction","title":"3.17. Command: Get Raw Transaction","text":"<p>The <code>getrawtransaction</code> command retrieves raw transaction data for a specific transaction, either as a serialized hex string or as a detailed JSON object.</p>"},{"location":"topics/services/rpc/#function-overview_10","title":"Function Overview","text":"<ul> <li> <p>Purpose: To retrieve transaction data for a specific transaction identified by its transaction ID</p> </li> <li> <p>Parameters:</p> <ul> <li><code>txid</code> (string, required): The transaction ID to look up</li> <li><code>verbose</code> (boolean, optional, default=false): If false, returns a hex-encoded string. If true, returns a JSON object with transaction details</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>If verbose=false: Returns a serialized, hex-encoded string of the transaction</li> <li>If verbose=true: Returns a JSON object with detailed transaction information</li> <li>On failure: Returns an error if the transaction cannot be found</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_11","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with transaction ID and verbosity flag</li> <li>Validates transaction ID format</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>Transaction Retrieval:</p> <ul> <li>Queries blockchain service to locate the transaction</li> <li>If transaction not found, returns appropriate error</li> </ul> </li> <li> <p>Response Formatting:</p> <ul> <li> <p>For non-verbose (false) response:</p> <ul> <li>Returns hex-encoded serialized transaction data</li> </ul> </li> <li> <p>For verbose (true) response:</p> <ul> <li>Constructs detailed JSON object with transaction information</li> <li>Includes metadata such as blockhash, confirmation count, and timestamp</li> <li>Processes all inputs and outputs with script details</li> </ul> </li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns the transaction data in the requested format</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_13","title":"(Success) Response Fields","text":"<p>When verbose=false:</p> <ul> <li>Returns a string containing the hex-encoded serialized transaction</li> </ul> <p>When verbose=true:</p> <ul> <li>hex: The serialized, hex-encoded transaction data</li> <li>txid: The transaction ID (same as requested)</li> <li>hash: The transaction hash (usually same as txid)</li> <li>size: The transaction size in bytes</li> <li>version: The transaction version number</li> <li>locktime: The transaction's locktime</li> <li>vin: Array of transaction inputs<ul> <li>txid: Transaction ID of the input</li> <li>vout: Output index of the input</li> <li>scriptSig: Script signature<ul> <li>asm: Assembly representation of the script</li> <li>hex: Hex-encoded script</li> </ul> </li> <li>sequence: Input sequence number</li> </ul> </li> <li>vout: Array of transaction outputs<ul> <li>value: The output value in BTC</li> <li>n: The output index number</li> <li>scriptPubKey: The output script<ul> <li>asm: Assembly representation of the script</li> <li>hex: Hex-encoded script</li> <li>reqSigs: Required signatures</li> <li>type: Script type</li> <li>addresses: Array of Bitcoin addresses</li> </ul> </li> </ul> </li> <li>blockhash: The block hash containing this transaction</li> <li>confirmations: Number of confirmations</li> <li>time: Block time</li> <li>blocktime: Block time in seconds since epoch</li> </ul>"},{"location":"topics/services/rpc/#318-command-invalidate-block","title":"3.18. Command: Invalidate Block","text":"<p>The <code>invalidateblock</code> command permanently marks a block as invalid, as if it violated a consensus rule.</p>"},{"location":"topics/services/rpc/#function-overview_11","title":"Function Overview","text":"<ul> <li> <p>Purpose: To manually invalidate a block in the blockchain</p> </li> <li> <p>Parameters:</p> <ul> <li><code>blockhash</code>: The hash of the block to invalidate</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns nil, indicating the block was successfully invalidated</li> <li>On failure: Returns an error if the block cannot be found or hash is invalid</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_12","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with block hash</li> <li>Validates block hash format</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>Block Invalidation:</p> <ul> <li>Sends invalidation request to blockchain service</li> <li>If invalidation fails, returns appropriate error</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns nil on successful invalidation</li> <li>Returns error if operation fails</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#important-notes_2","title":"Important Notes","text":"<ul> <li>This command should be used with extreme caution as it can cause chain reorganization</li> <li>Invalidating a block also invalidates all blocks built on top of it</li> </ul>"},{"location":"topics/services/rpc/#319-command-is-banned","title":"3.19. Command: Is Banned","text":"<p>The <code>isbanned</code> command checks if a specific network address is currently banned from connecting to the node.</p> <p>Note: This command accesses the underlying GRPC ban status methods which require API key authentication when accessed directly via GRPC. However, the RPC <code>isbanned</code> command handles this authentication internally.</p>"},{"location":"topics/services/rpc/#function-overview_12","title":"Function Overview","text":"<ul> <li> <p>Purpose: To determine if a network address is banned from node connections</p> </li> <li> <p>Parameters:</p> <ul> <li><code>address</code> (string, required): The network address to check, e.g. \"192.168.0.1\" or \"192.168.0.0/24\"</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns a boolean value - <code>true</code> if the address is banned, <code>false</code> if not</li> <li>On failure: Returns an error if the address format is invalid</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_13","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with network address</li> <li>Validates address format</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>Ban Status Check:</p> <ul> <li>Queries peer service to check if the address is in the banned list</li> <li>Retrieves current ban status and expiration time if banned</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns <code>true</code> if the address is currently banned</li> <li>Returns <code>false</code> if the address is not banned</li> <li>Returns error if address validation fails</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_14","title":"(Success) Response Fields","text":"<ul> <li>Returns boolean <code>true</code> if the address is banned</li> <li>Returns boolean <code>false</code> if the address is not banned</li> </ul>"},{"location":"topics/services/rpc/#important-notes_3","title":"Important Notes","text":"<ul> <li>Can be used to check both individual IP addresses and subnets</li> <li>Address bans may have an expiration time</li> <li>Works in conjunction with the <code>setban</code> command which is used to add or remove bans</li> </ul>"},{"location":"topics/services/rpc/#320-command-reassign","title":"3.20. Command: Reassign","text":"<p>The <code>reassign</code> command allows administrators to change the ownership of a specific UTXO by reassigning it to a new Bitcoin address.</p>"},{"location":"topics/services/rpc/#function-overview_13","title":"Function Overview","text":"<ul> <li> <p>Purpose: To reassign ownership of a specific transaction output (UTXO) to a new Bitcoin address</p> </li> <li> <p>Parameters:</p> <ul> <li><code>txid</code> (string, required): The transaction ID of the output to reassign</li> <li><code>vout</code> (numeric, required): The output index to reassign</li> <li><code>destination</code> (string, required): The Bitcoin address to reassign the UTXO to</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns <code>true</code> indicating the UTXO was successfully reassigned</li> <li>On failure: Returns an error if the UTXO cannot be found or reassigned</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_14","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with transaction ID, output index, and destination address</li> <li>Validates input parameters</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>UTXO and Address Validation:</p> <ul> <li>Checks if the specified UTXO exists</li> <li>Validates the destination address format and network compatibility</li> </ul> </li> <li> <p>Reassignment Operation:</p> <ul> <li>Creates a new scriptPubKey for the destination address</li> <li>Updates the UTXO record with the new ownership information</li> <li>Records the reassignment action for auditing purposes</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns <code>true</code> on successful reassignment operation</li> <li>Returns error if validation fails or database update fails</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_15","title":"(Success) Response Fields","text":"<ul> <li>Returns boolean <code>true</code> on successful reassignment operation</li> </ul>"},{"location":"topics/services/rpc/#important-notes_4","title":"Important Notes","text":"<ul> <li>Reassigning a UTXO changes its spending conditions without creating a new transaction</li> <li>This is an administrative function that bypasses normal Bitcoin transaction rules</li> <li>Should only be used in specific regulatory or recovery scenarios</li> <li>The operation creates a permanent record in the audit log</li> </ul>"},{"location":"topics/services/rpc/#321-command-reconsider-block","title":"3.21. Command: Reconsider Block","text":"<p>The <code>reconsiderblock</code> command removes the invalid status of a block and its descendants, allowing them to be reconsidered for inclusion in the blockchain. This command is typically used to undo the effects of <code>invalidateblock</code>.</p>"},{"location":"topics/services/rpc/#function-overview_14","title":"Function Overview","text":"<ul> <li> <p>Purpose: To remove invalid status from a previously invalidated block</p> </li> <li> <p>Parameters:</p> <ul> <li><code>blockhash</code>: The hash of the block to reconsider</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns nil, indicating the block was successfully reconsidered</li> <li>On failure: Returns an error if the block cannot be found or hash is invalid</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_15","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with block hash</li> <li>Validates block hash format</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>Block Reconsideration:</p> <ul> <li>Sends revalidation request to blockchain service</li> <li>If revalidation fails, returns appropriate error</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns nil on successful reconsideration</li> <li>Returns error if operation fails</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#important-notes_5","title":"Important Notes","text":"<ul> <li>This command removes the invalid status set by <code>invalidateblock</code></li> <li>The block and its descendants will be reconsidered for inclusion in the best chain</li> <li>May trigger chain reorganization if the reconsidered chain has more work</li> </ul>"},{"location":"topics/services/rpc/#322-command-send-raw-transaction","title":"3.22. Command: Send Raw Transaction","text":"<p>The <code>sendrawtransaction</code> RPC command in Bitcoin RPC is used to submit a pre-signed raw transaction to the network. This command broadcasts the raw transaction hex to the connected nodes in the blockchain network for inclusion in a block.</p>"},{"location":"topics/services/rpc/#function-overview_15","title":"Function Overview","text":"<ul> <li> <p>Purpose: To submit a raw, serialized, and hex-encoded transaction to the blockchain network.</p> </li> <li> <p>Parameters:</p> <ul> <li><code>cmd</code>: Contains the raw transaction data and any command-specific parameters.</li> <li><code>closeChan</code>: A channel that signals the function to close and stop processing, used for graceful shutdowns and interruption handling.</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns a result (e.g., transaction ID or confirmation message) indicating that the transaction was successfully broadcast.</li> <li>On failure: Returns an error detailing why the transaction could not be processed or broadcast.</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_16","title":"Process Flow","text":"<ol> <li> <p>Input Parsing:</p> <ul> <li>The function receives a command (<code>bsvjson.SendRawTransactionCmd</code>) which includes the hex-encoded string of the transaction.</li> <li>It checks if the hexadecimal string has an odd length and prepends a \"0\" if necessary to ensure correct decoding.</li> </ul> </li> <li> <p>Hex Decoding:</p> <ul> <li>Attempts to decode the hexadecimal string into bytes.</li> <li>If decoding fails, it returns an error using <code>rpcDecodeHexError</code>, indicating the hex string was malformed.</li> </ul> </li> <li> <p>Transaction Deserialization:</p> <ul> <li>Attempts to construct a transaction object from the decoded bytes using a transaction parsing library (assumed to be <code>bt.NewTxFromBytes</code>).</li> <li>If the transaction cannot be parsed, it returns an error stating the transaction is deserializable, indicating structural issues with the transaction data.</li> </ul> </li> <li> <p>Transaction Distribution Setup:</p> <ul> <li>Initializes a <code>Distributor</code> object responsible for handling the broadcasting of transactions to the Propagation servers.</li> <li>If the distributor cannot be created (due to configuration errors, connection issues, etc.), it returns an initialization error.</li> </ul> </li> <li> <p>Transaction Broadcasting:</p> <ul> <li>Calls a method (<code>d.SendTransaction</code>) on the distributor to send the transaction to the network.</li> <li>If broadcasting fails, it returns an error indicating that the transaction was rejected along with a message detailing the reason (e.g., network errors, validation failures on the network side).</li> </ul> </li> <li> <p>Success Response:</p> <ul> <li>If the transaction is successfully broadcast, the function returns a success response, which includes the transaction ID of the broadcast transaction</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#323-command-set-ban","title":"3.23. Command: Set Ban","text":"<p>The <code>setban</code> command adds or removes an IP address or subnet from the banned list. This command is used for network management and peer control.</p> <p>Note: The underlying GRPC ban methods (<code>BanPeer</code> and <code>UnbanPeer</code>) require API key authentication when accessed directly via GRPC. However, the RPC <code>setban</code> command handles this authentication internally.</p>"},{"location":"topics/services/rpc/#function-overview_16","title":"Function Overview","text":"<ul> <li> <p>Purpose: To manage banned IP addresses/subnets</p> </li> <li> <p>Parameters:</p> <ul> <li><code>ipOrSubnet</code>: The IP/Subnet to ban/unban (e.g., \"192.168.0.6\" or \"192.168.0.0/24\")</li> <li><code>command</code>: \"add\" to add a ban, \"remove\" to remove a ban</li> <li><code>bantime</code>: Time in seconds for how long the IP is banned (optional)</li> <li><code>absolute</code>: If true, bantime is interpreted as an absolute timestamp (optional)</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns nil</li> <li>On failure: Returns an error if parameters are invalid or operation fails</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_17","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with IP/subnet and command</li> <li>Validates input parameters</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>Ban List Management:</p> <ul> <li>Retrieves current ban list</li> <li>Processes add/remove command</li> <li>Updates ban list accordingly</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns nil on successful operation</li> <li>Returns error if operation fails</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#important-notes_6","title":"Important Notes","text":"<ul> <li> <p>For \"add\" command:</p> <ul> <li>If bantime is 0, defaults to 24 hours</li> <li>If absolute is true, bantime is treated as Unix timestamp</li> </ul> </li> <li> <p>The IP/subnet format must be valid</p> </li> <li>Removing a non-existent ban is considered an error</li> </ul>"},{"location":"topics/services/rpc/#324-command-stop","title":"3.24. Command: Stop","text":"<p>The <code>stop</code> command initiates a clean shutdown of the node, stopping all services in an orderly fashion.</p>"},{"location":"topics/services/rpc/#function-overview_17","title":"Function Overview","text":"<ul> <li> <p>Purpose: To safely shut down the node and all its services</p> </li> <li> <p>Parameters: None</p> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns a string message indicating the node is stopping</li> <li>The actual shutdown process happens asynchronously after the response is sent</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_18","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives stop request</li> <li>No parameters to validate</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>Shutdown Initiation:</p> <ul> <li>Logs shutdown request</li> <li>Sets internal shutdown flag</li> <li>Signals main process to initiate shutdown sequence</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns confirmation message that shutdown has been initiated</li> <li>Actual shutdown happens in the background after response is sent</li> </ul> </li> <li> <p>Shutdown Sequence (happens after response):</p> <ul> <li>Services are stopped in reverse order of initialization</li> <li>Connections are gracefully closed</li> <li>Data is persisted where necessary</li> <li>Process exits</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_16","title":"(Success) Response Fields","text":"<ul> <li>Returns string message: \"Bitcoin server stopping\"</li> </ul>"},{"location":"topics/services/rpc/#important-notes_7","title":"Important Notes","text":"<ul> <li>This is an administrative command that should be restricted to authorized users</li> <li>The node will complete the shutdown process even after sending the RPC response</li> <li>Any pending operations may be completed or aborted depending on their nature</li> <li>Clients should expect to lose connection to the node shortly after receiving the response</li> </ul>"},{"location":"topics/services/rpc/#325-command-submit-mining-solution","title":"3.25. Command: Submit Mining Solution","text":"<p>The <code>submitminingsolution</code> RPC command in Bitcoin RPC is used to submit a mining solution to the network. This command allows miners to propose a potential new block to be added to the blockchain.</p>"},{"location":"topics/services/rpc/#function-overview_18","title":"Function Overview","text":"<ul> <li> <p>Purpose: To submit a mining solution, which represents a potential new block, for validation and inclusion in the blockchain.</p> </li> <li> <p>Parameters:</p> <ul> <li><code>cmd</code>: Contains the JSON string of the mining solution and any command-specific parameters.</li> <li><code>closeChan</code>: A channel that signals the function to close and stop processing, used for graceful shutdowns and interruption handling.</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns <code>nil</code>, indicating that the mining solution was successfully submitted.</li> <li>On failure: Returns an error detailing why the mining solution could not be processed or submitted.</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_19","title":"Process Flow","text":"<ol> <li> <p>Command Parsing:</p> <ul> <li>The function receives a command (<code>bsvjson.SubmitMiningSolutionCmd</code>) which includes a JSON string representing the mining solution.</li> </ul> </li> <li> <p>JSON Unmarshaling:</p> <ul> <li>Attempts to unmarshal the JSON string into a <code>model.MiningSolution</code> struct.</li> <li>If unmarshaling fails, it returns an error, indicating that the JSON string was malformed or incompatible with the expected structure.</li> </ul> </li> <li> <p>Solution Submission:</p> <ul> <li>Calls the <code>SubmitMiningSolution</code> method on the <code>blockAssemblyClient</code> to submit the mining solution to the Block Assembly.</li> <li>The Block Assembly validates the solution and, if successful, propagates it to other nodes in the network.</li> <li>If submission fails, it returns an error indicating why the solution was rejected (e.g., invalid solution, network errors).</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_17","title":"(Success) Response Fields","text":"<ul> <li>Returns nil on success</li> </ul>"},{"location":"topics/services/rpc/#326-command-unfreeze","title":"3.26. Command: Unfreeze","text":"<p>The <code>unfreeze</code> command removes the freeze status from a previously frozen UTXO, allowing it to be spent again in transactions.</p>"},{"location":"topics/services/rpc/#function-overview_19","title":"Function Overview","text":"<ul> <li> <p>Purpose: To remove the frozen status from a specific transaction output (UTXO)</p> </li> <li> <p>Parameters:</p> <ul> <li><code>txid</code> (string, required): The transaction ID of the frozen output</li> <li><code>vout</code> (numeric, required): The output index to unfreeze</li> </ul> </li> <li> <p>Return Value:</p> <ul> <li>On success: Returns <code>true</code> indicating the UTXO was successfully unfrozen</li> <li>On failure: Returns an error if the UTXO cannot be found or unfrozen</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#process-flow_20","title":"Process Flow","text":"<ol> <li> <p>Request Processing:</p> <ul> <li>Receives request with transaction ID and output index</li> <li>Validates input parameters</li> <li>Initializes tracing and metrics</li> </ul> </li> <li> <p>UTXO Validation:</p> <ul> <li>Checks if the specified UTXO exists</li> <li>Validates that the UTXO is currently frozen</li> </ul> </li> <li> <p>Unfreezing Operation:</p> <ul> <li>Removes the frozen status from the UTXO in the state database</li> <li>Records the unfreezing action for auditing purposes</li> </ul> </li> <li> <p>Response Construction:</p> <ul> <li>Returns <code>true</code> on successful unfreeze operation</li> <li>Returns error if validation fails or database update fails</li> </ul> </li> </ol>"},{"location":"topics/services/rpc/#success-response-fields_18","title":"(Success) Response Fields","text":"<ul> <li>Returns boolean <code>true</code> on successful unfreeze operation</li> </ul>"},{"location":"topics/services/rpc/#important-notes_8","title":"Important Notes","text":"<ul> <li>Only previously frozen UTXOs can be unfrozen</li> <li>After unfreezing, the UTXO becomes available for spending in transactions</li> <li>This operation is an administrative function that should be used with appropriate authorization</li> </ul>"},{"location":"topics/services/rpc/#327-command-version","title":"3.27. Command: Version","text":"<p>The <code>version</code> command is used to retrieve the version information of the RPC server. The RPC server processes this command by constructing a response with the server version information and returning it to the client.</p> <p></p>"},{"location":"topics/services/rpc/#success-response-fields_19","title":"(Success) Response Fields","text":"<ul> <li>btcdjsonrpcapi: Object containing version information<ul> <li>versionString: Semantic version string</li> <li>major: Major version number</li> <li>minor: Minor version number</li> <li>patch: Patch version number</li> </ul> </li> </ul>"},{"location":"topics/services/rpc/#4-technology","title":"4. Technology","text":""},{"location":"topics/services/rpc/#http-server-and-restful-api","title":"HTTP Server and RESTful API","text":"<ul> <li>Usage: In <code>Server.go</code>, an HTTP server is set up to listen for requests and send responses.</li> <li>Functionality: Handling HTTP requests and responses, as well as routing, middleware support.</li> </ul>"},{"location":"topics/services/rpc/#authentication-and-security","title":"Authentication and Security","text":"<ul> <li>Basic Authentication: Handling basic HTTP authentication to secure server access.</li> </ul>"},{"location":"topics/services/rpc/#5-directory-structure-and-main-files","title":"5. Directory Structure and Main Files","text":"<p>The RPC service is located in the <code>services/rpc</code> directory. The main files and directories are as follows:</p> <pre><code>./servers/rpc\n\u251c\u2500\u2500 Server.go          # Main server application file: Initializes and runs the server, sets up configurations, and handles lifecycle events.\n\u2514\u2500\u2500 handlers.go        # Request handlers: Defines functions that process incoming requests based on type and content.\n</code></pre>"},{"location":"topics/services/rpc/#6-configuration-settings","title":"6. Configuration Settings","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the rpc Settings Reference.</p>"},{"location":"topics/services/rpc/#7-how-to-run","title":"7. How to run","text":"<p>To run the RPC Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -RPC=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the RPC Service locally.</p>"},{"location":"topics/services/rpc/#8-other-resources","title":"8. Other Resources","text":"<p>RPC Reference</p>"},{"location":"topics/services/subtreeValidation/","title":"\ud83d\udd0d Subtree Validation Service","text":""},{"location":"topics/services/subtreeValidation/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1. Receiving UTXOs and warming up the TXMeta Cache</li> <li>2.2. Receiving subtrees for validation</li> <li>2.3. Validating the Subtrees</li> <li>2.4. Subtree Locking Mechanism</li> <li>2.5. Distributed Pause Mechanism</li> </ul> </li> <li>gRPC Protobuf Definitions</li> <li>Data Model</li> <li>Technology</li> <li> <p>Directory Structure and Main Files</p> </li> <li> <p>How to Run</p> </li> <li>Configuration Settings</li> <li>Other Resources</li> </ol>"},{"location":"topics/services/subtreeValidation/#1-description","title":"1. Description","text":"<p>The Subtree Validator is responsible for ensuring the integrity and consistency of each received subtree before it is added to the subtree store. It performs several key functions:</p> <ol> <li> <p>Validation of Subtree Structure: Verifies that each received subtree adheres to the defined structure and format, and that its transactions are known and valid.</p> </li> <li> <p>Transaction Legitimacy: Ensures all transactions within subtrees are valid, including checks for double-spending.</p> </li> <li> <p>Decorates the Subtree with additional metadata: Adds metadata to the subtree, to facilitate faster block validation at a later stage (by the Block Validation Service).</p> <ul> <li>Specifically, the subtree metadata will contain all of the transaction input outpoints (TxInpoints). This decorated subtree can be validated and processed faster by the Block Validation Service, preventing unnecessary round trips to the UTXO Store.</li> </ul> </li> </ol> <p>Note: For information about how the Subtree Validation service is initialized during daemon startup and how it interacts with other services, see the Teranode Daemon Reference.</p> <p></p> <p>The Subtree Validation Service:</p> <ul> <li>Receives new subtrees from the P2P Service. The P2P Service has received them from other nodes on the network.</li> <li>Validates the subtrees, after fetching them from the remote asset server.</li> <li>Decorates the subtrees with additional metadata, and stores them in the Subtree Store.</li> </ul> <p>The P2P Service communicates with the Block Validation over either gRPC protocols.</p> <p></p> <p>The detailed component diagram below shows the internal architecture of the Subtree Validation Service with code-verified connections:</p> <p></p>"},{"location":"topics/services/subtreeValidation/#11-validator-integration","title":"1.1 Validator Integration","text":"<p>The Subtree Validation service interacts with the Validator service to validate transactions that might be missing during subtree processing. This interaction can happen in two different configurations:</p> <ol> <li> <p>Local Validator:</p> <ul> <li>When <code>validator.useLocalValidator=true</code> (recommended for production)</li> <li>The Validator is instantiated directly within the Subtree Validation service</li> <li>Direct method calls are used without network overhead</li> <li>This provides the best performance and lowest latency</li> </ul> </li> <li> <p>Remote Validator Service:</p> <ul> <li>When <code>validator.useLocalValidator=false</code></li> <li>The Subtree Validation service connects to a separate Validator service via gRPC</li> <li>Useful for development, testing, or specialized deployment scenarios</li> <li>Has higher latency due to additional network calls</li> </ul> </li> </ol> <p>This configuration is controlled by the settings passed to <code>GetValidatorClient()</code> in daemon.go.</p> <p>To improve performance, the Subtree Validation Service uses a caching mechanism for UTXO meta data (called <code>TX Meta Cache</code> for historical reasons). This prevents repeated fetch calls to the store by retaining recently loaded transactions in memory (for a limited time). This can be enabled or disabled via the <code>subtreevalidation_txMetaCacheEnabled</code> setting. The caching mechanism is implemented in the <code>txmetacache</code> package, and is used by the Subtree Validation Service:</p> <pre><code> // create a caching tx meta store\n    if gocore.Config().GetBool(\"subtreevalidation_txMetaCacheEnabled\", true) {\n        logger.Infof(\"Using cached version of tx meta store\")\n        u.utxoStore = txmetacache.NewTxMetaCache(ctx, ulogger.TestLogger{}, utxoStore)\n    } else {\n        u.utxoStore = utxoStore\n    }\n</code></pre> <p>If this caching mechanism is enabled, the Subtree Validation Service will listen to the <code>kafka_txmetaConfig</code> Kafka topic, where the Transaction Validator posts new UTXO meta data. This data is then stored in the cache for quick access during subtree validation.</p> <p>Finally, note that the Subtree Validation service benefits of the use of Lustre Fs (filesystem). Lustre is a type of parallel distributed file system, primarily used for large-scale cluster computing. This filesystem is designed to support high-performance, large-scale data storage and workloads. Specifically for Teranode, these volumes are meant to be temporary holding locations for short-lived file-based data that needs to be shared quickly between various services Teranode microservices make use of the Lustre file system in order to share subtree and tx data, eliminating the need for redundant propagation of subtrees over grpc or message queues. The services sharing Subtree data through this system can be seen here:</p> <p></p>"},{"location":"topics/services/subtreeValidation/#2-functionality","title":"2. Functionality","text":"<p>The subtree validator is a service that validates subtrees. After validating them, it will update the relevant stores accordingly.</p>"},{"location":"topics/services/subtreeValidation/#21-receiving-utxos-and-warming-up-the-txmeta-cache","title":"2.1. Receiving UTXOs and warming up the TXMeta Cache","text":"<ul> <li>The TX Validator service processes and validates new transactions.</li> <li>After validating transactions, The Tx Validator Service sends them (in UTXO Meta format) to the Subtree Validation Service via Kafka.</li> <li>The Subtree Validation Service stores these UTXO Meta Data in the Tx Meta Cache.</li> <li>At a later stage (next sections), the Subtree Validation Service will receive subtrees, composed of 1 million transactions. By having the Txs preloaded in a warmed up Tx Meta Cache, the Subtree Validation Service can quickly access the data required to validate the subtree.</li> </ul>"},{"location":"topics/services/subtreeValidation/#22-receiving-subtrees-for-validation","title":"2.2. Receiving subtrees for validation","text":"<ul> <li> <p>The P2P service is responsible for receiving new subtrees from the network. When a new subtree is found, it will notify the subtree validation service via the Kafka <code>kafka_subtreesConfig</code> producer.</p> </li> <li> <p>The subtree validation service will then check if the subtree is already known. If not, it will start the validation process.</p> </li> <li>Before validation, the service will \"lock\" the subtree, to avoid concurrent (and accidental) changes of the same subtree. To do this, the service will attempt to create a \"lock\" file in the shared subtree storage. If this succeeds, the subtree validation will then start.</li> <li>Once validated, we add it to the Subtree store, from where it will be retrieved later on (when a block using the subtrees gets validated).</li> </ul> <p>Receiving subtrees for validation via Kafka:</p> <p></p> <p>In addition to the P2P Service, the Block Validation service can also request for subtrees to be validated and added, together with its metadata, to the subtree store. Should the Block Validation service find, as part of the validation of a specific block, a subtree not known by the node, it can request its validation to the Subtree Validation service.</p> <p></p> <p>The detail of how the subtree is validated will be described in the next section.</p>"},{"location":"topics/services/subtreeValidation/#23-validating-the-subtrees","title":"2.3. Validating the Subtrees","text":"<p>In the previous section, the process of validating a subtree was described. Here, we will go into more detail about the validation process.</p> <p></p> <p>The validation process is as follows:</p> <ol> <li>First, the Validator will check if the subtree already exists in the Subtree Store. If it does, the subtree will not be validated again.</li> <li>If the subtree is not found in the Subtree Store, the Validator will fetch the subtree from the remote asset server.</li> <li>The Validator will create a subtree metadata object.</li> <li> <p>Next, the Validator will decorate all Txs. To do this, it will try the following approaches (in order):</p> <ul> <li>First, it will try to fetch the UTXO metadata from the tx metadata cache (in-memory).</li> <li>If the tx metadata is not found, it will try to fetch the tx metadata from the UTXO store.</li> <li>If the tx metadata is not found in the UTXO store, the Validator will fetch the UTXO from the remote asset server.</li> <li>If the tx is not found, the tx will be marked as invalid, and the subtree validation will fail.</li> </ul> </li> </ol>"},{"location":"topics/services/subtreeValidation/#24-subtree-locking-mechanism","title":"2.4. Subtree Locking Mechanism","text":"<p>To prevent concurrent validation of the same subtree, the service implements a file-based locking mechanism:</p> <ol> <li>Before validation begins, the service attempts to create a \"lock\" file for the specific subtree.</li> <li>If the lock file creation succeeds, the service proceeds with validation.</li> <li>If the lock file already exists, the service assumes another instance is already validating the subtree.</li> <li>This mechanism ensures efficient resource usage and prevents duplicate validation work.</li> </ol> <p>The locking implementation is designed to be resilient across distributed systems by leveraging the shared filesystem.</p>"},{"location":"topics/services/subtreeValidation/#25-distributed-pause-mechanism","title":"2.5. Distributed Pause Mechanism","text":"<p>The Subtree Validation service implements a distributed pause mechanism that coordinates pausing across multiple pods in a Kubernetes cluster. This mechanism is essential for preventing UTXO state conflicts during block validation.</p> <p></p> <p>Key Features:</p> <ol> <li>Distributed Lock: Uses a special lock file <code>__SUBTREE_PAUSE__.lock</code> in shared storage (quorum path)</li> <li>Heartbeat Updates: The lock file timestamp is updated every <code>timeout/2</code> seconds (default: 5 seconds) to indicate the pause is still active</li> <li>Automatic Crash Recovery: Stale locks (older than the configured timeout) are automatically detected and cleaned up</li> <li> <p>Two-Level Checking:</p> <ul> <li>Fast local atomic boolean check (no I/O)</li> <li>Reliable distributed lock check when local flag is false</li> <li>Graceful Fallback: Falls back to local-only pause if quorum is not initialized (for tests)</li> </ul> </li> </ol> <p>How It Works:</p> <ol> <li>When block validation starts on any pod, it calls <code>setPauseProcessing(ctx)</code></li> <li>A distributed lock is created in the shared storage with automatic heartbeat</li> <li> <p>All other pods check <code>isPauseActive()</code> before processing subtrees:</p> </li> <li> <p>First checks local atomic flag (fast path)</p> </li> <li>If local flag is false, checks for distributed lock</li> <li>If pause is active, subtree processing is skipped</li> <li>When block validation completes, the lock is released</li> <li>If a pod crashes during validation, the lock becomes stale after the timeout period and is automatically cleaned up by other pods</li> </ol> <p>Configuration:</p> <p>The distributed pause mechanism uses existing subtree validation settings:</p> <ul> <li><code>subtree_quorum_path</code>: Path to shared storage for lock files</li> <li><code>subtree_quorum_absolute_timeout</code>: Timeout for lock staleness (default: 30 seconds)</li> </ul>"},{"location":"topics/services/subtreeValidation/#3-grpc-protobuf-definitions","title":"3. gRPC Protobuf Definitions","text":"<p>The Subtree Validation Service uses gRPC for communication between nodes. The protobuf definitions used for defining the service methods and message formats can be seen here.</p>"},{"location":"topics/services/subtreeValidation/#4-data-model","title":"4. Data Model","text":"<ul> <li>Subtree Data Model: Contain lists of transaction IDs and their Merkle root.</li> <li>Extended Transaction Data Model: Include additional metadata to facilitate processing.</li> <li>UTXO Data Model: Include additional metadata to facilitate processing.</li> </ul>"},{"location":"topics/services/subtreeValidation/#5-technology","title":"5. Technology","text":"<ol> <li> <p>Go Programming Language (Golang).</p> </li> <li> <p>gRPC (Google Remote Procedure Call):</p> <ul> <li>Used for implementing server-client communication. gRPC is a high-performance, open-source framework that supports efficient communication between services.</li> </ul> </li> <li> <p>Data Stores:</p> <ul> <li>Integration with various stores: blob store, and UTXO store.</li> </ul> </li> <li> <p>Caching Mechanisms (ttlcache):</p> <ul> <li>Uses <code>ttlcache</code>, a Go library for in-memory caching with time-to-live settings, to avoid redundant processing and improve performance.</li> </ul> </li> <li> <p>Configuration Management (gocore):</p> <ul> <li>Uses <code>gocore</code> for configuration management, allowing dynamic configuration of service parameters.</li> </ul> </li> <li> <p>Networking and Protocol Buffers:</p> <ul> <li>Handles network communications and serializes structured data using Protocol Buffers, a language-neutral, platform-neutral, extensible mechanism for serializing structured data.</li> </ul> </li> <li> <p>Synchronization Primitives (sync):</p> <ul> <li>Utilizes Go's <code>sync</code> package for synchronization primitives like mutexes, aiding in managing concurrent access to shared resources.</li> </ul> </li> </ol>"},{"location":"topics/services/subtreeValidation/#6-directory-structure-and-main-files","title":"6. Directory Structure and Main Files","text":"<pre><code>./services/subtreevalidation\n\u251c\u2500\u2500 Client.go                               # Client-side implementation for gRPC subtree validation service interactions.\n\u251c\u2500\u2500 Interface.go                            # Defines interfaces related to subtree validation, facilitating abstraction and testing.\n\u251c\u2500\u2500 README.md                               # Project documentation including setup, usage, and examples.\n\u251c\u2500\u2500 Server.go                               # Server-side logic for the subtree validation service, handling RPC calls.\n\u251c\u2500\u2500 Server_test.go                          # Tests for the server implementation.\n\u251c\u2500\u2500 SubtreeValidation.go                    # Core logic for validating subtrees within a blockchain structure.\n\u251c\u2500\u2500 SubtreeValidation_test.go               # Unit tests for the subtree validation logic.\n\u251c\u2500\u2500 TryLockIfNotExists.go                   # Implementation of locking mechanism to avoid concurrent subtree validation.\n\u251c\u2500\u2500 metrics.go                              # Implementation of metrics collection for monitoring service performance.\n\u251c\u2500\u2500 processTxMetaUsingCache.go              # Logic for processing transaction metadata with a caching layer for efficiency.\n\u251c\u2500\u2500 processTxMetaUsingStore.go              # Handles processing of transaction metadata directly from storage, bypassing cache.\n\u251c\u2500\u2500 subtreeHandler.go                       # Handler for operations related to subtree processing and validation.\n\u251c\u2500\u2500 subtreeHandler_test.go                  # Unit tests for the subtree handler logic.\n\u251c\u2500\u2500 subtreevalidation_api                   # Directory containing Protocol Buffers definitions and generated code for the API.\n\u2502   \u251c\u2500\u2500 subtreevalidation_api.pb.go         # Generated Go code from .proto definitions, containing structs and methods.\n\u2502   \u251c\u2500\u2500 subtreevalidation_api.proto         # Protocol Buffers file defining the subtree validation service API.\n\u2502   \u2514\u2500\u2500 subtreevalidation_api_grpc.pb.go    # Generated Go code for gRPC client and server interfaces from the .proto service.\n\u251c\u2500\u2500 txmetaHandler.go                        # Manages operations related to transaction metadata, including validation and caching.\n\u2514\u2500\u2500 txmetaHandler_test.go                   # Unit tests for transaction metadata handling.\n</code></pre>"},{"location":"topics/services/subtreeValidation/#7-how-to-run","title":"7. How to Run","text":"<p>To run the Subtree Validation Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -SubtreeValidation=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Subtree Validation Service locally.</p>"},{"location":"topics/services/subtreeValidation/#8-configuration-settings","title":"8. Configuration Settings","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the subtree Validation Settings Reference.</p>"},{"location":"topics/services/subtreeValidation/#9-other-resources","title":"9. Other Resources","text":"<ul> <li>Subtree Validation Reference</li> <li>Handling Double Spends</li> </ul>"},{"location":"topics/services/utxoPersister/","title":"\ud83d\udd0d UTXO Persister Service","text":""},{"location":"topics/services/utxoPersister/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1 Service Initialization</li> <li>2.2 Receiving and Processing a new UTXO Set</li> <li>2.3 Processing Blocks and Creating UTXO Sets</li> </ul> </li> <li>Data Model</li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>How to run</li> <li>Configuration Settings</li> <li>Other Resources</li> </ol>"},{"location":"topics/services/utxoPersister/#1-description","title":"1. Description","text":"<p>The UTXO Persister primary function is to create and maintain an up-to-date Unspent Transaction Output (UTXO) file set for each block in the blockchain.</p> <p>To achieve its target, the UTXO Persister uses the output of the Block Persister service, and outputs an updated UTXO set file.</p> <p>The UTXO set file can be exported and used as an input for initializing the UTXO store in a new Teranode instance.</p> <ol> <li> <p>Input Processing:</p> <ul> <li> <p>The UTXO Persister works with the output of the Block Persister, which includes:</p> <ul> <li><code>utxo-additions</code>: New UTXOs created in a block,</li> <li><code>utxo-deletions</code>: UTXOs spent in a block,</li> <li><code>.block</code>: The block data,</li> <li><code>.subtree</code>: Subtree information for the block.</li> </ul> </li> </ul> </li> <li> <p>UTXO Set Generation:</p> <ul> <li>For each new block fileset detected, the UTXO Persister creates a 'utxo-set' file.</li> <li>This file represents the complete set of unspent transaction outputs up to and including the current block.</li> </ul> </li> <li> <p>File Monitoring and Processing:</p> <ul> <li>The service continuously monitors the shared storage for new .block files and additional related files.</li> <li>When new files are detected, it triggers the UTXO set creation process.</li> </ul> </li> <li> <p>Progress Tracking:</p> <ul> <li>The service maintains a <code>lastProcessed.dat</code> file to keep track of the last block height processed.</li> <li>This ensures continuity and allows the service to resume from the correct point after restarts or interruptions.</li> </ul> </li> <li> <p>Efficient Data Handling:</p> <ul> <li>The service uses optimized data structures and file formats to handle large volumes of UTXO data efficiently.</li> <li>It implements binary encoding for UTXOs and UTXO deletions to minimize storage requirements and improve processing speed.</li> </ul> </li> <li> <p>File Management:</p> <ul> <li>The service interacts with the storage system to read and write necessary files.</li> </ul> </li> </ol> <p>Note: For information about how the UTXO Persister service is initialized during daemon startup and how it interacts with other services, see the Teranode Daemon Reference.</p> <p></p> <p>The service interacts with the storage system to read and write necessary files (shared with the Block Persister service), and requests block information from the Blockchain service (or, optionally, directly from the Blockchain store)</p> <p></p> <p>The following diagram provides a deeper level of detail into the UTXO Persister Service's internal components and their interactions:</p> <p></p>"},{"location":"topics/services/utxoPersister/#2-functionality","title":"2. Functionality","text":""},{"location":"topics/services/utxoPersister/#21-service-initialization","title":"2.1 Service Initialization","text":"<ol> <li> <p>The service starts by creating connections to:</p> <ul> <li>A blob store (shared with the block persister) for file storage</li> <li>Either the Blockchain service or the Blockchain store for block information</li> </ul> </li> <li> <p>It reads the last processed height from <code>lastProcessed.dat</code>.</p> </li> <li> <p>The service then enters a loop, waiting for new blocks to process.</p> </li> </ol>"},{"location":"topics/services/utxoPersister/#22-receiving-and-processing-a-new-utxo-set","title":"2.2 Receiving and Processing a new UTXO Set","text":""},{"location":"topics/services/utxoPersister/#23-processing-blocks-and-creating-utxo-sets","title":"2.3 Processing Blocks and Creating UTXO Sets","text":"<p>The UTXO Persister processes blocks and creates UTXO sets as follows:</p> <p>1) Trigger and Block Height Check:</p> <ul> <li>The service is triggered to process the next block (via notification, timer, or startup).</li> <li>It checks if the next block to process is at least 100 blocks behind the current best block height.</li> </ul> <p>2) Block Headers Retrieval:</p> <ul> <li>If processing is needed, the service retrieves block headers from either the Blockchain Store or Blockchain Client.</li> <li>It verifies the chain continuity using these headers.</li> </ul> <p>3) Last Set Verification:</p> <ul> <li>The service verifies the last UTXO set using <code>verifyLastSet()</code> to ensure data integrity.</li> </ul> <p>4) Block Range Consolidation:</p> <ul> <li>A new Consolidator is created to process a range of blocks efficiently.</li> <li>The <code>ConsolidateBlockRange()</code> method is called to handle multiple blocks at once if needed.</li> </ul> <p>5) UTXO Set Preparation:</p> <ul> <li>The service calls <code>GetUTXOSetWithDeletionsMap()</code> to prepare the UTXO set for the new block.</li> <li>This retrieves the UTXO deletions from the Block Store and creates a deletions map.</li> </ul> <p>6) UTXO Set Creation:</p> <ul> <li>The <code>CreateUTXOSet()</code> method is called on the UTXOSet object.</li> <li> <p>This method:</p> </li> <li> <p>Retrieves the previous block's UTXO set from the Block Store.</p> </li> <li>Applies the deletions from the deletions map.</li> <li>Incorporates new UTXOs from the block's transactions.</li> <li>Writes the new UTXO set to the Block Store.</li> </ul> <p>7) Cleanup:</p> <ul> <li>If not skipped (based on configuration), the service deletes the previous block's UTXO set to save space.</li> </ul> <p>8) Update Last Processed Height:</p> <ul> <li>The service calls <code>writeLastHeight()</code> to update its record of the last processed block height.</li> </ul> <p>9) Trigger Next Block Processing:</p> <ul> <li>The service initiates the processing of the next block, continuing the cycle.</li> </ul> <p>If the current height is less than 100 blocks behind the best block height, the service waits for more confirmations before processing.</p>"},{"location":"topics/services/utxoPersister/#3-data-model","title":"3. Data Model","text":""},{"location":"topics/services/utxoPersister/#31-basic-structure","title":"3.1 Basic Structure","text":"<p>The UTXO set is essentially a collection of all unspent transaction outputs in the blockchain up to a specific block height. Each UTXO represents a piece of cryptocurrency that can be spent in future transactions.</p>"},{"location":"topics/services/utxoPersister/#32-utxo-components","title":"3.2 UTXO Components","text":"<p>The UTXOs are stored in a hierarchical structure, with the following components:</p> <p>UTXOWrapper:</p> <pre><code>type UTXOWrapper struct {\n    TxID     chainhash.Hash\n    Height   uint32\n    Coinbase bool\n    UTXOs    []*UTXO\n}\n</code></pre> <p>UTXO:</p> <pre><code>type UTXO struct {\n    Index  uint32\n    Value  uint64\n    Script []byte\n}\n</code></pre> <p>Field descriptions:</p> <ul> <li>TxID: The transaction ID where this UTXO was created (32 bytes)</li> <li>Index: The output index in the transaction (4 bytes)</li> <li>Value: The amount of cryptocurrency in this UTXO (8 bytes)</li> <li>Height: The block height where this UTXO was created (4 bytes)</li> <li>Script: The locking script that must be satisfied to spend this UTXO (variable length)</li> <li>Coinbase: A flag indicating whether this UTXO is from a coinbase transaction (1 bit, packed with Height)</li> </ul>"},{"location":"topics/services/utxoPersister/#33-binary-encoding","title":"3.3 Binary Encoding","text":"<p>The UTXO is encoded into a binary format for efficient storage and retrieval:</p> <ul> <li>32 bytes: TxID</li> <li>4 bytes: Index (little-endian)</li> <li>8 bytes: Value (little-endian)</li> <li>4 bytes: Height and Coinbase flag (Height &lt;&lt; 1 | CoinbaseFlag)</li> <li>4 bytes: Script length (little-endian)</li> <li>Variable bytes: Script</li> </ul>"},{"location":"topics/services/utxoPersister/#34-utxo-set-file","title":"3.4 UTXO Set File","text":"<p>The UTXO set for each block is stored in a file with the extension <code>utxo-set</code>. This file contains a series of encoded UTXOs representing all unspent outputs up to that block.</p>"},{"location":"topics/services/utxoPersister/#35-utxo-diff","title":"3.5 UTXO Diff","text":"<p>The UTXO Persister uses a diff-based approach to update the UTXO set:</p> <ul> <li><code>utxo-additions</code>: New UTXOs created in a block</li> <li><code>utxo-deletions</code>: UTXOs spent in a block</li> </ul>"},{"location":"topics/services/utxoPersister/#36-utxo-deletion-model","title":"3.6 UTXO Deletion Model","text":"<p>When a UTXO is spent, it's recorded in the <code>utxo-deletions</code> file. The deletion record contains:</p> <ul> <li>TxID (32 bytes)</li> <li>Index (4 bytes)</li> </ul>"},{"location":"topics/services/utxoPersister/#37-set-operations","title":"3.7 Set Operations","text":"<p>Creating a new UTXO set involves:</p> <ol> <li>Starting with the previous block's UTXO set</li> <li>Removing UTXOs listed in the current block's <code>utxo-deletions</code></li> <li>Adding UTXOs listed in the current block's <code>utxo-additions</code></li> </ol>"},{"location":"topics/services/utxoPersister/#38-persistence","title":"3.8 Persistence","text":"<p>The UTXO set is persisted using a FileStorer, which writes the data to a blob store.</p>"},{"location":"topics/services/utxoPersister/#4-technology","title":"4. Technology","text":"<ol> <li> <p>Programming Language:</p> <ul> <li>Go (Golang): The entire service is written in Go.</li> </ul> </li> <li> <p>Blockchain-specific Libraries:</p> <ul> <li>github.com/bsv-blockchain/go-bt/v2: A Bitcoin SV library for Go, used for handling Bitcoin transactions and blocks.</li> <li>github.com/bsv-blockchain/teranode: Custom package for Bitcoin SV operations.</li> </ul> </li> <li> <p>Storage:</p> <ul> <li>Blob Store: Used for reading block data, subtrees, and UTXO diff files, and for writing UTXO Set files.</li> </ul> </li> <li> <p>Configuration Management:</p> <ul> <li>github.com/ordishs/gocore: Used for configuration management (e.g., reading config values).</li> </ul> </li> </ol>"},{"location":"topics/services/utxoPersister/#5-directory-structure-and-main-files","title":"5. Directory Structure and Main Files","text":"<p>The Block Persister service is located in the <code>services/utxopersister</code> directory.</p> <pre><code>./services/utxopersister/\n\u2502\n\u251c\u2500\u2500 Server.go\n\u2502   Main implementation of the UTXO Persister server. It contains the core logic for\n\u2502   initializing the service, handling requests, and coordinating UTXO set updates.\n\u2502\n\u251c\u2500\u2500 UTXO.go\n\u2502   Defines the UTXO (Unspent Transaction Output) data structure and related methods.\n\u2502\n\u251c\u2500\u2500 UTXOSet.go\n\u2502   Implements the UTXOSet structure and related methods.\n\u2502\n\u251c\u2500\u2500 UTXODeletion.go\n\u2502   Implements the logic for UTXO deletions, which occur when UTXOs are spent in a transaction.\n|\n\u2514\u2500\u2500 filestorer/\n    \u2502\n    \u2514\u2500\u2500 FileStorer.go\n        Implements a custom file storage mechanism, optimized for the specific\n        needs of storing and retrieving UTXO data efficiently.\n</code></pre>"},{"location":"topics/services/utxoPersister/#6-how-to-run","title":"6. How to run","text":"<p>To run the UTXO Persister Service locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -UTXOPersister=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the UTXO Persister Service locally.</p>"},{"location":"topics/services/utxoPersister/#7-configuration-settings","title":"7. Configuration Settings","text":"<p>The UTXO Persister service relies on a set of configuration settings that control its behavior, performance, and resource usage. This section provides a comprehensive overview of these settings, organized by functional category, along with their impacts, dependencies, and recommended configurations for different deployment scenarios.</p>"},{"location":"topics/services/utxoPersister/#71-configuration-categories","title":"7.1 Configuration Categories","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the utxo Persister Settings Reference.</p>"},{"location":"topics/services/utxoPersister/#72-performance-tuning-settings","title":"7.2 Performance Tuning Settings","text":"<p>These settings control the I/O performance and memory usage patterns of the UTXO Persister service.</p> Setting Type Default Description Impact <code>utxoPersister_buffer_size</code> string <code>\"4KB\"</code> Controls the buffer size for reading from and writing to UTXO files Affects I/O performance and memory usage when processing UTXO data"},{"location":"topics/services/utxoPersister/#performance-tuning-interactions-and-dependencies","title":"Performance Tuning Interactions and Dependencies","text":"<p>The buffer size setting directly affects how efficiently the service reads and writes UTXO data:</p> <ul> <li>Larger buffer sizes (e.g., 64KB to 1MB) can significantly improve I/O throughput by reducing the number of system calls needed for file operations</li> <li>Smaller buffer sizes reduce memory usage but may increase CPU overhead due to more frequent I/O operations</li> <li>The optimal buffer size depends on the hardware characteristics, particularly disk I/O capabilities, available memory, and the size of typical UTXO files</li> </ul> <p>For high-throughput environments with fast storage systems (like SSDs), larger buffer sizes provide better performance. For memory-constrained environments, smaller buffers may be necessary despite the performance impact.</p>"},{"location":"topics/services/utxoPersister/#73-storage-management-settings","title":"7.3 Storage Management Settings","text":"<p>These settings control how UTXO data is stored and retained.</p> Setting Type Default Description Impact <code>blockpersister_skipUTXODelete</code> bool <code>false</code> When true, previous block's UTXO sets aren't deleted after processing Controls storage usage and retention policy for historical UTXO sets <code>blockstore</code> *url.URL <code>\"file://./data/blockstore\"</code> Specifies the URL for the block storage backend Determines where block data, including UTXO sets, are stored <code>txstore</code> *url.URL <code>\"\"</code> Specifies the URL for the transaction storage backend Determines where transaction data is stored for block processing <code>txmeta_store</code> *url.URL <code>\"\"</code> Specifies the URL for the UTXO metadata storage backend Determines where UTXO metadata is stored for efficient lookups"},{"location":"topics/services/utxoPersister/#storage-management-interactions-and-dependencies","title":"Storage Management Interactions and Dependencies","text":"<p>These settings determine the storage footprint and data persistence behavior of the service:</p> <ul> <li>When <code>blockpersister_skipUTXODelete</code> is <code>false</code> (default), the service maintains minimal storage by only keeping the UTXO set for the most recent processed block</li> <li>When set to <code>true</code>, the service preserves all historical UTXO sets, which increases storage requirements but enables historical analysis and validation</li> <li>The <code>blockstore</code> setting defines where all block-related data is stored, affecting both read and write performance based on the underlying storage system</li> <li>The <code>txstore</code> and <code>txmeta_store</code> settings determine where transaction data and UTXO metadata are stored, respectively, which impacts performance and data availability</li> </ul> <p>Storage requirements grow significantly when keeping historical UTXO sets, as each set contains the complete state of all unspent outputs at a given block height.</p>"},{"location":"topics/services/utxoPersister/#74-deployment-architecture-settings","title":"7.4 Deployment Architecture Settings","text":"<p>These settings control how the UTXO Persister interacts with other components in the system.</p> Setting Type Default Description Impact <code>direct</code> bool <code>true</code> Controls whether the service connects directly to the blockchain store or uses the client interface Affects performance and deployment architecture"},{"location":"topics/services/utxoPersister/#deployment-architecture-interactions-and-dependencies","title":"Deployment Architecture Interactions and Dependencies","text":"<p>The deployment architecture settings determine how the service integrates with the broader system:</p> <ul> <li>When <code>direct</code> is <code>true</code>, the service bypasses the blockchain client interface and connects directly to the blockchain store, which improves performance but requires the service to be deployed in the same process</li> <li>When <code>direct</code> is <code>false</code>, the service uses the blockchain client interface, allowing for distributed deployment at the cost of additional network overhead</li> </ul> <p>This setting has significant implications for system design and deployment flexibility. Direct access provides better performance but limits deployment options, while client-based access enables more flexible deployment topologies but may impact performance.</p>"},{"location":"topics/services/utxoPersister/#75-operational-controls-settings","title":"7.5 Operational Controls Settings","text":"<p>These settings control general operational aspects of the service.</p> Setting Type Default Description Impact <code>network</code> string <code>\"mainnet\"</code> Specifies the blockchain network (mainnet, testnet, regtest) Determines genesis hash and chain parameters used for validation"},{"location":"topics/services/utxoPersister/#operational-controls-interactions-and-dependencies","title":"Operational Controls Interactions and Dependencies","text":"<ul> <li>The <code>network</code> setting determines the genesis hash and chain parameters via chaincfg.GetChainParams(), which is critical for genesis block detection and UTXO set validation</li> <li>Genesis hash validation is used throughout the service to determine when to skip certain operations like UTXO deletion</li> </ul> <p>This setting is fundamental to the service's operation and must match the target blockchain network.</p>"},{"location":"topics/services/utxoPersister/#76-configuration-best-practices","title":"7.6 Configuration Best Practices","text":"<ol> <li> <p>Performance Monitoring: Regularly monitor I/O performance metrics when adjusting buffer sizes. Balance memory usage against throughput based on your specific hardware capabilities.</p> </li> <li> <p>Storage Planning: When using <code>blockpersister_skipUTXODelete=true</code>, implement a storage monitoring and management strategy. UTXO sets grow significantly over time and may require substantial storage capacity.</p> </li> <li> <p>Deployment Architecture: Choose direct access (<code>direct=true</code>) whenever possible for best performance, unless your system architecture specifically requires distributed deployment.</p> </li> <li> <p>Network Configuration: Ensure the <code>network</code> setting matches your target blockchain environment. Incorrect network configuration can lead to validation failures and data corruption.</p> </li> <li> <p>Storage Location: Use persistent, reliable storage locations for the <code>blockstore</code> setting in production environments, ideally on dedicated, high-performance storage systems.</p> </li> <li> <p>Backup Strategy: Implement regular backups of your UTXO data, especially the most recent UTXO set, to enable rapid recovery in case of data corruption or storage failures.</p> </li> <li> <p>Service Coordination: Ensure that blockchain services and UTXO Persister services are properly coordinated in terms of startup sequence and operational dependencies, particularly when using direct access mode.</p> </li> </ol>"},{"location":"topics/services/utxoPersister/#8-other-resources","title":"8. Other Resources","text":"<p>UTXO Persister Reference</p>"},{"location":"topics/services/validator/","title":"\ud83d\udd0d TX Validator","text":""},{"location":"topics/services/validator/#index","title":"Index","text":"<ol> <li>Description</li> <li>Functionality<ul> <li>2.1. Starting the Validator as a service</li> <li>2.2. Receiving Transaction Validation Requests</li> <li>2.3. Validating the Transaction</li> <li>2.3.1. Consensus Rules vs Policy Checks</li> <li>2.3.2. Transaction Format Extension</li> <li>2.4. Script Verification</li> <li>2.5. Error Handling and Transaction Rejection</li> <li>2.6. Concurrent Processing</li> <li>2.7. Post-validation: Updating stores and propagating the transaction</li> <li>2.7.1. Two-Phase Transaction Commit Process</li> </ul> </li> <li>gRPC Protobuf Definitions</li> <li>Data Model</li> <li>Technology</li> <li>Directory Structure and Main Files</li> <li>How to run</li> <li>Configuration</li> <li>Other Resources</li> </ol>"},{"location":"topics/services/validator/#1-description","title":"1. Description","text":"<p>The <code>Validator</code> (also called <code>Transaction Validator</code> or <code>Tx Validator</code>) is a go component responsible for:</p> <ol> <li>Receiving new transactions from the <code>Propagation</code>service,</li> <li>Validating them,</li> <li>Persisting the data into the utxo store,</li> <li> <p>Propagating the transactions to other services based on validation results:</p> <ul> <li>Block Assembly service: Direct gRPC calls for validated transactions (if the Tx is passed)</li> <li>Subtree Validation service: Kafka topic for transaction metadata (if the Tx is passed)</li> <li>P2P service: Kafka topic for rejected transaction notifications (if the tx is rejected)</li> </ul> </li> </ol>"},{"location":"topics/services/validator/#11-deployment-models","title":"1.1 Deployment Models","text":"<p>The Validator can be deployed in two distinct ways:</p> <ol> <li> <p>Local Validator (Recommended):</p> <ul> <li>The Validator is instantiated directly within other services (like the Propagation, Subtree Validation, and Legacy Services)</li> <li>This is the recommended approach for production deployments due to better performance</li> <li>No additional network calls are needed between services and validator</li> <li>Configuration: Set <code>validator.useLocalValidator=true</code> in your settings</li> </ul> </li> <li> <p>Remote Validator Service:</p> <ul> <li>The Validator runs as an independent service with a gRPC interface</li> <li>Other services connect to it remotely via gRPC</li> <li>This approach has higher latency due to additional network calls</li> <li>Useful for development, testing, or specialized deployment scenarios</li> <li>Configuration: Set <code>validator.useLocalValidator=false</code> and configure validator gRPC endpoint</li> </ul> </li> </ol> <p>The performance difference between these approaches can be significant, as the local validator approach eliminates network overhead between services.</p> <p>Note: For detailed information about how the daemon initializes services and manages dependencies, see the Teranode Daemon Reference.</p> <p></p> <p>The Validator, as a component, is instantiated as part of any service requiring to validate transactions. However, the Validator can also be started as a service, allowing to interact with it via gRPC or Kafka. This setup is not recommended, given its performance overhead.</p> <p>The Validator receives notifications about new Txs.</p> <p>Also, the <code>Validator</code> will accept subscriptions from the P2P Service, where rejected tx notifications are pushed to.</p> <p></p>"},{"location":"topics/services/validator/#detailed-component-diagram","title":"Detailed Component Diagram","text":"<p>The detailed component diagram below shows the internal architecture of the Validator Service:</p> <p></p> <p>The Validator notifies the Block Assembly service of new transactions through gRPC calls via the Block Assembly client interface.</p> <p>A node can start multiple parallel instances of the TX Validator. This translates into multiple pods within a Kubernetes cluster. Each instance / pod will have its own gRPC server, and will be able to receive and process transactions independently. GRPC load balancing allows to distribute the load across the multiple instances.</p>"},{"location":"topics/services/validator/#kafka-integration","title":"Kafka Integration","text":"<p>The Validator uses Kafka for several critical messaging paths:</p> <ol> <li> <p>Transaction Metadata Publishing:</p> <ul> <li>Successfully validated transactions have their metadata published to a Kafka topic</li> <li>Metadata includes transaction ID, size, fee, input and output information</li> <li>The <code>txmetaKafkaProducerClient</code> handles this publishing process</li> <li>Target topic is configured via <code>Kafka.TxMetaConfig</code> settings</li> </ul> </li> <li> <p>Rejected Transaction Notifications:</p> <ul> <li>When transactions fail validation, details are published to a rejection topic</li> <li>The <code>rejectedTxKafkaProducerClient</code> is responsible for these messages</li> <li>Includes rejection reason, transaction ID, and error classification</li> <li>Helps with network-wide tracking of invalid transactions</li> </ul> </li> <li> <p>Transaction Validation Requests:</p> <ul> <li>The Validator can also consume validation requests via Kafka</li> <li>This enables asynchronous transaction processing patterns</li> <li>Useful for high-throughput scenarios where immediate responses aren't needed</li> </ul> </li> </ol> <p>The Kafka integration provides resilience, allowing the system to handle temporary outages or service unavailability by buffering messages.</p>"},{"location":"topics/services/validator/#communication-patterns-summary","title":"Communication Patterns Summary","text":"<p>The Validator uses different communication patterns depending on the target service and use case:</p> <p>Outbound Communications (Validator \u2192 Other Services):</p> <ul> <li>Block Assembly: gRPC calls via <code>blockassembly.ClientI</code> interface - used for real-time transaction forwarding to mining candidates</li> <li>Subtree Validation: Kafka producer via <code>txmetaKafkaProducerClient</code> - used for transaction metadata publishing</li> <li>P2P Service: Kafka producer via <code>rejectedTxKafkaProducerClient</code> - used for rejected transaction notifications</li> </ul> <p>Inbound Communications (Other Services \u2192 Validator):</p> <ul> <li>Propagation Service: Direct method calls (local validator) or gRPC calls (remote validator)</li> <li>Subtree Validation Service: Direct method calls (local validator) or gRPC calls (remote validator)</li> <li>Kafka Consumers: Kafka messages via <code>consumerClient</code> - used for asynchronous validation requests</li> <li>HTTP API: REST endpoints for external validation requests</li> </ul> <p>The choice between local method calls and gRPC depends on the deployment model configured via the <code>useLocalValidator</code> setting.</p>"},{"location":"topics/services/validator/#2-functionality","title":"2. Functionality","text":""},{"location":"topics/services/validator/#21-starting-the-validator-as-a-service","title":"2.1. Starting the Validator as a service","text":"<p>Should the node require to start the validator as an independent service, the <code>services/validator/Server.go</code> will be instantiated as follows:</p> <p></p> <ul> <li> <p><code>NewServer</code> Function:</p> <ol> <li>Initialize a new <code>Server</code> instance.</li> <li>Create channels for new and dead subscriptions.</li> <li>Initialize a map for subscribers.</li> <li>Create a context for subscriptions and a corresponding cancellation function.</li> <li>Return the newly created <code>Server</code> instance.</li> </ol> </li> <li> <p><code>Init</code> Function:</p> <ol> <li>Create a new validator instance within the server.</li> <li>Handle any errors during the creation of the validator.</li> <li>Return the outcome (success or error).</li> </ol> </li> <li> <p><code>Start</code> Function:</p> <ol> <li>Start a goroutine to manage new and dead subscriptions.</li> <li>Start the gRPC server and handle any errors.</li> </ol> </li> </ul>"},{"location":"topics/services/validator/#22-receiving-transaction-validation-requests","title":"2.2. Receiving Transaction Validation Requests","text":"<p>The Propagation and Subtree Validation modules invoke the validator process in order to have new or previously missed Txs validated. The Propagation service is responsible for processing new Txs, while the Block Validation service is responsible for identifying missed Txs while processing blocks.</p> <p></p> <p>BlockValidation and Propagation invoke the validator process with and without batching. Batching is settings controlled, and improves the processing performance.</p> <ol> <li> <p>Transaction Propagation:</p> <ul> <li>The Propagation module <code>ProcessTransaction()</code> function invokes <code>Validate()</code> on the Validator client.</li> <li>The Validator validates the transaction.</li> </ul> </li> <li> <p>Subtree Validation:</p> <ul> <li>The SubtreeValidation module <code>blessMissingTransaction()</code> function invokes <code>Validate()</code> on the Validator client.</li> <li>The Validator validates the transaction.</li> </ul> </li> </ol>"},{"location":"topics/services/validator/#23-validating-the-transaction","title":"2.3. Validating the Transaction","text":"<p>For every transaction received, Teranode must validate:</p> <ul> <li>All inputs against the existing UTXO-set, verifying if the input(s) can be spent,<ul> <li>Notice that if Teranode detects a double-spend, the transaction that was received first must be considered the valid transaction.</li> </ul> </li> <li>Bitcoin consensus rules,</li> <li>Local policies (if any),</li> <li>Whether the script execution returns <code>true</code>.</li> </ul> <p>Teranode will consider a transaction that passes consensus rules, local policies and script validation as fully validated and fit to be included in the next possible block.</p> <p>The validation process includes several stages:</p> <ol> <li> <p>Basic Transaction Structure Validation:</p> <ul> <li>Verify inputs and outputs are present</li> <li>Check transaction size against policy limits</li> <li>Validate input and output value ranges</li> </ul> </li> <li> <p>Policy Validation:</p> <ul> <li>Apply configurable policy rules that can be enabled/disabled</li> <li>Check transaction fees against minimum requirements</li> <li>Enforce limits on script operations (sigops)</li> </ul> </li> <li> <p>Input Validation:</p> <ul> <li>Verify inputs exist in the UTXO set</li> <li>Ensure inputs are unspent (prevent double-spending)</li> <li>Validate input script format</li> </ul> </li> </ol> <p>New Txs are validated by the <code>ValidateTransaction()</code> function. To ensure the validity of the extended Tx, this is delegated to a BSV library: <code>github.com/TAAL-GmbH/arc/validator/default</code> (the default validator).</p> <p>We can see the exact steps being executed as part of the validation process below:</p> <pre><code>func (tv *TxValidator) ValidateTransaction(tx *bt.Tx, blockHeight uint32, validationOptions *Options) error {\n //\n // Each node will verify every transaction against a long checklist of criteria:\n //\n txSize := tx.Size()\n\n // 1) Neither lists of inputs nor outputs are empty\n if len(tx.Inputs) == 0 || len(tx.Outputs) == 0 {\n  return errors.NewTxInvalidError(\"transaction has no inputs or outputs\")\n }\n\n // 2) The transaction size in bytes is less than maxtxsizepolicy.\n if !validationOptions.SkipPolicyChecks {\n  if err := tv.checkTxSize(txSize); err != nil {\n   return err\n  }\n }\n\n // 3) check that each input value, as well as the sum, are in the allowed range of values (less than 21m coins)\n // 5) None of the inputs have hash=0, N=\u20131 (coinbase transactions should not be relayed)\n if err := tv.checkInputs(tx, blockHeight); err != nil {\n  return err\n }\n\n // 4) Each output value, as well as the total, must be within the allowed range of values (less than 21m coins,\n //    more than the dust threshold if 1 unless it's OP_RETURN, which is allowed to be 0)\n if err := tv.checkOutputs(tx, blockHeight); err != nil {\n  return err\n }\n\n // The transaction size in bytes is greater than or equal to 100 (BCH only check, not applicable to BSV)\n\n // The number of signature operations (SIGOPS) contained in the transaction is less than the signature operation limit\n // Note: This may be disabled for unlimited operation counts\n\n // The unlocking script (scriptSig) can only push numbers on the stack\n if tv.interpreter.Interpreter() != TxInterpreterGoBDK &amp;&amp; blockHeight &gt; tv.settings.ChainCfgParams.UahfForkHeight {\n  if err := tv.pushDataCheck(tx); err != nil {\n   return err\n  }\n }\n\n // 10) Reject if the sum of input values is less than sum of output values\n // 11) Reject if transaction fee would be too low (minRelayTxFee) to get into an empty block.\n if !validationOptions.SkipPolicyChecks {\n  if err := tv.checkFees(tx, feesToBtFeeQuote(tv.settings.Policy.GetMinMiningTxFee())); err != nil {\n   return err\n  }\n }\n\n // 12) The unlocking scripts for each input must validate against the corresponding output locking scripts\n // (Script verification is handled separately with multiple interpreter options)\n\n return nil\n}\n</code></pre> <p>The above represents an implementation of the core Teranode validation rules:</p> <ul> <li> <p>All transactions must exist and be unspent (does not apply to Coinbase transactions).</p> </li> <li> <p>All transaction inputs must have been present in either a transaction in an ancestor block, or in a transaction in the same block that is located before the transaction being validated.</p> </li> <li> <p>All transaction inputs must not have been spent by any transaction in ancestor blocks, or by any transaction in the same block that is not located after the transaction being validated</p> </li> <li> <p>The length of the script (scriptSig) in the Coinbase transaction must be between 2 and (including) 100 bytes.</p> </li> <li> <p>The transaction must be syntactically valid:</p> </li> <li> <p>A transaction must have at least one input</p> </li> <li> <p>A transaction must have at least one output</p> </li> <li> <p>The amount of satoshis in all outputs must be less than or equal to the amount of satoshis in the inputs, to avoid new BSV being introduced to the network</p> </li> <li> <p>The amount in all outputs must be between 0 and 21,000,000 BSV (2.1 * 10^15 satoshi)</p> </li> <li> <p>The amount of all inputs must be between 0 and 21,000,000 BSV. The sum of the amount over all inputs must not be larger than 21,000,000 BSV.</p> </li> <li> <p>A transaction must be final, meaning that either of the following conditions is met:</p> <ul> <li> <p>The sequence number in all inputs is equal to 0xffffffff, or</p> </li> <li> <p>The lock time is:</p> </li> <li> <p>Equal to zero, or</p> </li> <li> <p>&lt;500000000 and smaller than block height, or &gt;=500000000 and SMALLER THAN TIMESTAMP</p> <ul> <li>Note: This means that Teranode will deem non-final transactions invalid and REJECT these transactions. It is up to the user to create proper non-final transactions to ensure that Teranode is aware of them. For clarity, if a transaction has a locktime in the future, the Tx Validator will reject it.</li> </ul> </li> <li> <p>No output must be Pay-to-Script-Hash (P2SH)</p> </li> <li> <p>A new transaction must not have any output which includes P2SH, as creation of new P2SH transactions is not allowed.</p> </li> <li> <p>Historical P2SH transactions (if any) must still be supported by Teranode, allowing these transactions to be spent.</p> </li> <li> <p>A transaction must not spend frozen UTXOs (see 3.13 \u2013 Integration with Alert System)</p> </li> <li> <p>A node must not be able to spend a confiscated (re-assigned) transaction until 1,000 blocks after the transaction was re-assigned (confiscation maturity). The difference between block height and height at which the transaction was re-assigned must not be less than one thousand.</p> </li> </ul> </li> </ul>"},{"location":"topics/services/validator/#231-consensus-rules-vs-policy-checks","title":"2.3.1. Consensus Rules vs Policy Checks","text":"<p>In Bitcoin transaction validation, there are two distinct types of rules:</p> <ol> <li> <p>Consensus Rules: Mandatory rules that all nodes must enforce to maintain network consensus. Transactions violating consensus rules are always rejected. These include:</p> <ul> <li>Transaction structure and formatting</li> <li>Double-spend prevention</li> <li>Input and output value constraints</li> <li>Script execution validity</li> </ul> </li> <li> <p>Policy Rules: Node-specific preferences that determine which valid transactions to accept, relay, or mine. Policy rules can differ between nodes without breaking consensus. These include:</p> <ul> <li>Minimum transaction fees</li> <li>Maximum transaction size</li> <li>Script complexity limits</li> <li>Dust output thresholds</li> </ul> </li> </ol> <p>The TX Validator implements both types of rules, but provides the ability to skip policy checks when appropriate through the <code>SkipPolicyChecks</code> option.</p>"},{"location":"topics/services/validator/#skip-policy-checks-feature","title":"Skip Policy Checks Feature","text":"<p>The <code>SkipPolicyChecks</code> feature allows Teranode to validate transactions while bypassing certain policy-based validations. When enabled, the validator will:</p> <ul> <li>Skip transaction size policy checks</li> <li>Skip minimum fee requirements</li> <li>Apply consensus-only script verification rules</li> <li>Continue enforcing all consensus rules</li> </ul> <p>This feature is particularly important when validating transactions that:</p> <ul> <li>Are part of blocks mined by other miners (which have already been validated through proof-of-work)</li> <li>Need to be accepted regardless of local policy preferences</li> <li>Have already been confirmed on the blockchain</li> </ul> <pre><code>// Example of policy checks being conditionally applied:\nif !validationOptions.SkipPolicyChecks {\n    if err := tv.checkFees(tx, feesToBtFeeQuote(tv.settings.Policy.GetMinMiningTxFee())); err != nil {\n        return err\n    }\n}\n</code></pre> <p>When validating transactions from blocks mined by other nodes, policy checks should be skipped because these blocks are already valid due to proof-of-work, and the transactions must be accepted to maintain consensus, even if they don't meet local policy preferences.</p>"},{"location":"topics/services/validator/#skip-policy-checks-usage","title":"Skip Policy Checks - Usage","text":"<p>To use this feature:</p> <ul> <li>When directly calling the validator: Use the <code>WithSkipPolicyChecks(true)</code> option</li> <li>When using the gRPC API endpoint: Set the <code>skip_policy_checks</code> field to <code>true</code> in the <code>ValidateTransactionRequest</code> message</li> <li>In services like Subtree Validation: The option is applied automatically when validating block transactions</li> </ul>"},{"location":"topics/services/validator/#232-transaction-format-extension","title":"2.3.2. Transaction Format Extension","text":"<p>The Validator Service automatically handles transaction format conversion during the validation pipeline, enabling support for both standard Bitcoin format and Extended Format (BIP-239) transactions.</p>"},{"location":"topics/services/validator/#extension-process","title":"Extension Process","text":"<p>When a transaction arrives in standard Bitcoin format (non-extended), the validator automatically extends it before validation:</p> <ol> <li>Detection: The validator checks <code>tx.IsExtended()</code> before validation begins</li> <li>Parent Lookup: Queries the UTXO store for all parent transactions referenced by the transaction's inputs</li> <li>Input Decoration: For each input, the system retrieves and adds:</li> </ol> <pre><code>tx.Inputs[idx].PreviousTxSatoshis = parentTx.Outputs[vout].Satoshis\ntx.Inputs[idx].PreviousTxScript = parentTx.Outputs[vout].LockingScript\n</code></pre> <ol> <li>In-Memory Extension: Transaction is marked as extended (not persisted to storage)</li> <li>Validation: Proceeds with full validation using the extended data</li> </ol> <p>This extension process occurs transparently at multiple checkpoints in the validation pipeline:</p> <ul> <li><code>Validator.Validate()</code> in <code>services/validator/Validator.go</code> - Initial format check before validation</li> <li><code>Validator.validateConsensusRules()</code> in <code>services/validator/Validator.go</code> - Before consensus rule checks</li> <li><code>Validator.validateScripts()</code> in <code>services/validator/Validator.go</code> - Before script validation</li> </ul>"},{"location":"topics/services/validator/#implementation-details","title":"Implementation Details","text":"<p>Key Functions:</p> <ul> <li><code>getTransactionInputBlockHeightsAndExtendTx()</code> - Orchestrates the extension process, fetching parent data and decorating inputs</li> <li><code>getUtxoBlockHeightsAndExtendTx()</code> - Retrieves block heights and extends transactions in parallel</li> <li><code>PreviousOutputsDecorate()</code> - UTXO store method that decorates transaction inputs with previous output data</li> <li><code>extendTransaction()</code> - Wrapper function for the extension logic</li> </ul> <p>Performance Optimization:</p> <ul> <li>Parallel batch queries: Parent transactions are looked up concurrently using Go's errgroup pattern</li> <li>Configurable batching: UTXO store queries are batched based on <code>UtxoStore.GetBatcherSize</code> setting</li> <li>Idempotent operation: Already-decorated inputs are skipped automatically</li> <li>In-memory only: Extension happens entirely in memory with no disk I/O overhead</li> <li>Sub-millisecond lookups: Highly optimized UTXO store (Aerospike/SQL) combined with txmeta cache provides extremely fast parent transaction retrieval</li> </ul> <p>Code Example:</p> <pre><code>// From Validator.Validate() method\nif !tx.IsExtended() {\n    // Get block heights and extend the transaction\n    if utxoHeights, err = v.getTransactionInputBlockHeightsAndExtendTx(ctx, tx, txID); err != nil {\n        return nil, errors.NewProcessingError(\"[Validate][%s] error getting transaction input block heights\", txID, err)\n    }\n}\n</code></pre>"},{"location":"topics/services/validator/#error-handling","title":"Error Handling","text":"<p>If parent transactions cannot be found during the extension process:</p> <ul> <li>Returns <code>ErrTxMissingParent</code> error to the client</li> <li>Transaction validation fails gracefully</li> <li>Provides clear error message indicating which parent transaction is missing</li> <li>Client can retry after ensuring parent transactions are validated/confirmed</li> </ul> <p>Common scenarios requiring parent transactions:</p> <ul> <li>Child-pays-for-parent (CPFP): Transaction chains where child hasn't been validated yet</li> <li>Concurrent submissions: Parent and child transactions submitted simultaneously</li> <li>Block validation: Transactions referencing outputs from the same block being processed</li> </ul>"},{"location":"topics/services/validator/#format-flexibility-benefits","title":"Format Flexibility Benefits","text":"<p>This automatic extension mechanism provides several advantages:</p> <ol> <li>Backward Compatibility: Existing Bitcoin wallets work without modification</li> <li>No Client Changes Required: Wallets can continue using standard Bitcoin transaction format</li> <li>Optimized When Available: Extended format transactions skip the lookup step for faster processing</li> <li>Transparent Operation: Format handling is invisible to the client</li> <li>Storage Efficiency: All transactions stored in non-extended format regardless of ingress format</li> </ol> <p>For more details on transaction format handling across the system, see the Transaction Data Model documentation.</p>"},{"location":"topics/services/validator/#24-script-verification","title":"2.4. Script Verification","text":"<p>The Validator supports multiple script verification implementations through a flexible interpreter architecture. Three different script interpreters are supported:</p> <ol> <li> <p>GoBT Interpreter (<code>TxInterpreterGoBT</code>):</p> <ul> <li>Based on the Go-BT library</li> <li>Default interpreter for basic script validation</li> </ul> </li> <li> <p>GoSDK Interpreter (<code>TxInterpreterGoSDK</code>):</p> <ul> <li>Based on the Go-SDK library</li> <li>Provides advanced script validation capabilities</li> </ul> </li> <li> <p>GoBDK Interpreter (<code>TxInterpreterGoBDK</code>):</p> <ul> <li>Based on the Go-BDK library</li> <li>Optimized for performance in high-throughput scenarios</li> <li>Includes specialized Bitcoin script validation features</li> </ul> </li> </ol> <p>The script verification process:</p> <ol> <li> <p>Each transaction input's unlocking script is validated against its corresponding output's locking script</p> </li> <li> <p>The interpreter evaluates if the combined script executes successfully and leaves 'true' on the stack</p> </li> <li> <p>The script verification is context-aware, considering current block height and network parameters</p> </li> </ol> <p>Script verification can be configured using the <code>validator_scriptVerificationLibrary</code> setting, which defaults to \"VerificatorGoBT\".</p>"},{"location":"topics/services/validator/#25-error-handling-and-transaction-rejection","title":"2.5. Error Handling and Transaction Rejection","text":"<p>The Validator implements a structured error handling system to categorize and report different types of validation failures:</p> <ol> <li> <p>Error Types:</p> <ul> <li><code>TxInvalidError</code>: Generated when a transaction fails basic validation rules</li> <li><code>ProcessingError</code>: Occurs during transaction processing issues</li> <li><code>ConfigurationError</code>: Indicates validator configuration problems</li> <li><code>ServiceError</code>: Represents broader service-level issues</li> </ul> </li> <li> <p>Rejection Flow:</p> <ul> <li>When a transaction is rejected, detailed error information is captured</li> <li>Rejection reasons are categorized (e.g., invalid inputs, script failure, insufficient fees)</li> <li>Rejected transactions are published to a dedicated Kafka topic if configured</li> <li>The P2P service receives notifications about rejected transactions</li> </ul> </li> <li> <p>Error Propagation:</p> <ul> <li>Errors are wrapped and propagated through the system with context information</li> <li>GRPC error codes translate internal errors for API responses</li> <li>Detailed error messages assist in diagnosing validation issues</li> </ul> </li> </ol>"},{"location":"topics/services/validator/#26-concurrent-processing","title":"2.6. Concurrent Processing","text":"<p>The Validator leverages concurrency to optimize transaction processing performance:</p> <ol> <li> <p>Parallel UTXO Saving:</p> <ul> <li>The <code>saveInParallel</code> flag enables concurrent UTXO updates</li> <li>Improves throughput by processing multiple transactions simultaneously</li> </ul> </li> <li> <p>Batching:</p> <ul> <li>Transactions can be validated in batches for higher throughput</li> <li>Batch processing is configurable and used by both Propagation and Subtree Validation services</li> <li>The <code>TriggerBatcher()</code> method initiates batch processing when sufficient transactions accumulate</li> </ul> </li> <li> <p>Error Group Pattern:</p> <ul> <li>Uses Go's <code>errgroup</code> package for coordinated concurrent execution</li> <li>Maintains proper error propagation in concurrent processing flows</li> </ul> </li> <li> <p>Two-Phase Commit Process:</p> <ul> <li>The <code>twoPhaseCommitTransaction</code> method ensures atomic transaction processing</li> <li>Prevents partial updates in case of failures during concurrent processing</li> </ul> </li> </ol>"},{"location":"topics/services/validator/#27-post-validation-updating-stores-and-propagating-the-transaction","title":"2.7. Post-validation: Updating stores and propagating the transaction","text":"<p>Once a Tx is validated, the Validator will update the UTXO store with the new Tx data. Then, it will notify the Block Assembly service and any P2P subscribers about the new Tx.</p> <p></p> <ul> <li>The Server receives a validation request and calls the <code>Validate</code> method on the Validator struct.</li> <li> <p>If the transaction is valid:</p> <ul> <li>The Validator marks the transaction's input UTXOs as spent in the UTXO Store.</li> <li>The Validator registers the new transaction in the UTXO Store.</li> <li>The Validator sends transaction metadata to the Subtree Validation Service via Kafka topic (<code>txmeta</code>).</li> <li>The Validator sends the transaction to the Block Assembly Service via direct gRPC calls.</li> <li>The Validator stores the new UTXOs generated by the transaction in the UTXO Store.</li> </ul> </li> <li> <p>If the transaction is invalid:</p> <ul> <li>The Server sends invalid transaction notifications to all P2P Service subscribers.</li> <li>The rejected Tx is not stored or tracked in any store, and it is simply discarded.</li> </ul> </li> </ul> <p>We can see the submission to the Subtree Validation Service here:</p> <p></p> <p>We can dive deeper into the submission to the Block Assembly:</p> <p></p> <p>The Validator notifies the Block Assembly service of new transactions through gRPC calls via the Block Assembly client interface by calling the <code>Store()</code> method on the Block Assembly client.</p>"},{"location":"topics/services/validator/#configuration-settings-affecting-validator-behavior","title":"Configuration Settings Affecting Validator Behavior","text":"<p>The Validator service behavior is controlled by several key configuration parameters:</p> <ul> <li><code>KafkaMaxMessageBytes</code> (default: 1MB): Controls size-based routing - large transactions that exceed this threshold are routed via HTTP instead of Kafka to avoid message size limitations.</li> <li><code>UseLocalValidator</code> (default: false): Determines whether to use a local validator instance or connect to a remote validator service via gRPC.</li> <li><code>KafkaWorkers</code> (default: 0): Controls the number of concurrent Kafka message processing workers. When set to 0, Kafka consumer processing is disabled.</li> <li><code>HTTPRateLimit</code> (default: 1024): Sets the rate limit for HTTP API requests to prevent service overload.</li> <li><code>VerboseDebug</code> (default: false): Enables detailed validation logging for troubleshooting.</li> </ul>"},{"location":"topics/services/validator/#rejected-transaction-handling","title":"Rejected Transaction Handling","text":"<p>When the Transaction Validator Service identifies an invalid transaction, it employs a Kafka-based notification system to inform other components of the system. Here's an overview of this process:</p> <p></p> <ol> <li> <p>Transaction Validation:</p> <ul> <li>The Validator receives a transaction for validation via a gRPC call to the <code>ValidateTransaction</code> method.</li> <li>The Validator performs its checks, including script verification, UTXO spending, and other validation rules.</li> </ul> </li> <li> <p>Identification of Invalid Transactions:</p> <ul> <li>If the transaction fails any of the validation checks, it is deemed invalid (rejected).</li> </ul> </li> <li> <p>Notification of Rejected Transactions:</p> <ul> <li>When a transaction is rejected, the Validator publishes information about the rejected transaction to a dedicated Kafka topic (<code>rejectedTx</code>).</li> <li>This is done using the Rejected Tx Kafka Producer, which is configured via the <code>kafka_rejectedTxConfig</code> setting.</li> </ul> </li> <li> <p>Kafka Message Content:</p> <ul> <li> <p>The Kafka message for a rejected transaction typically includes:</p> <ul> <li>The transaction ID (hash)</li> <li>The reason for rejection (error message)</li> </ul> </li> </ul> </li> <li> <p>Consumption of Rejected Transaction Notifications:</p> <ul> <li>Other services in the system, such as the P2P Service, can subscribe to this Kafka topic.</li> <li>By consuming messages from this topic, these services receive notifications about rejected transactions and can take appropriate action (e.g., banning peers that send invalid transactions).</li> </ul> </li> </ol>"},{"location":"topics/services/validator/#271-two-phase-transaction-commit-process","title":"2.7.1. Two-Phase Transaction Commit Process","text":"<p>The Validator implements a two-phase commit process for transaction creation and addition to block assembly:</p> <ol> <li> <p>Phase 1 - Transaction Creation with Locked Flag:</p> <ul> <li>When a transaction is created, it is initially stored in the UTXO store with an \"locked\" flag set to <code>true</code>.</li> <li>This flag prevents the transaction outputs from being spent while it's in the process of being validated and added to block assembly, protecting against potential double-spend attempts.</li> </ul> </li> <li> <p>Phase 2 - Unsetting the Locked Flag:</p> <ul> <li>The locked flag is unset in two key scenarios:</li> </ul> <p>a. After Successful Addition to Block Assembly:</p> <pre><code>- When a transaction is successfully validated and added to the block assembly, the Validator service immediately unsets the \"locked\" flag (sets it to `false`).\n- This makes the transaction outputs available for spending in subsequent transactions, even before the transaction is mined in a block.\n</code></pre> <p>b. When Mined in a Block (Fallback Mechanism):</p> <pre><code>- As a fallback mechanism, if the flag hasn't been unset already, it will be unset when the transaction is mined in a block.\n- When the transaction is mined in a block and that block is processed by the Block Validation service, the \"locked\" flag is unset (set to `false`) during the `SetMinedMulti` operation.\n</code></pre> </li> <li> <p>Ignoring Locked Flag for Block Transactions:</p> <ul> <li>When processing transactions that are part of a block (as opposed to new transactions to include in an upcoming block), the validator can be configured to ignore the locked flag.</li> <li>This is necessary because transactions in a block have already been validated by miners and must be accepted regardless of their locked status.</li> <li>The validator uses the <code>WithIgnoreLocked</code> option to control this behavior during transaction validation.</li> </ul> </li> </ol> <p>This two-phase commit approach ensures that transactions are only made spendable after they've been successfully added to block assembly, reducing the risk of race conditions and double-spend attempts during the transaction processing lifecycle.</p> <p>For a comprehensive explanation of the two-phase commit process across the entire system, see the Two-Phase Transaction Commit Process documentation.</p>"},{"location":"topics/services/validator/#3-grpc-protobuf-definitions","title":"3. gRPC Protobuf Definitions","text":"<p>The Validator, when run as a service, uses gRPC for communication between nodes. The protobuf definitions used for defining the service methods and message formats can be found in the protobuf documentation.</p>"},{"location":"topics/services/validator/#4-data-model","title":"4. Data Model","text":"<p>The Validation Service processes transactions in multiple formats:</p> <ul> <li>Transaction Data Model: Comprehensive documentation covering both standard Bitcoin format and Extended Format (BIP-239), including automatic format conversion and storage strategies.</li> </ul>"},{"location":"topics/services/validator/#5-technology","title":"5. Technology","text":"<p>The code snippet you've provided utilizes a variety of technologies and libraries, each serving a specific purpose within the context of a Bitcoin SV (BSV) blockchain-related application. Here's a breakdown of these technologies:</p> <ol> <li> <p>Go (Golang): The programming language used for the entire codebase.</p> </li> <li> <p>gRPC: Google's Remote Procedure Call system, used here for server-client communication. It enables the server to expose specific methods that clients can call remotely. This is only used if the component is started as a service.</p> </li> <li> <p>Kafka (by Apache): A distributed streaming platform (optionally) used here for message handling. Kafka is used for distributing transaction validation data to the block assembly.</p> <ul> <li>Kafka in Teranode: Overview of Kafka integration with the Validator component</li> <li>Kafka Message Format: Details on message formats used in Kafka communication</li> </ul> </li> <li> <p>Sarama: A Go library for Apache Kafka.</p> </li> <li> <p>Go-Bitcoin: A Go library that provides utilities and tools for working with Bitcoin, including transaction parsing and manipulation.</p> </li> <li> <p>BSV Libraries: Go libraries from the bsv-blockchain organization for Bitcoin SV, used for transaction-related operations.</p> </li> <li> <p>Other Utilities and Libraries:</p> <ul> <li><code>sync/atomic</code>, <code>strings</code>, <code>strconv</code>, <code>time</code>, <code>io</code>, <code>net/url</code>, <code>os</code>, <code>bytes</code>, and other standard Go packages for various utility functions.</li> <li><code>github.com/ordishs/gocore</code> and <code>github.com/ordishs/go-utils/batcher</code>: Utility libraries, used for handling core functionalities and batch processing.</li> <li><code>github.com/opentracing/opentracing-go</code>: Used for distributed tracing.</li> </ul> </li> </ol>"},{"location":"topics/services/validator/#6-directory-structure-and-main-files","title":"6. Directory Structure and Main Files","text":"<pre><code>./services/validator\n\u251c\u2500\u2500 Client.go                    # Contains client-side logic for interacting with the Validator\n\u251c\u2500\u2500 Interface.go                 # Defines interfaces for the Validator\n\u251c\u2500\u2500 ScriptVerificatorGoBDK.go    # Implements script verification using a Go Bitcoin Development Kit\n\u251c\u2500\u2500 ScriptVerificatorGoBT.go     # Implements script verification using Go Bitcoin Tools\n\u251c\u2500\u2500 ScriptVerificatorGoSDK.go    # Implements script verification using a Go Software Development Kit\n\u251c\u2500\u2500 Server.go                    # Implements the server-side logic of the Validator\n\u251c\u2500\u2500 TxValidator.go               # Contains specific logic for validating transactions\n\u251c\u2500\u2500 Validator.go                 # Contains the main logic for validator functionalities\n\u251c\u2500\u2500 data.go                      # Contains data structures or constants used in the validator service\n\u251c\u2500\u2500 metrics.go                   # Contains code for metrics collection within the Validator\n\u251c\u2500\u2500 options.go                   # Defines configuration options or settings for the validator service\n\u251c\u2500\u2500 policy.go                    # Defines validation policies or rules\n\u2514\u2500\u2500 validator_api\n    \u251c\u2500\u2500 validator_api.pb.go          # Auto-generated Go code from validator_api.proto\n    \u251c\u2500\u2500 validator_api.proto          # Protocol Buffers definition file for the validator API\n    \u2514\u2500\u2500 validator_api_grpc.pb.go     # Auto-generated gRPC specific code from validator_api.proto\n</code></pre>"},{"location":"topics/services/validator/#7-how-to-run","title":"7. How to run","text":"<p>To run the Validator locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -Validator=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Validator locally.</p>"},{"location":"topics/services/validator/#8-configuration","title":"8. Configuration","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the Validator Settings Reference.</p>"},{"location":"topics/services/validator/#9-other-resources","title":"9. Other Resources","text":"<p>Validator Reference</p>"},{"location":"topics/stores/blob/","title":"\ud83d\uddc2\ufe0f Blob Server","text":""},{"location":"topics/stores/blob/#index","title":"Index","text":"<ul> <li>1. Description</li> <li>2. Architecture</li> <li>3. Technology<ul> <li>3.1 Overview</li> <li>3.2 Store Options</li> <li>3.3 Concurrent Access Patterns</li> <li>3.4 HTTP REST API Server</li> </ul> </li> <li>4. Data Model</li> <li>5. Use Cases<ul> <li>5.1. Asset Server (HTTP): Get Transactions</li> <li>5.2. Asset Server (HTTP): Get Subtrees</li> <li>5.3. Block Assembly</li> <li>5.4. Block Validation</li> <li>5.5 Propagation: TXStore Set()</li> </ul> </li> <li>6. Directory Structure and Main Files</li> <li>7. Locally Running the store</li> <li>8. Configuration Options</li> <li>9. Other Resources</li> </ul>"},{"location":"topics/stores/blob/#1-description","title":"1. Description","text":"<p>The Blob Server is a generic datastore that can be used for any specific data model. In the current Teranode implementation, it is used to store transactions (extended tx) and subtrees.</p> <p>The Blob Server provides a set of methods to interact with the TX and Subtree storage implementations.</p> <ol> <li> <p>Health: <code>Health(ctx)</code></p> <ul> <li>Purpose: Checks the health status of the Blob Server.</li> </ul> </li> <li> <p>Exists: <code>Exists(ctx, key)</code></p> <ul> <li>Purpose: Determines if a given key exists in the store.</li> </ul> </li> <li> <p>Get: <code>Get(ctx, key)</code></p> <ul> <li>Purpose: Retrieves the value associated with a given key.</li> </ul> </li> <li> <p>GetIoReader: <code>GetIoReader(ctx, key)</code></p> <ul> <li>Purpose: Retrieves an <code>io.ReadCloser</code> for the value associated with a given key, useful for streaming large data.</li> </ul> </li> <li> <p>Set: <code>Set(ctx, key, value, opts...)</code></p> <ul> <li>Purpose: Sets a key-value pair in the store.</li> </ul> </li> <li> <p>SetFromReader: <code>SetFromReader(ctx, key, value, opts...)</code></p> <ul> <li>Purpose: Sets a key-value pair in the store from an <code>io.ReadCloser</code>, useful for streaming large data.</li> </ul> </li> <li> <p>SetTTL: <code>SetTTL(ctx, key, ttl)</code></p> <ul> <li>Purpose: Sets a Time-To-Live for a given key.</li> </ul> </li> <li> <p>Del: <code>Del(ctx, key)</code></p> <ul> <li>Purpose: Deletes a key and its associated value from the store.</li> </ul> </li> <li> <p>Close: <code>Close(ctx)</code></p> <ul> <li>Purpose: Closes the Blob Server connection or any associated resources.</li> </ul> </li> </ol>"},{"location":"topics/stores/blob/#2-architecture","title":"2. Architecture","text":"<p>The Blob Server is a store interface, with implementations for Tx Store and Subtree Store.</p> <p></p> <p>The Blob Server implementations for Tx Store and Subtree Store are injected into the various services that require them. They are initialised in the <code>daemon/daemon_stores.go</code> file and passed into the services as a dependency. See below:</p> <pre><code>func getTxStore(logger ulogger.Logger) blob.Store {\n if txStore != nil {\n  return txStore\n }\n\n txStoreUrl, err, found := gocore.Config().GetURL(\"txstore\")\n if err != nil {\n  panic(err)\n }\n if !found {\n  panic(\"txstore config not found\")\n }\n txStore, err = blob.NewStore(logger, txStoreUrl, options.WithHashPrefix(10))\n if err != nil {\n  panic(err)\n }\n\n return txStore\n}\n\nfunc getSubtreeStore(logger ulogger.Logger) blob.Store {\n if subtreeStore != nil {\n  return subtreeStore\n }\n\n subtreeStoreUrl, err, found := gocore.Config().GetURL(\"subtreestore\")\n if err != nil {\n  panic(err)\n }\n if !found {\n  panic(\"subtreestore config not found\")\n }\n subtreeStore, err = blob.NewStore(logger, subtreeStoreUrl, options.WithHashPrefix(10))\n if err != nil {\n  panic(err)\n }\n\n return subtreeStore\n}\n</code></pre> <p>The following diagram provides a deeper level of detail into the Blob Store's internal components and their interactions:</p> <p></p>"},{"location":"topics/stores/blob/#3-technology","title":"3. Technology","text":""},{"location":"topics/stores/blob/#31-overview","title":"3.1 Overview","text":"<p>Key technologies involved:</p> <ol> <li> <p>Go Programming Language (Golang):</p> <ul> <li>A statically typed, compiled language known for its simplicity and efficiency, especially in concurrent operations and networked services.</li> <li>The primary language used for implementing the service's logic.</li> </ul> </li> </ol>"},{"location":"topics/stores/blob/#32-store-options","title":"3.2 Store Options","text":"<p>The Blob Server supports various backends, each suited to different storage requirements and environments.</p> <ul> <li> <p>Batcher: Provides batch processing capabilities for storage operations.</p> </li> <li> <p>File: Utilizes the local file system for storage.</p> </li> <li> <p>HTTP: Implements an HTTP client for interacting with a remote blob storage server.</p> </li> <li> <p>Local TTL: Provides local Time-to-Live (TTL) functionality for managing data expiration.</p> </li> <li> <p>Memory: In-memory storage for temporary and fast data access.</p> </li> <li> <p>Null: A no-operation store for testing or disabling storage features.</p> </li> <li> <p>Amazon S3: Integration with Amazon Simple Storage Service (S3) for cloud storage. Amazon S3</p> </li> </ul> <p>Each store option is implemented in its respective subdirectory within the <code>stores/blob/</code> directory.</p> <p>The system also includes a main server implementation (<code>server.go</code>) that provides an HTTP interface for blob storage operations.</p> <p>Options for configuring these stores are managed through the <code>options</code> package.</p>"},{"location":"topics/stores/blob/#33-concurrent-access-patterns","title":"3.3 Concurrent Access Patterns","text":"<p>The Blob Server includes a <code>ConcurrentBlob</code> implementation that provides thread-safe access to blob storage operations with optimized concurrent access patterns. This feature is particularly important in high-concurrency environments where the same blob might be requested multiple times simultaneously.</p>"},{"location":"topics/stores/blob/#key-features","title":"Key Features","text":"<ul> <li>Double-Checked Locking Pattern: Ensures that only one fetch operation occurs at a time for each unique key, while allowing concurrent operations on different keys</li> <li>Generic Type Support: Parametrized by a key type K that must satisfy <code>chainhash.Hash</code> constraints for type-safe handling</li> <li>Duplicate Operation Prevention: Avoids duplicate network or disk operations when multiple goroutines request the same blob simultaneously</li> <li>Efficient Resource Usage: Other goroutines wait for completion rather than duplicating work</li> </ul>"},{"location":"topics/stores/blob/#usage-pattern","title":"Usage Pattern","text":"<pre><code>// Create a concurrent blob instance\nconcurrentBlob := blob.NewConcurrentBlob[chainhash.Hash](blobStore, options...)\n\n// Get a blob with automatic fetching if not present\nreader, err := concurrentBlob.Get(ctx, key, fileType, func() (io.ReadCloser, error) {\n    // This function is called only if the blob doesn't exist\n    return fetchBlobFromSource(key)\n})\n</code></pre> <p>The <code>ConcurrentBlob</code> wrapper is particularly useful for services that need to fetch the same data concurrently, such as block validation or transaction processing services.</p>"},{"location":"topics/stores/blob/#34-http-rest-api-server","title":"3.4 HTTP REST API Server","text":"<p>The Blob Server includes a comprehensive HTTP REST API server implementation (<code>HTTPBlobServer</code>) that provides full HTTP access to blob storage operations. This server implements the standard <code>http.Handler</code> interface and can be easily integrated into existing HTTP server infrastructure.</p>"},{"location":"topics/stores/blob/#supported-http-endpoints","title":"Supported HTTP Endpoints","text":"<ul> <li>GET /health: Health check endpoint returning server status</li> <li>GET /{key}: Retrieve blob by key with optional range support</li> <li>POST /{key}: Store new blob data</li> <li>PUT /{key}: Update existing blob data</li> <li>DELETE /{key}: Delete blob by key</li> <li>GET /{key}/dah: Get Delete-At-Height information for a blob</li> <li>POST /{key}/dah: Set Delete-At-Height for a blob</li> </ul>"},{"location":"topics/stores/blob/#usage-example","title":"Usage Example","text":"<pre><code>// Create HTTP blob server\nhttpServer := blob.NewHTTPBlobServer(blobStore, logger)\n\n// Start HTTP server\nhttp.Handle(\"/blob/\", http.StripPrefix(\"/blob\", httpServer))\nlog.Fatal(http.ListenAndServe(\":8080\", nil))\n</code></pre> <p>The HTTP server is particularly useful for external integrations, debugging, and providing web-based access to blob storage functionality.</p>"},{"location":"topics/stores/blob/#4-data-model","title":"4. Data Model","text":"<ul> <li> <p>Subtree Data Model: Contain lists of transaction IDs and their Merkle root.</p> </li> <li> <p>Extended Transaction Data Model: Include additional metadata to facilitate processing.</p> </li> </ul>"},{"location":"topics/stores/blob/#5-use-cases","title":"5. Use Cases","text":""},{"location":"topics/stores/blob/#51-asset-server-http-get-transactions","title":"5.1. Asset Server (HTTP): Get Transactions","text":""},{"location":"topics/stores/blob/#52-asset-server-http-get-subtrees","title":"5.2. Asset Server (HTTP): Get Subtrees","text":""},{"location":"topics/stores/blob/#53-block-assembly","title":"5.3. Block Assembly","text":"<p>New Subtree and block mining scenarios:</p> <p></p> <p>Reorganizing subtrees:</p> <p></p>"},{"location":"topics/stores/blob/#54-block-validation","title":"5.4. Block Validation","text":"<p>Service:</p> <p></p> <p>gRPC endpoints:</p> <p></p>"},{"location":"topics/stores/blob/#55-propagation-txstore-set","title":"5.5 Propagation: TXStore Set()","text":""},{"location":"topics/stores/blob/#6-directory-structure-and-main-files","title":"6. Directory Structure and Main Files","text":"<pre><code>./stores/blob/\n\u251c\u2500\u2500 Interface.go                # Interface definitions for the project.\n\u251c\u2500\u2500 batcher                     # Batching functionality for efficient processing.\n\u2502   \u2514\u2500\u2500 batcher.go              # Main batcher functionality.\n\u251c\u2500\u2500 factory.go                  # Factory methods for creating instances.\n\u251c\u2500\u2500 file                        # File system based implementations.\n\u2502   \u251c\u2500\u2500 file.go                 # File system handling.\n\u2502   \u2514\u2500\u2500 file_test.go            # Test cases for file system functions.\n\u251c\u2500\u2500 http                        # HTTP client implementation for remote blob storage.\n\u2502   \u2514\u2500\u2500 http.go                 # HTTP specific functionality.\n\u251c\u2500\u2500 localttl                    # Local Time-to-Live functionality.\n\u2502   \u2514\u2500\u2500 localttl.go             # Local TTL handling.\n\u251c\u2500\u2500 memory                      # In-memory implementation.\n\u2502   \u2514\u2500\u2500 memory.go               # In-memory data handling.\n\u251c\u2500\u2500 null                        # Null implementation (no-op).\n\u2502   \u2514\u2500\u2500 null.go                 # Null pattern implementation.\n\u251c\u2500\u2500 options                     # Options and configurations.\n\u2502   \u251c\u2500\u2500 Options.go              # General options for the project.\n\u2502   \u2514\u2500\u2500 Options_test.go         # Test cases for options.\n\u251c\u2500\u2500 s3                          # Amazon S3 cloud storage implementation.\n\u2502   \u2514\u2500\u2500 s3.go                   # S3 specific functionality.\n\u251c\u2500\u2500 server.go                   # HTTP server implementation for blob storage.\n\u2514\u2500\u2500 server_test.go              # Test cases for the server implementation.\n</code></pre>"},{"location":"topics/stores/blob/#7-locally-running-the-store","title":"7. Locally Running the store","text":"<p>The Blob Server cannot be run independently. It is instantiated as part of the main.go initialization and directly used by the services that require it.</p>"},{"location":"topics/stores/blob/#8-configuration-options","title":"8. Configuration Options","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the ublob Store Settings Reference.</p>"},{"location":"topics/stores/blob/#9-other-resources","title":"9. Other Resources","text":"<p>Blob Server Reference</p>"},{"location":"topics/stores/utxo/","title":"\ud83d\uddc3\ufe0f UTXO Store","text":""},{"location":"topics/stores/utxo/#index","title":"Index","text":"<ol> <li>Description</li> <li>Architecture</li> <li>UTXO: Data Model<ul> <li>3.1. What is an UTXO?</li> <li>3.2. How are UTXOs stored?</li> <li>3.3. UTXO Meta Data</li> </ul> </li> <li>Use Cases<ul> <li>4.1. Asset Server</li> <li>4.2. Block Persister</li> <li>4.3. Block Assembly</li> <li>4.4. Block Validation</li> <li>4.5. Subtree Validation</li> <li>4.6. Transaction Validator</li> <li>4.7. UTXO Batch Processing and External Storage mode</li> <li>4.8. Alert System and UTXO Management</li> <li>4.9. Unmined Transaction Management</li> </ul> </li> <li>Technology<ul> <li>5.1. Language and Libraries</li> <li>5.2. Data Stores</li> <li>5.3. Data Purging</li> </ul> </li> <li>Performance Optimizations<ul> <li>6.1. Shared Buffer Optimization</li> </ul> </li> <li>Directory Structure and Main Files</li> <li>Running the Store Locally<ul> <li>How to run</li> </ul> </li> <li>Configuration and Settings</li> <li>Other Resources</li> </ol>"},{"location":"topics/stores/utxo/#1-description","title":"1. Description","text":"<p>The UTXO set represents the current state of ownership of all Satoshi tokens in existence. Except for Coinbase transactions, every other valid transaction spends at least one UTXO and creates zero or more new UTXOs, plus zero or more locked outputs. The UTXOs that are being \u2018spent\u2019 come from previously successful transactions, and these may be recorded in the current or a previous block. An unspent output may be created and spent in a few milliseconds, or it may remain unspent for decades, meaning that the unspent outputs must persist for as long as they remain unspent.</p> <p>The UTXO Store is responsible for tracking spendable UTXOs (the UTXO set), based on the longest honest chain-tip in the network. These are UTXOs that can be used as inputs in new transactions. The UTXO Store is an internal datastore used by some of the services, such as the Asset Server, the TX Validator and the Block Assembly. The main purpose of this store is to maintain the UTXO data on behalf of other micro-services.</p> <p>It handles the core functionalities of the UTXO Store:</p> <ul> <li>Health: Check the health status of the UTXO store service.</li> <li>Get: Retrieve a specific UTXO.</li> <li>GetMeta: Retrieve a specific UTXO meta data.</li> <li>Create: Add new UTXOs to the store.</li> <li>Spend/Unspend: Mark UTXOs as spent or reverse such markings, respectively.</li> <li>Delete: Remove UTXOs from the store.</li> <li>Block Height Management: Set and retrieve the current blockchain height, which can be crucial for determining the spendability of certain UTXOs based on locktime conditions.</li> <li>FreezeUTXOs / UnFreezeUTXOs: Mark UTXOs as frozen or unfrozen, in scenarios involving alert systems or temporary holds on specific UTXOs.</li> <li>ReAssignUTXO: Reassign a UTXO to a different owner.</li> </ul> <p>Principles:</p> <ul> <li>All operations are atomic.</li> <li>All data is shared across servers with standard sharing algorithms.</li> <li>In production, the data is stored in a Master and Replica configuration.</li> <li>No centralised broker - all clients know where each hash is stored.</li> <li>No cross-transaction state is stored.</li> </ul> <p>The UTXO Store includes functionality to freeze and unfreeze UTXOs, as well as re-assign them.</p> <ul> <li>BSV is the only blockchain that allows legal recourse for lost asset (token) recovery to legally rightful owners by design. The Alert System can also freeze assets based on court injunctions and legal notices.</li> <li>Teranode must be able to re-assign (a set of) UTXO(s) to another (specified) address at a specified block height.</li> </ul>"},{"location":"topics/stores/utxo/#2-architecture","title":"2. Architecture","text":"<p>The UTXO Store is a micro-service that is used by other micro-services to retrieve or store / modify UTXOs.</p> <p></p> <p>The UTXO Store uses a number of different datastores, either in-memory or persistent, to store the UTXOs.</p> <p></p>"},{"location":"topics/stores/utxo/#utxo-store-operations-flow","title":"UTXO Store Operations Flow","text":"<p>The following diagram illustrates the complete flow of UTXO operations across all services, including transaction validation, conflict resolution, block mining, reorganization handling, alert system operations, and subtree validation:</p> <p></p> <p>The UTXO store implementation is consistent within a Teranode node (every service connects to the same specific implementation), and it is defined via settings (<code>utxostore</code>), as it can be seen in the following code fragment (<code>main.go</code>):</p> <pre><code>func getUtxoStore(ctx context.Context, logger ulogger.Logger) utxostore.Interface {\n if utxoStore != nil {\n  return utxoStore\n }\n\n utxoStoreURL, err, found := gocore.Config().GetURL(\"utxostore\")\n if err != nil {\n  panic(err)\n }\n if !found {\n  panic(\"no utxostore setting found\")\n }\n utxoStore, err = utxo_factory.NewStore(ctx, logger, utxoStoreURL, \"main\")\n if err != nil {\n  panic(err)\n }\n\n return utxoStore\n}\n</code></pre> <p>The following diagram provides a deeper level of detail into the UTXO Store's internal components and their interactions:</p> <p></p> <p>The following datastores are supported (either in development / experimental or production mode):</p> <ol> <li> <p>Aerospike.</p> </li> <li> <p>Memory (In-Memory Store).</p> </li> <li> <p>Sql (Postgres and SQLLite).</p> </li> <li> <p>Nullstore.</p> </li> </ol> <p>Notice how SqlLite and the In-Memory store are in-memory, while Aerospike and Postgres are persistent (and shared with other services).</p> <p>More details about the specific stores can be found in the Technology section.</p>"},{"location":"topics/stores/utxo/#3-utxo-data-model","title":"3. UTXO: Data Model","text":""},{"location":"topics/stores/utxo/#31-what-is-an-utxo","title":"3.1. What is an UTXO?","text":"<p>The Teranode UTXO is no different from Bitcoin UTXO. The following is a description of the Bitcoin UTXO model, focusing on the BSV implementation:</p> <ul> <li>Transaction Outputs: When a transaction occurs on the blockchain, it creates \"transaction outputs,\" which are essentially chunks of cryptocurrency value. Each output specifies an amount and a condition under which it can be spent (a cryptographic script key that the receiver owns).</li> </ul> <p>Under the external library <code>github.com/ordishs/go-bt/output.go</code>, we can see the structure of a transaction output.</p> <pre><code>type Output struct {\n Satoshis      uint64          `json:\"satoshis\"`\n LockingScript *bscript.Script `json:\"locking_script\"`\n}\n</code></pre> <p>Components of the <code>Output</code> struct:</p> <ol> <li> <p>Satoshis (<code>uint64</code>):</p> <ul> <li>The amount of BSV cryptocurrency associated with this output.</li> <li>The unit \"Satoshis\" refers to the smallest unit of Bitcoin (1 Bitcoin = 100 million Satoshis).</li> </ul> </li> <li> <p>LockingScript (<code>*bscript.Script</code>):</p> <ul> <li>This field represents the conditions that must be met to spend the Satoshis in this output.</li> <li>The <code>LockingScript</code>, often referred to as the \"scriptPubKey\" in Bitcoin's technical documentation, is a script written in Bitcoin's scripting language.</li> <li>This script contains cryptographic conditions to unlock the funds.</li> </ul> </li> </ol> <p>Equally, we can see how a list of outputs is part of a transaction (<code>github.com/ordishs/go-bt/tx.go</code>):</p> <pre><code>type Tx struct {\n Inputs   []*Input  `json:\"inputs\"`\n Outputs  []*Output `json:\"outputs\"`\n Version  uint32    `json:\"version\"`\n LockTime uint32    `json:\"locktime\"`\n}\n</code></pre> <ul> <li>Unspent Transaction Outputs (UTXOs): A UTXO is a transaction output that hasn't been used as an input in a new transaction.</li> </ul> <p>When a transaction occurs, it consumes one or more UTXOs as inputs and creates new UTXOs as outputs. The sum of the input UTXOs represents the total amount of Bitcoin being transferred, and the outputs represent the distribution of this amount after the transaction.</p> <p>To \"own\" bitcoins means to control UTXOs on the blockchain that can be spent by the user (i.e., the user has the private key to unlock these UTXOs).</p> <p>When a user creates a new transaction, the transaction references these UTXOs as inputs, proving his ownership by fulfilling the spending conditions set in these UTXOs (signing the transaction with the user's private key).</p> <p>Independent UTXOs can be processed in parallel, potentially improving the efficiency of transaction validation.</p> <p>UTXOs now have an additional state: frozen. A frozen UTXO cannot be spent until it is unfrozen.</p> <p>To know more about UTXOs, please check https://protocol.bsvblockchain.org/transaction-lifecycle/transaction-inputs-and-outputs.</p>"},{"location":"topics/stores/utxo/#32-how-are-utxos-stored","title":"3.2. How are UTXOs stored?","text":"<p>When storing the UTXOs (Unspent Transaction Outputs) associated to a given Tx in Aerospike (see <code>stores/utxo/aerospike/aerospike.go</code>, our primary persisted datastore, the following information is kept:</p> <ol> <li> <p>Inputs: The transaction inputs, represented as a slice of byte arrays (<code>inputs</code> bin). Each input includes the previous transaction output it spends from, as well as additional data such as the script and satoshis amount.</p> </li> <li> <p>Outputs: The transaction outputs, represented as a slice of byte arrays (<code>outputs</code> bin). Each output contains the value in satoshis and the locking script.</p> </li> <li> <p>Version: The version number of the transaction (<code>version</code> bin).</p> </li> <li> <p>Fee: The fee associated with the transaction (<code>fee</code> bin).</p> </li> <li> <p>Size in Bytes: The size of the transaction in bytes (<code>sizeInBytes</code> bin).</p> </li> <li> <p>UTXOs: A map representing the UTXOs associated with this transaction (<code>utxos</code> bin). The keys are the UTXO hashes, and the values are empty strings initially. The values will be populated, once the UTXO is spent, with the spending Tx Id.</p> </li> <li> <p>Number of UTXOs: The number of UTXOs in the <code>utxos</code> map (<code>recordUtxos</code> bin).</p> </li> <li> <p>Spent UTXOs: A counter for the number of UTXOs that have been spent (<code>spentUtxos</code> bin).</p> </li> <li> <p>Block IDs: The IDs of the blocks that include this transaction (<code>blockIDs</code> bin).</p> </li> <li> <p>Coinbase Flag: A boolean indicating whether this transaction is a coinbase transaction (<code>isCoinbase</code> bin).</p> </li> </ol> <p>These bins collectively store the necessary data to track the transaction's inputs, outputs, and its state within the blockchain.</p> <p>Once the UTXO is spent, the spending tx_id will be saved.</p> <ul> <li> <p>To compute the hash of the key, the caller must know the complete data and calculate the hash before calling the API.</p> </li> <li> <p>The hashing logic can be found in <code>UTXOHash.go</code>:</p> </li> </ul> <pre><code>func UTXOHash(previousTxid *chainhash.Hash, index uint32, lockingScript []byte, satoshis uint64) (*chainhash.Hash, error) {\n if len(lockingScript) == 0 {\n  return nil, fmt.Errorf(\"locking script is nil\")\n }\n\n if satoshis == 0 {\n  return nil, fmt.Errorf(\"satoshis is 0\")\n }\n\n utxoHash := make([]byte, 0, 256)\n utxoHash = append(utxoHash, previousTxid.CloneBytes()...)\n utxoHash = append(utxoHash, bt.VarInt(index).Bytes()...)\n utxoHash = append(utxoHash, lockingScript...)\n utxoHash = append(utxoHash, bt.VarInt(satoshis).Bytes()...)\n\n hash := crypto.Sha256(utxoHash)\n chHash, err := chainhash.NewHash(hash)\n if err != nil {\n  return nil, err\n }\n\n return chHash, nil\n}\n</code></pre> <ul> <li>The existence of the key confirms the details of the UTXO are the same as what the caller has.</li> </ul>"},{"location":"topics/stores/utxo/#33-utxo-meta-data","title":"3.3. UTXO Meta Data","text":"<p>The <code>Data</code> struct (<code>stores/utxo/meta/data.go</code>), referred moving forward as <code>UTXO Meta Data</code>, provides a convenience structure to retrieve meta data associated to a transaction out of the UTXO Store.</p> <pre><code>type Data struct {\n Tx             *bt.Tx           `json:\"tx\"`\n ParentTxHashes []chainhash.Hash `json:\"parentTxHashes\"`\n BlockIDs       []uint32         `json:\"blockIDs\"`\n Fee            uint64           `json:\"fee\"`\n SizeInBytes    uint64           `json:\"sizeInBytes\"`\n IsCoinbase     bool             `json:\"isCoinbase\"`\n LockTime       uint32           `json:\"lockTime\"` // lock time can be different from the transaction lock time, for instance in coinbase transactions\n}\n</code></pre> <p>This is widely used by all services, given it is a comprehensive set of data going well beyond the extended Tx data set.</p> <p>To know more about the UTXO data model, please refer to the UTXO Data Model documentation.</p>"},{"location":"topics/stores/utxo/#4-use-cases","title":"4. Use Cases","text":""},{"location":"topics/stores/utxo/#41-asset-server","title":"4.1. Asset Server","text":"<ol> <li>The UI Dashboard sends a request to the AssetService for UTXO data.</li> <li>The AssetService forwards this request to the UTXO Store.</li> <li>The UTXO Store interacts with the underlying Datastore implementation to fetch the requested UTXO data and check the health status of the store.</li> <li>The Datastore implementation returns the UTXO data and health status to the UTXO Store.</li> <li>The UTXO Store sends this information back to the AssetService.</li> <li>Finally, the AssetService responds back to the UI Dashboard.</li> </ol> <p>To know more about the AssetService, please check its specific service documentation.</p>"},{"location":"topics/stores/utxo/#42-block-persister","title":"4.2. Block Persister","text":"<ol> <li> <p>The Block Persister service persists the block data to disk. As part of this job, it retrieves the utxo meta data for each Tx in each subtree in each block, and dumps it to the storage.</p> </li> <li> <p>Depending on the settings Block Persister operates under, the persister will request the utxo meta data in batches or one by one. In general, and depending on the specific UTXO store implementation, the batched processing is more efficient.</p> </li> </ol>"},{"location":"topics/stores/utxo/#43-block-assembly","title":"4.3. Block Assembly","text":"<p>Coinbase Transaction creation (UTXO step):</p> <ol> <li>The Block Assembly service (see <code>SubtreeProcessor.go</code>, <code>processCoinbaseUtxos</code> method) creates a new Coinbase transaction.</li> <li>The Block Assembly service sends a request to the UTXO Store to store the Coinbase UTXO.</li> <li>The UTXO Store interacts with the underlying Datastore implementation to store the Coinbase UTXO.</li> </ol> <p>Coinbase Transaction deletion (UTXO step):</p> <ol> <li>The Block Assembly service (see <code>SubtreeProcessor.go</code>, `` method)  deletes the Coinbase transaction. This is done when blocks are reorganised and previously tracked coinbase transactions lose validity.</li> <li>The Block Assembly service sends a request to the UTXO Store to delete the Coinbase UTXO.</li> <li>The UTXO Store interacts with the underlying Datastore implementation to delete the Coinbase UTXO.</li> </ol> <p>To know more about the Block Assembly, please check its specific service documentation.</p>"},{"location":"topics/stores/utxo/#44-block-validation","title":"4.4. Block Validation","text":"<ol> <li>The Block Validator service validates the block by checking the integrity of the transactions and UTXOs.</li> <li>Once a block is validated, the Block Validator service stores the coinbase tx and their associated UTXO data in the UTXO Store.</li> <li>The UTXO Store interacts with the underlying Datastore implementation to store the coinbase tx and UTXO data.</li> </ol>"},{"location":"topics/stores/utxo/#45-subtree-validation","title":"4.5. Subtree Validation","text":"<p>In order to validate subtrees, the Subtree Validation service will retrieve the UTXO meta data for each tx in the subtree. This is done by sending a request to the UTXO Store, either in batches or one by one, depending on the settings.</p>"},{"location":"topics/stores/utxo/#46-transaction-validator","title":"4.6. Transaction Validator","text":"<p>The Transaction Validator uses the UTXO Store to perform a number of UTXO related operations:</p> <ol> <li>Obtain the current block height from the UTXO Store.</li> <li>Mark a UTXO as spent. If needed, it can also request to unspend (revert) a UTXO.</li> <li>Store and delete new UTXOs.</li> <li>Retrieve previous outputs UTXO data.</li> </ol> <p>When marking a UTXO as spent, the store will check if the UTXO is known (by its hash), and whether it is spent or not. If it is spent, we will return one response message or another depending on whether the spending tx_id matches. See here:</p> <p></p> <p>Also notice how no transaction can be spent if frozen.</p> <p>On the other hand, we can see the process for unspending an UTXO here:</p> <p></p> <p>To know more about the Transaction Validator, please check its specific service documentation.</p>"},{"location":"topics/stores/utxo/#47-utxo-batch-processing-and-external-storage-mode","title":"4.7. UTXO Batch Processing and External Storage mode","text":"<p>Aerospike is the primary datastore for the UTXO store. However, to overcome Aerospike's record size limitation of 1MB, the system implements an external storage mechanism for large transactions.</p> <p>If a transaction is too large to fit in a single Aerospike record (indicated by a <code>RECORD_TOO_BIG</code> error), or if the system is configured to externalize all transactions, the UTXO store will store the full transaction data in an external storage (typically AWS S3, but any external storage can be used).</p> <p>In such cases, the full transaction data is stored externally, while metadata and UTXOs are still stored in Aerospike, potentially across multiple records. The Aerospike record will have an <code>external</code> flag set to true, indicating that the full transaction data is stored externally.</p> <p>When the UTXO data is needed, the system will first check the Aerospike record. If the `external flag is true, it will then retrieve the full transaction data from the external storage using the transaction hash as a key.</p>"},{"location":"topics/stores/utxo/#48-alert-system-and-utxo-management","title":"4.8. Alert System and UTXO Management","text":"<p>The UTXO Store supports advanced UTXO management features, which can be utilized by an alert system.</p> <p></p> <ol> <li> <p>Freezing UTXOs: The alert system can request to freeze specific UTXOs, preventing them from being spent.</p> <ul> <li>Checks if the UTXO exists</li> <li>If the UTXO is already spent, it returns \"SPENT\"</li> <li>If the UTXO is already frozen, it returns \"FROZEN\"</li> <li>Otherwise, it marks the UTXO as frozen by setting its spending TxID to 32 'FF' bytes</li> </ul> </li> <li> <p>Unfreezing UTXOs: Previously frozen UTXOs can be unfrozen, allowing them to be spent again.</p> <ul> <li>Checks if the UTXO exists and is frozen</li> <li>If frozen, it removes the freeze mark</li> <li>If not frozen, it returns an error</li> </ul> </li> <li> <p>Reassigning UTXOs: UTXOs can be reassigned to a new transaction output, but only if they are frozen first.</p> <ul> <li>Verifies the UTXO exists and is frozen</li> <li>Updates the UTXO hash to the new value</li> <li>Sets spendable block height to current + ReAssignedUtxoSpendableAfterBlocks</li> <li>Logs the reassignment for audit purposes</li> </ul> </li> </ol>"},{"location":"topics/stores/utxo/#49-unmined-transaction-management","title":"4.9. Unmined Transaction Management","text":"<p>The UTXO Store tracks unmined transactions to support transaction recovery and continuity across service restarts. This functionality is crucial for maintaining a reliable transaction processing pipeline.</p> <p></p> <p>Key Components:</p> <ul> <li><code>unminedSince</code> Field: A block height field in the UTXO store that, when set, indicates the transaction is unmined and tracks when it was first stored</li> <li><code>createdAt</code> Field: Timestamp tracking when the unmined transaction was added, used for ordering during recovery</li> <li><code>preserveUntil</code> Field: Protects parent transactions from deletion when they have unmined children</li> </ul> <p>Functionality:</p> <ol> <li>Transaction Tracking: When a transaction is validated but not yet mined, it's marked with the <code>unminedSince</code> field containing the current block height</li> <li>Recovery Interface: The <code>UnminedTxIterator</code> provides efficient iteration over all unmined transactions, enabling the Block Assembly service to reload them on startup</li> <li>Automatic Cleanup: Old unmined transactions are periodically cleaned up based on retention settings through <code>QueryOldUnminedTransactions</code></li> <li>Parent Protection: During cleanup, parent transactions of younger unmined transactions are protected using <code>PreserveTransactions</code></li> </ol> <p>Benefits:</p> <ul> <li>Service Resilience: Transactions persist across service restarts without requiring resubmission</li> <li>State Management: Clear separation between mined and unmined transactions</li> <li>Resource Management: Automatic cleanup prevents unbounded growth while preserving transaction dependencies</li> <li>Conflict Detection: Helps identify transactions that conflict with mined blocks</li> </ul>"},{"location":"topics/stores/utxo/#5-technology","title":"5. Technology","text":""},{"location":"topics/stores/utxo/#51-language-and-libraries","title":"5.1. Language and Libraries","text":"<ol> <li> <p>Go Programming Language (Golang): A statically typed, compiled language known for its simplicity and efficiency, especially in concurrent operations and networked services. The primary language used for implementing the service's logic.</p> </li> <li> <p>Bitcoin Transaction (BT) GoLang library: <code>github.com/ordishs/go-bt/</code> - a full featured Bitcoin transactions and transaction manipulation/functionality Go Library.</p> </li> </ol>"},{"location":"topics/stores/utxo/#52-data-stores","title":"5.2. Data Stores","text":"<p>The following datastores are supported (either in development / experimental or production mode):</p> <ol> <li> <p>Aerospike:</p> <ul> <li>A high-performance, NoSQL distributed database.</li> <li>Suitable for environments requiring high throughput and low latency.</li> <li>Handles large volumes of UTXO data with fast read/write capabilities.</li> <li>Aerospike is the reference datastore. Teranode has been guaranteed to process 1 million tps with Aerospike.</li> <li>Aerospike records can contain up to 1024 bytes.    </li> <li>For more information, please refer to the official Aerospike documentation: https://aerospike.com.</li> </ul> </li> <li> <p>Memory (In-Memory Store):</p> <ul> <li>Stores UTXOs directly in the application's memory.</li> <li>Offers the fastest access times but lacks persistence; data is lost if the service restarts.</li> <li>Useful for development or testing purposes.</li> </ul> </li> <li> <p>Sql:</p> <ul> <li>A SQL database, currently Postgresql and SqlLite are in scope, can be used to store UTXOs.</li> <li>Provides a balance of performance and persistence, suitable for medium to large-scale applications.</li> <li>Offers the ability to query and analyze UTXO data using SQL.</li> </ul> </li> <li> <p>Nullstore:</p> <ul> <li>A dummy or placeholder implementation, used for testing (when no actual storage is needed).</li> <li>Can be used to mock UTXO store functionality in a development or test environment.</li> </ul> </li> </ol> <p>Implementation Choice Considerations:</p> <ul> <li>The choice of implementation depends on the specific requirements of the BSV node, such as speed, data volume, persistence, and the operational environment.</li> <li>Memory-based stores are typically faster but may require additional persistence mechanisms.</li> <li>Databases like Aerospike provide a balance of speed and persistence, suitable for larger, more complex systems.</li> <li>Nullstore is more appropriate for testing, development, or lightweight applications.</li> </ul>"},{"location":"topics/stores/utxo/#53-data-purging","title":"5.3. Data Purging","text":"<p>Stored data is automatically purged a certain TTL (Time To Live) period after it is spent. This is done to prevent the datastore from growing indefinitely and to ensure that only relevant data (i.e. data that is spendable or recently spent) is kept in the store.</p>"},{"location":"topics/stores/utxo/#6-performance-optimizations","title":"6. Performance Optimizations","text":""},{"location":"topics/stores/utxo/#61-shared-buffer-optimization","title":"6.1. Shared Buffer Optimization","text":"<p>To improve performance when reading large batches of UTXOs, Teranode implements a shared buffer optimization that significantly reduces memory allocations and garbage collection pressure.</p>"},{"location":"topics/stores/utxo/#how-it-works","title":"How It Works","text":"<p>The shared buffer optimization is implemented in the Aerospike UTXO store's batch operations:</p> <ol> <li>Single Buffer Allocation: Instead of allocating individual buffers for each UTXO read operation, the system allocates a single large shared buffer</li> <li>Buffer Reuse: This buffer is reused across multiple UTXO reads within the same batch operation</li> <li>Slice References: Each UTXO's data is referenced as a slice of the shared buffer, avoiding data copying</li> </ol>"},{"location":"topics/stores/utxo/#performance-benefits","title":"Performance Benefits","text":"<ul> <li>Reduced Memory Allocations: Decreases the number of memory allocations from O(n) to O(1) for batch operations</li> <li>Lower GC Pressure: Fewer allocations mean less work for the garbage collector</li> <li>Improved Throughput: Particularly beneficial when reading UTXO sets for block validation or subtree processing</li> <li>Better Cache Locality: Contiguous memory access patterns improve CPU cache utilization</li> </ul>"},{"location":"topics/stores/utxo/#implementation-details","title":"Implementation Details","text":"<p>The optimization is automatically applied when:</p> <ul> <li>Batch reading operations are performed through <code>GetMetaBatch</code> or similar methods</li> <li>Multiple UTXOs are requested in a single operation</li> <li>The Aerospike store implementation is being used</li> </ul> <p>Example of the optimization in action:</p> <ul> <li>Without optimization: Reading 10,000 UTXOs requires 10,000 separate buffer allocations</li> <li>With optimization: Reading 10,000 UTXOs requires 1 shared buffer allocation plus slice operations</li> </ul> <p>This optimization is particularly effective for:</p> <ul> <li>Block validation (reading all UTXOs referenced in a block)</li> <li>Subtree validation (processing millions of transactions)</li> <li>UTXO set snapshots and exports</li> <li>High-throughput transaction validation</li> </ul>"},{"location":"topics/stores/utxo/#7-directory-structure-and-main-files","title":"7. Directory Structure and Main Files","text":"<pre><code>UTXO Store Package Structure (stores/utxo)\n\u251c\u2500\u2500 Interface.go                    # Defines the interface for the UTXO Store\n\u251c\u2500\u2500 _factory                        # Contains different store implementations\n\u2502   \u251c\u2500\u2500 aerospike.go                # Factory setup for Aerospike UTXO Store\n\u2502   \u251c\u2500\u2500 memory.go                   # Factory setup for in-memory UTXO Store\n\u2502   \u2514\u2500\u2500 utxo.go                     # Common UTXO store setup\n\u251c\u2500\u2500 aerospike                       # Aerospike-specific UTXO Store implementation\n\u2502   \u251c\u2500\u2500 aerospike.go                # Main Aerospike UTXO store implementation\n\u2502   \u2502   \u2514\u2500\u2500 Store                   # Struct representing the Aerospike UTXO store\n\u2502   \u2502       \u251c\u2500\u2500 url                 # The Aerospike URL\n\u2502   \u2502       \u251c\u2500\u2500 client              # The Aerospike client instance\n\u2502   \u2502       \u251c\u2500\u2500 namespace           # The Aerospike namespace used\n\u2502   \u2502       \u251c\u2500\u2500 setName             # The set name for storing UTXOs\n\u2502   \u2502       \u251c\u2500\u2500 expiration          # The expiration time for UTXO records\n\u2502   \u2502       \u251c\u2500\u2500 blockHeight         # The current blockchain height\n\u2502   \u2502       \u251c\u2500\u2500 logger              # The logger instance\n\u2502   \u2502       \u251c\u2500\u2500 storeBatcher        # Batcher for store operations\n\u2502   \u2502       \u251c\u2500\u2500 getBatcher          # Batcher for get operations\n\u2502   \u2502       \u251c\u2500\u2500 spendBatcher        # Batcher for spend operations\n\u2502   \u2502       \u251c\u2500\u2500 lastSpendBatcher    # Batcher for last spend operations\n\u2502   \u2502       \u251c\u2500\u2500 New                 # Initializes a new Aerospike UTXO Store\n\u2502   \u2502       \u251c\u2500\u2500 sendStoreBatch      # Processes and stores a batch of transactions\n\u2502   \u2502       \u251c\u2500\u2500 sendGetBatch        # Retrieves a batch of UTXO data\n\u2502   \u2502       \u251c\u2500\u2500 sendSpendBatchLua   # Processes a batch of UTXO spend operations using Lua script\n\u2502   \u2502       \u251c\u2500\u2500 SetBlockHeight      # Sets the current block height\n\u2502   \u2502       \u251c\u2500\u2500 GetBlockHeight      # Retrieves the current block height\n\u2502   \u2502       \u251c\u2500\u2500 Health              # Checks the health of the Aerospike UTXO store service\n\u2502   \u2502       \u251c\u2500\u2500 GetSpend            # Retrieves the spend status of a UTXO\n\u2502   \u2502       \u251c\u2500\u2500 GetMeta             # Retrieves metadata for a specific UTXO\n\u2502   \u2502       \u251c\u2500\u2500 Get                 # Retrieves specific UTXO data\n\u2502   \u2502       \u251c\u2500\u2500 Create              # Adds new UTXOs to the store\n\u2502   \u2502       \u251c\u2500\u2500 Spend               # Marks UTXOs as spent\n\u2502   \u2502       \u251c\u2500\u2500 Unspend             # Reverses the spent status of UTXOs\n\u2502   \u2502       \u251c\u2500\u2500 SetMinedMulti       # Sets multiple transactions as mined\n\u2502   \u2502       \u251c\u2500\u2500 SetMined            # Sets a transaction as mined\n\u2502   \u2502       \u2514\u2500\u2500 Delete              # Removes UTXOs from the store\n\u2502   \u251c\u2500\u2500 aerospike_server_test.go    # Tests for the Aerospike UTXO store\n\u2502   \u251c\u2500\u2500 alert_system.go             # Aerospike alert system implementation\n\u2502   \u251c\u2500\u2500 alert_system_test.go        # Tests for the Aerospike alert system\n\u2502   \u251c\u2500\u2500 metrics.go                  # Metrics collection for Aerospike operations\n\u2502   \u2514\u2500\u2500 spend.lua                   # Lua script for batch spend operations\n\u251c\u2500\u2500 memory                          # In-memory UTXO Store implementation\n\u2502   \u2514\u2500\u2500 memory.go                   # Main in-memory UTXO store implementation\n\u251c\u2500\u2500 meta                            # Metadata handling for UTXOs\n\u2502   \u251c\u2500\u2500 data.go                     # Defines metadata structures for UTXOs\n\u2502   \u2514\u2500\u2500 data_test.go                # Tests for UTXO metadata\n\u251c\u2500\u2500 nullstore                       # Null UTXO Store implementation for testing\n\u2502   \u2514\u2500\u2500 nullstore.go                # Main nullstore implementation\n\u251c\u2500\u2500 sql                             # SQL UTXO Store implementation\n\u2502   \u2514\u2500\u2500 sql.go                      # Main sql implementation (provides support for SqlLite and Postgres)\n\u251c\u2500\u2500 status.pb.go                    # Generated protocol buffer code for UTXO status\n\u251c\u2500\u2500 status.proto                    # Protocol buffer definition for UTXO status\n\u251c\u2500\u2500 utils.go                        # Utility functions for the UTXO Store\n\u2514\u2500\u2500 utils_test.go                   # Tests for utility functions\n</code></pre>"},{"location":"topics/stores/utxo/#8-running-the-store-locally","title":"8. Running the Store Locally","text":""},{"location":"topics/stores/utxo/#how-to-run","title":"How to run","text":"<p>To run the UTXO Store locally, you can execute the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_USERNAME] go run -UtxoStore=1\n</code></pre> <p>Please refer to the Locally Running Services Documentation document for more information on running the Bootstrap Service locally.</p>"},{"location":"topics/stores/utxo/#9-configuration-and-settings","title":"9. Configuration and Settings","text":"<p>For comprehensive configuration documentation including all settings, defaults, and interactions, see the UTXO Store Settings Reference.</p>"},{"location":"topics/stores/utxo/#10-other-resources","title":"10. Other Resources","text":"<p>UTXO Store Reference</p>"},{"location":"tutorials/developers/developerSetup/","title":"\ud83d\udda5 Developer Setup - Pre-requisites and Installation","text":"<p>This guide assists you in setting up the Teranode project on your machine. The below assumes you are running a recent version of Mac OS.</p>"},{"location":"tutorials/developers/developerSetup/#index","title":"Index","text":"<ol> <li>Install Go</li> <li>Set Go Environment Variables</li> <li>Python and Dependencies<ul> <li>3.1 Install Python (via Homebrew)</li> <li>3.2 (Recommended) Use a Python Virtual Environment to install PyYAML</li> <li>3.3 Install Dependencies Within the Virtual Environment</li> <li>3.4 Verify Installation</li> <li>Alternative: Use <code>pipx</code> (for CLI tools)</li> </ul> </li> <li>Clone the Project and Install Dependencies</li> <li> <p>Configure Settings</p> <ul> <li> <p>5.1 Introducing developer-specific settings in <code>settings_local.conf</code></p> </li> <li> <p>5.3 Verify</p> </li> </ul> </li> <li> <p>Prerequisites for Running the Node</p> <ul> <li>6.1 Install Docker for Mac</li> <li>6.2 Start Kafka and PostgreSQL</li> </ul> </li> <li>Run the Node</li> <li>Troubleshooting<ul> <li>8.1. Dependency errors and conflicts</li> <li>Next Steps</li> </ul> </li> </ol>"},{"location":"tutorials/developers/developerSetup/#1-install-go","title":"1. Install Go","text":"<p>Download and install the latest version of Go. As of October 2025, it's <code>1.25.2</code>.</p> <p>Go Installation Guide</p> <p>Test Installation: Open a new terminal and execute: <pre><code>go version\n</code></pre> It should display <code>go1.25.2</code> or above.</p>"},{"location":"tutorials/developers/developerSetup/#2-set-go-environment-variables","title":"2. Set Go Environment Variables","text":"<p>Add these lines to <code>.zprofile</code> or <code>.bash_profile</code>, depending on which one your development machine uses:</p> <pre><code>export PATH=\"$PATH:$(go env GOPATH)/bin\"\nexport GOPATH=\"$(go env GOPATH)\"\nexport GOBIN=\"$(go env GOPATH)/bin\"\n</code></pre> <p>Test Configuration: Open a new terminal and execute: <pre><code>echo $GOPATH\necho $GOBIN\n</code></pre></p> <p>Both should display paths related to Go.</p>"},{"location":"tutorials/developers/developerSetup/#3-python-and-dependencies","title":"3. Python and Dependencies","text":""},{"location":"tutorials/developers/developerSetup/#31-install-python-via-homebrew","title":"3.1 Install Python (via Homebrew)","text":"<pre><code>brew install python\n</code></pre> <p>By default, Homebrew will install Python 3.x and create symlinks like <code>python3</code> and <code>pip3</code>. You can optionally create a symlink for <code>python</code> and <code>pip</code> if you want shorter commands (but check if they already exist first):</p> <pre><code>ln -s /opt/homebrew/bin/python3 /opt/homebrew/bin/python  # might already exist\nln -s /opt/homebrew/bin/pip3 /opt/homebrew/bin/pip        # might already exist\n</code></pre>"},{"location":"tutorials/developers/developerSetup/#32-recommended-use-a-python-virtual-environment-to-install-pyyaml","title":"3.2 (Recommended) Use a Python Virtual Environment to install PyYAML","text":"<p>Because of PEP 668 and Homebrew\u2019s \u201cexternally-managed-environment\u201d setup, you can\u2019t do <code>pip install ...</code> directly into the system-wide Python.</p> <p>Instead, create and activate a virtual environment:</p> <p><pre><code>python3 -m venv ~/my_python_env     # choose any path you like\nsource ~/my_python_env/bin/activate\n</code></pre> After activating, your shell should show something like <code>(my_python_env)</code> as a prefix.</p>"},{"location":"tutorials/developers/developerSetup/#33-install-dependencies-within-the-virtual-environment","title":"3.3 Install Dependencies Within the Virtual Environment","text":"<p>Once your virtual environment is active, you can safely use <code>pip</code> to install packages without system conflicts:</p> <pre><code>python -m pip install --upgrade pip\npip install PyYAML\n</code></pre>"},{"location":"tutorials/developers/developerSetup/#34-verify-installation","title":"3.4 Verify Installation","text":"<p><pre><code>python -c \"import yaml; print(yaml.__version__)\"\n</code></pre> This should print out the installed PyYAML version (e.g., <code>6.0.2</code> or similar).</p>"},{"location":"tutorials/developers/developerSetup/#alternative-use-pipx-for-cli-tools-not-recommended-for-teranode-development","title":"Alternative: Use <code>pipx</code> (for CLI tools) - NOT recommended for Teranode Development","text":"<p>If you need PyYAML as part of a standalone command-line tool, you could use pipx instead: <pre><code>brew install pipx\npipx install PyYAML\n</code></pre> However, most Teranode workflows will need PyYAML as a library for scripts, so a virtual environment is usually best.</p>"},{"location":"tutorials/developers/developerSetup/#4-clone-the-project-and-install-dependencies","title":"4. Clone the Project and Install Dependencies","text":"<p>Clone the project:</p> <pre><code>git clone git@github.com:bsv-blockchain/teranode.git\n</code></pre> <p>Install all dependencies: Execute: <pre><code>cd teranode\n\n# This will install all required dependencies (protobuf, golangci-lint, etc.)\nmake install\n</code></pre></p> <p>Note: If you receive an error <code>ModuleNotFoundError: No module named \u2018yaml\u2019</code> error, refer to this issue for a potential fix. Example: <pre><code>PYTHONPATH=$HOME/Library/Python/3.9/lib/python/site-packages make install  #Make sure the path is correct for your own python version\n</code></pre></p>"},{"location":"tutorials/developers/developerSetup/#5-configure-settings","title":"5. Configure Settings","text":"<p>Teranode uses two configuration files:</p> <ul> <li><code>settings.conf</code> - Contains sensible defaults for all environments. You should NOT modify this file as part of the scope of this guide.</li> <li><code>settings_local.conf</code> - Contains developer-specific and deployment-specific settings</li> </ul>"},{"location":"tutorials/developers/developerSetup/#51-introducing-developer-specific-settings-in-settings_localconf","title":"5.1 Introducing developer-specific settings in <code>settings_local.conf</code>","text":"<ol> <li> <p>In your project directory, create a file <code>settings_local.conf</code>. This file is used for your personal development settings, and it's not tracked in source control.</p> </li> <li> <p>Introduce any settings override in <code>settings_local.conf</code> that you might require for your development. Use the <code>settings.conf</code> as a reference for common settings and their default values.</p> </li> <li> <p>The settings you are adding will use a prefix (settings context) that identifies your context. By default, your settings context should be <code>dev.</code>. You can further refine this by using a more specific prefix, such as <code>dev.john</code> or <code>dev.me</code>. However, it is recommended to use the default prefix <code>dev.</code>, and only refine it in very specific cases.</p> </li> </ol> <p>In order for your node to read your custom lines, you set <code>SETTINGS_CONTEXT</code> to match the prefix you used (i.e., <code>dev</code>).</p> <p>In zsh, open <code>~/.zprofile</code> (or <code>~/.zshrc</code>). In bash, open <code>~/.bash_profile</code> (or <code>~/.bashrc</code>).</p> <p>Add: <pre><code>export SETTINGS_CONTEXT=dev\n</code></pre> (If you have used a richer prefix, such as <code>dev.john</code>, you would set <code>SETTINGS_CONTEXT=dev.john</code>)</p> <p>After editing, reload your shell config: <pre><code>source ~/.zprofile\n</code></pre> (or the equivalent for your shell).</p>"},{"location":"tutorials/developers/developerSetup/#53-verify","title":"5.3 Verify","text":"<ol> <li> <p>Echo the environment variable to ensure it\u2019s set correctly:    <pre><code>echo $SETTINGS_CONTEXT\n</code></pre>    Should print <code>dev</code>.</p> </li> <li> <p>Run or restart your node. Check logs or console output to confirm it\u2019s picking up the lines with <code>dev</code>.</p> </li> </ol>"},{"location":"tutorials/developers/developerSetup/#6-prerequisites-for-running-the-node","title":"6. Prerequisites for Running the Node","text":""},{"location":"tutorials/developers/developerSetup/#61-install-docker-for-mac","title":"6.1 Install Docker for Mac","text":"<p>Kafka runs in Docker containers, so you'll need to install Docker for Mac:</p> <ol> <li>Download Docker Desktop for Mac from Docker Hub</li> <li>Double-click the downloaded <code>.dmg</code> file and drag the Docker app to your Applications folder</li> <li>Launch Docker Desktop from your Applications folder</li> <li>When prompted, authorize Docker with your system password</li> <li>Wait for Docker to start (the whale icon in the status bar will stop animating when Docker is ready)</li> </ol> <p>Verify Docker installation: <pre><code>docker --version\n</code></pre></p>"},{"location":"tutorials/developers/developerSetup/#62-start-kafka-and-postgresql","title":"6.2 Start Kafka and PostgreSQL","text":"<p>Once Docker is installed and running, start Kafka and PostgreSQL with:</p> <pre><code># Start Kafka in Docker\n./scripts/kafka.sh\n\n# Start PostgreSQL in Docker\n./scripts/postgres.sh\n</code></pre> <p>These scripts will set up Docker containers with the required services configured correctly for Teranode.</p> <p>Note: If you configure your settings to use Aerospike for UTXO storage, you'll also need to run the Aerospike script: <pre><code># Start Aerospike in Docker\n./scripts/aerospike.sh\n</code></pre></p>"},{"location":"tutorials/developers/developerSetup/#7-run-the-node","title":"7. Run the Node","text":"<p>You can run the entire node with the following command:</p> <pre><code>SETTINGS_CONTEXT=dev.[YOUR_CONTEXT] go run .\n</code></pre> <p>If no errors are seen, you have successfully installed the project and are ready to start working on the project or running the node.</p> <p>Note that the node is initialized in IDLE mode by default. You'll need to transition it to RUNNING mode to start processing transactions.</p>"},{"location":"tutorials/developers/developerSetup/#71-executing-the-teranode-cli-as-a-developer","title":"7.1. Executing the Teranode-CLI as a Developer","text":"<p>The Teranode-CLI allows you to interact with Teranode services. You can use it to transition the node to different states, query its current state, and perform various maintenance operations. For a comprehensive guide on using the Teranode-CLI as a developer, see the Developer's Guide to Teranode-CLI.</p>"},{"location":"tutorials/developers/developerSetup/#building-the-teranode-cli","title":"Building the Teranode-CLI","text":"<p>Build the Teranode-CLI tool with:</p> <pre><code>go build -o teranode-cli ./cmd/teranodecli\n</code></pre>"},{"location":"tutorials/developers/developerSetup/#executing-commands","title":"Executing Commands","text":"<p>Once built, you can run commands directly:</p> <pre><code># Get current FSM state\nSETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli getfsmstate\n\n# Set FSM state to RUNNING\nSETTINGS_CONTEXT=dev.[YOUR_CONTEXT] ./teranode-cli setfsmstate --fsmstate running\n</code></pre>"},{"location":"tutorials/developers/developerSetup/#available-commands","title":"Available Commands","text":"<p>The Teranode-CLI provides several commands:</p> Command Description <code>getfsmstate</code> Get the current FSM State <code>setfsmstate</code> Set the FSM State (with <code>--fsmstate</code> flag) <code>settings</code> View system configuration <code>aerospikereader</code> Read transaction data from Aerospike <code>filereader</code> Read and process files <code>seeder</code> Seed initial blockchain data <code>bitcointoutxoset</code> Convert Bitcoin data to UTXO set <code>utxopersister</code> Manage UTXO persistence <code>export-blocks</code> Export blockchain to CSV <code>import-blocks</code> Import blockchain from CSV <code>checkblocktemplate</code> Check block template"},{"location":"tutorials/developers/developerSetup/#getting-help","title":"Getting Help","text":"<p>For general help and a list of available commands:</p> <pre><code># General help\n./teranode-cli\n\n# Command-specific help\n./teranode-cli setfsmstate --help\n</code></pre> <p>The CLI will use your development settings as specified by your <code>SETTINGS_CONTEXT</code> environment variable.</p>"},{"location":"tutorials/developers/developerSetup/#transitioning-the-node-to-running-mode","title":"Transitioning the Node to RUNNING Mode","text":"<p>To transition the node from IDLE to RUNNING mode, use:</p> <pre><code># Get current FSM state\nSETTINGS_CONTEXT=dev ./teranode-cli getfsmstate\n\n# Set FSM state to RUNNING\nSETTINGS_CONTEXT=dev ./teranode-cli setfsmstate --fsmstate running\n</code></pre> <p>After executing these commands, your log should show a successful transition:</p> <pre><code>[Blockchain Client] FSM successfully transitioned from IDLE to state:RUNNING\n</code></pre>"},{"location":"tutorials/developers/developerSetup/#8-troubleshooting","title":"8. Troubleshooting","text":""},{"location":"tutorials/developers/developerSetup/#81-dependency-errors-and-conflicts","title":"8.1. Dependency errors and conflicts","text":"<p>Should you have errors with dependencies, try running the following commands:</p> <pre><code>go clean -cache\ngo clean -modcache\nrm go.sum\ngo mod tidy\ngo mod download\n</code></pre> <p>This will effectively reset your cache and re-download all dependencies.</p>"},{"location":"tutorials/developers/developerSetup/#next-steps","title":"Next Steps","text":"<ul> <li>Check our Git Commit Signing Setup Guide for Contributors</li> </ul>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/","title":"Teranode Public Repository: Fork and Pull Request Guidelines","text":""},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#overview","title":"Overview","text":"<p>The Teranode public repository is hosted at https://github.com/bsv-blockchain/teranode. Like most public repositories, direct commits to the main repository are not permitted. Instead, contributors must use a fork-and-pull-request workflow. This document provides a comprehensive guide to this process.</p>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding Forks<ul> <li>What is a Fork?</li> <li>Why Fork?</li> <li>Best Practices</li> </ul> </li> <li>Step 1: Create Your Fork</li> <li>Step 2: Clone Your Fork Locally</li> <li>Step 3: Protect Your Main Branch (Recommended)</li> <li>Step 4: Keep Your Fork Synchronized</li> <li>Step 5: Working on Features and Creating Pull Requests</li> <li>Quick Reference Commands</li> <li>Summary Workflow</li> <li>Need Help?</li> </ul>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#understanding-forks","title":"Understanding Forks","text":""},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#what-is-a-fork","title":"What is a Fork?","text":"<p>A fork is a complete copy of a repository that lives under your own GitHub account. When you fork a repository:</p> <ul> <li>Original Repository (<code>bsv-blockchain/teranode</code>): This is the authoritative source maintained by the BSV Blockchain organization. You cannot directly push changes here.</li> <li>Your Fork (<code>YOUR_USERNAME/teranode</code>): This is your personal copy where you have full control. You can create branches, commit changes, and experiment freely.</li> </ul>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#why-fork","title":"Why Fork?","text":"<p>Forking allows you to:</p> <ul> <li>Work independently without affecting the original repository</li> <li>Experiment with changes safely</li> <li>Propose changes back to the original project via pull requests</li> <li>Maintain your own version of the codebase while staying synchronized with updates</li> </ul>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#best-practices","title":"Best Practices","text":"<ul> <li>Keep your fork synchronized: Regularly sync with the upstream repository to avoid diverging too far from the main codebase</li> <li>One fork per contributor: You only need one fork per repository\u2014create multiple branches within your fork for different features</li> <li>Don't commit to your main branch: Keep your fork's <code>main</code> branch clean and aligned with upstream. Always work in feature branches</li> <li>Descriptive branch names: Use clear, descriptive names like <code>feature/add-logging</code> or <code>bugfix/connection-timeout</code></li> </ul>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#step-1-create-your-fork","title":"Step 1: Create Your Fork","text":"<ol> <li> <p>Navigate to https://github.com/bsv-blockchain/teranode</p> </li> <li> <p>Click the \"Fork\" button in the top-right corner of the page</p> <p></p> </li> <li> <p>Configure your fork:</p> </li> <li> <p>Owner: Select your GitHub account</p> </li> <li>Repository name: Keep it as <code>teranode</code> (or customize if needed)</li> <li>Description: Optional\u2014you can add a custom description</li> <li> <p>Copy the main branch only: \u2713 Check this box (recommended for cleaner setup)</p> <p></p> </li> <li> <p>Click \"Create fork\"</p> </li> </ol> <p>Your fork will now be available at <code>https://github.com/YOUR_USERNAME/teranode</code></p>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#step-2-clone-your-fork-locally","title":"Step 2: Clone Your Fork Locally","text":""},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#1-get-your-repository-url","title":"1. Get Your Repository URL","text":"<p>Navigate to your forked repository at <code>https://github.com/YOUR_USERNAME/teranode</code> and click the green \"Code\" button. Copy the SSH URL (recommended) or HTTPS URL.</p> <p></p>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#2-clone-the-repository","title":"2. Clone the Repository","text":"<p>Open your terminal and run:</p> <pre><code>git clone git@github.com:YOUR_GITHUB_USERNAME/teranode.git\n</code></pre> <p>Replace <code>YOUR_GITHUB_USERNAME</code> with your actual GitHub username.</p>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#3-add-the-upstream-remote","title":"3. Add the Upstream Remote","text":"<p>Navigate into your cloned repository and add a reference to the original repository:</p> <pre><code>cd teranode\n\n# Add the original repository as \"upstream\"\ngit remote add upstream https://github.com/bsv-blockchain/teranode.git\n</code></pre> <p>Why do we need an upstream remote?</p> <p>When you clone your fork, Git automatically sets up <code>origin</code> to point to your fork. However, you also need a way to pull in changes from the original repository as it evolves. By adding the upstream remote, you create a connection to the source repository, which allows you to:</p> <ul> <li>Fetch the latest updates from the main project as other contributors merge their changes</li> <li>Keep your fork synchronized with the official codebase</li> <li>Avoid merge conflicts by regularly incorporating upstream changes before they diverge too far from your work</li> <li>Base your pull requests on the latest code, ensuring compatibility with recent changes</li> </ul> <p>Think of it this way: <code>origin</code> is where you push your work, and <code>upstream</code> is where you pull updates from the main project.</p>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#4-verify-your-remotes","title":"4. Verify Your Remotes","text":"<p>Confirm your setup by running:</p> <pre><code>git remote -v\n</code></pre> <p>You should see output similar to:</p> <pre><code>origin    git@github.com:YOUR_GITHUB_USERNAME/teranode.git (fetch)\norigin    git@github.com:YOUR_GITHUB_USERNAME/teranode.git (push)\nupstream  https://github.com/bsv-blockchain/teranode.git (fetch)\nupstream  https://github.com/bsv-blockchain/teranode.git (push)\n</code></pre> <p>What this means:</p> <ul> <li>origin: Your fork (where you push your changes)</li> <li>upstream: The original repository (where you pull updates from)</li> </ul>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#step-3-protect-your-main-branch-recommended","title":"Step 3: Protect Your Main Branch (Recommended)","text":"<p>Important: To prevent accidental commits to your <code>main</code> branch, set up a Git hook that will block direct commits.</p>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#why-protect-main","title":"Why Protect Main?","text":"<ul> <li>Prevents sync issues: Committing to main causes your fork to diverge from upstream, creating complex merge conflicts</li> <li>Enforces best practices: Ensures all work happens in feature branches</li> <li>Cleaner pull requests: PRs from feature branches are easier to review</li> <li>Easier to reset: A clean main branch can always be safely reset to match upstream</li> </ul>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#set-up-the-pre-commit-hook","title":"Set Up the Pre-Commit Hook","text":"<p>Run the following commands in your repository:</p> <pre><code># Create the pre-commit hook\ncat &gt; .git/hooks/pre-commit &lt;&lt; 'EOF'\n#!/bin/bash\n\nbranch=\"$(git rev-parse --abbrev-ref HEAD)\"\n\nif [ \"$branch\" = \"main\" ]; then\n  echo \"\u274c ERROR: Direct commits to 'main' branch are not allowed!\"\n  echo \"\ud83d\udc49 Please create a feature branch instead:\"\n  echo \"   git checkout -b feature/your-feature-name\"\n  exit 1\nfi\nEOF\n\n# Make it executable\nchmod +x .git/hooks/pre-commit\n</code></pre> <p>Now, if you accidentally try to commit to main, you'll see:</p> <pre><code>\u274c ERROR: Direct commits to 'main' branch are not allowed!\n\ud83d\udc49 Please create a feature branch instead:\n   git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#what-if-i-accidentally-committed-to-main","title":"What If I Accidentally Committed to Main?","text":"<p>If you haven't pushed yet:</p> <pre><code># Save your work to a new branch\ngit branch feature/my-work\n\n# Reset main to match upstream\ngit checkout main\ngit reset --hard upstream/main\n\n# Continue working on your feature branch\ngit checkout feature/my-work\n</code></pre> <p>If you already pushed:</p> <pre><code># Create a branch with your work\ngit branch feature/my-work\n\n# Force reset main to match upstream\ngit checkout main\ngit reset --hard upstream/main\ngit push origin main --force\n\n# Continue working on your feature branch\ngit checkout feature/my-work\n</code></pre>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#step-4-keep-your-fork-synchronized","title":"Step 4: Keep Your Fork Synchronized","text":"<p>Before starting any new work, always sync your fork with the upstream repository to ensure you have the latest changes.</p>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#method-1-using-github-ui","title":"Method 1: Using GitHub UI","text":"<ol> <li>Go to your forked repository on GitHub</li> <li>Click the \"Sync fork\" button near the top of the page</li> <li> <p>Click \"Update branch\" if changes are available</p> <p></p> </li> </ol> <p>This merges all changes from <code>upstream/main</code> into your fork's <code>main</code> branch.</p>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#method-2-using-command-line","title":"Method 2: Using Command Line","text":"<pre><code># Ensure you're on your main branch\ngit checkout main\n\n# Fetch and merge changes from upstream\ngit fetch upstream\ngit reset --hard upstream/main\n\n# Push the updates to your fork on GitHub\ngit push origin main\n</code></pre> <p>Important: Always sync before creating a new feature branch to ensure you're working with the latest code.</p>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#step-5-working-on-features-and-creating-pull-requests","title":"Step 5: Working on Features and Creating Pull Requests","text":""},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<p>Always work in a dedicated branch, never directly in <code>main</code>:</p> <pre><code># First, make sure you're up to date\ngit checkout main\ngit fetch upstream\ngit reset --hard upstream/main\n\n# Create and switch to a new feature branch\ngit checkout -b feature/your-feature-name\n</code></pre> <p>Use descriptive branch names such as:</p> <ul> <li><code>feature/add-transaction-validation</code></li> <li><code>bugfix/fix-memory-leak</code></li> <li><code>docs/update-readme</code></li> </ul>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#2-make-your-changes","title":"2. Make Your Changes","text":"<p>Edit files, write code, and commit your changes:</p> <pre><code># Stage your changes\ngit add .\n\n# Commit with a clear, descriptive message\ngit commit -m \"Add transaction validation logic\"\n</code></pre>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#3-prepare-for-pull-request","title":"3. Prepare for Pull Request","text":"<p>Before submitting a PR, ensure your branch is up to date with upstream to avoid merge conflicts:</p> <pre><code># Fetch the latest changes from upstream\ngit fetch upstream\ngit reset --hard upstream/main\n</code></pre>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#4-resolve-merge-conflicts-if-any","title":"4. Resolve Merge Conflicts (if any)","text":"<p>If there are conflicts, Git will notify you. Open the conflicted files, resolve the conflicts manually, then:</p> <pre><code># After resolving conflicts\ngit add .\ngit commit -m \"Resolve merge conflicts with upstream\"\n</code></pre>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#5-push-your-branch","title":"5. Push Your Branch","text":"<p>Push your feature branch to your fork:</p> <pre><code># First push of a new branch\ngit push -u origin feature/your-feature-name\n\n# Subsequent pushes\ngit push\n</code></pre>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#6-create-a-pull-request","title":"6. Create a Pull Request","text":"<ol> <li>Go to your forked repository on GitHub (<code>https://github.com/YOUR_USERNAME/teranode</code>)</li> <li>Navigate to the \"Pull requests\" tab or click the \"Compare &amp; pull request\" button that appears after pushing</li> <li>Alternatively, go to \"Branches\" \u2192 find your feature branch \u2192 click \"New pull request\"</li> <li> <p>Ensure:</p> </li> <li> <p>Base repository: <code>bsv-blockchain/teranode</code></p> </li> <li>Base branch: <code>main</code></li> <li>Head repository: <code>YOUR_USERNAME/teranode</code></li> <li>Compare branch: <code>feature/your-feature-name</code></li> <li> <p>Fill in the PR title and description:</p> </li> <li> <p>Clearly explain what changes you made</p> </li> <li>Reference any related issues (e.g., \"Fixes #123\")</li> <li>Describe testing performed</li> <li>Click \"Create pull request\"</li> </ol>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#quick-reference-commands","title":"Quick Reference Commands","text":"<pre><code># Initial Setup\n# =============\n# Clone your fork\ngit clone git@github.com:YOUR_USERNAME/teranode.git\ncd teranode\n\n# Add upstream remote\ngit remote add upstream https://github.com/bsv-blockchain/teranode.git\n\n# Set up pre-commit hook to protect main branch\ncat &gt; .git/hooks/pre-commit &lt;&lt; 'EOF'\n#!/bin/bash\nbranch=\"$(git rev-parse --abbrev-ref HEAD)\"\nif [ \"$branch\" = \"main\" ]; then\n  echo \"\u274c ERROR: Direct commits to 'main' branch are not allowed!\"\n  echo \"\ud83d\udc49 Please create a feature branch instead:\"\n  echo \"   git checkout -b feature/your-feature-name\"\n  exit 1\nfi\nEOF\nchmod +x .git/hooks/pre-commit\n\n# Regular Workflow\n# ================\n# Sync with upstream\ngit checkout main\ngit fetch upstream\ngit reset --hard upstream/main\ngit push origin main\n\n# Create feature branch\ngit checkout -b feature/new-feature\n\n# Commit changes\ngit add .\ngit commit -m \"Descriptive commit message\"\n\n# Update feature branch with latest upstream\ngit fetch upstream\ngit reset --hard upstream/main\n\n# Push feature branch\ngit push -u origin feature/new-feature\n</code></pre>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#summary-workflow","title":"Summary Workflow","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Add upstream remote to track the original repository</li> <li>Set up pre-commit hook to protect your main branch</li> <li>Sync your fork regularly with upstream</li> <li>Create feature branches for all new work</li> <li>Commit and push your changes to your fork</li> <li>Update your feature branch with upstream changes before submitting</li> <li>Create a pull request from your feature branch to the upstream main branch</li> </ol>"},{"location":"tutorials/developers/forkAndPullRequestGuidelines/#need-help","title":"Need Help?","text":"<p>If you encounter issues or have questions about this workflow, please reach out to the team or consult the GitHub documentation on forking.</p>"},{"location":"tutorials/miners/minersGettingStarted/","title":"Getting Started with Teranode","text":""},{"location":"tutorials/miners/minersGettingStarted/#index","title":"Index","text":"<ul> <li>Introduction</li> <li>Prerequisites</li> <li>What is Teranode?</li> <li>Components Overview</li> <li>First-Time Setup<ul> <li>Step 1: Prepare Your Environment</li> <li>Step 2: Initial Setup</li> <li>Step 3: Start Teranode</li> <li>Step 4: Verify Installation</li> <li>Common Issues</li> </ul> </li> <li>Basic Operations<ul> <li>Checking Node Status</li> <li>Working with Transactions</li> <li>Monitoring Your Node</li> <li>Basic Maintenance</li> <li>Common Operations</li> <li>Next Steps</li> <li>Docker Compose Setup</li> <li>Kubernetes Deployment</li> </ul> </li> </ul>"},{"location":"tutorials/miners/minersGettingStarted/#introduction","title":"Introduction","text":"<p>This tutorial will guide you through your first steps with Teranode using Docker Compose. By the end of this guide, you'll have a running testnet Teranode instance suitable for testing and development.</p>"},{"location":"tutorials/miners/minersGettingStarted/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Basic understanding of blockchain technology</li> <li>Familiarity with command-line operations</li> <li>The AWS CLI</li> <li>Docker Engine 17.03+</li> <li>Docker Compose</li> <li>The Teranode Docker Compose file</li> <li>100GB+ available disk space</li> <li>Stable internet connection</li> </ul>"},{"location":"tutorials/miners/minersGettingStarted/#what-is-teranode","title":"What is Teranode?","text":"<p>Teranode is a scalable Bitcoin SV node implementation that:</p> <ul> <li>Processes over 1 million transactions per second</li> <li>Uses a microservices architecture</li> <li>Maintains full Bitcoin protocol compatibility</li> </ul>"},{"location":"tutorials/miners/minersGettingStarted/#components-overview","title":"Components Overview","text":"<p>Your Teranode Docker Compose setup will include:</p> <ol> <li> <p>Core Teranode Services</p> <ul> <li>Asset Server</li> <li>Block Assembly</li> <li>Block Validation</li> <li>Blockchain</li> <li>Legacy Gateway</li> <li>P2P</li> <li>Propagation</li> <li>Subtree Validation</li> </ul> </li> <li> <p>Optional Services</p> <ul> <li>Block Persister</li> <li>UTXO Persister</li> </ul> </li> <li> <p>Supporting Services</p> <ul> <li>Kafka for message queuing</li> <li>PostgreSQL for blockchain data</li> <li>Aerospike for UTXO storage</li> <li>Grafana and Prometheus for monitoring</li> </ul> </li> </ol>"},{"location":"tutorials/miners/minersGettingStarted/#first-time-setup","title":"First-Time Setup","text":""},{"location":"tutorials/miners/minersGettingStarted/#step-1-prepare-your-environment","title":"Step 1: Prepare Your Environment","text":"<ul> <li>Checkout the Teranode public repository:</li> </ul> <pre><code>cd $YOUR_WORKING_DIR\ngit clone git@github.com:bsv-blockchain/teranode.git\ncd teranode\n</code></pre>"},{"location":"tutorials/miners/minersGettingStarted/#step-2-initial-setup","title":"Step 2: Initial Setup","text":"<ul> <li>Go to the testnet docker compose path:</li> </ul> <pre><code>cd $YOUR_WORKING_DIR/teranode/deploy/docker/testnet\n</code></pre> <ul> <li>Pull required images:</li> </ul> <pre><code>docker-compose pull\n</code></pre>"},{"location":"tutorials/miners/minersGettingStarted/#step-3-start-teranode","title":"Step 3: Start Teranode","text":"<ul> <li>Launch all services:</li> </ul> <pre><code>docker-compose up -d\n</code></pre> <p>Force the node to transition to Run mode:</p> <pre><code>grpcurl -plaintext localhost:8087 blockchain_api.BlockchainAPI.Run\n</code></pre> <p>or LegacySync mode:</p> <pre><code>grpcurl -plaintext localhost:8087 blockchain_api.BlockchainAPI.LegacySync\n</code></pre> <ul> <li>Verify services are running:</li> </ul> <pre><code>docker-compose ps\n</code></pre> <ul> <li>Check individual service logs:</li> </ul> <pre><code># Example commands\ndocker-compose logs asset\ndocker-compose logs blockchain\n</code></pre> <ul> <li>Verify legacy sync status:</li> </ul> <p>When the node is started for the first time, its first action is to perform a initial blockchain sync. You can check the sync progress by checking the Legacy service logs:</p> <pre><code>docker-compose logs legacy\n</code></pre>"},{"location":"tutorials/miners/minersGettingStarted/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<ul> <li>Check service health:</li> </ul> <pre><code>curl http://localhost:8090/health\n</code></pre> <ul> <li> <p>Access monitoring dashboard:</p> <ul> <li>Open Grafana: http://localhost:3005</li> <li>Login with the default credentials: admin/admin</li> <li>Navigate to the \"Teranode - Service Overview\" dashboard for key metrics</li> <li>Explore other dashboards for detailed service metrics. For example, you can check the Legacy sync metrics in the \"Teranode - Legacy Service\" dashboard.</li> </ul> </li> </ul>"},{"location":"tutorials/miners/minersGettingStarted/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Services fail to start</p> <ul> <li>Check logs: <code>docker-compose logs</code></li> <li>Verify disk space: <code>df -h</code></li> <li>Ensure all ports are available</li> </ul> </li> <li> <p>Cannot connect to services</p> <ul> <li>Verify services are running: <code>docker-compose ps</code></li> <li>Check service logs for specific errors</li> <li>Ensure ports are not blocked by firewall</li> </ul> </li> </ol>"},{"location":"tutorials/miners/minersGettingStarted/#basic-operations","title":"Basic Operations","text":""},{"location":"tutorials/miners/minersGettingStarted/#checking-node-status","title":"Checking Node Status","text":"<ol> <li>View all services status:</li> </ol> <pre><code>docker-compose ps\n</code></pre> <ol> <li>Check blockchain sync:</li> </ol> <pre><code>curl http://localhost:8090/api/v1/blockstats\n</code></pre> <ol> <li>Monitor specific service logs:</li> </ol> <pre><code>docker-compose logs -f legacy\ndocker-compose logs -f blockchain\ndocker-compose logs -f asset\n</code></pre>"},{"location":"tutorials/miners/minersGettingStarted/#working-with-transactions","title":"Working with Transactions","text":"<ol> <li>Get transaction details:</li> </ol> <pre><code>curl http://localhost:8090/api/v1/tx/&lt;txid&gt;\n</code></pre>"},{"location":"tutorials/miners/minersGettingStarted/#monitoring-your-node","title":"Monitoring Your Node","text":"<ol> <li> <p>Access Grafana dashboards:</p> <ul> <li>Open http://localhost:3005</li> <li>Navigate to \"TERANODE Service Overview\"</li> </ul> </li> <li> <p>Key metrics to watch:</p> <ul> <li>Block queue length (should be near 0)</li> <li>Transaction processing rate</li> <li>Memory and CPU usage</li> <li>Disk space utilization</li> </ul> </li> </ol>"},{"location":"tutorials/miners/minersGettingStarted/#basic-maintenance","title":"Basic Maintenance","text":"<ol> <li>View logs:</li> </ol> <pre><code># All services\ndocker-compose logs\n\n# Specific service\ndocker-compose logs blockchain\n</code></pre> <ol> <li>Check disk usage:</li> </ol> <pre><code>df -h\n</code></pre> <ol> <li>Restart a specific service:</li> </ol> <pre><code>docker-compose restart blockchain\n</code></pre> <ol> <li>Restart all services:</li> </ol> <pre><code>docker-compose down\ndocker-compose up -d\n</code></pre>"},{"location":"tutorials/miners/minersGettingStarted/#common-operations","title":"Common Operations","text":"<ol> <li>Check current block height:</li> </ol> <pre><code>curl http://localhost:8090/api/v1/bestblockheader/json\n</code></pre> <ol> <li>Get block information:</li> </ol> <pre><code>curl http://localhost:8090/api/v1/block/&lt;blockhash&gt;\n</code></pre> <ol> <li>Check UTXO status:</li> </ol> <pre><code>curl http://localhost:8090/api/v1/utxo/&lt;utxohash&gt;\n</code></pre>"},{"location":"tutorials/miners/minersGettingStarted/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the How-to Guides for advanced tasks</li> <li>Review the Reference documentation for detailed endpoint information</li> </ul>"},{"location":"tutorials/miners/minersGettingStarted/#docker-compose-setup","title":"Docker Compose Setup","text":"<ol> <li>Installation Guide</li> <li>Starting and Stopping Teranode</li> <li>Configuration Guide</li> <li>Blockchain Synchronization</li> <li>Update Procedures</li> <li>Troubleshooting Guide</li> <li>Security Best Practices</li> </ol>"},{"location":"tutorials/miners/minersGettingStarted/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<ol> <li>Installation with Kubernetes Operator</li> <li>Starting and Stopping Teranode</li> <li>Configuration Guide</li> <li>Blockchain Synchronization</li> <li>Update Procedures</li> <li>Backup Procedures</li> <li>Troubleshooting Guide</li> <li>Security Best Practices</li> <li>Remote Debugging Guide</li> </ol>"}]}