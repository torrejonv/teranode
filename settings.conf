# This file should only contain Teranode defaults and settings that are common across different environments

# @group: CLIENT_NAMES compact
clientName                       = teranode
clientName.docker.host.teranode1 = teranode1
clientName.docker.host.teranode2 = teranode2
clientName.docker.host.teranode3 = teranode3
clientName.docker.ss.teranode1   = teranode1
clientName.docker.teranode1      = teranode1
clientName.docker.teranode2      = teranode2
clientName.docker.teranode3      = teranode3
clientName.docker.ci             = teranode1
clientName.docker.ci.teranode1   = teranode1
clientName.docker.ci.teranode2   = teranode2
clientName.docker.ci.teranode3   = teranode3
# @endgroup

# @group: DATA_DIRECTORY compact
DATADIR                                     = ./data
DATADIR.operator                            = /data
DATADIR.docker.context.testrunner           = ./../data
DATADIR.docker.teranode1.context.testrunner = ./../../data
DATADIR.docker.teranode2.context.testrunner = ./../../data
DATADIR.docker.teranode3.context.testrunner = ./../../data
# @endgroup

KAFKA_BLOCKS                     = blocks
KAFKA_BLOCKS.docker.ss.teranode1 = blocks1
KAFKA_BLOCKS.operator            = blocks-${clientName}

KAFKA_BLOCKS_FINAL                     = blocks-final
KAFKA_BLOCKS_FINAL.docker.ss.teranode1 = blocks-final1
KAFKA_BLOCKS_FINAL.operator            = blocks-final-${clientName}

KAFKA_HOSTS             = localhost:${KAFKA_PORT}
KAFKA_HOSTS.test        = 127.0.0.1:${KAFKA_PORT}
KAFKA_HOSTS.docker      = kafka-shared:${KAFKA_PORT}
KAFKA_HOSTS.docker.host = localhost:${KAFKA_PORT}

KAFKA_INVALID_BLOCKS                     = invalid-blocks
KAFKA_INVALID_BLOCKS.docker.ci.teranode1 = invalid-blocks1
KAFKA_INVALID_BLOCKS.docker.ci.teranode2 = invalid-blocks2
KAFKA_INVALID_BLOCKS.docker.ci.teranode3 = invalid-blocks3
KAFKA_INVALID_BLOCKS.docker.ss.teranode1 = invalid-blocks1
KAFKA_INVALID_BLOCKS.operator            = invalid-blocks-${clientName}

KAFKA_INVALID_SUBTREES                     = invalid-subtrees
KAFKA_INVALID_SUBTREES.docker.ci.teranode1 = invalid-subtrees1
KAFKA_INVALID_SUBTREES.docker.ci.teranode2 = invalid-subtrees2
KAFKA_INVALID_SUBTREES.docker.ci.teranode3 = invalid-subtrees3
KAFKA_INVALID_SUBTREES.docker.ss.teranode1 = invalid-subtrees1
KAFKA_INVALID_SUBTREES.operator            = invalid-subtrees-${clientName}

KAFKA_LEGACY_INV                     = legacy-inv
KAFKA_LEGACY_INV.docker.ss.teranode1 = legacy-inv1
KAFKA_LEGACY_INV.operator            = legacy-inv-${clientName}

KAFKA_PARTITIONS_HIGH                      = 8
KAFKA_PARTITIONS_HIGH.operator             = 512
KAFKA_PARTITIONS_HIGH.operator.teratestnet = 32

KAFKA_PARTITIONS_LOW = 1

KAFKA_PORT               = 9092
KAFKA_PORT.dev.kafkatool = 9094
KAFKA_PORT.docker.host   = 19092

KAFKA_REJECTEDTX                     = rejectedtx
KAFKA_REJECTEDTX.docker.ss.teranode1 = rejectedtx1
KAFKA_REJECTEDTX.operator            = rejectedtx-${clientName}

# use the same kafka cluster for the testing env
KAFKA_REPLICATION_FACTOR          = 1
KAFKA_REPLICATION_FACTOR.operator = 3

KAFKA_SCHEMA                              = kafka
KAFKA_SCHEMA.dev                          = memory
KAFKA_SCHEMA.test                         = memory
KAFKA_SCHEMA.docker.host.teranode1.daemon = memory
KAFKA_SCHEMA.docker.host.teranode2.daemon = memory
KAFKA_SCHEMA.docker.host.teranode3.daemon = memory

KAFKA_SUBTREES                     = subtrees
KAFKA_SUBTREES.docker.ss.teranode1 = subtrees1
KAFKA_SUBTREES.operator            = subtrees-${clientName}

# @group: KAFKA_TLS
KAFKA_ENABLE_TLS = false

KAFKA_TLS_SKIP_VERIFY = false
# @endgroup

# Enable debug logging from Sarama (Kafka client library)
# Set to true to see detailed Kafka connection and protocol logs
# Default: false (too verbose for production)
kafka_enable_debug_logging = false

KAFKA_TXMETA                     = txmeta
KAFKA_TXMETA.docker.ss.teranode1 = txmeta1
KAFKA_TXMETA.operator            = txmeta-${clientName}

KAFKA_UNITTEST = unittest

KAFKA_VALIDATORTXS                     = validatortxs
KAFKA_VALIDATORTXS.docker.ss.teranode1 = validatortxs1
KAFKA_VALIDATORTXS.operator            = validatortxs-${clientName}

# @group: PORT PREFIXES compact
PORT_PREFIX                       =
PORT_PREFIX.docker.host.teranode1 = 1
PORT_PREFIX.docker.host.teranode2 = 2
PORT_PREFIX.docker.host.teranode3 = 3
# @endgroup

# @group: PORTS compact
ALERT_P2P_PORT               = 9908 # this is set in the mainnet.json in services/alert/config
ASSET_HTTP_PORT              = 8090
BLOCKCHAIN_GRPC_PORT         = 8087
BLOCKCHAIN_HTTP_PORT         = 8082
BLOCK_ASSEMBLY_GRPC_PORT     = 8085
BLOCK_PERSISTER_HTTP_PORT    = 8083
BLOCK_VALIDATION_GRPC_PORT   = 8088
CENTRIFUGE_PORT              = 8892
COINBASE_GRPC_PORT           = 8093
FAUCET_HTTP_PORT             = 8097
HEALTH_CHECK_PORT            = 8000
JAEGER_PORT                  = 6831
JAEGER_PORT_HTTP             = 4318
LEGACY_GRPC_PORT             = 8099
LEGACY_HTTP_PORT             = 8098
P2P_BOOTSTRAP_PORT           = 9901
P2P_GRPC_PORT                = 9904
P2P_HTTP_PORT                = 9906
P2P_PORT                     = 9905
P2P_PORT_COINBASE            = 9907
POSTGRES_PORT                = 5432
PROFILE_PORT                 = 9091
PROFILE_PORT_TXBLASTER       = 9092
PROPAGATION_GRPC_PORT        = 8084
PROPAGATION_HTTP_PORT        = 8833
SUBTREE_VALIDATION_GRPC_PORT = 8086
TERANODE_RPC_PORT            = 9292
VALIDATOR_GRPC_PORT          = 8081
VALIDATOR_HTTP_PORT          = 8834
# @endgroup

advertisingInterval = 10s

# Advertising Configuration
# -------------------------
advertisingURL =

aerospike_debug = false

aerospike_host.docker.teranode1    = aerospike-1
aerospike_host.docker.teranode2    = aerospike-2
aerospike_host.docker.teranode3    = aerospike-3
aerospike_host.docker.ci           = localhost
aerospike_host.docker.ci.teranode1 = aerospike-1
aerospike_host.docker.ci.teranode2 = aerospike-2
aerospike_host.docker.ci.teranode3 = aerospike-3

# @group: aerospike_policies compact
# The following 3 policies are used for all read/write operations when the aerospike_useDefaultPolicies is false
# SleepBetweenRetries is not being used in the current implementation
# SleepMultiplier is not being used in the current implementation
# ExitFastOnExhaustedConnectionPool is not being used in the current implementation
aerospike_batchPolicy = aerospike:///?MaxRetries=5&SleepBetweenRetries=500ms&SleepMultiplier=1&TotalTimeout=64s&SocketTimeout=10s&ConcurrentNodes=0
aerospike_readPolicy  = aerospike:///?MaxRetries=5&SleepBetweenRetries=500ms&SleepMultiplier=1&TotalTimeout=1s&SocketTimeout=1s
aerospike_writePolicy = aerospike:///?MaxRetries=5&SleepBetweenRetries=500ms&SleepMultiplier=1&TotalTimeout=1s&SocketTimeout=1s
# @endgroup

aerospike_port.docker.teranode1      = 3100
aerospike_port.docker.teranode2      = 3200
aerospike_port.docker.teranode3      = 3300
aerospike_port.docker.ci             = 13100
aerospike_port.docker.ci.teranode1   = 13100
aerospike_port.docker.ci.teranode2   = 13200
aerospike_port.docker.ci.teranode3   = 13300
aerospike_port.docker.teranode1.test = 3100
aerospike_port.docker.teranode2.test = 3200
aerospike_port.docker.teranode3.test = 3300

aerospike_useDefaultBasePolicies = false

aerospike_useDefaultPolicies = false

aerospike_warmUp = true

alert_genesis_keys = "02a1589f2c8e1a4e7cbf28d4d6b676aa2f30811277883211027950e82a83eb2768 | 03aec1d40f02ac7f6df701ef8f629515812f1bcd949b6aa6c7a8dd778b748b2433 | 03ddb2806f3cc48aa36bd4aea6b9f1c7ed3ffc8b9302b198ca963f15beff123678 | 036846e3e8f4f944af644b6a6c6243889dd90d7b6c3593abb9ccf2acb8c9e606e2 | 03e45c9dd2b34829c1d27c8b5d16917dd0dc2c88fa0d7bad7bffb9b542229a9304"

alert_p2p_private_key = "e76c77795b43d2aacd564648bffebde74a4c31540357dad4a3694a561b4c4f1fbb0ba060a3015f7f367742500ef8486707e58032af1b4dfdb1203c790bcf2526"

alert_protocol_id = "/bitcoin/alert-system/1.0.0"

alert_store = sqlite:///alert

alert_topic_name = "bitcoin_alert_system"

asset_apiPrefix = /api/v1

asset_centrifugeListenAddress             = :${CENTRIFUGE_PORT}
asset_centrifugeListenAddress.dev         = localhost:${CENTRIFUGE_PORT}
asset_centrifugeListenAddress.docker.host = localhost:${PORT_PREFIX}${CENTRIFUGE_PORT}

# turn this on to activate the centrifuge server
asset_centrifuge_disable = false

asset_httpAddress                             = http://localhost:${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.docker                      = http://${clientName}:${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.docker.ci.externaltxblaster = http://localhost:${PORT_PREFIX}${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.docker.host                 = http://localhost:${PORT_PREFIX}${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.docker.m                    = http://asset:${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.docker.ss.teranode1         = http://asset-1:${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.operator                    = http://asset:8090${asset_apiPrefix}

asset_httpListenAddress             = :${ASSET_HTTP_PORT}
asset_httpListenAddress.dev         = localhost:${ASSET_HTTP_PORT}
asset_httpListenAddress.docker.host = :${PORT_PREFIX}${ASSET_HTTP_PORT}

# these define the publicly available asset endpoint other Teranodes can use to download subtree/blocks
# asset_httpPublicAddress      = "https://myteranode.example.com/api/v1"

blockMinedCacheMaxMB        = 256
blockMinedCacheMaxMB.docker = 32

blockPersisterStore = ${blockstore}

blockPersister_httpListenAddress             = :${BLOCK_PERSISTER_HTTP_PORT}
blockPersister_httpListenAddress.docker.host = :${PORT_PREFIX}${BLOCK_PERSISTER_HTTP_PORT}

blockPersister_persistAge        = 100
blockPersister_persistAge.docker = 0

blockPersister_persistSleep        = 1m
blockPersister_persistSleep.docker = 10ms

blockPersister_stateFile = ${DATADIR}/blockpersister_state.txt

block_checkDuplicateTransactionsConcurrency.docker.m = 32
block_checkDuplicateTransactionsConcurrency.operator = 32

# block concurrency settings
block_getAndValidateSubtreesConcurrency.docker.m = 32
block_getAndValidateSubtreesConcurrency.operator = 32

block_validOrderAndBlessedConcurrency.docker.m = 32
block_validOrderAndBlessedConcurrency.operator = 32

blockassembly_difficultyCache = true

blockassembly_disabled                                      = false
blockassembly_disabled.docker.teranode2.test.resilience.tc2 = true

blockassembly_grpcAddress                     = localhost:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.docker.m            = blockassembly:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.docker              = ${clientName}:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.docker.host         = localhost:${PORT_PREFIX}${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.operator            = k8s:///block-assembly.${clientName}.svc.cluster.local:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.docker.ss.teranode1 = blockassembly-1:${BLOCK_ASSEMBLY_GRPC_PORT}

blockassembly_grpcListenAddress             = :${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcListenAddress.dev         = localhost:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${BLOCK_ASSEMBLY_GRPC_PORT}

blockassembly_grpcMaxRetries                      = 3
blockassembly_grpcMaxRetries.operator.teratestnet = 5

blockassembly_grpcRetryBackoff                      = 2s
blockassembly_grpcRetryBackoff.operator.teratestnet = 5s

# the local DAH cache is only used when remote TTL stores are enabled
blockassembly_localDAHCache =

blockassembly_maxBlockReorgCatchup = 100

blockassembly_maxBlockReorgRollback = 100

blockassembly_moveBackBlockConcurrency.operator = 375

blockassembly_processRemainderTxHashesConcurrency.operator = 375

blockassembly_sendBatchSize = 1024

blockassembly_sendBatchTimeout = 5

blockassembly_subtreeProcessorBatcherSize = 32768

blockassembly_subtreeProcessorConcurrentReads = 375

blockassembly_subtreeTTL.docker.host = 0

blockassembly_useDynamicSubtreeSize = true

blockchain_grpcAddress                       = localhost:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker                = ${clientName}:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker.m              = blockchain:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker.teranode1.test = teranode1:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker.teranode2.test = teranode2:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker.teranode3.test = teranode3:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker.host           = localhost:${PORT_PREFIX}${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker.ss.teranode1   = blockchain-1:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.operator              = k8s:///blockchain.${clientName}.svc.cluster.local:${BLOCKCHAIN_GRPC_PORT}

blockchain_grpcListenAddress                        = :${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcListenAddress.docker                 = 0.0.0.0:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcListenAddress.docker.host            = localhost:${PORT_PREFIX}${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcListenAddress.docker.m               = :${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcListenAddress.dev                    = localhost:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcListenAddress.docker.teranode3.debug = :${BLOCKCHAIN_GRPC_PORT}

blockchain_httpListenAddress             = :${BLOCKCHAIN_HTTP_PORT}
blockchain_httpListenAddress.dev         = localhost:${BLOCKCHAIN_HTTP_PORT}
blockchain_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${BLOCKCHAIN_HTTP_PORT}

blockchain_initializeNodeInState =

# Blockchain Service Configuration
# --------------------------------
blockchain_maxRetries.docker.host = 3

# @group: blockchain_store compact
blockchainDB.docker.teranode1        = teranode1
blockchainDB.docker.teranode2        = teranode2
blockchainDB.docker.teranode3        = teranode3
blockchainDBUserPwd.docker.teranode1 = miner1
blockchainDBUserPwd.docker.teranode2 = miner2
blockchainDBUserPwd.docker.teranode3 = miner3
# @endgroup

blockchain_store                                          = sqlite:///blockchain
blockchain_store.dev                                      = postgres://teranode:teranode@localhost:${POSTGRES_PORT}/teranode
blockchain_store.dev.system.test                          = sqlitememory:///blockchain
blockchain_store.docker.ci.chainintegrity.teranode1       = postgres://miner1:miner1@localhost:${POSTGRES_PORT}/teranode1
blockchain_store.docker.ci.chainintegrity.teranode2       = postgres://miner2:miner2@localhost:${POSTGRES_PORT}/teranode2
blockchain_store.docker.ci.chainintegrity.teranode3       = postgres://miner3:miner3@localhost:${POSTGRES_PORT}/teranode3
blockchain_store.docker.host.teranode1.daemon             = sqlite:///teranode1/blockchain1
blockchain_store.docker.host.teranode2.daemon             = sqlite:///teranode2/blockchain2
blockchain_store.docker.host.teranode3.daemon             = sqlite:///teranode3/blockchain3
blockchain_store.docker.host.teranode1                    = postgres://miner1:miner1@localhost:1${POSTGRES_PORT}/teranode1
blockchain_store.docker.host.teranode2                    = postgres://miner2:miner2@localhost:1${POSTGRES_PORT}/teranode2
blockchain_store.docker.host.teranode3                    = postgres://miner3:miner3@localhost:1${POSTGRES_PORT}/teranode3
blockchain_store.docker.m                                 = postgres://teranode:teranode@postgres:${POSTGRES_PORT}/teranode
blockchain_store.docker.ss.teranode1                      = postgres://miner1:miner1@postgres:${POSTGRES_PORT}/teranode1
blockchain_store.docker                                   = postgres://${blockchainDBUserPwd}:${blockchainDBUserPwd}@postgres:${POSTGRES_PORT}/${blockchainDB}
blockchain_store.docker.teranode1.test                    = postgres://miner1:miner1@postgres:${POSTGRES_PORT}/teranode1
blockchain_store.docker.teranode2.test                    = postgres://miner2:miner2@postgres:${POSTGRES_PORT}/teranode2
blockchain_store.docker.teranode3.test                    = postgres://miner3:miner3@postgres:${POSTGRES_PORT}/teranode3
blockchain_store.docker.teranode1.test.context.testrunner = postgres://miner1:miner1@localhost:7432/teranode1
blockchain_store.docker.teranode2.test.context.testrunner = postgres://miner2:miner2@localhost:7432/teranode2
blockchain_store.docker.teranode3.test.context.testrunner = postgres://miner3:miner3@localhost:7432/teranode3
blockchain_store.docker.teranode1.context.testrunner      = postgres://${blockchainDBUserPwd}:${blockchainDBUserPwd}@localhost:${POSTGRES_PORT}/${blockchainDB}
blockchain_store.teratestnet                              = postgres://teranode:teranode@localhost:${POSTGRES_PORT}/teranode

# Set maximum block size in bytes we will mine. Size of the mined block will never exceed the maximum block size we will accept (excessiveblocksize)
blockmaxsize          = 0
blockmaxsize.docker.m = 4294967296
blockmaxsize.operator = 4294967296

blockstore                                          = file://${DATADIR}/blockstore?localTTLStore=file&localTTLStorePath=${DATADIR}/blockstore-ttl-1 | ${DATADIR}/blockstore-ttl-2
blockstore.dev                                      = file://${DATADIR}/blockstore?localTTLStore=file&localTTLStorePath=${DATADIR}/blockstore-ttl
blockstore.docker                                   = file://${DATADIR}/blockstore
blockstore.docker.host                              = file://${DATADIR}/${clientName}/blockstore
blockstore.operator                                 = file://${DATADIR}/${clientName}/blockstore
blockstore.docker.teranode1.test.context.testrunner = file://${DATADIR}/test/teranode1/blockstore
blockstore.docker.teranode2.test.context.testrunner = file://${DATADIR}/test/teranode2/blockstore
blockstore.docker.teranode3.test.context.testrunner = file://${DATADIR}/test/teranode3/blockstore
blockstore.docker.teranode1.context.testrunner      = file://${DATADIR}/teranode1/blockstore
blockstore.docker.teranode2.context.testrunner      = file://${DATADIR}/teranode2/blockstore
blockstore.docker.teranode3.context.testrunner      = file://${DATADIR}/teranode3/blockstore

# File Store Concurrency Limits
# Separate read/write semaphores prevent deadlocks where write operations block on pipe data
# while read operations (that would provide that data) wait for semaphore slots.
# These limits protect against Linux ulimit exhaustion by controlling concurrent file descriptors.
# Total: read + write should stay under your system's open file limit (ulimit -n)
# Default: 768 + 256 = 1024 (matches original single semaphore capacity)
# Tune based on workload: higher read limits benefit read-heavy workloads, higher write limits
# benefit write-heavy workloads. Monitor with: lsof -p <pid> | wc -l
# filestore_read_concurrency  = 768   # Concurrent read operations (Get, Exists, GetDAH, GetIoReader)
# filestore_write_concurrency = 256   # Concurrent write operations (Set, Del, SetDAH, SetFromReader)

# quick validation means it only uses the txmeta cache
# (ignored if blockvalidation_validation_max_retries is 0)
# This is only useful when all transactions are sent to all nodes, using something like ipv6
blockvalidation_fail_fast_validation = false

# limit this, this is happening in the background
blockvalidation_finalizeBlockValidationConcurrency = 8

blockvalidation_getMissingTransactions = 32

blockvalidation_grpcAddress             = localhost:${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcAddress.docker.m    = blockvalidation:${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcAddress.docker.host = localhost:${PORT_PREFIX}${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcAddress.operator    = k8s:///block-validation.${clientName}.svc.cluster.local:${BLOCK_VALIDATION_GRPC_PORT}

blockvalidation_grpcListenAddress             = :${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcListenAddress.dev         = localhost:${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${BLOCK_VALIDATION_GRPC_PORT}

blockvalidation_localSetTxMinedConcurrency = 8

blockvalidation_maxPreviousBlockHeadersToCheck = 100

# too big a batch and you get asset service timeout challenges
blockvalidation_missingTransactionsBatchSize = 5000

blockvalidation_processTxMetaUsingCache_BatchSize = 1024

blockvalidation_processTxMetaUsingCache_Concurrency = 128

# cache miss threshold is the number of missing txs to allow before we fail
blockvalidation_processTxMetaUsingCache_MissingTxThreshold = 1

blockvalidation_processTxMetaUsingStore_BatchSize = 1024

blockvalidation_processTxMetaUsingStore_Concurrency = 128

# store miss threshold is the number of missing txs when searching our store, ignored during BlockValidation
blockvalidation_processTxMetaUsingStore_MissingTxThreshold = 1

# block validation concurrency settings
# this is basically making subtree processing unlimited
# it is very important that new subtrees are processed as fast as possible
blockvalidation_subtreeFoundChConcurrency = 64

# Timeout for fetching subtrees from peers (default: 120s)
blockvalidation_subtree_fetch_timeout = 120s

# abandon subtree validation when missing total exceeds threshold, ignored during BlockValidation
# This is only useful when all transactions are sent to all nodes, using something like ipv6
blockvalidation_subtree_validation_abandon_threshold = 0

blockvalidation_validateBlockSubtreesConcurrency = 64

blockvalidation_validation_max_retries = 3

blockvalidation_validation_retry_sleep = 5s

coinbaseDB.docker.teranode1    = coinbase1
coinbaseDB.docker.teranode2    = coinbase2
coinbaseDB.docker.teranode3    = coinbase3
coinbaseDB.docker.ci           = coinbase1
coinbaseDB.docker.ci.teranode1 = coinbase1
coinbaseDB.docker.ci.teranode2 = coinbase2
coinbaseDB.docker.ci.teranode3 = coinbase3

coinbaseDBUserPwd.docker.teranode1    = coinbase1
coinbaseDBUserPwd.docker.teranode2    = coinbase2
coinbaseDBUserPwd.docker.teranode3    = coinbase3
coinbaseDBUserPwd.docker.ci           = coinbase1
coinbaseDBUserPwd.docker.ci.teranode1 = coinbase1
coinbaseDBUserPwd.docker.ci.teranode2 = coinbase2
coinbaseDBUserPwd.docker.ci.teranode3 = coinbase3

coinbase_arbitrary_text                      = /teranode/
coinbase_arbitrary_text.docker.teranode1     = /m1-eu/
coinbase_arbitrary_text.docker.teranode2     = /m2-us/
coinbase_arbitrary_text.docker.teranode3     = /m3-asia/
coinbase_arbitrary_text.docker.host          = /${clientName}/
coinbase_arbitrary_text.operator.teratestnet = /${clientName}-euc/
coinbase_arbitrary_text.operator.mainnet     = /${clientName}-euw/
coinbase_arbitrary_text.docker.ss.teranode1  = /m1-eu/

coinbase_grpcAddress                             = # This should be empty by default
coinbase_grpcAddress.dev                         = localhost:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.teratestnet                 = localhost:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.m                    = teranode-coinbase:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker                      = ${clientName}:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.teranode1            = coinbase1:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.teranode2            = coinbase2:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.teranode3            = coinbase3:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.host                 = localhost:${PORT_PREFIX}${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.ss.teranode1         = coinbase-1:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.operator.teratestnet        = k8s:///coinbase.${clientName}.svc.cluster.local:${COINBASE_GRPC_PORT}

coinbase_grpcListenAddress                             = :${COINBASE_GRPC_PORT}
coinbase_grpcListenAddress.dev                         = localhost:${COINBASE_GRPC_PORT}
coinbase_grpcListenAddress.docker.host                 = localhost:${PORT_PREFIX}${COINBASE_GRPC_PORT}

coinbase_notification_threshold.operator = 100000

coinbase_p2p_peer_id.dev                   = 12D3KooWBBV8PL949p46DJHwJkjESoPGCYhqHv1Ek1DkbQ6HGB8X
coinbase_p2p_peer_id.docker.teranode1      = 12D3KooWNQWh27xAsZRuXzANGQjLVJqXGVdp1errjLfc3wWvawZw
coinbase_p2p_peer_id.docker.teranode2      = 12D3KooWNhWUxABRjenSeCT3V4zVKnPqfSA3jvXQnPbVmcp1ZtYU
coinbase_p2p_peer_id.docker.teranode3      = 12D3KooWS6HPmwhqSDdS78rLqUQpM39Jf59XYGxJNE77W4WziGL6
coinbase_p2p_peer_id.docker.ss.teranode1   = 12D3KooWNQWh27xAsZRuXzANGQjLVJqXGVdp1errjLfc3wWvawZw
coinbase_p2p_peer_id.docker.host.teranode1 = 12D3KooWNQWh27xAsZRuXzANGQjLVJqXGVdp1errjLfc3wWvawZw
coinbase_p2p_peer_id.docker.host.teranode2 = 12D3KooWNhWUxABRjenSeCT3V4zVKnPqfSA3jvXQnPbVmcp1ZtYU
coinbase_p2p_peer_id.docker.host.teranode3 = 12D3KooWS6HPmwhqSDdS78rLqUQpM39Jf59XYGxJNE77W4WziGL6

coinbase_p2p_private_key.dev                   = 44a5a189fbad1d7bc0c59b33fbd5e485f2f4d3d8bf293838c56ce72e53b557171444c0bb7d5cf75112717084cee9e9e98651421b3cd29d721e43c0a51d81aa54
coinbase_p2p_private_key.docker.teranode1      = e76c77795b43d2aacd564648bffebde74a4c31540357dad4a3694a561b4c4f1fbb0ba060a3015f7f367742500ef8486707e58032af1b4dfdb1203c790bcf2526
coinbase_p2p_private_key.docker.teranode2      = 860616e0492a3050aa760440469acfe4f57cf5387a765f5227603c4f6aeac985bf6643d453a1d68a101e52766e9feb9721b95e34aa73e5ea6c69a44be43cab6d
coinbase_p2p_private_key.docker.teranode3      = 1d6a9c8963fdbb86eabc4d10cb1efdf418197cfc3f9779e3c8229663411ae5c8f1cee260eeeae89cb45aae6955230557eba5bf63ef38087ec6be91ab744326c7
coinbase_p2p_private_key.docker.ss.teranode1   = e76c77795b43d2aacd564648bffebde74a4c31540357dad4a3694a561b4c4f1fbb0ba060a3015f7f367742500ef8486707e58032af1b4dfdb1203c790bcf2526
coinbase_p2p_private_key.docker.host.teranode1 = e76c77795b43d2aacd564648bffebde74a4c31540357dad4a3694a561b4c4f1fbb0ba060a3015f7f367742500ef8486707e58032af1b4dfdb1203c790bcf2526
coinbase_p2p_private_key.docker.host.teranode2 = 860616e0492a3050aa760440469acfe4f57cf5387a765f5227603c4f6aeac985bf6643d453a1d68a101e52766e9feb9721b95e34aa73e5ea6c69a44be43cab6d
coinbase_p2p_private_key.docker.host.teranode3 = 1d6a9c8963fdbb86eabc4d10cb1efdf418197cfc3f9779e3c8229663411ae5c8f1cee260eeeae89cb45aae6955230557eba5bf63ef38087ec6be91ab744326c7

coinbase_p2p_static_peers.dev                   = /ip4/127.0.0.1/tcp/${P2P_PORT}/p2p/12D3KooWMQira6uh4rptNzMP5sojTdNXyveAWMKJi5ySoepVXGxo
coinbase_p2p_static_peers.dev.system.test       =
coinbase_p2p_static_peers.docker.m              =
coinbase_p2p_static_peers.docker.host.teranode1 = /dns/localhost/tcp/2${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW | /dns/localhost/tcp/3${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
coinbase_p2p_static_peers.docker.host.teranode2 = /dns/localhost/tcp/1${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/localhost/tcp/3${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
coinbase_p2p_static_peers.docker.host.teranode3 = /dns/localhost/tcp/1${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/localhost/tcp/2${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW
coinbase_p2p_static_peers.docker.teranode1      = /dns/teranode2/tcp/${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW | /dns/teranode3/tcp/${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
coinbase_p2p_static_peers.docker.teranode2      = /dns/teranode1/tcp/${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/teranode3/tcp/${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
coinbase_p2p_static_peers.docker.teranode3      = /dns/teranode1/tcp/${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/teranode2/tcp/${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW

coinbase_should_wait = false

coinbase_store                                       = sqlite:///coinbase
coinbase_store.dev                                   = postgres://teranode:teranode@localhost:${POSTGRES_PORT}/coinbase
coinbase_store.dev.system.test                       = sqlitememory:///coinbase
coinbase_store.docker                                = postgres://${coinbaseDBUserPwd}:${coinbaseDBUserPwd}@postgres:${POSTGRES_PORT}/${coinbaseDB}
coinbase_store.docker.ci.chainintegrity.teranode1    = postgres://coinbase1:coinbase1@localhost:${POSTGRES_PORT}/coinbase1
coinbase_store.docker.ci.chainintegrity.teranode2    = postgres://coinbase2:coinbase2@localhost:${POSTGRES_PORT}/coinbase2
coinbase_store.docker.ci.chainintegrity.teranode3    = postgres://coinbase3:coinbase3@localhost:${POSTGRES_PORT}/coinbase3
coinbase_store.docker.host                           = postgres://coinbase${PORT_PREFIX}:coinbase${PORT_PREFIX}@localhost:1${POSTGRES_PORT}/coinbase${PORT_PREFIX}
coinbase_store.docker.m                              = postgres://coinbase1:coinbase1@postgres:${POSTGRES_PORT}/coinbase1
coinbase_store.host.ss.teranode1                     = postgres://coinbase1:coinbase1@postgres:${POSTGRES_PORT}/coinbase1

coinbase_store_dbTimeoutMillis = 5000

coinbase_test_mode = false

# coinbase are shared on the same server
coinbase_wait_for_peers                 = true
coinbase_wait_for_peers.dev.system.test = false
coinbase_wait_for_peers.dev.kafka       = false
coinbase_wait_for_peers.operator        = false

coinbase_wallet_private_key                       = ${PK1}
coinbase_wallet_private_key.docker.teranode1      = ${PK1}
coinbase_wallet_private_key.docker.teranode2      = ${PK2}
coinbase_wallet_private_key.docker.teranode3      = ${PK3}
coinbase_wallet_private_key.docker.ss.teranode1   = ${PK1}
coinbase_wallet_private_key.docker.host.teranode1 = ${PK1}
coinbase_wallet_private_key.docker.host.teranode2 = ${PK2}
coinbase_wallet_private_key.docker.host.teranode3 = ${PK3}

# @group: dashboard
# Vite dev server ports (comma-separated)
# dashboard_devServerPorts = 5173,4173

# Dashboard UI
# ------------------------
dashboard_enabled = true

# WebSocket path
# dashboard_websocketPath = /connection/websocket

# WebSocket port for development environment
# dashboard_websocketPort = 8090
# @endgroup

double_spend_window_millis = 0

# policy settings
# use these if you do not want unbounded scaling
excessiveblocksize                      = 10737418240
excessiveblocksize.docker.teranode2.tc2 = 1000

# end of policy settings
faucet_httpListenAddress             = :${FAUCET_HTTP_PORT}
faucet_httpListenAddress.dev         = localhost:${FAUCET_HTTP_PORT}
faucet_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${FAUCET_HTTP_PORT}
faucet_httpListenAddress.docker.m    = :${FAUCET_HTTP_PORT}

fsm_state_change_delay.docker.teranode1 = 1s # for testing, we want to delay the state change and have time to capture the state
fsm_state_change_delay.docker.teranode2 = 1s # for testing, we want to delay the state change and have time to capture the state
fsm_state_change_delay.docker.teranode3 = 1s # for testing, we want to delay the state change and have time to capture the state

fsm_state_restore = false

global_blockHeightRetention = 288

# @group: gocore
# Core Stats Configuration
# ------------------------
gocore_stats_reported_time_threshold = 24h

stats_prefix = gocore
# @endgroup

# TODO: change api key and move out of settings
grpc_admin_api_key = testkey

# Grpc Resolver Configuration
grpc_resolver          = dns
grpc_resolver.operator = kubernetes

health_check_httpListenAddress                                = :${HEALTH_CHECK_PORT}
health_check_httpListenAddress.docker.host                    = :${PORT_PREFIX}${HEALTH_CHECK_PORT}
health_check_httpListenAddress.docker.host.teranode1.coinbase = :48000
health_check_httpListenAddress.docker.host.teranode2.coinbase = :48001
health_check_httpListenAddress.docker.host.teranode3.coinbase = :48002
health_check_httpListenAddress.docker.teranode1.test.coinbase = :48000
health_check_httpListenAddress.docker.teranode2.test.coinbase = :48001
health_check_httpListenAddress.docker.teranode3.test.coinbase = :48002

http_sign_response = true

http_timeout = 30000

# HTTP streaming timeout in milliseconds for large file downloads (subtree data, blocks during catchup)
# This is longer than http_timeout to accommodate large subtree data files which can be 100+ MB
# Default: 300000ms (5 minutes)
http_streaming_timeout = 300000

# IPV6 Addresses
# --------------
# ipv6_addresses = ff02::1234

k8s_resolver_ttl     = 10
k8s_resolver_ttl.dev = 0

kafka_blocksConfig = ${KAFKA_SCHEMA}://${KAFKA_HOSTS}/${KAFKA_BLOCKS}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&retention=60000&flush_bytes=64&consumerTimeout=1800000

kafka_blocksFinalConfig = ${KAFKA_SCHEMA}://${KAFKA_HOSTS}/${KAFKA_BLOCKS_FINAL}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&retention=60000&flush_bytes=64&consumerTimeout=1800000

kafka_invalidBlocksConfig = ${KAFKA_SCHEMA}://${KAFKA_HOSTS}/${KAFKA_INVALID_BLOCKS}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&retention=600000&flush_bytes=1024&flush_messages=10000&flush_frequency=1s&replay=0

kafka_invalidSubtreesConfig = ${KAFKA_SCHEMA}://${KAFKA_HOSTS}/${KAFKA_INVALID_SUBTREES}?partitions=${KAFKA_PARTITIONS_HIGH}&replication=${KAFKA_REPLICATION_FACTOR}&retention=60000&segment_bytes=33554432&flush_bytes=64&flush_messages=1&replay=0

kafka_legacyInvConfig = ${KAFKA_SCHEMA}://${KAFKA_HOSTS}/${KAFKA_LEGACY_INV}?partitions=${KAFKA_PARTITIONS_HIGH}&replication=${KAFKA_REPLICATION_FACTOR}&retention=6000&flush_bytes=1024&flush_messages=16&flush_frequency=1s

kafka_rejectedTxConfig = ${KAFKA_SCHEMA}://${KAFKA_HOSTS}/${KAFKA_REJECTEDTX}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&retention=600000&flush_bytes=1024&flush_messages=10000&flush_frequency=1s&replay=0

# subtree processing when propagation distributes to all nodes it super fast.
# sutree processing with no tx distrubtion is much slower and can take minutes for a very large subtree
# Excessive timeouts can mask performance issues.
# Monitor p99 latency and adjust if most subtrees complete faster than these limits.
kafka_subtreesConfig = ${KAFKA_SCHEMA}://${KAFKA_HOSTS}/${KAFKA_SUBTREES}?partitions=${KAFKA_PARTITIONS_HIGH}&replication=${KAFKA_REPLICATION_FACTOR}&retention=1800000&segment_bytes=33554432&flush_bytes=64&flush_messages=1&replay=0&maxProcessingTime=30000&sessionTimeout=120000&heartbeatInterval=20000&offsetReset=latest

kafka_txmetaConfig = ${KAFKA_SCHEMA}://${KAFKA_HOSTS}/${KAFKA_TXMETA}?partitions=${KAFKA_PARTITIONS_HIGH}&replication=${KAFKA_REPLICATION_FACTOR}&retention=60000&flush_bytes=1024&flush_messages=10000&flush_frequency=1s

kafka_unitTest = ${KAFKA_SCHEMA}://localhost:${KAFKA_PORT}/${KAFKA_UNITTEST}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&retention=600000&flush_bytes=1024&flush_messages=10000&flush_frequency=1s

# tx validation order is critical so we cannot have mutiple partitions :(
kafka_validatortxsConfig                      =
kafka_validatortxsConfig.operator             = ${KAFKA_SCHEMA}://${KAFKA_HOSTS}/${KAFKA_VALIDATORTXS}?partitions=${KAFKA_PARTITIONS_HIGH}&replication=${KAFKA_REPLICATION_FACTOR}&retention=60000&flush_bytes=8192&flush_messages=10000&flush_frequency=1s
kafka_validatortxsConfig.operator.teratestnet =

legacy_allowSyncCandidateFromLocalPeers.docker = true

# legacy_config_AddPeers = 18.199.12.185:8333 | 3.213.100.250:8333 | 44.213.141.106:8333

# Any config from the legacy Config struct can be set here
# they need to be defined with a default value so they can be overridden from the env
legacy_config_ConnectPeers   =
# legacy_config_ConnectPeers = 18.199.12.185:8333 | 3.213.100.250:8333 | 44.213.141.106:8333
# legacy_config_ConnectPeers = 3.123.101.88:18333 | 23.22.19.204:18333

legacy_config_DataDir          = ${DATADIR}/legacy
legacy_config_DataDir.operator = ${DATADIR}/${clientName}/legacy

legacy_config_DisableCheckpoints = false

# this forces IPv4 only, set to empty for IPv4 & IPv6 support
legacy_config_Listeners = "0.0.0.0"

legacy_config_MaxPeers = 20

legacy_config_MaxPeersPerIP = 5

legacy_config_Upnp          = false
legacy_config_Upnp.dev      = true
legacy_config_Upnp.docker   = true
legacy_config_Upnp.docker.m = false

legacy_grpcAddress             = localhost:${LEGACY_GRPC_PORT}
legacy_grpcAddress.docker.host = localhost:${PORT_PREFIX}${LEGACY_GRPC_PORT}
legacy_grpcAddress.docker.m    = legacy:${LEGACY_GRPC_PORT}
legacy_grpcAddress.operator    = k8s:///legacy.${clientName}.svc.cluster.local:${LEGACY_GRPC_PORT}

legacy_grpcListenAddress             = :${LEGACY_GRPC_PORT}
legacy_grpcListenAddress.dev         = localhost:${LEGACY_GRPC_PORT}
legacy_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${LEGACY_GRPC_PORT}

legacy_httpAddress                     = http://localhost:${LEGACY_HTTP_PORT}
legacy_httpAddress.docker              = http://${clientName}:${LEGACY_HTTP_PORT}
legacy_httpAddress.docker.host         = http://localhost:${PORT_PREFIX}${LEGACY_HTTP_PORT}
legacy_httpAddress.docker.m            = http://legacy:${LEGACY_HTTP_PORT}
legacy_httpAddress.docker.ss.teranode1 = http://legacy-1:${LEGACY_HTTP_PORT}
legacy_httpAddress.operator            = http://legacy.${clientName}.svc.cluster.local:${LEGACY_HTTP_PORT}

legacy_httpListenAddress             = :${LEGACY_HTTP_PORT}
legacy_httpListenAddress.dev         = localhost:${LEGACY_HTTP_PORT}
legacy_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${LEGACY_HTTP_PORT}

legacy_outpointBatcherConcurrency = 32

legacy_outpointBatcherSize = 1024

legacy_printInvMessages     = false
legacy_printInvMessages.dev = true

legacy_spendBatcherConcurrency = 32

legacy_spendBatcherSize = 1024

legacy_storeBatcherConcurrency = 32

legacy_storeBatcherSize = 1024

legacy_workingDir             = ${DATADIR}/legacy
legacy_workingDir.docker.host = ${DATADIR}/${clientName}/legacy

# node listen mode: full (default, normal operation) or listen_only (receive only, no outbound messages)
listen_mode     = full
listen_mode.dev = listen_only

# only enable full listen_mode if you're asset_httpPublicAddress is publicly available for other nodes to download from
local_test_start_from_state =

# @group: logger
logLevel       = INFO
# logLevel.dev = DEBUG

# Logging Configuration
# ---------------------
logger = zerolog

logger_output_format =  | %-25s | %-7s | %s |

logger_show_socket_info = true

logger_show_timestamps = true
# @endgroup

maxscriptnumlengthpolicy.dev.system.test.toomanyopstest = 1

maxscriptsizepolicy.dev.system.test.oversizedscripttest = 10000

maxtxsigopscountspolicy = 4294967295

maxtxsizepolicy                            = 100000000
maxtxsizepolicy.dev.system.test.txsizetest = 10000

min_block_height_for_e2e = 100

# @group: miner_wallet_private_keys compact
PK1 = L56TgyTpDdvL3W24SMoALYotibToSCySQeo4pThLKxw6EFR6f93Q
PK2 = KyAwSjuXZNgj78w3W7mR1fVMbPFu2heaCJJkWK5Yy58NZ4xafV6k
PK3 = L3NVjmwg3nC7ZPrwMVF6FXiG1a1RZ89nhizmJVctGztRKLYrhtFL
PK4 = L3NVjmwg3nC7ZPrwMVF6FXiG1a1RZ89nhizmJVctGztRKLYrhtFL
PK5 = L3NVjmwg3nC7ZPrwMVF6FXiG1a1RZ89nhizmJVctGztRKLYrhtFL
PK6 = L3NVjmwg3nC7ZPrwMVF6FXiG1a1RZ89nhizmJVctGztRKLYrhtFL
# @endgroup

miner_wallet_private_keys                      = ${PK1}
miner_wallet_private_keys.operator.teratestnet = ${PK1} | ${PK2} | ${PK3}
miner_wallet_private_keys.operator.mainnet     = ${PK1} | ${PK2} | ${PK3}
miner_wallet_private_keys.docker               = ${PK1} | ${PK2} | ${PK3}

# This is the minimum fee per kB for transactions and is specified
# like in bitcoin-sv in BSV/kB. Therefore 0.00000001 is 1 satoshi per kB.
# Unless this value is exactly 0, transaction fees will always round
# to a minimum of 1 satoshi.
minminingtxfee             = 0.00000001
minminingtxfee.dev         = 0
minminingtxfee.docker      = 0
minminingtxfee.docker.m    = 0.00000001
minminingtxfee.teratestnet = 0

# network can be  main, stn, test and regtest.
network                                   = mainnet
network.dev                               = regtest # used by unit tests in your IDE
network.teratestnet                       = teratestnet
network.test                              = regtest # used by long tests
network.docker                            = regtest
network.docker.host.teranode1.legacy      = testnet
network.docker.host.teranode2.legacy      = testnet
network.docker.host.teranode3.legacy      = testnet
network.docker.teranode1.test.tnf6        = testnet
network.docker.teranode2.test.tnf6        = custom
network.docker.teranode3.test.tnf6        = testnet
network.docker.teranode2.test.tnf6.stage2 = testnet
network.operator                          = mainnet
network.operator.testnet                  = testnet
network.operator.teratestnet              = teratestnet
network.dev.legacy.testnet                = testnet
network.dev.legacy.mainnet                = mainnet

# use separator | to list multiple advertise addresses (optional, for nodes behind proxies)
p2p_advertise_addresses             =
p2p_advertise_addresses.dev         =
p2p_advertise_addresses.teratestnet =

p2p_ban_duration = 24h

p2p_ban_threshold = 100

p2p_bestblock_topic = bestblock

p2p_block_topic = block

p2p_bootstrapAddresses                      =
p2p_bootstrapAddresses.dev                  = /dns4/teranode-bootstrap-stage.bsvb.tech/tcp/9901/p2p/12D3KooWJ6kQHAR65xkA34NABsNVAJyVxPWh8JUSo1vtZsTyw4GD
p2p_bootstrapAddresses.teratestnet          = /dns4/teranode-bootstrap-stage.bsvb.tech/tcp/9901/p2p/12D3KooWJ6kQHAR65xkA34NABsNVAJyVxPWh8JUSo1vtZsTyw4GD
p2p_bootstrapAddresses.docker.m             = /dns4/teranode-bootstrap.bsvb.tech/tcp/9901/p2p/12D3KooWESmhNAN8s6NPdGNvJH3zJ4wMKDxapXKNUe2DzkAwKYqK
p2p_bootstrapAddresses.operator             = /dns4/teranode-bootstrap.bsvb.tech/tcp/9901/p2p/12D3KooWESmhNAN8s6NPdGNvJH3zJ4wMKDxapXKNUe2DzkAwKYqK
p2p_bootstrapAddresses.operator.teratestnet = /dns4/teranode-bootstrap-stage.bsvb.tech/tcp/9901/p2p/12D3KooWJ6kQHAR65xkA34NABsNVAJyVxPWh8JUSo1vtZsTyw4GD

# Enable persistent connections to bootstrap servers (they will reconnect after disconnect)
p2p_bootstrap_persistent = true

# Do not set protocol ID for DHT directly unless user is doing private
# networking, by default let teranode set it to a sane default
#
# p2p_dht_protocol_id = /teranode

p2p_dht_use_private = true

# Enable NAT hole punching using DCUtR protocol (default: false)
# This helps establish direct connections through NAT without port forwarding
p2p_enable_hole_punching = true

p2p_enable_nat_port_map = true

# NAT Traversal Settings
# These settings help nodes behind NAT/firewall establish connections
# Enable NAT port mapping using UPnP/NAT-PMP (default: false)
# This attempts to automatically configure your router to forward P2P ports
p2p_enable_nat_service = true

# Enable Circuit Relay v2 (default: false)
# This allows nodes to relay traffic when direct connections fail
p2p_enable_relay = true

# Force sync from a specific peer ID, overriding automatic peer selection
# Empty string (default) means automatic selection based on best chain
# Format: libp2p peer ID (e.g., "12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW")
p2p_force_sync_peer =

# If true, fall back to pruned nodes when no full nodes are available (default: true)
# When enabled, selects the youngest pruned node (smallest lag) to minimize UTXO pruning risk
p2p_allow_pruned_node_fallback = true

p2p_grpcAddress             = localhost:${P2P_GRPC_PORT}
p2p_grpcAddress.docker.host = localhost:${PORT_PREFIX}${P2P_GRPC_PORT}
p2p_grpcAddress.docker.m    = peer:${P2P_GRPC_PORT}
p2p_grpcAddress.operator    = k8s:///peer.${clientName}.svc.cluster.local:${P2P_GRPC_PORT}

p2p_grpcListenAddress             = :${P2P_GRPC_PORT}
p2p_grpcListenAddress.dev         = localhost:${P2P_GRPC_PORT}
p2p_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${P2P_GRPC_PORT}

p2p_handshake_topic = handshake

p2p_httpAddress                     = localhost:${P2P_HTTP_PORT}
p2p_httpAddress.dev                 = localhost:${P2P_HTTP_PORT}
p2p_httpAddress.operator            = peer.${clientName}.svc.cluster.local:${P2P_HTTP_PORT}
p2p_httpAddress.docker.m            = peer:${P2P_HTTP_PORT}
p2p_httpAddress.docker.host         = localhost:${PORT_PREFIX}${P2P_HTTP_PORT}
p2p_httpAddress.docker.ss.teranode1 = p2p-1:${P2P_HTTP_PORT}

p2p_httpListenAddress             = :${P2P_HTTP_PORT}
p2p_httpListenAddress.dev         = localhost:${P2P_HTTP_PORT}
p2p_httpListenAddress.docker      = localhost:${P2P_HTTP_PORT}
p2p_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${P2P_HTTP_PORT}
p2p_httpListenAddress.docker.m    = :${P2P_HTTP_PORT}
p2p_httpListenAddress.docker.ss   = 0.0.0.0:${P2P_HTTP_PORT}

# use separator | to list multiple listen addresses
p2p_listen_addresses     = 0.0.0.0
p2p_listen_addresses.dev = 127.0.0.1

p2p_mining_on_topic = miningon

# Optimise connection retries for better peer discovery
p2p_optimise_retries = true

# Peer Cache Settings
# Peer caching remembers successful peer connections for faster network recovery after restarts
# Directory for storing the peer cache file (default: current DATADIR)
# The cache file will always be named "teranode_peers.json"
# Leave empty to store teranode_peers.json in the current working directory
p2p_peer_cache_dir          = ${DATADIR}
p2p_peer_cache_dir.operator = ${DATADIR}/${clientName}

p2p_peer_id.teratestnet           =
p2p_peer_id.docker.teranode1      = 12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG
p2p_peer_id.docker.teranode2      = 12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW
p2p_peer_id.docker.teranode3      = 12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_peer_id.docker.host.teranode1 = 12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG
p2p_peer_id.docker.host.teranode2 = 12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW
p2p_peer_id.docker.host.teranode3 = 12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_peer_id.docker.ss.teranode1   = 12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG

p2p_port             = ${P2P_PORT}
p2p_port.docker.host = ${PORT_PREFIX}${P2P_PORT}

p2p_port_coinbase             = ${P2P_PORT}
p2p_port_coinbase.dev         = ${P2P_PORT_COINBASE}
p2p_port_coinbase.docker      = ${P2P_PORT_COINBASE}
p2p_port_coinbase.docker.host = ${PORT_PREFIX}${P2P_PORT_COINBASE}
p2p_port_coinbase.operator    = ${P2P_PORT_COINBASE}

# create your own private key and peer ID using cmd/keygen, or have Teranode auto-generate
p2p_private_key.docker.teranode1         = c8a1b91ae120878d91a04c904e0d565aa44b2575c1bb30a729bd3e36e2a1d5e6067216fa92b1a1a7e30d0aaabe288e25f1efc0830f309152638b61d84be6b71d
p2p_private_key.docker.teranode2         = 89a2d8acf5b2e60fd969914c326c63cde50675a47897c0eaacc02eb6ff8665585d4d059f977910472bcb75040617632019cc0749443fdc66d331b61c8cfb4b0f
p2p_private_key.docker.teranode3         = d77a7cac7833f2c0263ed7b9aaeb8dda1effaf8af948d570ed8f7a93bd3c418d6efee7bdd82ddb80484be84ba0c78ea07251a3ba2b45b2b3367fd5e2f0284e7c
p2p_private_key.docker.host.teranode1    = c8a1b91ae120878d91a04c904e0d565aa44b2575c1bb30a729bd3e36e2a1d5e6067216fa92b1a1a7e30d0aaabe288e25f1efc0830f309152638b61d84be6b71d
p2p_private_key.docker.host.teranode2    = 89a2d8acf5b2e60fd969914c326c63cde50675a47897c0eaacc02eb6ff8665585d4d059f977910472bcb75040617632019cc0749443fdc66d331b61c8cfb4b0f
p2p_private_key.docker.host.teranode3    = d77a7cac7833f2c0263ed7b9aaeb8dda1effaf8af948d570ed8f7a93bd3c418d6efee7bdd82ddb80484be84ba0c78ea07251a3ba2b45b2b3367fd5e2f0284e7c
p2p_private_key.docker.ss.teranode1      = c8a1b91ae120878d91a04c904e0d565aa44b2575c1bb30a729bd3e36e2a1d5e6067216fa92b1a1a7e30d0aaabe288e25f1efc0830f309152638b61d84be6b71d
p2p_private_key.operator.mainnet.stage.1 = a4a361d2e1d3b73a4a50857794bf0a2dd148bf18531203dc403220f2365e06e3c12bcc09668d9603cd9f0e223ca101a59e9677721bd4d85a21f24d6faf79f22e

# The P2P topic to publish rejected transaction messages
p2p_rejected_tx_topic = rejected_tx

# Allow sharing of private IP addresses in peer lists (default: false)
# Set to true if you need peers to discover each other using private IPs (e.g., in private networks)
p2p_share_private_addresses = false

p2p_shared_key = 285b49e6d910726a70f205086c39cbac6d8dcc47839053a21b1f614773bbc137

# p2p_static_peers is optional, the node will use the bootstrap addresses to find peers regardless
p2p_static_peers                              =
p2p_static_peers.docker.teranode1             = /dns/teranode2/tcp/${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW | /dns/teranode3/tcp/${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_static_peers.docker.teranode2             = /dns/teranode1/tcp/${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/teranode3/tcp/${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_static_peers.docker.teranode3             = /dns/teranode1/tcp/${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/teranode2/tcp/${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW
p2p_static_peers.docker.m                     =
p2p_static_peers.docker.host.teranode1        = /dns/localhost/tcp/2${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW | /dns/localhost/tcp/3${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_static_peers.docker.host.teranode2        = /dns/localhost/tcp/1${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/localhost/tcp/3${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_static_peers.docker.host.teranode3        = /dns/localhost/tcp/1${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/localhost/tcp/2${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW
p2p_static_peers.docker.host.teranode1.legacy =

p2p_subtree_topic = subtree

# Number of stuck attempts before switching sync peers (default: 1)
p2p_sync_stuck_retry_threshold = 1

peerStatus_timeout = 5m

# Profiler Configuration
# ---------------------------------------
profilerAddr             = :${PROFILE_PORT}
profilerAddr.dev         = localhost:${PROFILE_PORT}
profilerAddr.docker.host = localhost:${PORT_PREFIX}${PROFILE_PORT}

prometheusEndpoint = /metrics

# @group: propagation compact
PROD_T1 = dns:///prod-teranet-1.teratestnet.ubsv.dev:${PROPAGATION_GRPC_PORT}
PROD_T2 = dns:///prod-teranet-2.teratestnet.ubsv.dev:${PROPAGATION_GRPC_PORT}
PROD_T3 = dns:///prod-teranet-3.teratestnet.ubsv.dev:${PROPAGATION_GRPC_PORT}
# @endgroup

# Note the following settings can be a pipe separated list
propagation_grpcAddresses                              = localhost:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker                       = teranode1:${PROPAGATION_GRPC_PORT} | teranode2:${PROPAGATION_GRPC_PORT} | teranode3:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.teranode1.test        = teranode1:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.teranode2.test        = teranode2:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.teranode3.test        = teranode3:${PROPAGATION_GRPC_PORT}
# propagation_grpcAddresses.docker.host                = localhost:1${PROPAGATION_GRPC_PORT} | localhost:$2${PROPAGATION_GRPC_PORT} | localhost:3${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.host                  = localhost:${PORT_PREFIX}${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.host.teranode1.daemon = localhost:${PORT_PREFIX}${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.host.teranode2.daemon = localhost:${PORT_PREFIX}${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.host.teranode3.daemon = localhost:${PORT_PREFIX}${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.m                     = propagation:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.ss.teranode1          = propagation-1:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.operator.teratestnet.prod    = ${PROD_T1} | ${PROD_T2} | ${PROD_T3}
propagation_grpcAddresses.operator                     = k8s:///propagation.${clientName}.svc.cluster.local:${PROPAGATION_GRPC_PORT}

propagation_grpcListenAddress             = :${PROPAGATION_GRPC_PORT}
propagation_grpcListenAddress.dev         = localhost:${PROPAGATION_GRPC_PORT}
propagation_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${PROPAGATION_GRPC_PORT}

propagation_grpcMaxConnectionAge          = 30s
propagation_grpcMaxConnectionAge.operator = 5m

propagation_httpAddresses                     = http://localhost:${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.docker              = http://${clientName}:${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.docker.host         = http://localhost:${PORT_PREFIX}${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.docker.m            = http://propagation:${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.operator            = http://propagation.${clientName}.svc.cluster.local:${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.docker.ss.teranode1 = http://propagation-1:${PROPAGATION_HTTP_PORT}

# 6 nodes
propagation_httpListenAddress             = :${PROPAGATION_HTTP_PORT}
propagation_httpListenAddress.dev         = localhost:${PROPAGATION_HTTP_PORT}
propagation_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${PROPAGATION_HTTP_PORT}

# set this to 0 to disable the propagation client from sending transactions to the propagation service in batches
propagation_sendBatchSize = 100

propagation_sendBatchTimeout = 10

# The alert service uses this to connect to the rpc
rpc_address = http://localhost:${TERANODE_RPC_PORT}

rpc_listener_url                       = http://:${TERANODE_RPC_PORT}
rpc_listener_url.docker.host.teranode1 = http://:1${TERANODE_RPC_PORT}
rpc_listener_url.docker.host.teranode2 = http://:2${TERANODE_RPC_PORT}
rpc_listener_url.docker.host.teranode3 = http://:3${TERANODE_RPC_PORT}
rpc_listener_url.docker.m              = http://:${TERANODE_RPC_PORT}

rpc_max_clients = 3

rpc_pass = bitcoin

rpc_user = bitcoin

securityLevelGRPC = 0

# Security Configuration
# ---------------------------------------
securityLevelHTTP = 0

server_certFile = certs/teranode.crt

server_keyFile = certs/teranode.key

# how long to wait before deleting spent utxos (seconds)
spent_utxo_ttl        = 3600 # = 1 hour
spent_utxo_ttl.dev    = 10 # = 10 seconds
spent_utxo_ttl.docker = 10 # = 10 seconds

# Alert Service Configuration
# ---------------------------------------
startAlert                                = true
startAlert.docker.host.teranode1.coinbase = false
startAlert.docker.host.teranode2.coinbase = false
startAlert.docker.host.teranode3.coinbase = false
startAlert.docker.teranode1.test.coinbase = false
startAlert.docker.teranode2.test.coinbase = false
startAlert.docker.teranode3.test.coinbase = false
startAlert.dev.system.test                = false
startAlert.docker.host                    = false
startAlert.docker.m                       = false
startAlert.operator                       = false

# Asset Service Configuration
# ---------------------------------------
startAsset                                      = true
startAsset.docker.host.teranode1.coinbase       = false
startAsset.docker.host.teranode2.coinbase       = false
startAsset.docker.host.teranode3.coinbase       = false
startAsset.docker.teranode1.test.coinbase       = false
startAsset.docker.teranode2.test.coinbase       = false
startAsset.docker.teranode3.test.coinbase       = false
startAsset.docker.m                             = false
startAsset.operator                             = false
startAsset.docker.teranode2.test.resilience.tc6 = false

# Block Assembly Service Configuration
# ---------------------------------------
startBlockAssembly                                      = true
startBlockAssembly.docker.host.teranode1.coinbase       = false
startBlockAssembly.docker.host.teranode2.coinbase       = false
startBlockAssembly.docker.host.teranode3.coinbase       = false
startBlockAssembly.docker.teranode1.test.coinbase       = false
startBlockAssembly.docker.teranode2.test.coinbase       = false
startBlockAssembly.docker.teranode3.test.coinbase       = false
startBlockAssembly.dev.system.test.blockassembly        = false
startBlockAssembly.docker.m                             = false
startBlockAssembly.operator                             = false
startBlockAssembly.docker.teranode2.test.resilience.tc2 = false

# Block Persister Service Configuration
# ---------------------------------------
startBlockPersister                                = false
startBlockPersister.docker.host.teranode1.coinbase = false
startBlockPersister.docker.host.teranode2.coinbase = false
startBlockPersister.docker.host.teranode3.coinbase = false
startBlockPersister.docker                         = true
startBlockPersister.docker.m                       = false
startBlockPersister.m                              = false
startBlockPersister.dev                            = true
startBlockPersister.docker.teranode1.test.coinbase = false
startBlockPersister.docker.teranode2.test.coinbase = false
startBlockPersister.docker.teranode3.test.coinbase = false
startBlockPersister.dev.system.test                = false
startBlockPersister.operator                       = false
startBlockPersister.teratestnet                    = true

# Block Validation Service Configuration
# ---------------------------------------
startBlockValidation                                      = true
startBlockValidation.docker.host.teranode1.coinbase       = false
startBlockValidation.docker.host.teranode2.coinbase       = false
startBlockValidation.docker.host.teranode3.coinbase       = false
startBlockValidation.docker.m                             = false
startBlockValidation.operator                             = false
startBlockValidation.docker.teranode1.test.coinbase       = false
startBlockValidation.docker.teranode2.test.coinbase       = false
startBlockValidation.docker.teranode3.test.coinbase       = false
startBlockValidation.docker.teranode2.test.resilience.tc3 = false

# BlockChain Service Configuration
# ---------------------------------------
startBlockchain                                      = true
startBlockchain.docker.host.teranode1.coinbase       = false
startBlockchain.docker.host.teranode2.coinbase       = false
startBlockchain.docker.host.teranode3.coinbase       = false
startBlockchain.docker.teranode1.test.stopBlockchain = false
startBlockchain.docker.m                             = false
startBlockchain.operator                             = false
startBlockchain.docker.teranode1.test.coinbase       = false
startBlockchain.docker.teranode2.test.coinbase       = false
startBlockchain.docker.teranode3.test.coinbase       = false
startBlockchain.docker.teranode2.test.resilience.tc4 = false

# Coinbase Tracker Service Configuration
# ---------------------------------------
startCoinbase                                = false
startCoinbase.dev.system.test.stopcoinbase   = false
startCoinbase.docker.m                       = false
startCoinbase.operator                       = false
startCoinbase.docker.teranode1.tc1           = false
startCoinbase.docker.teranode2.tc1           = false
startCoinbase.docker.teranode3.tc1           = false
startCoinbase.docker.host.teranode1          = true
startCoinbase.docker.host.teranode2          = true
startCoinbase.docker.host.teranode3          = true
startCoinbase.docker.teranode1.test.coinbase = true
startCoinbase.docker.teranode2.test.coinbase = true
startCoinbase.docker.teranode3.test.coinbase = true

# Faucet Service Configuration
# ---------------------------------------
startFaucet                                = true
startFaucet.docker.host.teranode1.coinbase = true
startFaucet.docker.host.teranode2.coinbase = true
startFaucet.docker.host.teranode3.coinbase = true
startFaucet.dev.system.test                = false
startFaucet.operator                       = false
startFaucet.docker                         = false
startFaucet.docker.m                       = false
startFaucet.docker.teranode1.test.coinbase = true
startFaucet.docker.teranode2.test.coinbase = true
startFaucet.docker.teranode3.test.coinbase = true

# Legacy Service Configuration
# ---------------------------------------
startLegacy                                = true
startLegacy.dev.system.test                = false
startLegacy.docker.host.teranode1.coinbase = false
startLegacy.docker.host.teranode2.coinbase = false
startLegacy.docker.host.teranode3.coinbase = false
startLegacy.docker                         = false
startLegacy.docker.m                       = false
startLegacy.operator                       = false
startLegacy.docker.teranode1.test.legacy   = true
startLegacy.docker.teranode1.test.coinbase = false
startLegacy.docker.teranode2.test.coinbase = false
startLegacy.docker.teranode3.test.coinbase = false
startLegacy.docker.teratestnet             = false
startLegacy.teratestnet                    = false

# P2P Configuration
# -----------------
startP2P                                      = true
startP2P.docker.host.teranode1.coinbase       = false
startP2P.docker.host.teranode2.coinbase       = false
startP2P.docker.host.teranode3.coinbase       = false
startP2P.docker.teranode1.test.coinbase       = false
startP2P.docker.teranode2.test.coinbase       = false
startP2P.docker.teranode3.test.coinbase       = false
startP2P.dev.system.test                      = false
startP2P.operator                             = false
startP2P.docker.m                             = false
startP2P.docker.teranode1.test.stopP2P        = false
startP2P.docker.teranode2.test.stopP2P        = false
startP2P.docker.teranode3.test.stopP2P        = false
startP2P.docker.teranode2.tc3                 = false
startP2P.docker.teranode2.test.tnf6.stage1    = false
startP2P.docker.teranode2.test.tnf6.stage2    = true
startP2P.docker.teranode2.test.resilience.tc5 = false

# Propagation Service Configuration
# ---------------------------------
startPropagation                                      = true
startPropagation.docker.host.teranode1.coinbase       = false
startPropagation.docker.host.teranode2.coinbase       = false
startPropagation.docker.host.teranode3.coinbase       = false
startPropagation.docker.m                             = false
startPropagation.operator                             = false
startPropagation.docker.teranode1.test.coinbase       = false
startPropagation.docker.teranode2.test.coinbase       = false
startPropagation.docker.teranode3.test.coinbase       = false
startPropagation.docker.teranode2.test.resilience.tc1 = false

# rpc service
# -----------
startRPC                                = true
startRPC.docker.host.teranode1.coinbase = false
startRPC.docker.host.teranode2.coinbase = false
startRPC.docker.host.teranode3.coinbase = false
startRPC.docker.teranode1.test.coinbase = false
startRPC.docker.teranode2.test.coinbase = false
startRPC.docker.teranode3.test.coinbase = false
startRPC.operator                       = false

# Subtree Validation Service Configuration
# ----------------------------------------
startSubtreeValidation                                = true
startSubtreeValidation.docker.host.teranode1.coinbase = false
startSubtreeValidation.docker.host.teranode2.coinbase = false
startSubtreeValidation.docker.host.teranode3.coinbase = false
startSubtreeValidation.docker.m                       = false
startSubtreeValidation.operator                       = false
startSubtreeValidation.docker.teranode1.test.coinbase = false
startSubtreeValidation.docker.teranode2.test.coinbase = false
startSubtreeValidation.docker.teranode3.test.coinbase = false

# UTXO Persister Service Configuration
# ------------------------------------
startUTXOPersister = false

# Validator Service Configuration
# ----------------------------------------
# startValidator is set to true to start the validator as a service
startValidator        = false
startValidator.docker = true

# @group: subtree_config
# Subtree Configuration
# ------------------------
initial_merkle_items_per_subtree                                = 1024
initial_merkle_items_per_subtree.test                           = 32768
initial_merkle_items_per_subtree.docker                         = 8
initial_merkle_items_per_subtree.docker.m                       = 1024
initial_merkle_items_per_subtree.docker.teranode1.test          = 2
initial_merkle_items_per_subtree.docker.teranode2.test          = 2
initial_merkle_items_per_subtree.docker.teranode3.test          = 2
initial_merkle_items_per_subtree.docker.teranode1.test.tna1Test = 32
initial_merkle_items_per_subtree.docker.teranode2.test.tna1Test = 32
initial_merkle_items_per_subtree.docker.teranode3.test.tna1Test = 32
initial_merkle_items_per_subtree.docker.teranode1.test.tnc1Test = 32768
initial_merkle_items_per_subtree.docker.teranode2.test.tnc1Test = 65536
initial_merkle_items_per_subtree.docker.teranode3.test.tnc1Test = 131072
initial_merkle_items_per_subtree.docker.teranode1.test.tnj1Test = 16
initial_merkle_items_per_subtree.docker.teranode1.test.tnb1Test = 32768
initial_merkle_items_per_subtree.docker.teranode2.test.tnb1Test = 32768
initial_merkle_items_per_subtree.docker.teranode3.test.tnb1Test = 32768

maximum_merkle_items_per_subtree                       =
maximum_merkle_items_per_subtree.dev                   = 32768
maximum_merkle_items_per_subtree.docker                = 32768
maximum_merkle_items_per_subtree.docker.m              = 32768
maximum_merkle_items_per_subtree.test                  = 32768
maximum_merkle_items_per_subtree.docker.teranode1.test = 2
maximum_merkle_items_per_subtree.docker.teranode2.test = 2
maximum_merkle_items_per_subtree.docker.teranode3.test = 2

minimum_merkle_items_per_subtree.dev                   = 1024
minimum_merkle_items_per_subtree.docker.teranode1.test = 2
minimum_merkle_items_per_subtree.docker.teranode2.test = 2
minimum_merkle_items_per_subtree.docker.teranode3.test = 2
# @endgroup

subtree_quorum_absolute_timeout = 30s

subtree_quorum_path             = ${DATADIR}/subtree_quorum
subtree_quorum_path.docker      = ${DATADIR}/subtreestore/subtree_quorum
subtree_quorum_path.docker.host = ${DATADIR}/${clientName}/subtree_quorum
subtree_quorum_path.operator    = ${DATADIR}/${clientName}/subtreestore/quorum

subtreestore                                          = file://${DATADIR}/subtreestore?localTTLStore=file&localTTLStorePath=${DATADIR}/subtreestore-ttl-1 | ${DATADIR}/subtreestore-ttl-2
subtreestore.dev                                      = file://${DATADIR}/subtreestore?localTTLStore=file&localTTLStorePath=${DATADIR}/subtreestore-ttl
subtreestore.mainnet                                  = file://${DATADIR}/subtreestore
subtreestore.docker                                   = file://${DATADIR}/subtreestore
subtreestore.teratestnet                              = file://${DATADIR}/subtreestore
subtreestore.docker.host                              = file://${DATADIR}/${clientName}/subtreestore
subtreestore.docker.host.teranode1.legacy             = file://${DATADIR}/${clientName}/subtreestore
subtreestore.operator                                 = file://${DATADIR}/${clientName}/subtreestore
subtreestore.operator.teratestnet                     = file://${DATADIR}/${clientName}/subtreestore
subtreestore.docker.teranode1.test.context.testrunner = file://${DATADIR}/test/teranode1/subtreestore
subtreestore.docker.teranode2.test.context.testrunner = file://${DATADIR}/test/teranode2/subtreestore
subtreestore.docker.teranode3.test.context.testrunner = file://${DATADIR}/test/teranode3/subtreestore

subtreevalidation_getMissingTransactions = 32

subtreevalidation_grpcAddress             = localhost:${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcAddress.docker.m    = subtreevalidation:${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcAddress.docker.host = localhost:${PORT_PREFIX}${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcAddress.operator    = k8s:///subtree-validator.${clientName}.svc.cluster.local:${SUBTREE_VALIDATION_GRPC_PORT}

subtreevalidation_grpcListenAddress             = :${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcListenAddress.dev         = localhost:${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${SUBTREE_VALIDATION_GRPC_PORT}

subtreevalidation_processTxMetaUsingCache_BatchSize = 1024

subtreevalidation_processTxMetaUsingCache_Concurrency = 32

# Cache miss threshold is the number of missing txs to allow before we fail
# This is useful when we broadcast txs to the network via propagation or IPV6
# However, with no broadcasting, we can't rely on the txs being in the cache
# so there is no optimisation to be had
subtreevalidation_processTxMetaUsingCache_MissingTxThreshold = 0

subtreevalidation_spendBatcherSize = 8192

subtreevalidation_txMetaCacheEnabled = true

subtreevalidation_pauseTimeout = 5m

temp_store             = file://${DATADIR}/tempstore
temp_store.docker.host = file://${DATADIR}/${clientName}/tempstore
temp_store.operator    = file://${DATADIR}/${clientName}/tempstore

# E2E test configuration
test_run_mode        = dev
test_run_mode.docker = ci

# testing environment is reusing the same kafka cluster for cost saving so we need to redefine the topics to differentiate them =
# testing environment is reusing the same kafka cluster for cost saving so we need to redefine the topics to differentiate them =

tracing_SampleRate = 0.1

# careful! this variable only works for tminer-lo-1he OTEL tracer. If you're using open tracing you need to set JAEGER_AGENT_HOST
tracing_collector_url             = jaeger-cluster-agent.jaeger.svc.cluster.local:${JAEGER_PORT_HTTP}
tracing_collector_url.dev         = localhost:${JAEGER_PORT_HTTP}
tracing_collector_url.docker.host = localhost:${JAEGER_PORT_HTTP}
tracing_collector_url.docker      = jaeger:${JAEGER_PORT_HTTP}
tracing_collector_url.operator    = jaeger-agent.jaeger.svc.cluster.local:${JAEGER_PORT_HTTP}

# Prometheus and Tracing Configuration
# ---------------------------------------
tracing_enabled.dev.system.test = false
tracing_enabled.dev             = false

# Tx Meta Data Store Service Configuration
# ----------------------------------------
# for mainnet it's recommended to use a higher value, depending on your resources
# txMetaCacheMaxMB = 32768 # 32GGB
txMetaCacheMaxMB   = 1024 # 1GB

txMetaCacheTrimRatio = 5

# Used by tx blaster to receive rejected txs
# tx_blaster_p2p_peer_id = 12D3KooWLRaBuYhHPcj8ixqwiVLtJ6YxMMSJRDwPYKVN5HWSnUWo

# tx_blaster_p2p_private_key = f19576f6a8ea51ebf5a1fb05cdfea7b66bfa8ecdcec58232d064f3343fb19ab29d99c9e7339b479ed4ee48d7bb7a078bcc9e2ad2355e7e6ba94297e97ecb7744

# p2p_static_peers is optional, the node will use the bootstrap addresses to find peers regardless
tx_blaster_p2p_static_peers =

tx_blaster_profilerAddr     = :${PROFILE_PORT_TXBLASTER}
tx_blaster_profilerAddr.dev = localhost:${PROFILE_PORT_TXBLASTER}

# tx_blaster_spam_rate = 10

tx_blaster_staggerWorkersTimeMs = 20

txstore = null:///

useLocalValidator = true

# CGO Signer and Verifier Configuration
# -------------------------------------
use_cgo_signer = true

use_cgo_verifier = true

use_datadog_profiler = false

# 54000 # 15 hours - we have the space
utxostore                                                  = null:///
utxostore.dev                                              = sqlite:///utxostore
utxostore.dev.legacy                                       = aerospike://localhost:3000/utxo-store?set=utxo&externalStore=file://${DATADIR}/external
utxostore.teratestnet                                      = aerospike://localhost:3000/utxo-store?set=utxo&externalStore=file://${DATADIR}/external
utxostore.docker.ci.chainintegrity.teranode1               = postgres://miner1:miner1@localhost:${POSTGRES_PORT}/teranode1
utxostore.docker.ci.chainintegrity.teranode2               = postgres://miner2:miner2@localhost:${POSTGRES_PORT}/teranode2
utxostore.docker.ci.chainintegrity.teranode3               = postgres://miner3:miner3@localhost:${POSTGRES_PORT}/teranode3
utxostore.docker.host.teranode1.postgres                   = postgres://miner${PORT_PREFIX}:miner${PORT_PREFIX}@localhost:1${POSTGRES_PORT}/teranode${PORT_PREFIX}?logging=true
utxostore.docker.host                                      = aerospike://localhost:3${PORT_PREFIX}00/test?WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNoutxo-store&set=utxo&logging=true&externalStore=file://${DATADIR}/teranode${PORT_PREFIX}/external?persistSubDir=sv-node/external&hashPrefix=2&s3URL=s3://s3.amazonaws.com/ubsv-teranode${PORT_PREFIX}-external-store?region=us-east-1
utxostore.docker.host.teranode1.daemon                     = sqlite:///teranode1/utxo1
utxostore.docker.host.teranode2.daemon                     = sqlite:///teranode2/utxo2
utxostore.docker.host.teranode3.daemon                     = sqlite:///teranode3/utxo3
utxostore.docker.m                                         = aerospike://aerospike:3000/utxo-store?WarmUp=0&ConnectionQueueSize=16&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&set=utxo&externalStore=file://${DATADIR}/external%3FhashPrefix=2
utxostore.docker.ss.teranode1                              = postgres://miner1:miner1@postgres:${POSTGRES_PORT}/teranode1
# utxostore.docker                                         = aerospike://${aerospike_host}:${aerospike_port}/utxo-store?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&externalStore=file://${DATADIR}/${clientName}/external
utxostore.docker.teranode1.test                            = aerospike://aerospike-1:3100/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&externalStore=file://${DATADIR}/external
utxostore.docker.teranode2.test                            = aerospike://aerospike-2:3200/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&externalStore=file://${DATADIR}/external
utxostore.docker.teranode3.test                            = aerospike://aerospike-3:3300/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&externalStore=file://${DATADIR}/external
utxostore.docker.teranode1.context.testrunner              = aerospike://localhost:3100/utxo-store?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&externalStore=file://${DATADIR}/teranode1/external
utxostore.docker.teranode2.context.testrunner              = aerospike://localhost:3200/utxo-store?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&externalStore=file://${DATADIR}/teranode2/external
utxostore.docker.teranode3.context.testrunner              = aerospike://localhost:3300/utxo-store?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&externalStore=file://${DATADIR}/teranode3/external
utxostore.docker.teranode1.test.nightly                    = postgres://postgres:postgres@postgres:${POSTGRES_PORT}/teranode1
utxostore.docker.teranode2.test.nightly                    = postgres://postgres:postgres@postgres:${POSTGRES_PORT}/teranode2
utxostore.docker.teranode3.test.nightly                    = postgres://postgres:postgres@postgres:${POSTGRES_PORT}/teranode3
utxostore.docker.teranode1.test.nightly.context.testrunner = postgres://postgres:postgres@localhost:${POSTGRES_PORT}/teranode1
utxostore.docker.teranode2.test.nightly.context.testrunner = postgres://postgres:postgres@localhost:${POSTGRES_PORT}/teranode2
utxostore.docker.teranode3.test.nightly.context.testrunner = postgres://postgres:postgres@localhost:${POSTGRES_PORT}/teranode3
utxostore.docker.teranode1.test.postgres                   = postgres://miner1:miner1@postgres:${POSTGRES_PORT}/teranode1
utxostore.docker.teranode2.test.postgres                   = postgres://miner2:miner2@postgres:${POSTGRES_PORT}/teranode2
utxostore.docker.teranode3.test.postgres                   = postgres://miner3:miner3@postgres:${POSTGRES_PORT}/teranode3
utxostore.docker.teranode1.test.context.testrunner         = postgres://teranode1:teranode1@localhost:${POSTGRES_PORT}/teranode1
utxostore.docker.teranode2.test.context.testrunner         = postgres://teranode2:teranode2@localhost:${POSTGRES_PORT}/teranode2
utxostore.docker.teranode3.test.context.testrunner         = postgres://teranode3:teranode3@localhost:${POSTGRES_PORT}/teranode3
utxostore.dev.system.test                                  = sqlitememory:///utxostore
utxostore.dev.system.test.postgres                         = postgres://teranode:teranode@localhost:${POSTGRES_PORT}/teranode?expiration=5m
utxostore.test                                             = sqlite:///utxostore

# txostore.docker.host
# replace utxostore_dbTimeoutMillis with settings.UtxoStore.DBTimeout
utxostore_dbTimeoutDuration = 5s

utxostore_dbTimeoutMillis = 5000

utxostore_getBatcherDurationMillis = 10

utxostore_getBatcherSize = 4096

utxostore_maxMinedBatchSize = 1024

utxostore_maxMinedRoutines = 128

utxostore_outpointBatcherConcurrency = 32

utxostore_outpointBatcherDurationMillis = 5

utxostore_outpointBatcherSize = 4096

utxostore_spendBatcherConcurrency = 16

# The spendBatcher values depend on the size of your Aerospike cluster and the size of the blocks
# During IBD at high peak times (e.g. around block ~755000) you might need to decrease the batcherSize to 48 and concurrency to 1
# You will see Aerospike DEVICE_OVERLOAD errors if it can't keep up with the batcherSize and concurrency
# You can also increase the max-write-cache in the aerospike configuration, using memory to handle the peak load when the disk can't keep up
utxostore_spendBatcherDurationMillis = 10

utxostore_spendBatcherSize = 1024

utxostore_storeBatcherDurationMillis = 10

utxostore_storeBatcherSize = 2048

# this value determines if a caching mechanism will be used for external transactions
# if you have the memory available, it will speed up your IBD
# and it is required for large blocks which load in the same tx multiple times, e.g. 814337
utxostore_useExternalTxCache = true

# utxos per record, if larger it will externalize the transaction
# ! do not change this after starting a node !
utxostore_utxoBatchSize          = 128
utxostore_utxoBatchSize.docker   = 50
utxostore_utxoBatchSize.docker.m = 512

# delay before sending batch to blockvalidation, use for testing subtree processing retries
validator_blockvalidation_delay = 0

# tx validator sending batches to blockvalidation
validator_blockvalidation_maxRetries = 5

validator_blockvalidation_retrySleep = 2s

validator_grpcAddress                     = 0.0.0.0:${VALIDATOR_GRPC_PORT}
validator_grpcAddress.docker              = ${clientName}:${VALIDATOR_GRPC_PORT}
validator_grpcAddress.docker.m            = validator:${VALIDATOR_GRPC_PORT}
validator_grpcAddress.docker.host         = 0.0.0.0:${PORT_PREFIX}${VALIDATOR_GRPC_PORT}
validator_grpcAddress.operator            = k8s:///validator.${clientName}.svc.cluster.local:${VALIDATOR_GRPC_PORT}
validator_grpcAddress.docker.ss.teranode1 = validator-1:${VALIDATOR_GRPC_PORT}

validator_grpcListenAddress             = :${VALIDATOR_GRPC_PORT}
validator_grpcListenAddress.dev         = localhost:${VALIDATOR_GRPC_PORT}
validator_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${VALIDATOR_GRPC_PORT}

validator_httpAddress             = http://localhost:${VALIDATOR_HTTP_PORT}
validator_httpAddress.docker      = http://${clientName}:${VALIDATOR_HTTP_PORT}
validator_httpAddress.docker.host = http://localhost:${PORT_PREFIX}${VALIDATOR_HTTP_PORT}

validator_httpListenAddress             = :${VALIDATOR_HTTP_PORT}
validator_httpListenAddress.dev         = localhost:${VALIDATOR_HTTP_PORT}
validator_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${VALIDATOR_HTTP_PORT}

validator_kafka_maxMessageBytes = 1048500

validator_sendBatchSize                                = 0
validator_sendBatchSize.docker.m                       = 1000
validator_sendBatchSize.docker.teranode1.test.tnb1Test = 10
validator_sendBatchSize.docker.teranode2.test.tnb1Test = 10
validator_sendBatchSize.docker.teranode3.test.tnb1Test = 10
validator_sendBatchSize.operator                       = 1000

validator_sendBatchTimeout          = 10
validator_sendBatchTimeout.docker.m = 5
validator_sendBatchTimeout.operator = 5

validator_sendBatchWorkers = 1

coinbase_rpc_url = http://${clientName}:9292
COINBASE_DB_URL.docker =
coinbase_Store.docker =
LOG_LEVEL =
COINBASE_RPC_USER =
COINBASE_RPC_PASS =
COINBASE_DB_TIMEOUT_MS =
COINBASE_GRPC_ADDRESS =
COINBASE_HTTP_PORT =
COINBASE_WALLET_PRIVATE_KEY.docker.teranode1 =
COINBASE_WALLET_PRIVATE_KEY.docker.teranode2 =
COINBASE_WALLET_PRIVATE_KEY.docker.teranode3 =